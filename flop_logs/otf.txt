bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 04:36:54 Starting App Insight Logger for task:  runTaskLet
2021/06/20 04:36:54 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612
2021/06/20 04:36:54 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info
bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 04:36:54 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status
[2021-06-20T04:36:54.297308] Entering context manager injector.
[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15'])
Script type = COMMAND
[2021-06-20T04:36:54.942306] Command=python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
[2021-06-20T04:36:54.942613] Entering Run History Context Manager.
[2021-06-20T04:36:55.527148] Command Working Directory=/mnt/batch/tasks/shared/LS_root/jobs/prunetrain/azureml/onthefly2/wd/azureml/OnTheFly2
[2021-06-20T04:36:55.527422] Starting Linux command : python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/data/torch/cifar-100-python.tar.gz
0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%Extracting ./dataset/data/torch/cifar-100-python.tar.gz to ./dataset/data/torch
2021/06/20 04:36:59 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 1
FilteredData: 0.
==> creating model 'vgg11_bn_flat'
    Total params: 9.27M
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375

Epoch: [1 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
cifar.py:338: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [1][0/391]	Time 0.316 (0.316)	Data 0.132 (0.132)	Loss 6.1210 (6.1210)	Acc@1 0.781 (0.781)	Acc@5 0.781 (0.781)
Epoch: [1][10/391]	Time 0.013 (0.040)	Data 0.001 (0.013)	Loss 9.4672 (7.1785)	Acc@1 0.781 (1.562)	Acc@5 6.250 (6.534)
Epoch: [1][20/391]	Time 0.012 (0.027)	Data 0.002 (0.008)	Loss 7.2636 (7.6863)	Acc@1 0.781 (1.562)	Acc@5 2.344 (6.510)
Epoch: [1][30/391]	Time 0.013 (0.023)	Data 0.001 (0.006)	Loss 6.1552 (7.3376)	Acc@1 0.781 (1.462)	Acc@5 6.250 (6.552)
Epoch: [1][40/391]	Time 0.014 (0.020)	Data 0.001 (0.005)	Loss 5.8377 (7.0318)	Acc@1 5.469 (1.524)	Acc@5 12.500 (6.460)
Epoch: [1][50/391]	Time 0.012 (0.019)	Data 0.002 (0.004)	Loss 5.9928 (6.8436)	Acc@1 0.000 (1.409)	Acc@5 3.906 (6.419)
Epoch: [1][60/391]	Time 0.013 (0.018)	Data 0.001 (0.004)	Loss 5.7998 (6.6873)	Acc@1 3.125 (1.511)	Acc@5 7.031 (6.416)
Epoch: [1][70/391]	Time 0.013 (0.017)	Data 0.001 (0.003)	Loss 6.0332 (6.5856)	Acc@1 3.125 (1.496)	Acc@5 10.156 (6.327)
Epoch: [1][80/391]	Time 0.013 (0.017)	Data 0.001 (0.003)	Loss 5.8240 (6.4995)	Acc@1 2.344 (1.485)	Acc@5 8.594 (6.318)
Epoch: [1][90/391]	Time 0.013 (0.016)	Data 0.001 (0.003)	Loss 5.7833 (6.4351)	Acc@1 0.781 (1.459)	Acc@5 6.250 (6.422)
Epoch: [1][100/391]	Time 0.013 (0.016)	Data 0.001 (0.003)	Loss 5.8505 (6.3845)	Acc@1 2.344 (1.462)	Acc@5 7.031 (6.374)
Epoch: [1][110/391]	Time 0.013 (0.016)	Data 0.001 (0.003)	Loss 5.8668 (6.3321)	Acc@1 1.562 (1.429)	Acc@5 8.594 (6.426)
Epoch: [1][120/391]	Time 0.013 (0.015)	Data 0.001 (0.003)	Loss 5.7605 (6.2867)	Acc@1 2.344 (1.420)	Acc@5 7.812 (6.437)
Epoch: [1][130/391]	Time 0.013 (0.015)	Data 0.001 (0.003)	Loss 5.7503 (6.2471)	Acc@1 0.781 (1.425)	Acc@5 5.469 (6.453)
Epoch: [1][140/391]	Time 0.014 (0.015)	Data 0.002 (0.002)	Loss 5.7301 (6.2126)	Acc@1 2.344 (1.457)	Acc@5 7.031 (6.483)
Epoch: [1][150/391]	Time 0.014 (0.015)	Data 0.001 (0.002)	Loss 5.7410 (6.1830)	Acc@1 0.000 (1.454)	Acc@5 3.125 (6.504)
Epoch: [1][160/391]	Time 0.010 (0.015)	Data 0.004 (0.002)	Loss 5.7633 (6.1571)	Acc@1 1.562 (1.436)	Acc@5 3.906 (6.434)
Epoch: [1][170/391]	Time 0.010 (0.015)	Data 0.005 (0.002)	Loss 5.7531 (6.1315)	Acc@1 2.344 (1.416)	Acc@5 5.469 (6.515)
Epoch: [1][180/391]	Time 0.014 (0.015)	Data 0.001 (0.002)	Loss 5.7049 (6.1082)	Acc@1 0.781 (1.450)	Acc@5 5.469 (6.561)
Epoch: [1][190/391]	Time 0.016 (0.015)	Data 0.000 (0.002)	Loss 5.6971 (6.0866)	Acc@1 3.125 (1.477)	Acc@5 6.250 (6.589)
Epoch: [1][200/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.6689 (6.0665)	Acc@1 1.562 (1.450)	Acc@5 6.250 (6.623)
Epoch: [1][210/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.6192 (6.0486)	Acc@1 1.562 (1.440)	Acc@5 6.250 (6.613)
Epoch: [1][220/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.6834 (6.0308)	Acc@1 0.781 (1.481)	Acc@5 9.375 (6.706)
Epoch: [1][230/391]	Time 0.011 (0.014)	Data 0.004 (0.002)	Loss 5.6207 (6.0146)	Acc@1 1.562 (1.488)	Acc@5 7.812 (6.734)
Epoch: [1][240/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.6062 (5.9992)	Acc@1 3.906 (1.485)	Acc@5 7.812 (6.752)
Epoch: [1][250/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.6187 (5.9840)	Acc@1 0.781 (1.500)	Acc@5 8.594 (6.792)
Epoch: [1][260/391]	Time 0.013 (0.014)	Data 0.002 (0.002)	Loss 5.5960 (5.9704)	Acc@1 3.125 (1.491)	Acc@5 7.812 (6.843)
Epoch: [1][270/391]	Time 0.013 (0.014)	Data 0.002 (0.002)	Loss 5.5757 (5.9564)	Acc@1 1.562 (1.490)	Acc@5 7.031 (6.855)
Epoch: [1][280/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.5815 (5.9437)	Acc@1 1.562 (1.496)	Acc@5 10.938 (6.934)
Epoch: [1][290/391]	Time 0.013 (0.014)	Data 0.002 (0.002)	Loss 5.5590 (5.9307)	Acc@1 1.562 (1.544)	Acc@5 11.719 (7.045)
Epoch: [1][300/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.5068 (5.9177)	Acc@1 3.906 (1.555)	Acc@5 9.375 (7.112)
Epoch: [1][310/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.5166 (5.9047)	Acc@1 2.344 (1.578)	Acc@5 10.156 (7.177)
Epoch: [1][320/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.4805 (5.8920)	Acc@1 3.125 (1.587)	Acc@5 8.594 (7.216)
Epoch: [1][330/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 5.4171 (5.8790)	Acc@1 3.125 (1.622)	Acc@5 12.500 (7.329)
Epoch: [1][340/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.3714 (5.8665)	Acc@1 4.688 (1.652)	Acc@5 11.719 (7.451)
Epoch: [1][350/391]	Time 0.016 (0.014)	Data 0.002 (0.002)	Loss 5.4568 (5.8537)	Acc@1 2.344 (1.663)	Acc@5 13.281 (7.561)
Epoch: [1][360/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 5.2845 (5.8393)	Acc@1 2.344 (1.720)	Acc@5 14.844 (7.758)
Epoch: [1][370/391]	Time 0.014 (0.014)	Data 0.001 (0.002)	Loss 5.3066 (5.8258)	Acc@1 1.562 (1.767)	Acc@5 12.500 (7.926)
Epoch: [1][380/391]	Time 0.014 (0.014)	Data 0.002 (0.002)	Loss 5.2620 (5.8109)	Acc@1 3.125 (1.815)	Acc@5 14.062 (8.130)
Epoch: [1][390/391]	Time 0.212 (0.014)	Data 0.001 (0.002)	Loss 5.3360 (5.7957)	Acc@1 3.750 (1.872)	Acc@5 15.000 (8.388)
cifar.py:425: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
num momentum params: 26
[0.1, 5.795697383117676, 4.298566193580627, 1.872, 3.46, tensor(0.1957, device='cuda:0', grad_fn=<DivBackward0>), 5.6352622509002686, 0.5011606216430664]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [2 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [2][0/391]	Time 0.023 (0.023)	Data 0.145 (0.145)	Loss 5.2343 (5.2343)	Acc@1 1.562 (1.562)	Acc@5 14.062 (14.062)
Epoch: [2][10/391]	Time 0.013 (0.014)	Data 0.001 (0.014)	Loss 5.1420 (5.1868)	Acc@1 4.688 (3.693)	Acc@5 17.969 (17.685)
Epoch: [2][20/391]	Time 0.015 (0.014)	Data 0.001 (0.008)	Loss 5.1915 (5.1881)	Acc@1 2.344 (3.869)	Acc@5 19.531 (17.374)
Epoch: [2][30/391]	Time 0.014 (0.014)	Data 0.002 (0.006)	Loss 5.1023 (5.1649)	Acc@1 3.906 (3.755)	Acc@5 17.969 (17.994)
Epoch: [2][40/391]	Time 0.010 (0.014)	Data 0.002 (0.005)	Loss 5.1934 (5.1495)	Acc@1 3.125 (3.944)	Acc@5 14.844 (18.121)
Epoch: [2][50/391]	Time 0.020 (0.014)	Data 0.001 (0.004)	Loss 5.1889 (5.1469)	Acc@1 6.250 (3.906)	Acc@5 15.625 (18.229)
Epoch: [2][60/391]	Time 0.013 (0.014)	Data 0.002 (0.004)	Loss 5.1170 (5.1408)	Acc@1 3.906 (4.022)	Acc@5 18.750 (18.507)
Epoch: [2][70/391]	Time 0.013 (0.014)	Data 0.002 (0.004)	Loss 5.0603 (5.1301)	Acc@1 3.125 (4.016)	Acc@5 21.875 (18.706)
Epoch: [2][80/391]	Time 0.011 (0.014)	Data 0.002 (0.003)	Loss 5.0541 (5.1233)	Acc@1 3.125 (4.032)	Acc@5 19.531 (18.769)
Epoch: [2][90/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 5.0468 (5.1134)	Acc@1 4.688 (4.087)	Acc@5 23.438 (19.050)
Epoch: [2][100/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 5.0156 (5.1047)	Acc@1 3.125 (4.162)	Acc@5 20.312 (19.253)
Epoch: [2][110/391]	Time 0.015 (0.014)	Data 0.002 (0.003)	Loss 4.9123 (5.0970)	Acc@1 7.812 (4.237)	Acc@5 20.312 (19.313)
Epoch: [2][120/391]	Time 0.014 (0.014)	Data 0.000 (0.003)	Loss 5.0290 (5.0808)	Acc@1 3.906 (4.378)	Acc@5 23.438 (19.641)
Epoch: [2][130/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 4.9221 (5.0744)	Acc@1 5.469 (4.431)	Acc@5 19.531 (19.710)
Epoch: [2][140/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 4.8321 (5.0619)	Acc@1 10.938 (4.610)	Acc@5 24.219 (19.963)
Epoch: [2][150/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 4.9296 (5.0515)	Acc@1 3.125 (4.584)	Acc@5 20.312 (20.137)
Epoch: [2][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.8115 (5.0404)	Acc@1 7.031 (4.663)	Acc@5 25.000 (20.191)
Epoch: [2][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.6224 (5.0310)	Acc@1 7.031 (4.665)	Acc@5 28.906 (20.363)
Epoch: [2][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.9575 (5.0211)	Acc@1 4.688 (4.770)	Acc@5 19.531 (20.571)
Epoch: [2][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 5.0049 (5.0112)	Acc@1 4.688 (4.917)	Acc@5 21.875 (20.840)
Epoch: [2][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.8277 (5.0026)	Acc@1 4.688 (5.010)	Acc@5 23.438 (21.039)
Epoch: [2][210/391]	Time 0.014 (0.013)	Data 0.002 (0.002)	Loss 4.7130 (4.9929)	Acc@1 9.375 (5.076)	Acc@5 25.781 (21.212)
Epoch: [2][220/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 4.8279 (4.9856)	Acc@1 6.250 (5.186)	Acc@5 23.438 (21.345)
Epoch: [2][230/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 4.7194 (4.9763)	Acc@1 9.375 (5.242)	Acc@5 24.219 (21.530)
Epoch: [2][240/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 4.7235 (4.9686)	Acc@1 7.812 (5.349)	Acc@5 29.688 (21.726)
Epoch: [2][250/391]	Time 0.011 (0.013)	Data 0.003 (0.002)	Loss 4.6322 (4.9584)	Acc@1 9.375 (5.447)	Acc@5 30.469 (21.931)
Epoch: [2][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.7254 (4.9500)	Acc@1 5.469 (5.541)	Acc@5 28.125 (22.040)
Epoch: [2][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.6185 (4.9404)	Acc@1 8.594 (5.613)	Acc@5 31.250 (22.232)
Epoch: [2][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.7876 (4.9298)	Acc@1 8.594 (5.683)	Acc@5 25.781 (22.400)
Epoch: [2][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.5830 (4.9203)	Acc@1 10.156 (5.745)	Acc@5 30.469 (22.605)
Epoch: [2][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.6979 (4.9132)	Acc@1 4.688 (5.798)	Acc@5 28.906 (22.744)
Epoch: [2][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.5781 (4.9042)	Acc@1 9.375 (5.871)	Acc@5 28.125 (22.905)
Epoch: [2][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.5856 (4.8950)	Acc@1 11.719 (5.946)	Acc@5 31.250 (23.141)
Epoch: [2][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.5247 (4.8868)	Acc@1 9.375 (6.023)	Acc@5 32.812 (23.298)
Epoch: [2][340/391]	Time 0.011 (0.013)	Data 0.002 (0.002)	Loss 4.5792 (4.8808)	Acc@1 8.594 (6.055)	Acc@5 26.562 (23.371)
Epoch: [2][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.5818 (4.8713)	Acc@1 7.812 (6.157)	Acc@5 27.344 (23.584)
Epoch: [2][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.4555 (4.8617)	Acc@1 14.844 (6.244)	Acc@5 34.375 (23.797)
Epoch: [2][370/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 4.5465 (4.8520)	Acc@1 11.719 (6.322)	Acc@5 33.594 (23.993)
Epoch: [2][380/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 4.5323 (4.8444)	Acc@1 10.156 (6.381)	Acc@5 27.344 (24.157)
Epoch: [2][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 4.4901 (4.8360)	Acc@1 11.250 (6.442)	Acc@5 36.250 (24.322)
num momentum params: 26
[0.1, 4.836037633514405, 3.7671382999420167, 6.442, 10.05, tensor(0.1718, device='cuda:0', grad_fn=<DivBackward0>), 5.150939702987671, 0.40003013610839844]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [3 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [3][0/391]	Time 0.034 (0.034)	Data 0.175 (0.175)	Loss 4.4632 (4.4632)	Acc@1 10.938 (10.938)	Acc@5 35.938 (35.938)
Epoch: [3][10/391]	Time 0.012 (0.016)	Data 0.002 (0.017)	Loss 4.6137 (4.5022)	Acc@1 6.250 (9.801)	Acc@5 28.125 (33.026)
Epoch: [3][20/391]	Time 0.013 (0.015)	Data 0.002 (0.010)	Loss 4.2005 (4.4744)	Acc@1 15.625 (10.268)	Acc@5 40.625 (33.780)
Epoch: [3][30/391]	Time 0.013 (0.014)	Data 0.001 (0.007)	Loss 4.2710 (4.4481)	Acc@1 13.281 (10.862)	Acc@5 38.281 (34.047)
Epoch: [3][40/391]	Time 0.013 (0.014)	Data 0.001 (0.006)	Loss 4.2708 (4.4337)	Acc@1 9.375 (10.614)	Acc@5 35.156 (34.318)
Epoch: [3][50/391]	Time 0.013 (0.014)	Data 0.001 (0.005)	Loss 4.5654 (4.4219)	Acc@1 10.938 (10.968)	Acc@5 28.125 (34.528)
Epoch: [3][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 4.4459 (4.4290)	Acc@1 9.375 (10.771)	Acc@5 32.031 (34.260)
Epoch: [3][70/391]	Time 0.014 (0.013)	Data 0.001 (0.004)	Loss 4.5597 (4.4268)	Acc@1 5.469 (10.651)	Acc@5 25.781 (34.199)
Epoch: [3][80/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 4.2986 (4.4212)	Acc@1 10.938 (10.571)	Acc@5 35.156 (34.259)
Epoch: [3][90/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 4.3350 (4.4077)	Acc@1 10.156 (10.792)	Acc@5 39.844 (34.693)
Epoch: [3][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 4.2326 (4.4009)	Acc@1 13.281 (10.907)	Acc@5 39.062 (34.708)
Epoch: [3][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 4.3247 (4.3948)	Acc@1 9.375 (10.902)	Acc@5 38.281 (34.727)
Epoch: [3][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 4.2753 (4.3901)	Acc@1 16.406 (11.080)	Acc@5 39.062 (34.866)
Epoch: [3][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 4.2866 (4.3812)	Acc@1 12.500 (11.212)	Acc@5 32.812 (35.174)
Epoch: [3][140/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 4.2986 (4.3704)	Acc@1 10.156 (11.253)	Acc@5 35.938 (35.478)
Epoch: [3][150/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 4.3326 (4.3636)	Acc@1 12.500 (11.372)	Acc@5 30.469 (35.612)
Epoch: [3][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.2579 (4.3550)	Acc@1 14.062 (11.496)	Acc@5 38.281 (35.889)
Epoch: [3][170/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 4.5430 (4.3482)	Acc@1 8.594 (11.541)	Acc@5 30.469 (36.116)
Epoch: [3][180/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 4.2052 (4.3377)	Acc@1 9.375 (11.663)	Acc@5 38.281 (36.317)
Epoch: [3][190/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 4.1748 (4.3342)	Acc@1 10.156 (11.711)	Acc@5 46.875 (36.396)
Epoch: [3][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.1367 (4.3297)	Acc@1 12.500 (11.723)	Acc@5 38.281 (36.423)
Epoch: [3][210/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 4.0390 (4.3242)	Acc@1 17.969 (11.808)	Acc@5 43.750 (36.452)
Epoch: [3][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.3883 (4.3176)	Acc@1 12.500 (11.931)	Acc@5 31.250 (36.517)
Epoch: [3][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.3299 (4.3134)	Acc@1 12.500 (12.003)	Acc@5 32.031 (36.597)
Epoch: [3][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.2327 (4.3090)	Acc@1 12.500 (12.108)	Acc@5 39.844 (36.693)
Epoch: [3][250/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 4.2602 (4.3009)	Acc@1 15.625 (12.114)	Acc@5 35.156 (36.840)
Epoch: [3][260/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.9777 (4.2938)	Acc@1 17.969 (12.183)	Acc@5 45.312 (37.042)
Epoch: [3][270/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 4.0945 (4.2869)	Acc@1 16.406 (12.298)	Acc@5 48.438 (37.235)
Epoch: [3][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.9718 (4.2774)	Acc@1 18.750 (12.397)	Acc@5 45.312 (37.492)
Epoch: [3][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.7841 (4.2697)	Acc@1 22.656 (12.454)	Acc@5 46.875 (37.610)
Epoch: [3][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.7724 (4.2618)	Acc@1 21.875 (12.539)	Acc@5 51.562 (37.786)
Epoch: [3][310/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 4.1106 (4.2511)	Acc@1 16.406 (12.681)	Acc@5 39.844 (38.033)
Epoch: [3][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.0467 (4.2454)	Acc@1 18.750 (12.775)	Acc@5 46.875 (38.143)
Epoch: [3][330/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.9501 (4.2395)	Acc@1 14.844 (12.847)	Acc@5 46.094 (38.260)
Epoch: [3][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.9271 (4.2328)	Acc@1 13.281 (12.954)	Acc@5 45.312 (38.414)
Epoch: [3][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.9869 (4.2253)	Acc@1 14.062 (13.072)	Acc@5 45.312 (38.606)
Epoch: [3][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 4.0255 (4.2186)	Acc@1 15.625 (13.117)	Acc@5 46.094 (38.764)
Epoch: [3][370/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.8642 (4.2111)	Acc@1 17.188 (13.199)	Acc@5 48.438 (38.940)
Epoch: [3][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.9218 (4.2041)	Acc@1 18.750 (13.269)	Acc@5 48.438 (39.089)
Epoch: [3][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 4.0311 (4.1961)	Acc@1 12.500 (13.352)	Acc@5 41.250 (39.220)
num momentum params: 26
[0.1, 4.196125597839355, 3.476620638370514, 13.352, 15.53, tensor(0.1484, device='cuda:0', grad_fn=<DivBackward0>), 5.114928960800171, 0.3730449676513672]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [4 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [4][0/391]	Time 0.020 (0.020)	Data 0.156 (0.156)	Loss 3.7669 (3.7669)	Acc@1 18.750 (18.750)	Acc@5 51.562 (51.562)
Epoch: [4][10/391]	Time 0.013 (0.014)	Data 0.001 (0.015)	Loss 3.9337 (3.8818)	Acc@1 15.625 (17.685)	Acc@5 42.969 (45.668)
Epoch: [4][20/391]	Time 0.016 (0.013)	Data 0.001 (0.009)	Loss 4.0982 (3.8638)	Acc@1 8.594 (18.118)	Acc@5 36.719 (46.280)
Epoch: [4][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 3.8109 (3.8612)	Acc@1 20.312 (18.246)	Acc@5 47.656 (47.026)
Epoch: [4][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 3.9661 (3.8520)	Acc@1 21.094 (18.464)	Acc@5 49.219 (47.313)
Epoch: [4][50/391]	Time 0.014 (0.014)	Data 0.001 (0.004)	Loss 4.0232 (3.8580)	Acc@1 19.531 (18.398)	Acc@5 42.969 (47.258)
Epoch: [4][60/391]	Time 0.013 (0.014)	Data 0.001 (0.004)	Loss 3.9426 (3.8614)	Acc@1 10.938 (18.251)	Acc@5 46.875 (47.080)
Epoch: [4][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 3.8561 (3.8655)	Acc@1 21.094 (17.969)	Acc@5 51.562 (47.029)
Epoch: [4][80/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 3.7656 (3.8430)	Acc@1 17.188 (18.306)	Acc@5 52.344 (47.666)
Epoch: [4][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.6625 (3.8357)	Acc@1 21.875 (18.475)	Acc@5 53.125 (47.708)
Epoch: [4][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.8077 (3.8260)	Acc@1 20.312 (18.533)	Acc@5 50.000 (48.012)
Epoch: [4][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.8537 (3.8172)	Acc@1 18.750 (18.539)	Acc@5 49.219 (48.100)
Epoch: [4][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.4580 (3.8121)	Acc@1 22.656 (18.530)	Acc@5 56.250 (48.341)
Epoch: [4][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.8617 (3.8062)	Acc@1 17.188 (18.601)	Acc@5 45.312 (48.509)
Epoch: [4][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.6656 (3.7972)	Acc@1 25.781 (18.839)	Acc@5 53.125 (48.720)
Epoch: [4][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.7000 (3.7927)	Acc@1 20.312 (18.957)	Acc@5 50.781 (48.763)
Epoch: [4][160/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 3.6984 (3.7897)	Acc@1 18.750 (18.964)	Acc@5 46.875 (48.787)
Epoch: [4][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.7589 (3.7844)	Acc@1 14.062 (19.001)	Acc@5 50.000 (48.949)
Epoch: [4][180/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.5478 (3.7790)	Acc@1 24.219 (19.104)	Acc@5 56.250 (49.124)
Epoch: [4][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.6276 (3.7720)	Acc@1 22.656 (19.229)	Acc@5 52.344 (49.305)
Epoch: [4][200/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 3.4904 (3.7623)	Acc@1 22.656 (19.454)	Acc@5 55.469 (49.452)
Epoch: [4][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.6106 (3.7574)	Acc@1 24.219 (19.498)	Acc@5 46.094 (49.556)
Epoch: [4][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.7383 (3.7512)	Acc@1 21.875 (19.616)	Acc@5 47.656 (49.657)
Epoch: [4][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.7184 (3.7458)	Acc@1 20.312 (19.683)	Acc@5 46.875 (49.807)
Epoch: [4][240/391]	Time 0.009 (0.013)	Data 0.005 (0.002)	Loss 3.8132 (3.7391)	Acc@1 16.406 (19.768)	Acc@5 50.000 (49.994)
Epoch: [4][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.6334 (3.7353)	Acc@1 21.094 (19.830)	Acc@5 53.125 (50.075)
Epoch: [4][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.5701 (3.7274)	Acc@1 21.094 (19.944)	Acc@5 53.125 (50.293)
Epoch: [4][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4791 (3.7215)	Acc@1 24.219 (20.062)	Acc@5 57.031 (50.430)
Epoch: [4][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.7725 (3.7198)	Acc@1 16.406 (20.057)	Acc@5 48.438 (50.425)
Epoch: [4][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4598 (3.7150)	Acc@1 15.625 (20.103)	Acc@5 57.812 (50.545)
Epoch: [4][300/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 3.4215 (3.7106)	Acc@1 27.344 (20.214)	Acc@5 55.469 (50.618)
Epoch: [4][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3763 (3.7043)	Acc@1 25.000 (20.312)	Acc@5 62.500 (50.791)
Epoch: [4][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.6329 (3.6992)	Acc@1 25.000 (20.366)	Acc@5 54.688 (50.866)
Epoch: [4][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3553 (3.6955)	Acc@1 23.438 (20.390)	Acc@5 57.031 (50.921)
Epoch: [4][340/391]	Time 0.018 (0.013)	Data 0.002 (0.002)	Loss 3.4865 (3.6899)	Acc@1 21.094 (20.471)	Acc@5 50.000 (51.063)
Epoch: [4][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3408 (3.6826)	Acc@1 29.688 (20.606)	Acc@5 62.500 (51.242)
Epoch: [4][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.6789 (3.6773)	Acc@1 19.531 (20.717)	Acc@5 52.344 (51.353)
Epoch: [4][370/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.5459 (3.6733)	Acc@1 23.438 (20.734)	Acc@5 54.688 (51.455)
Epoch: [4][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.6251 (3.6686)	Acc@1 18.750 (20.839)	Acc@5 53.125 (51.562)
Epoch: [4][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.8358 (3.6654)	Acc@1 20.000 (20.906)	Acc@5 45.000 (51.646)
num momentum params: 26
[0.1, 3.6653772924804686, 3.0151377868652345, 20.906, 24.33, tensor(0.1411, device='cuda:0', grad_fn=<DivBackward0>), 5.136517524719238, 0.3695719242095947]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [5 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [5][0/391]	Time 0.021 (0.021)	Data 0.141 (0.141)	Loss 3.2506 (3.2506)	Acc@1 32.031 (32.031)	Acc@5 57.812 (57.812)
Epoch: [5][10/391]	Time 0.013 (0.013)	Data 0.001 (0.014)	Loss 3.2494 (3.4398)	Acc@1 30.469 (25.426)	Acc@5 58.594 (55.185)
Epoch: [5][20/391]	Time 0.013 (0.013)	Data 0.001 (0.009)	Loss 3.2267 (3.4049)	Acc@1 28.125 (25.372)	Acc@5 62.500 (56.622)
Epoch: [5][30/391]	Time 0.014 (0.013)	Data 0.001 (0.006)	Loss 3.3209 (3.4125)	Acc@1 28.906 (25.403)	Acc@5 58.594 (56.477)
Epoch: [5][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 3.2998 (3.4044)	Acc@1 27.344 (25.457)	Acc@5 61.719 (57.241)
Epoch: [5][50/391]	Time 0.013 (0.013)	Data 0.002 (0.004)	Loss 3.4580 (3.4145)	Acc@1 21.094 (25.184)	Acc@5 60.156 (57.169)
Epoch: [5][60/391]	Time 0.014 (0.013)	Data 0.001 (0.004)	Loss 3.2912 (3.4184)	Acc@1 28.906 (25.051)	Acc@5 54.688 (56.865)
Epoch: [5][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 3.4388 (3.4251)	Acc@1 21.875 (25.044)	Acc@5 58.594 (56.668)
Epoch: [5][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.3965 (3.4258)	Acc@1 29.688 (25.174)	Acc@5 55.469 (56.723)
Epoch: [5][90/391]	Time 0.019 (0.013)	Data 0.001 (0.003)	Loss 3.3118 (3.4205)	Acc@1 25.000 (25.309)	Acc@5 57.812 (56.834)
Epoch: [5][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.3135 (3.4135)	Acc@1 26.562 (25.487)	Acc@5 57.031 (56.946)
Epoch: [5][110/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 3.3317 (3.4150)	Acc@1 28.125 (25.331)	Acc@5 53.125 (56.820)
Epoch: [5][120/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 3.4475 (3.4143)	Acc@1 27.344 (25.291)	Acc@5 56.250 (56.799)
Epoch: [5][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.5295 (3.4171)	Acc@1 21.094 (25.233)	Acc@5 56.250 (56.858)
Epoch: [5][140/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.2405 (3.4128)	Acc@1 30.469 (25.410)	Acc@5 54.688 (56.898)
Epoch: [5][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1156 (3.4049)	Acc@1 36.719 (25.730)	Acc@5 63.281 (57.073)
Epoch: [5][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3828 (3.3991)	Acc@1 25.781 (25.767)	Acc@5 62.500 (57.211)
Epoch: [5][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4824 (3.3954)	Acc@1 22.656 (25.863)	Acc@5 57.031 (57.328)
Epoch: [5][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2468 (3.3931)	Acc@1 26.562 (25.868)	Acc@5 60.938 (57.368)
Epoch: [5][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4571 (3.3892)	Acc@1 25.000 (26.023)	Acc@5 53.906 (57.432)
Epoch: [5][200/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 3.3140 (3.3901)	Acc@1 23.438 (25.972)	Acc@5 56.250 (57.381)
Epoch: [5][210/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 3.0389 (3.3890)	Acc@1 36.719 (26.052)	Acc@5 68.750 (57.453)
Epoch: [5][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4557 (3.3869)	Acc@1 22.656 (26.096)	Acc@5 60.156 (57.526)
Epoch: [5][230/391]	Time 0.015 (0.013)	Data 0.002 (0.002)	Loss 3.2703 (3.3858)	Acc@1 28.125 (26.062)	Acc@5 66.406 (57.586)
Epoch: [5][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3020 (3.3764)	Acc@1 26.562 (26.190)	Acc@5 59.375 (57.858)
Epoch: [5][250/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 3.1307 (3.3774)	Acc@1 28.125 (26.161)	Acc@5 61.719 (57.865)
Epoch: [5][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4584 (3.3739)	Acc@1 28.125 (26.203)	Acc@5 56.250 (57.941)
Epoch: [5][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2931 (3.3707)	Acc@1 26.562 (26.294)	Acc@5 56.250 (58.017)
Epoch: [5][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1391 (3.3674)	Acc@1 27.344 (26.362)	Acc@5 65.625 (58.118)
Epoch: [5][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3168 (3.3642)	Acc@1 27.344 (26.431)	Acc@5 59.375 (58.204)
Epoch: [5][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4905 (3.3595)	Acc@1 20.312 (26.443)	Acc@5 58.594 (58.324)
Epoch: [5][310/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.0406 (3.3565)	Acc@1 30.469 (26.462)	Acc@5 71.094 (58.433)
Epoch: [5][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3507 (3.3535)	Acc@1 24.219 (26.543)	Acc@5 54.688 (58.453)
Epoch: [5][330/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 3.0944 (3.3493)	Acc@1 38.281 (26.650)	Acc@5 61.719 (58.547)
Epoch: [5][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3900 (3.3472)	Acc@1 32.031 (26.679)	Acc@5 60.938 (58.628)
Epoch: [5][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4065 (3.3439)	Acc@1 27.344 (26.738)	Acc@5 61.719 (58.718)
Epoch: [5][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1431 (3.3409)	Acc@1 27.344 (26.762)	Acc@5 66.406 (58.808)
Epoch: [5][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1400 (3.3399)	Acc@1 31.250 (26.760)	Acc@5 64.844 (58.874)
Epoch: [5][380/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 3.0157 (3.3352)	Acc@1 35.156 (26.854)	Acc@5 67.188 (58.983)
Epoch: [5][390/391]	Time 0.016 (0.013)	Data 0.001 (0.002)	Loss 3.4123 (3.3352)	Acc@1 22.500 (26.844)	Acc@5 56.250 (58.996)
num momentum params: 26
[0.1, 3.3352313398742677, 2.9556623888015747, 26.844, 25.62, tensor(0.1468, device='cuda:0', grad_fn=<DivBackward0>), 5.102367162704468, 0.3717200756072998]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [6 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [6][0/391]	Time 0.027 (0.027)	Data 0.136 (0.136)	Loss 3.4150 (3.4150)	Acc@1 21.875 (21.875)	Acc@5 58.594 (58.594)
Epoch: [6][10/391]	Time 0.013 (0.014)	Data 0.001 (0.014)	Loss 3.2258 (3.2417)	Acc@1 22.656 (29.119)	Acc@5 64.844 (61.648)
Epoch: [6][20/391]	Time 0.013 (0.014)	Data 0.001 (0.008)	Loss 3.2337 (3.2115)	Acc@1 25.781 (29.018)	Acc@5 61.719 (62.760)
Epoch: [6][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 3.1377 (3.1890)	Acc@1 31.250 (29.410)	Acc@5 64.062 (63.029)
Epoch: [6][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 3.2940 (3.1801)	Acc@1 28.906 (29.554)	Acc@5 64.844 (63.415)
Epoch: [6][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 3.2396 (3.1861)	Acc@1 28.125 (29.550)	Acc@5 64.062 (63.312)
Epoch: [6][60/391]	Time 0.014 (0.013)	Data 0.001 (0.004)	Loss 3.0880 (3.1882)	Acc@1 34.375 (29.675)	Acc@5 63.281 (63.128)
Epoch: [6][70/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 3.0165 (3.1764)	Acc@1 39.062 (30.007)	Acc@5 67.188 (63.413)
Epoch: [6][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.9028 (3.1684)	Acc@1 36.719 (30.305)	Acc@5 64.062 (63.455)
Epoch: [6][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.0394 (3.1704)	Acc@1 28.125 (30.349)	Acc@5 67.188 (63.453)
Epoch: [6][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.0260 (3.1658)	Acc@1 32.812 (30.337)	Acc@5 65.625 (63.513)
Epoch: [6][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.0868 (3.1696)	Acc@1 37.500 (30.103)	Acc@5 64.062 (63.450)
Epoch: [6][120/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1036 (3.1627)	Acc@1 25.781 (30.159)	Acc@5 67.188 (63.662)
Epoch: [6][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2405 (3.1643)	Acc@1 29.688 (30.129)	Acc@5 61.719 (63.615)
Epoch: [6][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1772 (3.1687)	Acc@1 28.906 (30.086)	Acc@5 65.625 (63.497)
Epoch: [6][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2950 (3.1659)	Acc@1 25.000 (30.179)	Acc@5 60.156 (63.607)
Epoch: [6][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8395 (3.1633)	Acc@1 34.375 (30.163)	Acc@5 72.656 (63.660)
Epoch: [6][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9124 (3.1592)	Acc@1 36.719 (30.332)	Acc@5 71.094 (63.811)
Epoch: [6][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1501 (3.1575)	Acc@1 29.688 (30.344)	Acc@5 66.406 (63.950)
Epoch: [6][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2328 (3.1525)	Acc@1 25.781 (30.416)	Acc@5 62.500 (64.054)
Epoch: [6][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8749 (3.1489)	Acc@1 41.406 (30.574)	Acc@5 71.875 (64.164)
Epoch: [6][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1892 (3.1487)	Acc@1 29.688 (30.572)	Acc@5 60.938 (64.129)
Epoch: [6][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4296 (3.1472)	Acc@1 26.562 (30.529)	Acc@5 59.375 (64.179)
Epoch: [6][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0768 (3.1461)	Acc@1 30.469 (30.503)	Acc@5 67.188 (64.161)
Epoch: [6][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0947 (3.1444)	Acc@1 28.906 (30.598)	Acc@5 62.500 (64.169)
Epoch: [6][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3945 (3.1468)	Acc@1 27.344 (30.609)	Acc@5 59.375 (64.062)
Epoch: [6][260/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 3.2425 (3.1470)	Acc@1 32.031 (30.666)	Acc@5 60.156 (64.051)
Epoch: [6][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2492 (3.1486)	Acc@1 28.906 (30.656)	Acc@5 62.500 (63.988)
Epoch: [6][280/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.1518 (3.1464)	Acc@1 29.688 (30.716)	Acc@5 64.844 (64.051)
Epoch: [6][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9955 (3.1482)	Acc@1 36.719 (30.759)	Acc@5 69.531 (64.011)
Epoch: [6][300/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.1936 (3.1496)	Acc@1 30.469 (30.785)	Acc@5 63.281 (64.031)
Epoch: [6][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1010 (3.1474)	Acc@1 28.125 (30.748)	Acc@5 66.406 (64.113)
Epoch: [6][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9888 (3.1455)	Acc@1 30.469 (30.749)	Acc@5 67.188 (64.199)
Epoch: [6][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0331 (3.1439)	Acc@1 30.469 (30.761)	Acc@5 64.062 (64.218)
Epoch: [6][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8809 (3.1392)	Acc@1 35.156 (30.870)	Acc@5 66.406 (64.298)
Epoch: [6][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.3609 (3.1371)	Acc@1 32.812 (30.934)	Acc@5 57.812 (64.343)
Epoch: [6][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1958 (3.1364)	Acc@1 34.375 (30.947)	Acc@5 67.969 (64.383)
Epoch: [6][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8899 (3.1346)	Acc@1 39.062 (30.976)	Acc@5 67.188 (64.431)
Epoch: [6][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9173 (3.1322)	Acc@1 33.594 (30.967)	Acc@5 70.312 (64.510)
Epoch: [6][390/391]	Time 0.015 (0.013)	Data 0.000 (0.002)	Loss 3.2685 (3.1321)	Acc@1 23.750 (30.964)	Acc@5 57.500 (64.510)
num momentum params: 26
[0.1, 3.1320773046112063, 2.7056687331199645, 30.964, 30.24, tensor(0.1612, device='cuda:0', grad_fn=<DivBackward0>), 5.068876266479492, 0.3748795986175537]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [7 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [7][0/391]	Time 0.025 (0.025)	Data 0.148 (0.148)	Loss 3.1477 (3.1477)	Acc@1 27.344 (27.344)	Acc@5 61.719 (61.719)
Epoch: [7][10/391]	Time 0.013 (0.015)	Data 0.001 (0.015)	Loss 3.1338 (3.1082)	Acc@1 28.125 (30.682)	Acc@5 63.281 (64.205)
Epoch: [7][20/391]	Time 0.013 (0.014)	Data 0.001 (0.008)	Loss 3.0097 (3.0823)	Acc@1 36.719 (32.775)	Acc@5 69.531 (65.216)
Epoch: [7][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.9977 (3.0566)	Acc@1 33.594 (32.913)	Acc@5 66.406 (66.129)
Epoch: [7][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 3.2684 (3.0556)	Acc@1 35.156 (32.793)	Acc@5 60.156 (66.368)
Epoch: [7][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 3.0827 (3.0471)	Acc@1 33.594 (33.119)	Acc@5 67.188 (66.667)
Epoch: [7][60/391]	Time 0.014 (0.013)	Data 0.001 (0.004)	Loss 2.9631 (3.0317)	Acc@1 34.375 (33.414)	Acc@5 72.656 (67.123)
Epoch: [7][70/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.9565 (3.0203)	Acc@1 33.594 (33.814)	Acc@5 67.969 (67.320)
Epoch: [7][80/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 3.2609 (3.0292)	Acc@1 30.469 (33.449)	Acc@5 61.719 (67.197)
Epoch: [7][90/391]	Time 0.009 (0.014)	Data 0.003 (0.003)	Loss 2.7914 (3.0242)	Acc@1 32.812 (33.422)	Acc@5 77.344 (67.376)
Epoch: [7][100/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 3.0274 (3.0219)	Acc@1 38.281 (33.493)	Acc@5 72.656 (67.520)
Epoch: [7][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.8215 (3.0117)	Acc@1 39.844 (33.791)	Acc@5 68.750 (67.758)
Epoch: [7][120/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 3.0827 (3.0144)	Acc@1 29.688 (33.723)	Acc@5 66.406 (67.672)
Epoch: [7][130/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 3.1050 (3.0160)	Acc@1 25.000 (33.653)	Acc@5 67.969 (67.671)
Epoch: [7][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9581 (3.0136)	Acc@1 32.812 (33.721)	Acc@5 69.531 (67.753)
Epoch: [7][150/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 2.8401 (3.0059)	Acc@1 39.844 (33.878)	Acc@5 67.188 (67.917)
Epoch: [7][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9886 (3.0138)	Acc@1 32.812 (33.827)	Acc@5 66.406 (67.765)
Epoch: [7][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9195 (3.0148)	Acc@1 38.281 (33.845)	Acc@5 68.750 (67.658)
Epoch: [7][180/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.0640 (3.0175)	Acc@1 33.594 (33.861)	Acc@5 67.969 (67.567)
Epoch: [7][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0017 (3.0161)	Acc@1 29.688 (33.868)	Acc@5 68.750 (67.605)
Epoch: [7][200/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 3.0430 (3.0170)	Acc@1 39.062 (33.936)	Acc@5 66.406 (67.553)
Epoch: [7][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1401 (3.0156)	Acc@1 33.594 (34.005)	Acc@5 64.844 (67.602)
Epoch: [7][220/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.9977 (3.0191)	Acc@1 38.281 (33.997)	Acc@5 63.281 (67.527)
Epoch: [7][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0670 (3.0235)	Acc@1 31.250 (33.888)	Acc@5 71.094 (67.455)
Epoch: [7][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0396 (3.0209)	Acc@1 39.844 (34.041)	Acc@5 68.750 (67.583)
Epoch: [7][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0441 (3.0214)	Acc@1 35.156 (34.030)	Acc@5 67.188 (67.611)
Epoch: [7][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1356 (3.0172)	Acc@1 29.688 (34.174)	Acc@5 64.844 (67.699)
Epoch: [7][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0643 (3.0174)	Acc@1 31.250 (34.185)	Acc@5 68.750 (67.718)
Epoch: [7][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9132 (3.0165)	Acc@1 35.156 (34.219)	Acc@5 70.312 (67.741)
Epoch: [7][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1412 (3.0171)	Acc@1 22.656 (34.152)	Acc@5 65.625 (67.732)
Epoch: [7][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9598 (3.0153)	Acc@1 35.938 (34.240)	Acc@5 67.969 (67.769)
Epoch: [7][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9992 (3.0152)	Acc@1 35.938 (34.259)	Acc@5 69.531 (67.868)
Epoch: [7][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9409 (3.0150)	Acc@1 32.031 (34.280)	Acc@5 71.094 (67.896)
Epoch: [7][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2233 (3.0168)	Acc@1 31.250 (34.215)	Acc@5 67.188 (67.896)
Epoch: [7][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0112 (3.0148)	Acc@1 39.844 (34.348)	Acc@5 67.188 (67.930)
Epoch: [7][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1185 (3.0148)	Acc@1 32.812 (34.395)	Acc@5 63.281 (67.889)
Epoch: [7][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.2940 (3.0153)	Acc@1 30.469 (34.427)	Acc@5 63.281 (67.893)
Epoch: [7][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7866 (3.0133)	Acc@1 43.750 (34.493)	Acc@5 67.969 (67.937)
Epoch: [7][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9323 (3.0124)	Acc@1 37.500 (34.547)	Acc@5 65.625 (67.977)
Epoch: [7][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 3.1203 (3.0137)	Acc@1 33.750 (34.558)	Acc@5 66.250 (67.954)
num momentum params: 26
[0.1, 3.013698516921997, 2.858780837059021, 34.558, 29.55, tensor(0.1788, device='cuda:0', grad_fn=<DivBackward0>), 5.119776010513306, 0.3791310787200928]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [8 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [8][0/391]	Time 0.024 (0.024)	Data 0.143 (0.143)	Loss 2.8780 (2.8780)	Acc@1 34.375 (34.375)	Acc@5 75.000 (75.000)
Epoch: [8][10/391]	Time 0.013 (0.014)	Data 0.001 (0.014)	Loss 2.6827 (2.8810)	Acc@1 41.406 (36.861)	Acc@5 72.656 (71.378)
Epoch: [8][20/391]	Time 0.013 (0.013)	Data 0.001 (0.008)	Loss 2.7720 (2.8789)	Acc@1 38.281 (37.314)	Acc@5 68.750 (71.466)
Epoch: [8][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.8088 (2.9132)	Acc@1 42.188 (37.374)	Acc@5 70.312 (70.514)
Epoch: [8][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 3.0119 (2.9209)	Acc@1 32.812 (36.890)	Acc@5 66.406 (70.389)
Epoch: [8][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 3.1499 (2.9185)	Acc@1 34.375 (36.780)	Acc@5 64.844 (70.435)
Epoch: [8][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.7898 (2.9149)	Acc@1 39.062 (37.103)	Acc@5 74.219 (70.710)
Epoch: [8][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.1355 (2.9290)	Acc@1 32.031 (36.818)	Acc@5 67.969 (70.555)
Epoch: [8][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7158 (2.9278)	Acc@1 41.406 (36.941)	Acc@5 74.219 (70.476)
Epoch: [8][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.8461 (2.9237)	Acc@1 38.281 (37.079)	Acc@5 69.531 (70.587)
Epoch: [8][100/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.8561 (2.9275)	Acc@1 37.500 (36.796)	Acc@5 77.344 (70.583)
Epoch: [8][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.8370 (2.9247)	Acc@1 40.625 (36.803)	Acc@5 77.344 (70.664)
Epoch: [8][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7128 (2.9200)	Acc@1 42.188 (36.938)	Acc@5 73.438 (70.693)
Epoch: [8][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9607 (2.9161)	Acc@1 32.031 (37.005)	Acc@5 69.531 (70.754)
Epoch: [8][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8989 (2.9189)	Acc@1 38.281 (36.951)	Acc@5 68.750 (70.612)
Epoch: [8][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1368 (2.9236)	Acc@1 36.719 (36.879)	Acc@5 65.625 (70.525)
Epoch: [8][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7211 (2.9264)	Acc@1 44.531 (36.927)	Acc@5 77.344 (70.434)
Epoch: [8][170/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9154 (2.9306)	Acc@1 33.594 (36.879)	Acc@5 71.094 (70.354)
Epoch: [8][180/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.1241 (2.9351)	Acc@1 36.719 (36.788)	Acc@5 68.750 (70.256)
Epoch: [8][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9657 (2.9313)	Acc@1 39.844 (36.989)	Acc@5 68.750 (70.304)
Epoch: [8][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8991 (2.9266)	Acc@1 37.500 (37.119)	Acc@5 71.875 (70.445)
Epoch: [8][210/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9922 (2.9250)	Acc@1 35.156 (37.141)	Acc@5 69.531 (70.509)
Epoch: [8][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0346 (2.9238)	Acc@1 33.594 (37.083)	Acc@5 72.656 (70.567)
Epoch: [8][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1568 (2.9245)	Acc@1 33.594 (37.047)	Acc@5 62.500 (70.509)
Epoch: [8][240/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.2117 (2.9233)	Acc@1 29.688 (37.030)	Acc@5 62.500 (70.533)
Epoch: [8][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0230 (2.9229)	Acc@1 37.500 (37.036)	Acc@5 69.531 (70.596)
Epoch: [8][260/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.8853 (2.9258)	Acc@1 40.625 (37.021)	Acc@5 74.219 (70.546)
Epoch: [8][270/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.0973 (2.9310)	Acc@1 31.250 (36.932)	Acc@5 68.750 (70.448)
Epoch: [8][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9047 (2.9305)	Acc@1 42.188 (37.030)	Acc@5 69.531 (70.454)
Epoch: [8][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0135 (2.9302)	Acc@1 40.625 (37.060)	Acc@5 68.750 (70.463)
Epoch: [8][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0414 (2.9302)	Acc@1 33.594 (37.067)	Acc@5 67.969 (70.471)
Epoch: [8][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9294 (2.9334)	Acc@1 36.719 (37.015)	Acc@5 71.094 (70.438)
Epoch: [8][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8299 (2.9353)	Acc@1 43.750 (36.996)	Acc@5 67.969 (70.424)
Epoch: [8][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9357 (2.9349)	Acc@1 34.375 (37.073)	Acc@5 67.188 (70.456)
Epoch: [8][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7346 (2.9342)	Acc@1 42.188 (37.156)	Acc@5 73.438 (70.455)
Epoch: [8][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8213 (2.9300)	Acc@1 40.625 (37.262)	Acc@5 71.094 (70.568)
Epoch: [8][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8208 (2.9303)	Acc@1 36.719 (37.312)	Acc@5 72.656 (70.548)
Epoch: [8][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0452 (2.9322)	Acc@1 32.812 (37.287)	Acc@5 72.656 (70.515)
Epoch: [8][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0731 (2.9322)	Acc@1 41.406 (37.279)	Acc@5 65.625 (70.548)
Epoch: [8][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.5322 (2.9312)	Acc@1 48.750 (37.292)	Acc@5 77.500 (70.624)
num momentum params: 26
[0.1, 2.9311736633300782, 2.3888316309452056, 37.292, 37.22, tensor(0.1962, device='cuda:0', grad_fn=<DivBackward0>), 5.088280200958252, 0.37213993072509766]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [9 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [9][0/391]	Time 0.024 (0.024)	Data 0.152 (0.152)	Loss 2.6536 (2.6536)	Acc@1 42.188 (42.188)	Acc@5 76.562 (76.562)
Epoch: [9][10/391]	Time 0.013 (0.014)	Data 0.001 (0.015)	Loss 2.9468 (2.8796)	Acc@1 42.969 (38.636)	Acc@5 74.219 (72.727)
Epoch: [9][20/391]	Time 0.013 (0.013)	Data 0.001 (0.008)	Loss 2.7895 (2.8969)	Acc@1 47.656 (39.174)	Acc@5 72.656 (71.540)
Epoch: [9][30/391]	Time 0.010 (0.013)	Data 0.005 (0.006)	Loss 2.9116 (2.8961)	Acc@1 40.625 (39.189)	Acc@5 70.312 (71.573)
Epoch: [9][40/391]	Time 0.014 (0.013)	Data 0.001 (0.005)	Loss 2.8375 (2.8951)	Acc@1 35.156 (38.796)	Acc@5 73.438 (71.418)
Epoch: [9][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 3.0203 (2.8776)	Acc@1 32.812 (39.062)	Acc@5 68.750 (71.875)
Epoch: [9][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.8486 (2.8738)	Acc@1 35.938 (38.973)	Acc@5 75.000 (72.016)
Epoch: [9][70/391]	Time 0.014 (0.013)	Data 0.001 (0.004)	Loss 2.9769 (2.8801)	Acc@1 39.844 (38.996)	Acc@5 68.750 (71.842)
Epoch: [9][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.8620 (2.8720)	Acc@1 42.188 (39.255)	Acc@5 75.000 (72.106)
Epoch: [9][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.9586 (2.8700)	Acc@1 35.938 (39.432)	Acc@5 71.094 (72.270)
Epoch: [9][100/391]	Time 0.010 (0.013)	Data 0.007 (0.003)	Loss 2.7392 (2.8565)	Acc@1 43.750 (39.759)	Acc@5 78.906 (72.540)
Epoch: [9][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.9080 (2.8621)	Acc@1 38.281 (39.597)	Acc@5 69.531 (72.544)
Epoch: [9][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.0601 (2.8691)	Acc@1 34.375 (39.405)	Acc@5 69.531 (72.534)
Epoch: [9][130/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 2.7254 (2.8649)	Acc@1 39.062 (39.361)	Acc@5 75.000 (72.609)
Epoch: [9][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6452 (2.8601)	Acc@1 47.656 (39.556)	Acc@5 74.219 (72.800)
Epoch: [9][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8277 (2.8573)	Acc@1 41.406 (39.611)	Acc@5 71.094 (72.889)
Epoch: [9][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0395 (2.8617)	Acc@1 42.188 (39.480)	Acc@5 63.281 (72.831)
Epoch: [9][170/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9542 (2.8613)	Acc@1 38.281 (39.487)	Acc@5 71.094 (72.880)
Epoch: [9][180/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.1160 (2.8622)	Acc@1 35.938 (39.460)	Acc@5 71.094 (72.872)
Epoch: [9][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7186 (2.8609)	Acc@1 40.625 (39.484)	Acc@5 75.781 (72.828)
Epoch: [9][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7947 (2.8609)	Acc@1 37.500 (39.509)	Acc@5 78.906 (72.835)
Epoch: [9][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9687 (2.8628)	Acc@1 37.500 (39.496)	Acc@5 70.312 (72.793)
Epoch: [9][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7695 (2.8631)	Acc@1 39.062 (39.519)	Acc@5 75.781 (72.748)
Epoch: [9][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9318 (2.8623)	Acc@1 43.750 (39.543)	Acc@5 67.188 (72.744)
Epoch: [9][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7888 (2.8581)	Acc@1 42.188 (39.617)	Acc@5 75.781 (72.896)
Epoch: [9][250/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7475 (2.8595)	Acc@1 38.281 (39.536)	Acc@5 74.219 (72.899)
Epoch: [9][260/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 3.0381 (2.8611)	Acc@1 35.156 (39.562)	Acc@5 71.094 (72.896)
Epoch: [9][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1575 (2.8605)	Acc@1 35.156 (39.607)	Acc@5 67.188 (72.930)
Epoch: [9][280/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8733 (2.8620)	Acc@1 38.281 (39.599)	Acc@5 75.000 (72.890)
Epoch: [9][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8397 (2.8614)	Acc@1 36.719 (39.599)	Acc@5 72.656 (72.901)
Epoch: [9][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7696 (2.8626)	Acc@1 44.531 (39.558)	Acc@5 74.219 (72.869)
Epoch: [9][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8349 (2.8620)	Acc@1 36.719 (39.537)	Acc@5 74.219 (72.920)
Epoch: [9][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7574 (2.8644)	Acc@1 46.875 (39.525)	Acc@5 73.438 (72.895)
Epoch: [9][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7260 (2.8644)	Acc@1 46.094 (39.553)	Acc@5 78.125 (72.916)
Epoch: [9][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7857 (2.8625)	Acc@1 42.969 (39.587)	Acc@5 73.438 (72.927)
Epoch: [9][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0997 (2.8644)	Acc@1 35.156 (39.530)	Acc@5 67.188 (72.874)
Epoch: [9][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7569 (2.8644)	Acc@1 41.406 (39.536)	Acc@5 75.000 (72.896)
Epoch: [9][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8593 (2.8645)	Acc@1 40.625 (39.532)	Acc@5 76.562 (72.915)
Epoch: [9][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9385 (2.8663)	Acc@1 36.719 (39.483)	Acc@5 73.438 (72.894)
Epoch: [9][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5921 (2.8671)	Acc@1 47.500 (39.466)	Acc@5 77.500 (72.886)
num momentum params: 26
[0.1, 2.867117988128662, 2.330473172664642, 39.466, 38.54, tensor(0.2121, device='cuda:0', grad_fn=<DivBackward0>), 5.127200603485107, 0.3738839626312256]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [10 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [10][0/391]	Time 0.026 (0.026)	Data 0.158 (0.158)	Loss 2.6740 (2.6740)	Acc@1 41.406 (41.406)	Acc@5 77.344 (77.344)
Epoch: [10][10/391]	Time 0.013 (0.014)	Data 0.001 (0.015)	Loss 2.7446 (2.7705)	Acc@1 38.281 (41.548)	Acc@5 75.781 (75.071)
Epoch: [10][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.7975 (2.7807)	Acc@1 41.406 (41.406)	Acc@5 74.219 (75.074)
Epoch: [10][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.8580 (2.7871)	Acc@1 43.750 (41.658)	Acc@5 75.781 (74.420)
Epoch: [10][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.6937 (2.7900)	Acc@1 42.188 (41.482)	Acc@5 80.469 (74.371)
Epoch: [10][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 3.0514 (2.7851)	Acc@1 33.594 (41.483)	Acc@5 70.312 (74.418)
Epoch: [10][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.8203 (2.7867)	Acc@1 40.625 (41.470)	Acc@5 73.438 (74.360)
Epoch: [10][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.7480 (2.7791)	Acc@1 40.625 (41.472)	Acc@5 75.781 (74.692)
Epoch: [10][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6871 (2.7783)	Acc@1 39.844 (41.551)	Acc@5 77.344 (74.740)
Epoch: [10][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7073 (2.7751)	Acc@1 42.969 (41.612)	Acc@5 80.469 (74.845)
Epoch: [10][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.0231 (2.7795)	Acc@1 36.719 (41.368)	Acc@5 69.531 (74.822)
Epoch: [10][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7802 (2.7835)	Acc@1 46.094 (41.265)	Acc@5 72.656 (74.761)
Epoch: [10][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7169 (2.7849)	Acc@1 41.406 (41.277)	Acc@5 76.562 (74.761)
Epoch: [10][130/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.6774 (2.7877)	Acc@1 46.094 (41.359)	Acc@5 77.344 (74.750)
Epoch: [10][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6728 (2.7889)	Acc@1 39.062 (41.284)	Acc@5 78.906 (74.773)
Epoch: [10][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0027 (2.7916)	Acc@1 35.156 (41.282)	Acc@5 69.531 (74.715)
Epoch: [10][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8725 (2.7889)	Acc@1 36.719 (41.431)	Acc@5 71.094 (74.791)
Epoch: [10][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6293 (2.7927)	Acc@1 53.125 (41.434)	Acc@5 77.344 (74.708)
Epoch: [10][180/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.9226 (2.7903)	Acc@1 44.531 (41.553)	Acc@5 68.750 (74.776)
Epoch: [10][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7812 (2.7905)	Acc@1 44.531 (41.574)	Acc@5 72.656 (74.795)
Epoch: [10][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8325 (2.7964)	Acc@1 36.719 (41.430)	Acc@5 76.562 (74.646)
Epoch: [10][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6830 (2.8013)	Acc@1 48.438 (41.328)	Acc@5 76.562 (74.552)
Epoch: [10][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7748 (2.8070)	Acc@1 42.188 (41.297)	Acc@5 75.000 (74.385)
Epoch: [10][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9312 (2.8085)	Acc@1 40.625 (41.339)	Acc@5 71.094 (74.337)
Epoch: [10][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7009 (2.8085)	Acc@1 43.750 (41.409)	Acc@5 78.906 (74.300)
Epoch: [10][250/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8328 (2.8064)	Acc@1 43.750 (41.478)	Acc@5 70.312 (74.371)
Epoch: [10][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8800 (2.8045)	Acc@1 45.312 (41.595)	Acc@5 69.531 (74.371)
Epoch: [10][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7645 (2.8053)	Acc@1 40.625 (41.602)	Acc@5 77.344 (74.369)
Epoch: [10][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6395 (2.8057)	Acc@1 50.000 (41.645)	Acc@5 75.781 (74.366)
Epoch: [10][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.4154 (2.8085)	Acc@1 33.594 (41.664)	Acc@5 57.812 (74.302)
Epoch: [10][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6166 (2.8098)	Acc@1 42.188 (41.640)	Acc@5 79.688 (74.273)
Epoch: [10][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5734 (2.8084)	Acc@1 50.781 (41.668)	Acc@5 80.469 (74.314)
Epoch: [10][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8070 (2.8072)	Acc@1 40.625 (41.640)	Acc@5 75.781 (74.377)
Epoch: [10][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5682 (2.8077)	Acc@1 44.531 (41.635)	Acc@5 82.812 (74.375)
Epoch: [10][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9917 (2.8088)	Acc@1 43.750 (41.596)	Acc@5 71.094 (74.370)
Epoch: [10][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8329 (2.8102)	Acc@1 44.531 (41.582)	Acc@5 75.781 (74.341)
Epoch: [10][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.1337 (2.8122)	Acc@1 33.594 (41.558)	Acc@5 66.406 (74.299)
Epoch: [10][370/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9043 (2.8120)	Acc@1 36.719 (41.577)	Acc@5 72.656 (74.286)
Epoch: [10][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7526 (2.8142)	Acc@1 37.500 (41.558)	Acc@5 79.688 (74.254)
Epoch: [10][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.7866 (2.8150)	Acc@1 48.750 (41.588)	Acc@5 70.000 (74.190)
num momentum params: 26
[0.1, 2.815003917541504, 2.2359458315372467, 41.588, 40.54, tensor(0.2265, device='cuda:0', grad_fn=<DivBackward0>), 5.1123223304748535, 0.38332486152648926]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [512, 512, 3, 3]
Before - module.bn8.weight: [512]
Before - module.bn8.bias: [512]
Before - module.fc.weight: [100, 512]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [64, 3, 3, 3] >> [64, 3, 3, 3]
[module.bn1.weight]: 64 >> 64
running_mean [64]
running_var [64]
num_batches_tracked []
[module.conv2.weight]: [128, 64, 3, 3] >> [128, 64, 3, 3]
[module.bn2.weight]: 128 >> 128
running_mean [128]
running_var [128]
num_batches_tracked []
[module.conv3.weight]: [256, 128, 3, 3] >> [256, 128, 3, 3]
[module.bn3.weight]: 256 >> 256
running_mean [256]
running_var [256]
num_batches_tracked []
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.bn4.weight]: 256 >> 256
running_mean [256]
running_var [256]
num_batches_tracked []
[module.conv5.weight]: [512, 256, 3, 3] >> [512, 256, 3, 3]
[module.bn5.weight]: 512 >> 512
running_mean [512]
running_var [512]
num_batches_tracked []
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.bn6.weight]: 512 >> 512
running_mean [512]
running_var [512]
num_batches_tracked []
[module.conv7.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.bn7.weight]: 512 >> 512
running_mean [512]
running_var [512]
num_batches_tracked []
[module.conv8.weight]: [512, 512, 3, 3] >> [438, 512, 3, 3]
[module.bn8.weight]: 512 >> 438
running_mean [438]
running_var [438]
num_batches_tracked []
[module.fc.weight]: [100, 512] >> [100, 438]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [64, 3, 3, 3]
After - module.bn1.weight: [64]
After - module.bn1.bias: [64]
After - module.conv2.weight: [128, 64, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [512, 512, 3, 3]
After - module.bn7.weight: [512]
After - module.bn7.bias: [512]
After - module.conv8.weight: [438, 512, 3, 3]
After - module.bn8.weight: [438]
After - module.bn8.bias: [438]
After - module.fc.weight: [100, 438]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [438, 512, 3, 3]
fc --> [438, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 6200229888, 8073216, 438
fc, 16819200, 43800, 0
===================
FLOP REPORT: 30736746000000.0 60502400000.0 151444248 151256 2678 17.02051544189453
[INFO] Storing checkpoint...

Epoch: [11 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [11][0/391]	Time 0.098 (0.098)	Data 0.135 (0.135)	Loss 2.7734 (2.7734)	Acc@1 42.188 (42.188)	Acc@5 76.562 (76.562)
Epoch: [11][10/391]	Time 0.014 (0.021)	Data 0.001 (0.014)	Loss 2.7918 (2.6784)	Acc@1 43.750 (44.886)	Acc@5 72.656 (77.273)
Epoch: [11][20/391]	Time 0.013 (0.017)	Data 0.001 (0.008)	Loss 2.8348 (2.7065)	Acc@1 45.312 (44.531)	Acc@5 70.312 (76.228)
Epoch: [11][30/391]	Time 0.014 (0.016)	Data 0.001 (0.006)	Loss 2.6998 (2.7264)	Acc@1 37.500 (43.674)	Acc@5 79.688 (76.361)
Epoch: [11][40/391]	Time 0.012 (0.016)	Data 0.001 (0.005)	Loss 2.4944 (2.7138)	Acc@1 53.125 (44.017)	Acc@5 81.250 (76.715)
Epoch: [11][50/391]	Time 0.014 (0.015)	Data 0.001 (0.004)	Loss 2.4196 (2.7150)	Acc@1 53.125 (44.118)	Acc@5 83.594 (76.654)
Epoch: [11][60/391]	Time 0.012 (0.015)	Data 0.001 (0.004)	Loss 2.9818 (2.7282)	Acc@1 35.938 (43.865)	Acc@5 69.531 (76.319)
Epoch: [11][70/391]	Time 0.013 (0.015)	Data 0.001 (0.003)	Loss 2.7340 (2.7300)	Acc@1 44.531 (43.684)	Acc@5 78.125 (76.485)
Epoch: [11][80/391]	Time 0.014 (0.014)	Data 0.001 (0.003)	Loss 2.4946 (2.7314)	Acc@1 50.781 (43.654)	Acc@5 78.906 (76.447)
Epoch: [11][90/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 2.8257 (2.7395)	Acc@1 40.625 (43.518)	Acc@5 74.219 (76.228)
Epoch: [11][100/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 2.9490 (2.7537)	Acc@1 39.844 (43.270)	Acc@5 70.312 (75.874)
Epoch: [11][110/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 2.6684 (2.7521)	Acc@1 45.312 (43.384)	Acc@5 79.688 (75.922)
Epoch: [11][120/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.5347 (2.7555)	Acc@1 48.438 (43.233)	Acc@5 78.906 (75.865)
Epoch: [11][130/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.8411 (2.7628)	Acc@1 41.406 (43.016)	Acc@5 71.875 (75.740)
Epoch: [11][140/391]	Time 0.014 (0.014)	Data 0.001 (0.002)	Loss 2.9064 (2.7678)	Acc@1 44.531 (42.996)	Acc@5 72.656 (75.670)
Epoch: [11][150/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.7380 (2.7766)	Acc@1 46.875 (42.772)	Acc@5 75.781 (75.600)
Epoch: [11][160/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.6225 (2.7739)	Acc@1 47.656 (42.843)	Acc@5 76.562 (75.650)
Epoch: [11][170/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.9211 (2.7735)	Acc@1 38.281 (42.864)	Acc@5 75.000 (75.653)
Epoch: [11][180/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 3.0131 (2.7714)	Acc@1 38.281 (42.891)	Acc@5 73.438 (75.786)
Epoch: [11][190/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.7194 (2.7717)	Acc@1 39.844 (42.883)	Acc@5 78.906 (75.753)
Epoch: [11][200/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.9591 (2.7707)	Acc@1 38.281 (42.872)	Acc@5 71.875 (75.816)
Epoch: [11][210/391]	Time 0.014 (0.014)	Data 0.001 (0.002)	Loss 2.6514 (2.7736)	Acc@1 47.656 (42.887)	Acc@5 77.344 (75.726)
Epoch: [11][220/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.9118 (2.7742)	Acc@1 35.938 (42.856)	Acc@5 74.219 (75.771)
Epoch: [11][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5618 (2.7782)	Acc@1 51.562 (42.793)	Acc@5 84.375 (75.714)
Epoch: [11][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6148 (2.7760)	Acc@1 46.094 (42.871)	Acc@5 78.906 (75.720)
Epoch: [11][250/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8328 (2.7774)	Acc@1 44.531 (42.916)	Acc@5 73.438 (75.629)
Epoch: [11][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9149 (2.7797)	Acc@1 38.281 (42.858)	Acc@5 71.875 (75.599)
Epoch: [11][270/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4662 (2.7779)	Acc@1 49.219 (42.931)	Acc@5 81.250 (75.603)
Epoch: [11][280/391]	Time 0.010 (0.013)	Data 0.001 (0.002)	Loss 2.8368 (2.7770)	Acc@1 38.281 (42.924)	Acc@5 70.312 (75.587)
Epoch: [11][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6817 (2.7779)	Acc@1 47.656 (42.963)	Acc@5 79.688 (75.577)
Epoch: [11][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7198 (2.7782)	Acc@1 39.062 (42.953)	Acc@5 79.688 (75.584)
Epoch: [11][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9961 (2.7785)	Acc@1 40.625 (42.964)	Acc@5 75.000 (75.608)
Epoch: [11][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8721 (2.7804)	Acc@1 42.188 (42.874)	Acc@5 76.562 (75.591)
Epoch: [11][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8836 (2.7812)	Acc@1 32.812 (42.825)	Acc@5 77.344 (75.552)
Epoch: [11][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8897 (2.7813)	Acc@1 44.531 (42.838)	Acc@5 74.219 (75.532)
Epoch: [11][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8468 (2.7819)	Acc@1 42.969 (42.869)	Acc@5 71.875 (75.521)
Epoch: [11][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0269 (2.7835)	Acc@1 39.844 (42.861)	Acc@5 67.188 (75.452)
Epoch: [11][370/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4930 (2.7824)	Acc@1 45.312 (42.868)	Acc@5 78.906 (75.432)
Epoch: [11][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7805 (2.7828)	Acc@1 44.531 (42.885)	Acc@5 69.531 (75.427)
Epoch: [11][390/391]	Time 0.057 (0.013)	Data 0.001 (0.002)	Loss 2.8441 (2.7818)	Acc@1 36.250 (42.940)	Acc@5 76.250 (75.476)
num momentum params: 26
[0.1, 2.7817924614715577, 2.2271045351028445, 42.94, 41.41, tensor(0.2382, device='cuda:0', grad_fn=<DivBackward0>), 5.232003211975098, 0.4094979763031006]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [12 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [12][0/391]	Time 0.025 (0.025)	Data 0.138 (0.138)	Loss 2.6076 (2.6076)	Acc@1 48.438 (48.438)	Acc@5 77.344 (77.344)
Epoch: [12][10/391]	Time 0.014 (0.014)	Data 0.001 (0.014)	Loss 2.7831 (2.6274)	Acc@1 46.094 (46.449)	Acc@5 74.219 (79.190)
Epoch: [12][20/391]	Time 0.012 (0.014)	Data 0.001 (0.008)	Loss 2.5839 (2.6538)	Acc@1 46.094 (45.759)	Acc@5 78.906 (77.976)
Epoch: [12][30/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.6934 (2.6597)	Acc@1 44.531 (45.640)	Acc@5 78.906 (78.276)
Epoch: [12][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.6305 (2.6654)	Acc@1 46.094 (45.446)	Acc@5 78.125 (77.973)
Epoch: [12][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.4123 (2.6734)	Acc@1 53.125 (45.711)	Acc@5 81.250 (77.788)
Epoch: [12][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.5831 (2.6825)	Acc@1 44.531 (45.300)	Acc@5 78.125 (77.741)
Epoch: [12][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6230 (2.6734)	Acc@1 50.781 (45.643)	Acc@5 78.125 (77.960)
Epoch: [12][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5240 (2.6755)	Acc@1 45.312 (45.496)	Acc@5 80.469 (77.971)
Epoch: [12][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6278 (2.6841)	Acc@1 44.531 (45.209)	Acc@5 81.250 (77.842)
Epoch: [12][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.9511 (2.6913)	Acc@1 40.625 (45.042)	Acc@5 74.219 (77.893)
Epoch: [12][110/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.7443 (2.6978)	Acc@1 44.531 (44.954)	Acc@5 79.688 (77.766)
Epoch: [12][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6413 (2.6989)	Acc@1 48.438 (44.977)	Acc@5 77.344 (77.634)
Epoch: [12][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8608 (2.7102)	Acc@1 38.281 (44.704)	Acc@5 78.906 (77.445)
Epoch: [12][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7735 (2.7134)	Acc@1 43.750 (44.609)	Acc@5 79.688 (77.388)
Epoch: [12][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7110 (2.7155)	Acc@1 42.969 (44.588)	Acc@5 79.688 (77.287)
Epoch: [12][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6916 (2.7155)	Acc@1 48.438 (44.667)	Acc@5 73.438 (77.218)
Epoch: [12][170/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7764 (2.7149)	Acc@1 43.750 (44.691)	Acc@5 72.656 (77.184)
Epoch: [12][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4444 (2.7107)	Acc@1 53.906 (44.842)	Acc@5 84.375 (77.249)
Epoch: [12][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8927 (2.7108)	Acc@1 33.594 (44.797)	Acc@5 75.781 (77.274)
Epoch: [12][200/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6040 (2.7120)	Acc@1 52.344 (44.834)	Acc@5 79.688 (77.165)
Epoch: [12][210/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7557 (2.7171)	Acc@1 46.094 (44.761)	Acc@5 71.875 (77.036)
Epoch: [12][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7650 (2.7192)	Acc@1 40.625 (44.634)	Acc@5 78.906 (76.983)
Epoch: [12][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7793 (2.7243)	Acc@1 44.531 (44.511)	Acc@5 71.875 (76.931)
Epoch: [12][240/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7582 (2.7286)	Acc@1 42.188 (44.457)	Acc@5 77.344 (76.870)
Epoch: [12][250/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.5638 (2.7255)	Acc@1 51.562 (44.572)	Acc@5 80.469 (76.908)
Epoch: [12][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5385 (2.7224)	Acc@1 50.000 (44.657)	Acc@5 78.125 (77.006)
Epoch: [12][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7519 (2.7259)	Acc@1 40.625 (44.546)	Acc@5 75.781 (76.952)
Epoch: [12][280/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 3.0662 (2.7298)	Acc@1 37.500 (44.409)	Acc@5 74.219 (76.893)
Epoch: [12][290/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7110 (2.7315)	Acc@1 48.438 (44.384)	Acc@5 77.344 (76.861)
Epoch: [12][300/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8529 (2.7321)	Acc@1 39.844 (44.388)	Acc@5 77.344 (76.858)
Epoch: [12][310/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7503 (2.7337)	Acc@1 48.438 (44.355)	Acc@5 74.219 (76.824)
Epoch: [12][320/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7704 (2.7373)	Acc@1 42.188 (44.281)	Acc@5 75.000 (76.791)
Epoch: [12][330/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7131 (2.7391)	Acc@1 44.531 (44.272)	Acc@5 77.344 (76.751)
Epoch: [12][340/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5244 (2.7382)	Acc@1 46.875 (44.325)	Acc@5 84.375 (76.794)
Epoch: [12][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5279 (2.7368)	Acc@1 43.750 (44.369)	Acc@5 82.031 (76.821)
Epoch: [12][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4902 (2.7367)	Acc@1 53.906 (44.384)	Acc@5 78.906 (76.822)
Epoch: [12][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6751 (2.7362)	Acc@1 42.188 (44.394)	Acc@5 82.031 (76.836)
Epoch: [12][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7389 (2.7390)	Acc@1 42.969 (44.304)	Acc@5 77.344 (76.794)
Epoch: [12][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.7896 (2.7409)	Acc@1 41.250 (44.244)	Acc@5 77.500 (76.776)
num momentum params: 26
[0.1, 2.7409270533752443, 2.64181765794754, 44.244, 34.32, tensor(0.2488, device='cuda:0', grad_fn=<DivBackward0>), 5.1105217933654785, 0.3731684684753418]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [13 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [13][0/391]	Time 0.030 (0.030)	Data 0.150 (0.150)	Loss 2.6495 (2.6495)	Acc@1 46.875 (46.875)	Acc@5 79.688 (79.688)
Epoch: [13][10/391]	Time 0.012 (0.014)	Data 0.001 (0.015)	Loss 2.8050 (2.6674)	Acc@1 41.406 (45.810)	Acc@5 75.000 (78.480)
Epoch: [13][20/391]	Time 0.013 (0.014)	Data 0.001 (0.008)	Loss 2.6560 (2.6268)	Acc@1 45.312 (47.173)	Acc@5 78.125 (79.241)
Epoch: [13][30/391]	Time 0.014 (0.014)	Data 0.001 (0.006)	Loss 2.8352 (2.6427)	Acc@1 44.531 (46.220)	Acc@5 74.219 (78.982)
Epoch: [13][40/391]	Time 0.013 (0.014)	Data 0.001 (0.005)	Loss 2.7222 (2.6388)	Acc@1 43.750 (46.551)	Acc@5 75.000 (79.040)
Epoch: [13][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.7635 (2.6494)	Acc@1 42.969 (46.507)	Acc@5 75.781 (78.569)
Epoch: [13][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.7606 (2.6603)	Acc@1 40.625 (46.196)	Acc@5 77.344 (78.394)
Epoch: [13][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7351 (2.6780)	Acc@1 46.875 (45.896)	Acc@5 75.781 (78.070)
Epoch: [13][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5574 (2.6828)	Acc@1 47.656 (45.737)	Acc@5 78.125 (77.884)
Epoch: [13][90/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.9921 (2.6919)	Acc@1 42.969 (45.664)	Acc@5 67.188 (77.704)
Epoch: [13][100/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.6112 (2.6884)	Acc@1 46.875 (45.707)	Acc@5 82.812 (77.947)
Epoch: [13][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4283 (2.6818)	Acc@1 50.781 (45.876)	Acc@5 83.594 (78.083)
Epoch: [13][120/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.8111 (2.6835)	Acc@1 45.312 (45.835)	Acc@5 75.000 (77.996)
Epoch: [13][130/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6579 (2.6894)	Acc@1 46.875 (45.730)	Acc@5 77.344 (77.910)
Epoch: [13][140/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7863 (2.6960)	Acc@1 43.750 (45.595)	Acc@5 76.562 (77.765)
Epoch: [13][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7442 (2.6983)	Acc@1 46.875 (45.530)	Acc@5 78.125 (77.665)
Epoch: [13][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7315 (2.6996)	Acc@1 45.312 (45.565)	Acc@5 79.688 (77.664)
Epoch: [13][170/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6745 (2.7028)	Acc@1 53.125 (45.500)	Acc@5 74.219 (77.604)
Epoch: [13][180/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 2.6838 (2.7049)	Acc@1 46.875 (45.433)	Acc@5 75.000 (77.573)
Epoch: [13][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7091 (2.7006)	Acc@1 41.406 (45.484)	Acc@5 76.562 (77.642)
Epoch: [13][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8560 (2.7029)	Acc@1 43.750 (45.491)	Acc@5 77.344 (77.631)
Epoch: [13][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6463 (2.7050)	Acc@1 46.094 (45.442)	Acc@5 79.688 (77.610)
Epoch: [13][220/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9905 (2.7092)	Acc@1 38.281 (45.383)	Acc@5 77.344 (77.584)
Epoch: [13][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9642 (2.7120)	Acc@1 35.156 (45.241)	Acc@5 73.438 (77.560)
Epoch: [13][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6358 (2.7132)	Acc@1 50.000 (45.222)	Acc@5 74.219 (77.503)
Epoch: [13][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9109 (2.7136)	Acc@1 38.281 (45.132)	Acc@5 76.562 (77.471)
Epoch: [13][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6481 (2.7141)	Acc@1 46.094 (45.175)	Acc@5 78.906 (77.449)
Epoch: [13][270/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6341 (2.7163)	Acc@1 46.094 (45.096)	Acc@5 77.344 (77.364)
Epoch: [13][280/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9995 (2.7161)	Acc@1 44.531 (45.146)	Acc@5 71.094 (77.372)
Epoch: [13][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7577 (2.7166)	Acc@1 48.438 (45.197)	Acc@5 76.562 (77.357)
Epoch: [13][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6918 (2.7193)	Acc@1 43.750 (45.159)	Acc@5 75.781 (77.318)
Epoch: [13][310/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.4673 (2.7170)	Acc@1 56.250 (45.250)	Acc@5 83.594 (77.409)
Epoch: [13][320/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7324 (2.7202)	Acc@1 47.656 (45.266)	Acc@5 78.125 (77.317)
Epoch: [13][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5193 (2.7227)	Acc@1 45.312 (45.180)	Acc@5 84.375 (77.287)
Epoch: [13][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6034 (2.7230)	Acc@1 47.656 (45.166)	Acc@5 78.906 (77.291)
Epoch: [13][350/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7650 (2.7228)	Acc@1 43.750 (45.175)	Acc@5 75.781 (77.279)
Epoch: [13][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6645 (2.7229)	Acc@1 45.312 (45.185)	Acc@5 78.125 (77.266)
Epoch: [13][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8544 (2.7218)	Acc@1 35.938 (45.188)	Acc@5 77.344 (77.308)
Epoch: [13][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8199 (2.7230)	Acc@1 42.969 (45.142)	Acc@5 80.469 (77.286)
Epoch: [13][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.6761 (2.7238)	Acc@1 50.000 (45.110)	Acc@5 76.250 (77.278)
num momentum params: 26
[0.1, 2.723786199798584, 2.373022447824478, 45.11, 39.46, tensor(0.2561, device='cuda:0', grad_fn=<DivBackward0>), 5.133336544036865, 0.3733632564544678]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [14 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [14][0/391]	Time 0.029 (0.029)	Data 0.155 (0.155)	Loss 2.4568 (2.4568)	Acc@1 52.344 (52.344)	Acc@5 78.906 (78.906)
Epoch: [14][10/391]	Time 0.013 (0.014)	Data 0.001 (0.015)	Loss 2.7796 (2.5912)	Acc@1 41.406 (48.295)	Acc@5 76.562 (78.835)
Epoch: [14][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.8066 (2.6222)	Acc@1 42.188 (47.470)	Acc@5 78.125 (78.832)
Epoch: [14][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.6691 (2.6461)	Acc@1 46.875 (47.051)	Acc@5 78.125 (78.453)
Epoch: [14][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.5586 (2.6442)	Acc@1 47.656 (46.970)	Acc@5 81.250 (78.754)
Epoch: [14][50/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.5382 (2.6492)	Acc@1 53.125 (47.304)	Acc@5 82.031 (78.523)
Epoch: [14][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6401 (2.6407)	Acc@1 49.219 (47.464)	Acc@5 81.250 (78.753)
Epoch: [14][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.7414 (2.6540)	Acc@1 43.750 (47.029)	Acc@5 78.906 (78.752)
Epoch: [14][80/391]	Time 0.014 (0.013)	Data 0.002 (0.003)	Loss 2.6698 (2.6579)	Acc@1 51.562 (47.068)	Acc@5 75.781 (78.646)
Epoch: [14][90/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.7341 (2.6646)	Acc@1 43.750 (46.961)	Acc@5 78.906 (78.374)
Epoch: [14][100/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.5849 (2.6647)	Acc@1 52.344 (46.875)	Acc@5 80.469 (78.442)
Epoch: [14][110/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5670 (2.6651)	Acc@1 49.219 (46.868)	Acc@5 78.906 (78.435)
Epoch: [14][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.8966 (2.6597)	Acc@1 42.188 (47.049)	Acc@5 73.438 (78.506)
Epoch: [14][130/391]	Time 0.015 (0.013)	Data 0.001 (0.003)	Loss 2.7727 (2.6614)	Acc@1 46.094 (46.935)	Acc@5 74.219 (78.560)
Epoch: [14][140/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.8514 (2.6651)	Acc@1 44.531 (46.864)	Acc@5 73.438 (78.491)
Epoch: [14][150/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7176 (2.6690)	Acc@1 47.656 (46.808)	Acc@5 79.688 (78.404)
Epoch: [14][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5927 (2.6672)	Acc@1 45.312 (46.788)	Acc@5 78.906 (78.392)
Epoch: [14][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8510 (2.6700)	Acc@1 46.094 (46.829)	Acc@5 76.562 (78.381)
Epoch: [14][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7147 (2.6737)	Acc@1 52.344 (46.836)	Acc@5 74.219 (78.289)
Epoch: [14][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8065 (2.6727)	Acc@1 45.312 (46.961)	Acc@5 74.219 (78.342)
Epoch: [14][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7192 (2.6722)	Acc@1 47.656 (46.972)	Acc@5 78.125 (78.331)
Epoch: [14][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5868 (2.6754)	Acc@1 48.438 (46.945)	Acc@5 76.562 (78.255)
Epoch: [14][220/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5625 (2.6731)	Acc@1 44.531 (46.935)	Acc@5 82.031 (78.390)
Epoch: [14][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7644 (2.6746)	Acc@1 41.406 (46.926)	Acc@5 76.562 (78.348)
Epoch: [14][240/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.4843 (2.6776)	Acc@1 51.562 (46.843)	Acc@5 81.250 (78.271)
Epoch: [14][250/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5058 (2.6784)	Acc@1 50.000 (46.831)	Acc@5 83.594 (78.221)
Epoch: [14][260/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.8075 (2.6780)	Acc@1 44.531 (46.887)	Acc@5 72.656 (78.215)
Epoch: [14][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4735 (2.6786)	Acc@1 55.469 (46.892)	Acc@5 79.688 (78.220)
Epoch: [14][280/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5504 (2.6811)	Acc@1 52.344 (46.831)	Acc@5 85.156 (78.186)
Epoch: [14][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8296 (2.6843)	Acc@1 42.969 (46.765)	Acc@5 75.000 (78.122)
Epoch: [14][300/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6221 (2.6848)	Acc@1 46.094 (46.750)	Acc@5 74.219 (78.091)
Epoch: [14][310/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7689 (2.6836)	Acc@1 45.312 (46.747)	Acc@5 78.906 (78.090)
Epoch: [14][320/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7925 (2.6827)	Acc@1 40.625 (46.753)	Acc@5 79.688 (78.118)
Epoch: [14][330/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4883 (2.6829)	Acc@1 51.562 (46.774)	Acc@5 76.562 (78.108)
Epoch: [14][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7075 (2.6842)	Acc@1 43.750 (46.721)	Acc@5 77.344 (78.070)
Epoch: [14][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7739 (2.6877)	Acc@1 45.312 (46.675)	Acc@5 76.562 (78.020)
Epoch: [14][360/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 3.0633 (2.6909)	Acc@1 36.719 (46.594)	Acc@5 67.969 (77.963)
Epoch: [14][370/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4932 (2.6921)	Acc@1 47.656 (46.534)	Acc@5 85.156 (77.978)
Epoch: [14][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7546 (2.6928)	Acc@1 46.094 (46.528)	Acc@5 77.344 (77.965)
Epoch: [14][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8354 (2.6918)	Acc@1 45.000 (46.588)	Acc@5 75.000 (77.996)
num momentum params: 26
[0.1, 2.6918133949279786, 2.2603272259235383, 46.588, 41.0, tensor(0.2644, device='cuda:0', grad_fn=<DivBackward0>), 5.102938175201416, 0.37749814987182617]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [15 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [15][0/391]	Time 0.029 (0.029)	Data 0.148 (0.148)	Loss 2.4998 (2.4998)	Acc@1 45.312 (45.312)	Acc@5 83.594 (83.594)
Epoch: [15][10/391]	Time 0.012 (0.014)	Data 0.002 (0.015)	Loss 2.6526 (2.5530)	Acc@1 47.656 (47.585)	Acc@5 82.812 (81.321)
Epoch: [15][20/391]	Time 0.013 (0.013)	Data 0.001 (0.008)	Loss 2.6849 (2.5497)	Acc@1 48.438 (48.847)	Acc@5 79.688 (81.101)
Epoch: [15][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.4732 (2.5476)	Acc@1 49.219 (48.690)	Acc@5 82.812 (80.948)
Epoch: [15][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.8151 (2.5633)	Acc@1 42.969 (48.666)	Acc@5 77.344 (80.412)
Epoch: [15][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.5504 (2.5781)	Acc@1 45.312 (48.315)	Acc@5 85.156 (80.331)
Epoch: [15][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6295 (2.5895)	Acc@1 46.875 (48.207)	Acc@5 84.375 (80.341)
Epoch: [15][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6905 (2.6045)	Acc@1 44.531 (47.887)	Acc@5 80.469 (80.095)
Epoch: [15][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5017 (2.6085)	Acc@1 51.562 (47.926)	Acc@5 81.250 (79.909)
Epoch: [15][90/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.6304 (2.6108)	Acc@1 50.781 (47.974)	Acc@5 78.906 (79.833)
Epoch: [15][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5848 (2.6277)	Acc@1 46.875 (47.703)	Acc@5 81.250 (79.579)
Epoch: [15][110/391]	Time 0.015 (0.013)	Data 0.001 (0.003)	Loss 2.6527 (2.6348)	Acc@1 45.312 (47.670)	Acc@5 79.688 (79.322)
Epoch: [15][120/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.4288 (2.6343)	Acc@1 53.906 (47.740)	Acc@5 82.812 (79.300)
Epoch: [15][130/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6684 (2.6387)	Acc@1 42.188 (47.829)	Acc@5 77.344 (79.115)
Epoch: [15][140/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7669 (2.6417)	Acc@1 46.094 (47.656)	Acc@5 78.906 (79.111)
Epoch: [15][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7237 (2.6422)	Acc@1 42.969 (47.646)	Acc@5 79.688 (79.160)
Epoch: [15][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7686 (2.6444)	Acc@1 42.188 (47.598)	Acc@5 77.344 (79.071)
Epoch: [15][170/391]	Time 0.017 (0.013)	Data 0.001 (0.002)	Loss 2.5311 (2.6437)	Acc@1 54.688 (47.597)	Acc@5 82.031 (79.121)
Epoch: [15][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8041 (2.6426)	Acc@1 49.219 (47.669)	Acc@5 75.781 (79.200)
Epoch: [15][190/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8925 (2.6454)	Acc@1 42.969 (47.558)	Acc@5 71.094 (79.209)
Epoch: [15][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7872 (2.6485)	Acc@1 46.875 (47.450)	Acc@5 75.781 (79.155)
Epoch: [15][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6930 (2.6512)	Acc@1 49.219 (47.456)	Acc@5 80.469 (79.099)
Epoch: [15][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5742 (2.6535)	Acc@1 46.094 (47.444)	Acc@5 84.375 (79.005)
Epoch: [15][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9163 (2.6575)	Acc@1 43.750 (47.382)	Acc@5 75.000 (78.984)
Epoch: [15][240/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7383 (2.6601)	Acc@1 43.750 (47.270)	Acc@5 79.688 (78.987)
Epoch: [15][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7761 (2.6640)	Acc@1 45.312 (47.192)	Acc@5 78.906 (78.922)
Epoch: [15][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7509 (2.6653)	Acc@1 44.531 (47.210)	Acc@5 78.125 (78.942)
Epoch: [15][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5151 (2.6643)	Acc@1 47.656 (47.238)	Acc@5 81.250 (78.935)
Epoch: [15][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4548 (2.6634)	Acc@1 53.906 (47.223)	Acc@5 85.156 (78.973)
Epoch: [15][290/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8412 (2.6639)	Acc@1 42.188 (47.286)	Acc@5 75.781 (78.936)
Epoch: [15][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5395 (2.6665)	Acc@1 55.469 (47.233)	Acc@5 82.031 (78.911)
Epoch: [15][310/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7987 (2.6678)	Acc@1 42.969 (47.156)	Acc@5 76.562 (78.894)
Epoch: [15][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6838 (2.6689)	Acc@1 42.969 (47.150)	Acc@5 82.812 (78.870)
Epoch: [15][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8483 (2.6689)	Acc@1 40.625 (47.170)	Acc@5 76.562 (78.887)
Epoch: [15][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6875 (2.6699)	Acc@1 47.656 (47.157)	Acc@5 75.000 (78.886)
Epoch: [15][350/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6121 (2.6680)	Acc@1 52.344 (47.213)	Acc@5 76.562 (78.900)
Epoch: [15][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4522 (2.6664)	Acc@1 51.562 (47.252)	Acc@5 82.031 (78.934)
Epoch: [15][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5848 (2.6672)	Acc@1 49.219 (47.239)	Acc@5 80.469 (78.936)
Epoch: [15][380/391]	Time 0.014 (0.013)	Data 0.002 (0.002)	Loss 2.7372 (2.6720)	Acc@1 49.219 (47.176)	Acc@5 75.000 (78.826)
Epoch: [15][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6794 (2.6726)	Acc@1 48.750 (47.148)	Acc@5 77.500 (78.812)
num momentum params: 26
[0.1, 2.6725554964447022, 2.2049174439907073, 47.148, 42.11, tensor(0.2706, device='cuda:0', grad_fn=<DivBackward0>), 5.082448482513428, 0.3780031204223633]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [16 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [16][0/391]	Time 0.028 (0.028)	Data 0.152 (0.152)	Loss 2.5495 (2.5495)	Acc@1 52.344 (52.344)	Acc@5 85.156 (85.156)
Epoch: [16][10/391]	Time 0.013 (0.014)	Data 0.001 (0.015)	Loss 2.6279 (2.6082)	Acc@1 51.562 (47.443)	Acc@5 85.938 (81.037)
Epoch: [16][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.7566 (2.6049)	Acc@1 46.094 (48.103)	Acc@5 78.906 (80.469)
Epoch: [16][30/391]	Time 0.013 (0.014)	Data 0.001 (0.006)	Loss 2.6546 (2.5882)	Acc@1 49.219 (48.765)	Acc@5 78.906 (80.444)
Epoch: [16][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.6131 (2.5928)	Acc@1 52.344 (48.990)	Acc@5 78.906 (80.069)
Epoch: [16][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.5762 (2.5994)	Acc@1 51.562 (48.836)	Acc@5 81.250 (80.224)
Epoch: [16][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.5138 (2.6034)	Acc@1 51.562 (48.860)	Acc@5 83.594 (80.136)
Epoch: [16][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.8094 (2.6139)	Acc@1 46.094 (48.614)	Acc@5 73.438 (79.853)
Epoch: [16][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 3.0931 (2.6318)	Acc@1 39.844 (47.955)	Acc@5 69.531 (79.581)
Epoch: [16][90/391]	Time 0.015 (0.013)	Data 0.001 (0.003)	Loss 2.7951 (2.6416)	Acc@1 47.656 (47.905)	Acc@5 77.344 (79.387)
Epoch: [16][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7711 (2.6365)	Acc@1 42.188 (48.043)	Acc@5 82.812 (79.541)
Epoch: [16][110/391]	Time 0.013 (0.013)	Data 0.002 (0.003)	Loss 2.7623 (2.6391)	Acc@1 44.531 (47.924)	Acc@5 76.562 (79.533)
Epoch: [16][120/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 2.8722 (2.6393)	Acc@1 42.969 (47.973)	Acc@5 80.469 (79.610)
Epoch: [16][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6005 (2.6391)	Acc@1 47.656 (48.020)	Acc@5 82.812 (79.580)
Epoch: [16][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5070 (2.6413)	Acc@1 54.688 (48.050)	Acc@5 79.688 (79.499)
Epoch: [16][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7359 (2.6420)	Acc@1 43.750 (47.967)	Acc@5 78.125 (79.496)
Epoch: [16][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4331 (2.6432)	Acc@1 52.344 (47.889)	Acc@5 84.375 (79.459)
Epoch: [16][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6190 (2.6443)	Acc@1 49.219 (47.830)	Acc@5 78.125 (79.395)
Epoch: [16][180/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7492 (2.6460)	Acc@1 44.531 (47.760)	Acc@5 76.562 (79.355)
Epoch: [16][190/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8347 (2.6451)	Acc@1 41.406 (47.775)	Acc@5 78.906 (79.344)
Epoch: [16][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4075 (2.6440)	Acc@1 54.688 (47.866)	Acc@5 85.938 (79.377)
Epoch: [16][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9039 (2.6443)	Acc@1 37.500 (47.812)	Acc@5 75.781 (79.391)
Epoch: [16][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8258 (2.6446)	Acc@1 44.531 (47.815)	Acc@5 71.094 (79.401)
Epoch: [16][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7822 (2.6469)	Acc@1 45.312 (47.778)	Acc@5 78.125 (79.397)
Epoch: [16][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6158 (2.6450)	Acc@1 49.219 (47.828)	Acc@5 79.688 (79.451)
Epoch: [16][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7872 (2.6407)	Acc@1 48.438 (47.946)	Acc@5 79.688 (79.535)
Epoch: [16][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6847 (2.6403)	Acc@1 46.094 (47.902)	Acc@5 77.344 (79.565)
Epoch: [16][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8732 (2.6438)	Acc@1 45.312 (47.852)	Acc@5 75.781 (79.483)
Epoch: [16][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8488 (2.6481)	Acc@1 46.875 (47.781)	Acc@5 74.219 (79.396)
Epoch: [16][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3506 (2.6485)	Acc@1 55.469 (47.812)	Acc@5 84.375 (79.398)
Epoch: [16][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6036 (2.6476)	Acc@1 48.438 (47.833)	Acc@5 80.469 (79.389)
Epoch: [16][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7061 (2.6493)	Acc@1 46.094 (47.772)	Acc@5 83.594 (79.401)
Epoch: [16][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6143 (2.6526)	Acc@1 46.094 (47.683)	Acc@5 80.469 (79.371)
Epoch: [16][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5719 (2.6514)	Acc@1 53.906 (47.751)	Acc@5 78.906 (79.331)
Epoch: [16][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7591 (2.6523)	Acc@1 46.094 (47.755)	Acc@5 75.781 (79.268)
Epoch: [16][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4778 (2.6536)	Acc@1 41.406 (47.665)	Acc@5 86.719 (79.262)
Epoch: [16][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5132 (2.6548)	Acc@1 53.906 (47.648)	Acc@5 82.031 (79.240)
Epoch: [16][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7421 (2.6554)	Acc@1 43.750 (47.635)	Acc@5 78.125 (79.243)
Epoch: [16][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5805 (2.6553)	Acc@1 46.875 (47.623)	Acc@5 81.250 (79.265)
Epoch: [16][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.6638 (2.6548)	Acc@1 52.500 (47.674)	Acc@5 77.500 (79.274)
num momentum params: 26
[0.1, 2.6548274966430663, 2.1437790489196775, 47.674, 43.79, tensor(0.2764, device='cuda:0', grad_fn=<DivBackward0>), 5.080712556838989, 0.3749051094055176]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [17 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [17][0/391]	Time 0.028 (0.028)	Data 0.138 (0.138)	Loss 2.7427 (2.7427)	Acc@1 50.781 (50.781)	Acc@5 77.344 (77.344)
Epoch: [17][10/391]	Time 0.014 (0.014)	Data 0.001 (0.014)	Loss 2.6871 (2.6318)	Acc@1 44.531 (47.869)	Acc@5 80.469 (80.540)
Epoch: [17][20/391]	Time 0.013 (0.014)	Data 0.001 (0.008)	Loss 2.3214 (2.5855)	Acc@1 58.594 (49.591)	Acc@5 83.594 (80.618)
Epoch: [17][30/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.5049 (2.5565)	Acc@1 46.875 (50.227)	Acc@5 84.375 (81.048)
Epoch: [17][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.4981 (2.5433)	Acc@1 51.562 (50.229)	Acc@5 82.812 (81.460)
Epoch: [17][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.8067 (2.5385)	Acc@1 44.531 (50.337)	Acc@5 80.469 (81.434)
Epoch: [17][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.7635 (2.5470)	Acc@1 45.312 (50.077)	Acc@5 78.906 (81.455)
Epoch: [17][70/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5798 (2.5713)	Acc@1 48.438 (49.626)	Acc@5 82.031 (81.030)
Epoch: [17][80/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.7043 (2.5713)	Acc@1 50.000 (49.720)	Acc@5 76.562 (80.941)
Epoch: [17][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4202 (2.5727)	Acc@1 54.688 (49.562)	Acc@5 80.469 (81.001)
Epoch: [17][100/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 2.8428 (2.5764)	Acc@1 40.625 (49.443)	Acc@5 81.250 (81.080)
Epoch: [17][110/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.8515 (2.5823)	Acc@1 41.406 (49.296)	Acc@5 74.219 (80.997)
Epoch: [17][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5849 (2.5866)	Acc@1 45.312 (49.219)	Acc@5 79.688 (80.817)
Epoch: [17][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4449 (2.5902)	Acc@1 50.000 (49.141)	Acc@5 85.156 (80.761)
Epoch: [17][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5588 (2.5898)	Acc@1 49.219 (49.252)	Acc@5 85.156 (80.840)
Epoch: [17][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8021 (2.5929)	Acc@1 47.656 (49.270)	Acc@5 75.781 (80.758)
Epoch: [17][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 3.0706 (2.6037)	Acc@1 33.594 (48.986)	Acc@5 73.438 (80.561)
Epoch: [17][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5053 (2.6052)	Acc@1 51.562 (49.022)	Acc@5 80.469 (80.514)
Epoch: [17][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6066 (2.6043)	Acc@1 43.750 (49.046)	Acc@5 81.250 (80.559)
Epoch: [17][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7675 (2.6154)	Acc@1 42.969 (48.765)	Acc@5 73.438 (80.297)
Epoch: [17][200/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4836 (2.6147)	Acc@1 53.906 (48.818)	Acc@5 82.031 (80.251)
Epoch: [17][210/391]	Time 0.019 (0.013)	Data 0.001 (0.002)	Loss 2.5802 (2.6150)	Acc@1 50.781 (48.826)	Acc@5 78.906 (80.191)
Epoch: [17][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6731 (2.6180)	Acc@1 42.969 (48.734)	Acc@5 77.344 (80.069)
Epoch: [17][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7236 (2.6203)	Acc@1 44.531 (48.667)	Acc@5 81.250 (80.039)
Epoch: [17][240/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7652 (2.6234)	Acc@1 44.531 (48.609)	Acc@5 80.469 (80.002)
Epoch: [17][250/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6408 (2.6261)	Acc@1 50.781 (48.546)	Acc@5 80.469 (79.937)
Epoch: [17][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7113 (2.6255)	Acc@1 41.406 (48.512)	Acc@5 76.562 (79.942)
Epoch: [17][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7858 (2.6264)	Acc@1 42.969 (48.536)	Acc@5 73.438 (79.918)
Epoch: [17][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5505 (2.6277)	Acc@1 52.344 (48.521)	Acc@5 80.469 (79.910)
Epoch: [17][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3632 (2.6267)	Acc@1 50.000 (48.515)	Acc@5 86.719 (79.959)
Epoch: [17][300/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5889 (2.6275)	Acc@1 47.656 (48.541)	Acc@5 80.469 (79.931)
Epoch: [17][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6604 (2.6260)	Acc@1 54.688 (48.553)	Acc@5 75.781 (79.936)
Epoch: [17][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5765 (2.6257)	Acc@1 50.000 (48.584)	Acc@5 80.469 (79.933)
Epoch: [17][330/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5486 (2.6276)	Acc@1 54.688 (48.551)	Acc@5 79.688 (79.907)
Epoch: [17][340/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5541 (2.6311)	Acc@1 45.312 (48.451)	Acc@5 80.469 (79.809)
Epoch: [17][350/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.5927 (2.6325)	Acc@1 50.781 (48.453)	Acc@5 83.594 (79.779)
Epoch: [17][360/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8533 (2.6333)	Acc@1 40.625 (48.444)	Acc@5 75.000 (79.755)
Epoch: [17][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5317 (2.6316)	Acc@1 53.125 (48.518)	Acc@5 77.344 (79.774)
Epoch: [17][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8113 (2.6335)	Acc@1 46.094 (48.483)	Acc@5 78.125 (79.765)
Epoch: [17][390/391]	Time 0.016 (0.013)	Data 0.000 (0.002)	Loss 2.7455 (2.6330)	Acc@1 53.750 (48.490)	Acc@5 72.500 (79.780)
num momentum params: 26
[0.1, 2.633042910232544, 2.4012286961078644, 48.49, 40.09, tensor(0.2820, device='cuda:0', grad_fn=<DivBackward0>), 5.11682915687561, 0.3756144046783447]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [18 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [18][0/391]	Time 0.031 (0.031)	Data 0.139 (0.139)	Loss 2.3391 (2.3391)	Acc@1 53.906 (53.906)	Acc@5 86.719 (86.719)
Epoch: [18][10/391]	Time 0.012 (0.015)	Data 0.001 (0.014)	Loss 2.4936 (2.5445)	Acc@1 55.469 (51.065)	Acc@5 78.125 (81.392)
Epoch: [18][20/391]	Time 0.013 (0.014)	Data 0.001 (0.008)	Loss 2.2191 (2.5060)	Acc@1 61.719 (51.600)	Acc@5 88.281 (82.106)
Epoch: [18][30/391]	Time 0.013 (0.014)	Data 0.001 (0.006)	Loss 2.5097 (2.5216)	Acc@1 53.125 (50.958)	Acc@5 82.031 (81.930)
Epoch: [18][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.6068 (2.5349)	Acc@1 49.219 (50.324)	Acc@5 82.812 (81.612)
Epoch: [18][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.4641 (2.5381)	Acc@1 53.125 (50.322)	Acc@5 83.594 (81.725)
Epoch: [18][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.5048 (2.5414)	Acc@1 45.312 (50.179)	Acc@5 84.375 (81.673)
Epoch: [18][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5774 (2.5458)	Acc@1 46.875 (50.165)	Acc@5 78.125 (81.624)
Epoch: [18][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7388 (2.5615)	Acc@1 45.312 (49.990)	Acc@5 80.469 (81.385)
Epoch: [18][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7338 (2.5609)	Acc@1 44.531 (49.931)	Acc@5 82.812 (81.413)
Epoch: [18][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7758 (2.5641)	Acc@1 48.438 (49.930)	Acc@5 77.344 (81.351)
Epoch: [18][110/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.6520 (2.5746)	Acc@1 45.312 (49.704)	Acc@5 83.594 (81.222)
Epoch: [18][120/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.6090 (2.5726)	Acc@1 50.000 (49.755)	Acc@5 79.688 (81.153)
Epoch: [18][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3547 (2.5808)	Acc@1 54.688 (49.529)	Acc@5 82.031 (80.886)
Epoch: [18][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6778 (2.5830)	Acc@1 50.000 (49.568)	Acc@5 82.031 (80.801)
Epoch: [18][150/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.4403 (2.5818)	Acc@1 54.688 (49.576)	Acc@5 81.250 (80.779)
Epoch: [18][160/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5957 (2.5830)	Acc@1 45.312 (49.505)	Acc@5 77.344 (80.697)
Epoch: [18][170/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8535 (2.5905)	Acc@1 46.094 (49.424)	Acc@5 71.094 (80.565)
Epoch: [18][180/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6139 (2.5914)	Acc@1 49.219 (49.413)	Acc@5 75.781 (80.525)
Epoch: [18][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3303 (2.5945)	Acc@1 53.125 (49.354)	Acc@5 85.156 (80.481)
Epoch: [18][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.2768 (2.5950)	Acc@1 52.344 (49.324)	Acc@5 87.500 (80.543)
Epoch: [18][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4505 (2.5963)	Acc@1 53.906 (49.282)	Acc@5 82.812 (80.495)
Epoch: [18][220/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7080 (2.5961)	Acc@1 47.656 (49.258)	Acc@5 77.344 (80.493)
Epoch: [18][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4614 (2.5970)	Acc@1 55.469 (49.226)	Acc@5 84.375 (80.499)
Epoch: [18][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5770 (2.5960)	Acc@1 45.312 (49.287)	Acc@5 87.500 (80.469)
Epoch: [18][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5372 (2.5962)	Acc@1 50.781 (49.278)	Acc@5 80.469 (80.494)
Epoch: [18][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6458 (2.5968)	Acc@1 42.969 (49.207)	Acc@5 84.375 (80.526)
Epoch: [18][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7212 (2.6002)	Acc@1 50.000 (49.124)	Acc@5 73.438 (80.408)
Epoch: [18][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4943 (2.6029)	Acc@1 47.656 (49.074)	Acc@5 84.375 (80.391)
Epoch: [18][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7126 (2.6066)	Acc@1 47.656 (48.991)	Acc@5 80.469 (80.321)
Epoch: [18][300/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6740 (2.6087)	Acc@1 46.094 (48.962)	Acc@5 78.125 (80.277)
Epoch: [18][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6825 (2.6094)	Acc@1 41.406 (48.942)	Acc@5 77.344 (80.240)
Epoch: [18][320/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9519 (2.6126)	Acc@1 41.406 (48.868)	Acc@5 75.781 (80.182)
Epoch: [18][330/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6054 (2.6136)	Acc@1 50.000 (48.855)	Acc@5 81.250 (80.152)
Epoch: [18][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4598 (2.6157)	Acc@1 51.562 (48.827)	Acc@5 83.594 (80.095)
Epoch: [18][350/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6277 (2.6174)	Acc@1 51.562 (48.803)	Acc@5 81.250 (80.044)
Epoch: [18][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.8376 (2.6213)	Acc@1 42.969 (48.706)	Acc@5 77.344 (79.977)
Epoch: [18][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5474 (2.6207)	Acc@1 50.781 (48.713)	Acc@5 82.031 (80.003)
Epoch: [18][380/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9040 (2.6200)	Acc@1 42.969 (48.677)	Acc@5 72.656 (79.993)
Epoch: [18][390/391]	Time 0.016 (0.013)	Data 0.001 (0.002)	Loss 2.9198 (2.6232)	Acc@1 37.500 (48.640)	Acc@5 77.500 (79.900)
num momentum params: 26
[0.1, 2.6232299617004395, 2.167961651086807, 48.64, 43.9, tensor(0.2853, device='cuda:0', grad_fn=<DivBackward0>), 5.111853122711182, 0.3876183032989502]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [19 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [19][0/391]	Time 0.029 (0.029)	Data 0.132 (0.132)	Loss 2.1575 (2.1575)	Acc@1 61.719 (61.719)	Acc@5 89.844 (89.844)
Epoch: [19][10/391]	Time 0.013 (0.015)	Data 0.001 (0.014)	Loss 2.6528 (2.5684)	Acc@1 43.750 (50.497)	Acc@5 78.125 (80.185)
Epoch: [19][20/391]	Time 0.013 (0.014)	Data 0.001 (0.008)	Loss 2.7607 (2.5367)	Acc@1 47.656 (50.372)	Acc@5 81.250 (81.585)
Epoch: [19][30/391]	Time 0.014 (0.014)	Data 0.001 (0.006)	Loss 2.5259 (2.5373)	Acc@1 50.781 (50.655)	Acc@5 76.562 (81.326)
Epoch: [19][40/391]	Time 0.013 (0.014)	Data 0.001 (0.005)	Loss 2.4466 (2.5305)	Acc@1 48.438 (50.724)	Acc@5 85.938 (81.879)
Epoch: [19][50/391]	Time 0.013 (0.013)	Data 0.002 (0.004)	Loss 2.5754 (2.5475)	Acc@1 49.219 (50.352)	Acc@5 76.562 (81.449)
Epoch: [19][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6465 (2.5688)	Acc@1 46.875 (49.885)	Acc@5 80.469 (81.186)
Epoch: [19][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6608 (2.5625)	Acc@1 46.094 (50.077)	Acc@5 78.906 (81.217)
Epoch: [19][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6913 (2.5670)	Acc@1 50.000 (50.039)	Acc@5 79.688 (80.980)
Epoch: [19][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.8064 (2.5766)	Acc@1 45.312 (49.837)	Acc@5 76.562 (80.864)
Epoch: [19][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6177 (2.5750)	Acc@1 50.000 (49.845)	Acc@5 75.781 (80.886)
Epoch: [19][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5886 (2.5762)	Acc@1 43.750 (49.754)	Acc@5 77.344 (80.968)
Epoch: [19][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4993 (2.5841)	Acc@1 54.688 (49.587)	Acc@5 83.594 (80.850)
Epoch: [19][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5832 (2.5817)	Acc@1 55.469 (49.630)	Acc@5 79.688 (80.886)
Epoch: [19][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4706 (2.5780)	Acc@1 49.219 (49.673)	Acc@5 85.938 (81.006)
Epoch: [19][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5434 (2.5816)	Acc@1 51.562 (49.633)	Acc@5 80.469 (80.831)
Epoch: [19][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6700 (2.5886)	Acc@1 48.438 (49.578)	Acc@5 78.906 (80.658)
Epoch: [19][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7562 (2.5878)	Acc@1 48.438 (49.680)	Acc@5 78.125 (80.647)
Epoch: [19][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7680 (2.5904)	Acc@1 41.406 (49.534)	Acc@5 81.250 (80.620)
Epoch: [19][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7442 (2.5891)	Acc@1 43.750 (49.538)	Acc@5 84.375 (80.710)
Epoch: [19][200/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4034 (2.5912)	Acc@1 57.031 (49.596)	Acc@5 85.156 (80.663)
Epoch: [19][210/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5704 (2.5968)	Acc@1 48.438 (49.515)	Acc@5 80.469 (80.591)
Epoch: [19][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5907 (2.5964)	Acc@1 50.000 (49.523)	Acc@5 78.906 (80.543)
Epoch: [19][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4879 (2.5984)	Acc@1 51.562 (49.479)	Acc@5 81.250 (80.462)
Epoch: [19][240/391]	Time 0.011 (0.013)	Data 0.007 (0.002)	Loss 2.4365 (2.6017)	Acc@1 54.688 (49.378)	Acc@5 82.031 (80.423)
Epoch: [19][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.2692 (2.5977)	Acc@1 57.031 (49.490)	Acc@5 87.500 (80.512)
Epoch: [19][260/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4588 (2.5982)	Acc@1 50.000 (49.491)	Acc@5 82.812 (80.472)
Epoch: [19][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8687 (2.5999)	Acc@1 40.625 (49.403)	Acc@5 80.469 (80.457)
Epoch: [19][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4237 (2.6004)	Acc@1 55.469 (49.349)	Acc@5 84.375 (80.483)
Epoch: [19][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5051 (2.6011)	Acc@1 53.125 (49.340)	Acc@5 81.250 (80.517)
Epoch: [19][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6721 (2.6036)	Acc@1 46.094 (49.245)	Acc@5 78.125 (80.528)
Epoch: [19][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5001 (2.6053)	Acc@1 42.969 (49.171)	Acc@5 85.156 (80.474)
Epoch: [19][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4709 (2.6042)	Acc@1 53.906 (49.228)	Acc@5 80.469 (80.452)
Epoch: [19][330/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 3.1365 (2.6047)	Acc@1 35.156 (49.228)	Acc@5 71.875 (80.459)
Epoch: [19][340/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4931 (2.6049)	Acc@1 51.562 (49.294)	Acc@5 82.031 (80.453)
Epoch: [19][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5919 (2.6044)	Acc@1 50.000 (49.341)	Acc@5 85.156 (80.475)
Epoch: [19][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5932 (2.6057)	Acc@1 52.344 (49.318)	Acc@5 77.344 (80.449)
Epoch: [19][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9002 (2.6107)	Acc@1 48.438 (49.229)	Acc@5 70.312 (80.355)
Epoch: [19][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5819 (2.6126)	Acc@1 44.531 (49.126)	Acc@5 80.469 (80.331)
Epoch: [19][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.4301 (2.6123)	Acc@1 52.500 (49.130)	Acc@5 85.000 (80.340)
num momentum params: 26
[0.1, 2.6122722100067137, 2.2123148000240325, 49.13, 42.57, tensor(0.2887, device='cuda:0', grad_fn=<DivBackward0>), 5.10607385635376, 0.3828902244567871]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [438, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [438]
Non Pruning Epoch - module.bn8.bias: [438]
Non Pruning Epoch - module.fc.weight: [100, 438]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [20 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [438, 512, 3, 3]
Epoch: [20][0/391]	Time 0.029 (0.029)	Data 0.143 (0.143)	Loss 2.5563 (2.5563)	Acc@1 47.656 (47.656)	Acc@5 80.469 (80.469)
Epoch: [20][10/391]	Time 0.013 (0.014)	Data 0.001 (0.014)	Loss 2.2257 (2.5035)	Acc@1 57.031 (51.776)	Acc@5 86.719 (81.747)
Epoch: [20][20/391]	Time 0.013 (0.014)	Data 0.001 (0.008)	Loss 2.4915 (2.4866)	Acc@1 53.125 (52.418)	Acc@5 78.906 (82.440)
Epoch: [20][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.3191 (2.4983)	Acc@1 55.469 (51.714)	Acc@5 89.062 (82.510)
Epoch: [20][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.4123 (2.4968)	Acc@1 56.250 (51.562)	Acc@5 82.812 (82.431)
Epoch: [20][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6814 (2.5041)	Acc@1 50.000 (51.455)	Acc@5 78.125 (82.200)
Epoch: [20][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6313 (2.5287)	Acc@1 53.125 (50.717)	Acc@5 78.906 (81.762)
Epoch: [20][70/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.3627 (2.5346)	Acc@1 60.156 (50.781)	Acc@5 83.594 (81.514)
Epoch: [20][80/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.4274 (2.5308)	Acc@1 53.906 (50.781)	Acc@5 82.031 (81.597)
Epoch: [20][90/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5645 (2.5399)	Acc@1 50.000 (50.644)	Acc@5 82.031 (81.319)
Epoch: [20][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4948 (2.5382)	Acc@1 50.781 (50.596)	Acc@5 81.250 (81.436)
Epoch: [20][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.3431 (2.5353)	Acc@1 50.781 (50.718)	Acc@5 83.594 (81.433)
Epoch: [20][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6067 (2.5443)	Acc@1 48.438 (50.581)	Acc@5 80.469 (81.334)
Epoch: [20][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7094 (2.5520)	Acc@1 49.219 (50.483)	Acc@5 78.906 (81.238)
Epoch: [20][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7788 (2.5630)	Acc@1 46.875 (50.150)	Acc@5 75.781 (81.062)
Epoch: [20][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5890 (2.5699)	Acc@1 47.656 (50.062)	Acc@5 79.688 (80.955)
Epoch: [20][160/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4046 (2.5708)	Acc@1 58.594 (50.039)	Acc@5 82.812 (80.944)
Epoch: [20][170/391]	Time 0.014 (0.013)	Data 0.002 (0.002)	Loss 2.6341 (2.5754)	Acc@1 50.781 (49.991)	Acc@5 78.906 (80.857)
Epoch: [20][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5370 (2.5741)	Acc@1 50.000 (49.961)	Acc@5 78.125 (80.879)
Epoch: [20][190/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5290 (2.5728)	Acc@1 53.125 (50.000)	Acc@5 78.125 (80.960)
Epoch: [20][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7094 (2.5718)	Acc@1 46.875 (50.008)	Acc@5 75.781 (80.982)
Epoch: [20][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4935 (2.5728)	Acc@1 53.906 (50.041)	Acc@5 82.812 (80.961)
Epoch: [20][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3702 (2.5766)	Acc@1 53.125 (49.951)	Acc@5 84.375 (80.935)
Epoch: [20][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6214 (2.5817)	Acc@1 50.000 (49.824)	Acc@5 77.344 (80.793)
Epoch: [20][240/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5889 (2.5815)	Acc@1 50.000 (49.896)	Acc@5 84.375 (80.819)
Epoch: [20][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3661 (2.5860)	Acc@1 58.594 (49.795)	Acc@5 81.250 (80.764)
Epoch: [20][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7704 (2.5876)	Acc@1 44.531 (49.758)	Acc@5 82.031 (80.723)
Epoch: [20][270/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6433 (2.5920)	Acc@1 50.781 (49.680)	Acc@5 79.688 (80.653)
Epoch: [20][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5939 (2.5926)	Acc@1 52.344 (49.664)	Acc@5 81.250 (80.630)
Epoch: [20][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4573 (2.5907)	Acc@1 57.031 (49.721)	Acc@5 83.594 (80.697)
Epoch: [20][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.9027 (2.5920)	Acc@1 44.531 (49.678)	Acc@5 76.562 (80.684)
Epoch: [20][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4848 (2.5925)	Acc@1 50.000 (49.711)	Acc@5 78.906 (80.645)
Epoch: [20][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3298 (2.5905)	Acc@1 59.375 (49.783)	Acc@5 82.031 (80.695)
Epoch: [20][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7268 (2.5920)	Acc@1 48.438 (49.759)	Acc@5 82.031 (80.672)
Epoch: [20][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5518 (2.5942)	Acc@1 50.000 (49.693)	Acc@5 81.250 (80.645)
Epoch: [20][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6413 (2.5960)	Acc@1 51.562 (49.688)	Acc@5 78.125 (80.600)
Epoch: [20][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4384 (2.5948)	Acc@1 57.031 (49.725)	Acc@5 85.938 (80.618)
Epoch: [20][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7149 (2.5947)	Acc@1 42.969 (49.745)	Acc@5 76.562 (80.620)
Epoch: [20][380/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5854 (2.5949)	Acc@1 50.781 (49.793)	Acc@5 82.812 (80.618)
Epoch: [20][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.9416 (2.5968)	Acc@1 37.500 (49.732)	Acc@5 75.000 (80.570)
num momentum params: 26
[0.1, 2.596798289718628, 2.2380236637592317, 49.732, 42.33, tensor(0.2924, device='cuda:0', grad_fn=<DivBackward0>), 5.103567123413086, 0.37243032455444336]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [438, 512, 3, 3]
Before - module.bn8.weight: [438]
Before - module.bn8.bias: [438]
Before - module.fc.weight: [100, 438]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [64, 3, 3, 3] >> [56, 3, 3, 3]
[module.bn1.weight]: 64 >> 56
running_mean [56]
running_var [56]
num_batches_tracked []
[module.conv2.weight]: [128, 64, 3, 3] >> [126, 56, 3, 3]
[module.bn2.weight]: 128 >> 126
running_mean [126]
running_var [126]
num_batches_tracked []
[module.conv3.weight]: [256, 128, 3, 3] >> [254, 126, 3, 3]
[module.bn3.weight]: 256 >> 254
running_mean [254]
running_var [254]
num_batches_tracked []
[module.conv4.weight]: [256, 256, 3, 3] >> [255, 254, 3, 3]
[module.bn4.weight]: 256 >> 255
running_mean [255]
running_var [255]
num_batches_tracked []
[module.conv5.weight]: [512, 256, 3, 3] >> [493, 255, 3, 3]
[module.bn5.weight]: 512 >> 493
running_mean [493]
running_var [493]
num_batches_tracked []
[module.conv6.weight]: [512, 512, 3, 3] >> [471, 493, 3, 3]
[module.bn6.weight]: 512 >> 471
running_mean [471]
running_var [471]
num_batches_tracked []
[module.conv7.weight]: [512, 512, 3, 3] >> [441, 471, 3, 3]
[module.bn7.weight]: 512 >> 441
running_mean [441]
running_var [441]
num_batches_tracked []
[module.conv8.weight]: [438, 512, 3, 3] >> [299, 441, 3, 3]
[module.bn8.weight]: 438 >> 299
running_mean [299]
running_var [299]
num_batches_tracked []
[module.fc.weight]: [100, 438] >> [100, 299]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [56, 3, 3, 3]
After - module.bn1.weight: [56]
After - module.bn1.bias: [56]
After - module.conv2.weight: [126, 56, 3, 3]
After - module.bn2.weight: [126]
After - module.bn2.bias: [126]
After - module.conv3.weight: [254, 126, 3, 3]
After - module.bn3.weight: [254]
After - module.bn3.bias: [254]
After - module.conv4.weight: [255, 254, 3, 3]
After - module.bn4.weight: [255]
After - module.bn4.bias: [255]
After - module.conv5.weight: [493, 255, 3, 3]
After - module.bn5.weight: [493]
After - module.bn5.bias: [493]
After - module.conv6.weight: [471, 493, 3, 3]
After - module.bn6.weight: [471]
After - module.bn6.bias: [471]
After - module.conv7.weight: [441, 471, 3, 3]
After - module.bn7.weight: [441]
After - module.bn7.bias: [441]
After - module.conv8.weight: [299, 441, 3, 3]
After - module.bn8.weight: [299]
After - module.bn8.bias: [299]
After - module.fc.weight: [100, 299]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [56, 3, 3, 3]
conv2 --> [126, 56, 3, 3]
conv3 --> [254, 126, 3, 3]
conv4 --> [255, 254, 3, 3]
conv5 --> [493, 255, 3, 3]
conv6 --> [471, 493, 3, 3]
conv7 --> [441, 471, 3, 3]
conv8 --> [299, 441, 3, 3]
fc --> [299, 100]
1, 620089344, 1548288, 56
2, 6795436032, 16257024, 126
3, 8406042624, 18434304, 254
4, 17012229120, 37307520, 255
5, 9848010240, 18102960, 493
6, 18189854208, 33437232, 471
7, 5742793728, 7477596, 441
8, 3645637632, 4746924, 299
fc, 11481600, 29900, 0
===================
FLOP REPORT: 27449833800000.0 56224000000.0 137341748 140560 2395 13.820587158203125
[INFO] Storing checkpoint...

Epoch: [21 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [21][0/391]	Time 0.368 (0.368)	Data 0.154 (0.154)	Loss 2.7312 (2.7312)	Acc@1 50.000 (50.000)	Acc@5 81.250 (81.250)
Epoch: [21][10/391]	Time 0.014 (0.046)	Data 0.001 (0.015)	Loss 2.5843 (2.5445)	Acc@1 50.000 (51.349)	Acc@5 79.688 (82.741)
Epoch: [21][20/391]	Time 0.014 (0.031)	Data 0.002 (0.009)	Loss 2.5997 (2.5237)	Acc@1 46.875 (51.488)	Acc@5 76.562 (82.478)
Epoch: [21][30/391]	Time 0.014 (0.025)	Data 0.001 (0.006)	Loss 2.5192 (2.5120)	Acc@1 51.562 (51.865)	Acc@5 79.688 (82.686)
Epoch: [21][40/391]	Time 0.013 (0.022)	Data 0.001 (0.005)	Loss 2.5242 (2.5164)	Acc@1 48.438 (51.715)	Acc@5 82.812 (82.755)
Epoch: [21][50/391]	Time 0.012 (0.020)	Data 0.001 (0.004)	Loss 2.6517 (2.5035)	Acc@1 51.562 (52.022)	Acc@5 77.344 (82.858)
Epoch: [21][60/391]	Time 0.013 (0.019)	Data 0.001 (0.004)	Loss 2.3875 (2.5103)	Acc@1 52.344 (51.921)	Acc@5 82.812 (82.556)
Epoch: [21][70/391]	Time 0.013 (0.018)	Data 0.002 (0.004)	Loss 2.5622 (2.5267)	Acc@1 48.438 (51.276)	Acc@5 78.125 (82.163)
Epoch: [21][80/391]	Time 0.013 (0.018)	Data 0.001 (0.003)	Loss 2.5418 (2.5410)	Acc@1 53.125 (50.829)	Acc@5 78.906 (81.935)
Epoch: [21][90/391]	Time 0.014 (0.017)	Data 0.001 (0.003)	Loss 2.4482 (2.5342)	Acc@1 50.000 (51.065)	Acc@5 86.719 (82.057)
Epoch: [21][100/391]	Time 0.012 (0.017)	Data 0.001 (0.003)	Loss 2.7335 (2.5423)	Acc@1 47.656 (50.851)	Acc@5 81.250 (81.938)
Epoch: [21][110/391]	Time 0.012 (0.016)	Data 0.001 (0.003)	Loss 2.6630 (2.5542)	Acc@1 47.656 (50.493)	Acc@5 78.906 (81.771)
Epoch: [21][120/391]	Time 0.013 (0.016)	Data 0.001 (0.003)	Loss 2.4712 (2.5587)	Acc@1 55.469 (50.529)	Acc@5 83.594 (81.612)
Epoch: [21][130/391]	Time 0.013 (0.016)	Data 0.001 (0.003)	Loss 2.6895 (2.5622)	Acc@1 45.312 (50.519)	Acc@5 78.125 (81.536)
Epoch: [21][140/391]	Time 0.013 (0.016)	Data 0.001 (0.002)	Loss 2.3997 (2.5601)	Acc@1 52.344 (50.471)	Acc@5 84.375 (81.555)
Epoch: [21][150/391]	Time 0.012 (0.015)	Data 0.001 (0.002)	Loss 2.7401 (2.5596)	Acc@1 46.875 (50.445)	Acc@5 74.219 (81.591)
Epoch: [21][160/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.4712 (2.5648)	Acc@1 53.125 (50.272)	Acc@5 82.031 (81.512)
Epoch: [21][170/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.6194 (2.5661)	Acc@1 47.656 (50.251)	Acc@5 80.469 (81.456)
Epoch: [21][180/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.3344 (2.5694)	Acc@1 57.812 (50.186)	Acc@5 84.375 (81.349)
Epoch: [21][190/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.4883 (2.5706)	Acc@1 49.219 (50.184)	Acc@5 82.812 (81.324)
Epoch: [21][200/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.5735 (2.5735)	Acc@1 44.531 (50.128)	Acc@5 82.812 (81.297)
Epoch: [21][210/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.5329 (2.5773)	Acc@1 49.219 (50.144)	Acc@5 83.594 (81.320)
Epoch: [21][220/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.7157 (2.5803)	Acc@1 46.875 (50.095)	Acc@5 77.344 (81.225)
Epoch: [21][230/391]	Time 0.013 (0.015)	Data 0.001 (0.002)	Loss 2.5716 (2.5816)	Acc@1 53.125 (50.051)	Acc@5 80.469 (81.247)
Epoch: [21][240/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.6481 (2.5827)	Acc@1 47.656 (50.010)	Acc@5 78.906 (81.253)
Epoch: [21][250/391]	Time 0.014 (0.014)	Data 0.001 (0.002)	Loss 2.6608 (2.5843)	Acc@1 52.344 (49.963)	Acc@5 78.906 (81.206)
Epoch: [21][260/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.6602 (2.5876)	Acc@1 46.094 (49.865)	Acc@5 80.469 (81.136)
Epoch: [21][270/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.8195 (2.5896)	Acc@1 44.531 (49.821)	Acc@5 80.469 (81.129)
Epoch: [21][280/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.5738 (2.5914)	Acc@1 50.000 (49.830)	Acc@5 83.594 (81.111)
Epoch: [21][290/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.9858 (2.5929)	Acc@1 36.719 (49.774)	Acc@5 75.781 (81.022)
Epoch: [21][300/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.8068 (2.5922)	Acc@1 43.750 (49.782)	Acc@5 78.906 (81.003)
Epoch: [21][310/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.6114 (2.5886)	Acc@1 45.312 (49.834)	Acc@5 80.469 (81.064)
Epoch: [21][320/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.5470 (2.5877)	Acc@1 50.000 (49.859)	Acc@5 83.594 (81.089)
Epoch: [21][330/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.7349 (2.5893)	Acc@1 46.875 (49.832)	Acc@5 73.438 (81.023)
Epoch: [21][340/391]	Time 0.013 (0.014)	Data 0.001 (0.002)	Loss 2.4016 (2.5918)	Acc@1 54.688 (49.755)	Acc@5 86.719 (81.007)
Epoch: [21][350/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.4525 (2.5894)	Acc@1 52.344 (49.795)	Acc@5 79.688 (81.039)
Epoch: [21][360/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.7004 (2.5898)	Acc@1 49.219 (49.792)	Acc@5 76.562 (81.021)
Epoch: [21][370/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.6656 (2.5921)	Acc@1 50.781 (49.724)	Acc@5 78.906 (80.987)
Epoch: [21][380/391]	Time 0.014 (0.014)	Data 0.002 (0.002)	Loss 2.5404 (2.5929)	Acc@1 53.906 (49.715)	Acc@5 82.031 (80.965)
Epoch: [21][390/391]	Time 0.242 (0.014)	Data 0.001 (0.002)	Loss 2.9244 (2.5926)	Acc@1 43.750 (49.730)	Acc@5 67.500 (80.936)
num momentum params: 26
[0.1, 2.592626275939941, 2.119240528345108, 49.73, 44.73, tensor(0.2946, device='cuda:0', grad_fn=<DivBackward0>), 5.622392892837524, 0.48459863662719727]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [22 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [22][0/391]	Time 0.033 (0.033)	Data 0.151 (0.151)	Loss 2.4412 (2.4412)	Acc@1 52.344 (52.344)	Acc@5 83.594 (83.594)
Epoch: [22][10/391]	Time 0.012 (0.015)	Data 0.001 (0.015)	Loss 2.5949 (2.5515)	Acc@1 49.219 (50.284)	Acc@5 84.375 (81.818)
Epoch: [22][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.6308 (2.5055)	Acc@1 47.656 (51.228)	Acc@5 81.250 (82.552)
Epoch: [22][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.4227 (2.4560)	Acc@1 62.500 (52.646)	Acc@5 80.469 (83.543)
Epoch: [22][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.5832 (2.4806)	Acc@1 55.469 (52.096)	Acc@5 78.906 (82.984)
Epoch: [22][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.4956 (2.5027)	Acc@1 52.344 (51.869)	Acc@5 84.375 (82.843)
Epoch: [22][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6500 (2.5169)	Acc@1 46.094 (51.370)	Acc@5 79.688 (82.697)
Epoch: [22][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6400 (2.5182)	Acc@1 49.219 (51.386)	Acc@5 78.125 (82.636)
Epoch: [22][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5896 (2.5234)	Acc@1 50.781 (51.264)	Acc@5 81.250 (82.465)
Epoch: [22][90/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.4521 (2.5270)	Acc@1 51.562 (51.236)	Acc@5 83.594 (82.246)
Epoch: [22][100/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.6340 (2.5278)	Acc@1 53.125 (51.191)	Acc@5 78.906 (82.279)
Epoch: [22][110/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.6041 (2.5299)	Acc@1 52.344 (51.133)	Acc@5 82.031 (82.193)
Epoch: [22][120/391]	Time 0.016 (0.013)	Data 0.001 (0.003)	Loss 2.6020 (2.5301)	Acc@1 49.219 (51.052)	Acc@5 78.125 (82.160)
Epoch: [22][130/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.7652 (2.5357)	Acc@1 44.531 (51.068)	Acc@5 75.000 (81.912)
Epoch: [22][140/391]	Time 0.014 (0.013)	Data 0.002 (0.002)	Loss 2.6880 (2.5408)	Acc@1 50.781 (51.014)	Acc@5 78.906 (81.782)
Epoch: [22][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5831 (2.5402)	Acc@1 50.781 (51.014)	Acc@5 82.031 (81.783)
Epoch: [22][160/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6124 (2.5408)	Acc@1 46.875 (50.995)	Acc@5 78.125 (81.750)
Epoch: [22][170/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6497 (2.5487)	Acc@1 46.094 (50.809)	Acc@5 78.906 (81.689)
Epoch: [22][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7108 (2.5524)	Acc@1 50.000 (50.708)	Acc@5 82.031 (81.673)
Epoch: [22][190/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6727 (2.5537)	Acc@1 46.875 (50.691)	Acc@5 79.688 (81.643)
Epoch: [22][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5314 (2.5565)	Acc@1 57.031 (50.599)	Acc@5 79.688 (81.608)
Epoch: [22][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3558 (2.5548)	Acc@1 52.344 (50.678)	Acc@5 89.062 (81.654)
Epoch: [22][220/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5385 (2.5595)	Acc@1 55.469 (50.594)	Acc@5 78.906 (81.582)
Epoch: [22][230/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4098 (2.5576)	Acc@1 50.000 (50.653)	Acc@5 85.156 (81.652)
Epoch: [22][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3991 (2.5576)	Acc@1 52.344 (50.678)	Acc@5 84.375 (81.649)
Epoch: [22][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4726 (2.5574)	Acc@1 57.031 (50.694)	Acc@5 80.469 (81.686)
Epoch: [22][260/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6647 (2.5581)	Acc@1 47.656 (50.673)	Acc@5 78.906 (81.663)
Epoch: [22][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.2841 (2.5580)	Acc@1 57.031 (50.677)	Acc@5 84.375 (81.651)
Epoch: [22][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4696 (2.5621)	Acc@1 53.906 (50.623)	Acc@5 82.031 (81.598)
Epoch: [22][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5564 (2.5631)	Acc@1 53.906 (50.609)	Acc@5 81.250 (81.545)
Epoch: [22][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4923 (2.5646)	Acc@1 46.875 (50.618)	Acc@5 85.938 (81.523)
Epoch: [22][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6283 (2.5654)	Acc@1 53.906 (50.653)	Acc@5 81.250 (81.459)
Epoch: [22][320/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4065 (2.5661)	Acc@1 55.469 (50.655)	Acc@5 86.719 (81.454)
Epoch: [22][330/391]	Time 0.021 (0.013)	Data 0.002 (0.002)	Loss 2.4273 (2.5664)	Acc@1 51.562 (50.618)	Acc@5 84.375 (81.453)
Epoch: [22][340/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8777 (2.5657)	Acc@1 43.750 (50.664)	Acc@5 73.438 (81.442)
Epoch: [22][350/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.5692 (2.5693)	Acc@1 51.562 (50.599)	Acc@5 83.594 (81.364)
Epoch: [22][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7872 (2.5729)	Acc@1 46.875 (50.502)	Acc@5 78.125 (81.343)
Epoch: [22][370/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5629 (2.5735)	Acc@1 51.562 (50.491)	Acc@5 82.031 (81.332)
Epoch: [22][380/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7730 (2.5756)	Acc@1 43.750 (50.445)	Acc@5 78.125 (81.285)
Epoch: [22][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.8582 (2.5794)	Acc@1 47.500 (50.394)	Acc@5 78.750 (81.222)
num momentum params: 26
[0.1, 2.5794354247283935, 2.198749839067459, 50.394, 42.84, tensor(0.2979, device='cuda:0', grad_fn=<DivBackward0>), 5.082038402557373, 0.37668442726135254]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [23 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [23][0/391]	Time 0.035 (0.035)	Data 0.150 (0.150)	Loss 2.4425 (2.4425)	Acc@1 54.688 (54.688)	Acc@5 82.031 (82.031)
Epoch: [23][10/391]	Time 0.013 (0.015)	Data 0.001 (0.015)	Loss 2.5793 (2.5515)	Acc@1 49.219 (51.491)	Acc@5 82.812 (82.599)
Epoch: [23][20/391]	Time 0.012 (0.014)	Data 0.001 (0.008)	Loss 2.3211 (2.5045)	Acc@1 59.375 (52.083)	Acc@5 87.500 (83.073)
Epoch: [23][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.3682 (2.5011)	Acc@1 52.344 (51.764)	Acc@5 87.500 (83.543)
Epoch: [23][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.4766 (2.4724)	Acc@1 53.906 (52.611)	Acc@5 83.594 (83.880)
Epoch: [23][50/391]	Time 0.012 (0.013)	Data 0.002 (0.004)	Loss 2.5263 (2.4909)	Acc@1 51.562 (52.359)	Acc@5 82.031 (83.609)
Epoch: [23][60/391]	Time 0.014 (0.013)	Data 0.002 (0.004)	Loss 2.4082 (2.4924)	Acc@1 56.250 (52.177)	Acc@5 84.375 (83.594)
Epoch: [23][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5712 (2.4944)	Acc@1 51.562 (52.267)	Acc@5 82.031 (83.374)
Epoch: [23][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5191 (2.4936)	Acc@1 51.562 (52.344)	Acc@5 85.938 (83.362)
Epoch: [23][90/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5017 (2.4909)	Acc@1 49.219 (52.515)	Acc@5 82.812 (83.302)
Epoch: [23][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5673 (2.4923)	Acc@1 51.562 (52.529)	Acc@5 83.594 (83.215)
Epoch: [23][110/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.6622 (2.4883)	Acc@1 50.000 (52.752)	Acc@5 80.469 (83.284)
Epoch: [23][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5684 (2.4898)	Acc@1 48.438 (52.576)	Acc@5 82.031 (83.226)
Epoch: [23][130/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5211 (2.4967)	Acc@1 49.219 (52.314)	Acc@5 82.812 (83.051)
Epoch: [23][140/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7291 (2.5060)	Acc@1 50.781 (52.189)	Acc@5 79.688 (82.846)
Epoch: [23][150/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6978 (2.5145)	Acc@1 48.438 (52.106)	Acc@5 81.250 (82.719)
Epoch: [23][160/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.2470 (2.5153)	Acc@1 56.250 (52.009)	Acc@5 84.375 (82.623)
Epoch: [23][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4550 (2.5182)	Acc@1 52.344 (51.923)	Acc@5 86.719 (82.593)
Epoch: [23][180/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4682 (2.5184)	Acc@1 48.438 (51.839)	Acc@5 82.031 (82.579)
Epoch: [23][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7640 (2.5181)	Acc@1 51.562 (51.980)	Acc@5 78.906 (82.522)
Epoch: [23][200/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4409 (2.5205)	Acc@1 56.250 (51.947)	Acc@5 81.250 (82.455)
Epoch: [23][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8353 (2.5295)	Acc@1 40.625 (51.722)	Acc@5 81.250 (82.313)
Epoch: [23][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6679 (2.5319)	Acc@1 52.344 (51.598)	Acc@5 78.125 (82.296)
Epoch: [23][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3259 (2.5349)	Acc@1 59.375 (51.552)	Acc@5 85.938 (82.197)
Epoch: [23][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5935 (2.5351)	Acc@1 52.344 (51.585)	Acc@5 78.125 (82.203)
Epoch: [23][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4205 (2.5345)	Acc@1 55.469 (51.640)	Acc@5 83.594 (82.137)
Epoch: [23][260/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7704 (2.5370)	Acc@1 44.531 (51.527)	Acc@5 82.812 (82.136)
Epoch: [23][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6260 (2.5377)	Acc@1 48.438 (51.496)	Acc@5 80.469 (82.129)
Epoch: [23][280/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4572 (2.5352)	Acc@1 52.344 (51.546)	Acc@5 82.812 (82.176)
Epoch: [23][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6906 (2.5381)	Acc@1 46.875 (51.447)	Acc@5 82.031 (82.165)
Epoch: [23][300/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.9339 (2.5402)	Acc@1 44.531 (51.396)	Acc@5 75.781 (82.109)
Epoch: [23][310/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4065 (2.5426)	Acc@1 56.250 (51.377)	Acc@5 83.594 (82.044)
Epoch: [23][320/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.4974 (2.5438)	Acc@1 51.562 (51.368)	Acc@5 83.594 (82.041)
Epoch: [23][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8445 (2.5438)	Acc@1 45.312 (51.376)	Acc@5 75.781 (82.048)
Epoch: [23][340/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7353 (2.5475)	Acc@1 46.875 (51.267)	Acc@5 74.219 (81.956)
Epoch: [23][350/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7104 (2.5496)	Acc@1 48.438 (51.202)	Acc@5 77.344 (81.920)
Epoch: [23][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6006 (2.5529)	Acc@1 47.656 (51.132)	Acc@5 79.688 (81.858)
Epoch: [23][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8225 (2.5553)	Acc@1 43.750 (51.097)	Acc@5 77.344 (81.814)
Epoch: [23][380/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5865 (2.5573)	Acc@1 52.344 (51.091)	Acc@5 82.031 (81.779)
Epoch: [23][390/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5325 (2.5594)	Acc@1 51.250 (51.020)	Acc@5 80.000 (81.746)
num momentum params: 26
[0.1, 2.559432122039795, 2.0912534201145174, 51.02, 46.62, tensor(0.3011, device='cuda:0', grad_fn=<DivBackward0>), 4.997123718261719, 0.3937695026397705]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [24 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [24][0/391]	Time 0.039 (0.039)	Data 0.150 (0.150)	Loss 2.3347 (2.3347)	Acc@1 55.469 (55.469)	Acc@5 87.500 (87.500)
Epoch: [24][10/391]	Time 0.012 (0.015)	Data 0.002 (0.015)	Loss 2.5936 (2.5539)	Acc@1 50.000 (50.710)	Acc@5 82.031 (82.315)
Epoch: [24][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.4978 (2.4779)	Acc@1 50.000 (52.493)	Acc@5 86.719 (82.961)
Epoch: [24][30/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.5015 (2.4955)	Acc@1 56.250 (51.941)	Acc@5 82.031 (82.913)
Epoch: [24][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.4579 (2.4978)	Acc@1 50.781 (52.096)	Acc@5 82.031 (82.546)
Epoch: [24][50/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.3550 (2.4966)	Acc@1 55.469 (52.298)	Acc@5 86.719 (82.675)
Epoch: [24][60/391]	Time 0.014 (0.013)	Data 0.001 (0.004)	Loss 2.5337 (2.4889)	Acc@1 53.125 (52.280)	Acc@5 83.594 (82.941)
Epoch: [24][70/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4192 (2.4951)	Acc@1 53.125 (52.135)	Acc@5 85.156 (82.757)
Epoch: [24][80/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.4576 (2.5057)	Acc@1 53.906 (52.016)	Acc@5 85.938 (82.687)
Epoch: [24][90/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 2.4659 (2.5159)	Acc@1 58.594 (51.837)	Acc@5 82.031 (82.383)
Epoch: [24][100/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.2925 (2.5182)	Acc@1 57.031 (51.671)	Acc@5 82.812 (82.356)
Epoch: [24][110/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 2.6254 (2.5260)	Acc@1 51.562 (51.527)	Acc@5 76.562 (82.193)
Epoch: [24][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6053 (2.5232)	Acc@1 49.219 (51.556)	Acc@5 85.156 (82.231)
Epoch: [24][130/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.3973 (2.5230)	Acc@1 54.688 (51.551)	Acc@5 82.812 (82.228)
Epoch: [24][140/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4917 (2.5240)	Acc@1 53.906 (51.540)	Acc@5 84.375 (82.148)
Epoch: [24][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7245 (2.5303)	Acc@1 49.219 (51.293)	Acc@5 81.250 (82.130)
Epoch: [24][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5782 (2.5308)	Acc@1 50.781 (51.262)	Acc@5 81.250 (82.094)
Epoch: [24][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4607 (2.5370)	Acc@1 54.688 (51.106)	Acc@5 82.031 (82.004)
Epoch: [24][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4126 (2.5381)	Acc@1 56.250 (51.135)	Acc@5 82.812 (81.971)
Epoch: [24][190/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.2889 (2.5378)	Acc@1 57.031 (51.096)	Acc@5 82.812 (81.994)
Epoch: [24][200/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4241 (2.5368)	Acc@1 53.125 (51.158)	Acc@5 88.281 (82.000)
Epoch: [24][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7503 (2.5402)	Acc@1 47.656 (51.092)	Acc@5 78.125 (81.972)
Epoch: [24][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4216 (2.5441)	Acc@1 55.469 (51.057)	Acc@5 82.812 (81.851)
Epoch: [24][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4559 (2.5452)	Acc@1 54.688 (51.089)	Acc@5 82.812 (81.822)
Epoch: [24][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5514 (2.5464)	Acc@1 52.344 (51.050)	Acc@5 78.906 (81.808)
Epoch: [24][250/391]	Time 0.018 (0.013)	Data 0.001 (0.002)	Loss 2.4550 (2.5483)	Acc@1 52.344 (51.074)	Acc@5 78.906 (81.754)
Epoch: [24][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4409 (2.5514)	Acc@1 55.469 (50.976)	Acc@5 84.375 (81.705)
Epoch: [24][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4975 (2.5503)	Acc@1 52.344 (50.974)	Acc@5 82.031 (81.711)
Epoch: [24][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8741 (2.5544)	Acc@1 43.750 (50.884)	Acc@5 78.125 (81.614)
Epoch: [24][290/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4540 (2.5552)	Acc@1 53.125 (50.843)	Acc@5 85.156 (81.602)
Epoch: [24][300/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 2.5758 (2.5558)	Acc@1 46.094 (50.844)	Acc@5 82.812 (81.613)
Epoch: [24][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4119 (2.5574)	Acc@1 58.594 (50.816)	Acc@5 82.812 (81.567)
Epoch: [24][320/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4258 (2.5620)	Acc@1 52.344 (50.728)	Acc@5 82.812 (81.479)
Epoch: [24][330/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.3801 (2.5615)	Acc@1 52.344 (50.725)	Acc@5 83.594 (81.481)
Epoch: [24][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4610 (2.5609)	Acc@1 53.125 (50.747)	Acc@5 85.156 (81.461)
Epoch: [24][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6326 (2.5593)	Acc@1 48.438 (50.810)	Acc@5 82.812 (81.488)
Epoch: [24][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5659 (2.5587)	Acc@1 50.000 (50.814)	Acc@5 80.469 (81.512)
Epoch: [24][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8874 (2.5606)	Acc@1 43.750 (50.807)	Acc@5 71.875 (81.444)
Epoch: [24][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4803 (2.5607)	Acc@1 46.094 (50.783)	Acc@5 83.594 (81.443)
Epoch: [24][390/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5428 (2.5611)	Acc@1 45.000 (50.798)	Acc@5 82.500 (81.438)
num momentum params: 26
[0.1, 2.561134903945923, 2.0710440468788147, 50.798, 45.16, tensor(0.3017, device='cuda:0', grad_fn=<DivBackward0>), 5.005487680435181, 0.3852119445800781]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [25 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [25][0/391]	Time 0.034 (0.034)	Data 0.164 (0.164)	Loss 2.3994 (2.3994)	Acc@1 57.031 (57.031)	Acc@5 83.594 (83.594)
Epoch: [25][10/391]	Time 0.013 (0.015)	Data 0.001 (0.016)	Loss 2.3953 (2.3707)	Acc@1 56.250 (57.031)	Acc@5 85.938 (84.304)
Epoch: [25][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.1907 (2.4393)	Acc@1 60.156 (54.241)	Acc@5 86.719 (83.371)
Epoch: [25][30/391]	Time 0.012 (0.014)	Data 0.001 (0.007)	Loss 2.6232 (2.4592)	Acc@1 50.000 (53.755)	Acc@5 85.156 (83.493)
Epoch: [25][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.2331 (2.4487)	Acc@1 53.906 (53.411)	Acc@5 87.500 (83.784)
Epoch: [25][50/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.4125 (2.4612)	Acc@1 53.906 (53.202)	Acc@5 85.156 (83.410)
Epoch: [25][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.5797 (2.4781)	Acc@1 52.344 (52.920)	Acc@5 82.031 (83.094)
Epoch: [25][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.4397 (2.4806)	Acc@1 52.344 (52.729)	Acc@5 84.375 (83.000)
Epoch: [25][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5150 (2.4969)	Acc@1 55.469 (52.517)	Acc@5 85.156 (82.610)
Epoch: [25][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4172 (2.5032)	Acc@1 53.125 (52.266)	Acc@5 83.594 (82.435)
Epoch: [25][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5443 (2.5072)	Acc@1 54.688 (52.228)	Acc@5 83.594 (82.348)
Epoch: [25][110/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.5371 (2.5083)	Acc@1 51.562 (52.245)	Acc@5 81.250 (82.278)
Epoch: [25][120/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5018 (2.5058)	Acc@1 54.688 (52.357)	Acc@5 81.250 (82.283)
Epoch: [25][130/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.7127 (2.5151)	Acc@1 52.344 (52.171)	Acc@5 77.344 (82.228)
Epoch: [25][140/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6387 (2.5225)	Acc@1 50.000 (51.995)	Acc@5 78.906 (82.064)
Epoch: [25][150/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5561 (2.5227)	Acc@1 55.469 (52.064)	Acc@5 75.000 (82.011)
Epoch: [25][160/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5976 (2.5284)	Acc@1 47.656 (51.868)	Acc@5 82.812 (81.954)
Epoch: [25][170/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5089 (2.5325)	Acc@1 49.219 (51.754)	Acc@5 84.375 (81.935)
Epoch: [25][180/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6309 (2.5342)	Acc@1 51.562 (51.830)	Acc@5 78.906 (81.824)
Epoch: [25][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7030 (2.5342)	Acc@1 51.562 (51.857)	Acc@5 75.000 (81.819)
Epoch: [25][200/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7040 (2.5364)	Acc@1 45.312 (51.803)	Acc@5 80.469 (81.779)
Epoch: [25][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5387 (2.5405)	Acc@1 53.125 (51.692)	Acc@5 80.469 (81.668)
Epoch: [25][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4266 (2.5410)	Acc@1 56.250 (51.704)	Acc@5 83.594 (81.681)
Epoch: [25][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5161 (2.5418)	Acc@1 46.875 (51.627)	Acc@5 82.031 (81.707)
Epoch: [25][240/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.7069 (2.5407)	Acc@1 46.094 (51.650)	Acc@5 79.688 (81.704)
Epoch: [25][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4016 (2.5385)	Acc@1 49.219 (51.709)	Acc@5 85.156 (81.701)
Epoch: [25][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4441 (2.5399)	Acc@1 56.250 (51.703)	Acc@5 83.594 (81.669)
Epoch: [25][270/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6425 (2.5406)	Acc@1 43.750 (51.704)	Acc@5 81.250 (81.677)
Epoch: [25][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7134 (2.5419)	Acc@1 49.219 (51.704)	Acc@5 78.906 (81.700)
Epoch: [25][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5882 (2.5414)	Acc@1 50.781 (51.691)	Acc@5 80.469 (81.731)
Epoch: [25][300/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.5709 (2.5427)	Acc@1 52.344 (51.705)	Acc@5 84.375 (81.725)
Epoch: [25][310/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6867 (2.5439)	Acc@1 45.312 (51.663)	Acc@5 82.812 (81.705)
Epoch: [25][320/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7224 (2.5485)	Acc@1 50.000 (51.558)	Acc@5 78.125 (81.639)
Epoch: [25][330/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 3.0010 (2.5501)	Acc@1 45.312 (51.532)	Acc@5 74.219 (81.637)
Epoch: [25][340/391]	Time 0.010 (0.013)	Data 0.003 (0.002)	Loss 2.6621 (2.5521)	Acc@1 50.781 (51.494)	Acc@5 78.125 (81.610)
Epoch: [25][350/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5251 (2.5521)	Acc@1 50.000 (51.445)	Acc@5 79.688 (81.644)
Epoch: [25][360/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.5856 (2.5515)	Acc@1 53.906 (51.456)	Acc@5 82.812 (81.659)
Epoch: [25][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4370 (2.5498)	Acc@1 48.438 (51.428)	Acc@5 84.375 (81.711)
Epoch: [25][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4974 (2.5499)	Acc@1 50.781 (51.439)	Acc@5 81.250 (81.691)
Epoch: [25][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.4974 (2.5500)	Acc@1 52.500 (51.428)	Acc@5 83.750 (81.692)
num momentum params: 26
[0.1, 2.5499849477386474, 2.2066632103919983, 51.428, 43.49, tensor(0.3047, device='cuda:0', grad_fn=<DivBackward0>), 5.010894060134888, 0.37767672538757324]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [26 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [26][0/391]	Time 0.036 (0.036)	Data 0.165 (0.165)	Loss 2.2819 (2.2819)	Acc@1 57.812 (57.812)	Acc@5 85.156 (85.156)
Epoch: [26][10/391]	Time 0.013 (0.015)	Data 0.001 (0.016)	Loss 2.5846 (2.4767)	Acc@1 50.781 (51.491)	Acc@5 80.469 (83.736)
Epoch: [26][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.5334 (2.4729)	Acc@1 52.344 (52.418)	Acc@5 84.375 (83.743)
Epoch: [26][30/391]	Time 0.013 (0.014)	Data 0.001 (0.007)	Loss 2.9436 (2.4809)	Acc@1 46.094 (52.495)	Acc@5 75.781 (83.493)
Epoch: [26][40/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 2.3676 (2.4742)	Acc@1 57.031 (52.668)	Acc@5 83.594 (83.346)
Epoch: [26][50/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.5637 (2.4789)	Acc@1 49.219 (52.849)	Acc@5 82.031 (83.471)
Epoch: [26][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.4329 (2.4917)	Acc@1 53.125 (52.702)	Acc@5 85.156 (82.953)
Epoch: [26][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.5618 (2.4899)	Acc@1 49.219 (52.762)	Acc@5 85.156 (82.967)
Epoch: [26][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.3153 (2.4841)	Acc@1 59.375 (52.826)	Acc@5 85.156 (82.967)
Epoch: [26][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.2829 (2.4855)	Acc@1 57.031 (52.601)	Acc@5 85.938 (82.916)
Epoch: [26][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.2942 (2.4879)	Acc@1 58.594 (52.614)	Acc@5 90.625 (82.952)
Epoch: [26][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5806 (2.4900)	Acc@1 44.531 (52.541)	Acc@5 80.469 (82.862)
Epoch: [26][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4729 (2.4921)	Acc@1 51.562 (52.563)	Acc@5 88.281 (82.903)
Epoch: [26][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6891 (2.4990)	Acc@1 49.219 (52.403)	Acc@5 81.250 (82.699)
Epoch: [26][140/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.8444 (2.5046)	Acc@1 43.750 (52.261)	Acc@5 77.344 (82.619)
Epoch: [26][150/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5009 (2.5153)	Acc@1 49.219 (52.044)	Acc@5 82.031 (82.409)
Epoch: [26][160/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 2.7801 (2.5240)	Acc@1 49.219 (51.926)	Acc@5 78.125 (82.274)
Epoch: [26][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3829 (2.5303)	Acc@1 54.688 (51.709)	Acc@5 85.938 (82.232)
Epoch: [26][180/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.4307 (2.5304)	Acc@1 55.469 (51.748)	Acc@5 82.031 (82.243)
Epoch: [26][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4971 (2.5331)	Acc@1 53.906 (51.685)	Acc@5 82.031 (82.191)
Epoch: [26][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5338 (2.5301)	Acc@1 53.906 (51.765)	Acc@5 83.594 (82.264)
Epoch: [26][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4763 (2.5282)	Acc@1 51.562 (51.836)	Acc@5 81.250 (82.279)
Epoch: [26][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8324 (2.5338)	Acc@1 41.406 (51.718)	Acc@5 80.469 (82.233)
Epoch: [26][230/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5503 (2.5366)	Acc@1 50.781 (51.742)	Acc@5 79.688 (82.177)
Epoch: [26][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6775 (2.5366)	Acc@1 54.688 (51.789)	Acc@5 78.125 (82.193)
Epoch: [26][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4797 (2.5362)	Acc@1 56.250 (51.827)	Acc@5 85.156 (82.221)
Epoch: [26][260/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 2.6236 (2.5407)	Acc@1 46.094 (51.727)	Acc@5 78.125 (82.124)
Epoch: [26][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4854 (2.5397)	Acc@1 53.125 (51.759)	Acc@5 81.250 (82.126)
Epoch: [26][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6267 (2.5402)	Acc@1 50.781 (51.765)	Acc@5 79.688 (82.084)
Epoch: [26][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5570 (2.5416)	Acc@1 50.000 (51.764)	Acc@5 85.156 (82.053)
Epoch: [26][300/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5715 (2.5407)	Acc@1 52.344 (51.749)	Acc@5 79.688 (82.036)
Epoch: [26][310/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5740 (2.5399)	Acc@1 48.438 (51.741)	Acc@5 82.031 (82.056)
Epoch: [26][320/391]	Time 0.014 (0.013)	Data 0.000 (0.002)	Loss 2.8322 (2.5398)	Acc@1 40.625 (51.760)	Acc@5 80.469 (82.087)
Epoch: [26][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6524 (2.5407)	Acc@1 51.562 (51.711)	Acc@5 85.156 (82.067)
Epoch: [26][340/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.8530 (2.5432)	Acc@1 44.531 (51.661)	Acc@5 76.562 (82.036)
Epoch: [26][350/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6912 (2.5440)	Acc@1 50.000 (51.676)	Acc@5 82.031 (82.036)
Epoch: [26][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6232 (2.5441)	Acc@1 47.656 (51.662)	Acc@5 82.812 (82.072)
Epoch: [26][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.2896 (2.5429)	Acc@1 52.344 (51.668)	Acc@5 85.156 (82.103)
Epoch: [26][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4723 (2.5440)	Acc@1 49.219 (51.638)	Acc@5 82.812 (82.056)
Epoch: [26][390/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6522 (2.5466)	Acc@1 47.500 (51.578)	Acc@5 82.500 (82.016)
num momentum params: 26
[0.1, 2.5465755783843993, 2.039380966424942, 51.578, 46.53, tensor(0.3061, device='cuda:0', grad_fn=<DivBackward0>), 5.059762954711914, 0.3752443790435791]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [27 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [27][0/391]	Time 0.033 (0.033)	Data 0.159 (0.159)	Loss 2.5753 (2.5753)	Acc@1 51.562 (51.562)	Acc@5 79.688 (79.688)
Epoch: [27][10/391]	Time 0.013 (0.015)	Data 0.001 (0.016)	Loss 2.4661 (2.4641)	Acc@1 54.688 (52.983)	Acc@5 82.812 (82.244)
Epoch: [27][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.4927 (2.4594)	Acc@1 54.688 (52.939)	Acc@5 82.812 (82.738)
Epoch: [27][30/391]	Time 0.020 (0.014)	Data 0.001 (0.007)	Loss 2.4913 (2.4597)	Acc@1 50.781 (52.823)	Acc@5 82.031 (83.140)
Epoch: [27][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.7060 (2.4523)	Acc@1 48.438 (53.335)	Acc@5 73.438 (83.289)
Epoch: [27][50/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.4929 (2.4652)	Acc@1 50.000 (53.324)	Acc@5 82.812 (83.134)
Epoch: [27][60/391]	Time 0.013 (0.013)	Data 0.002 (0.004)	Loss 2.4059 (2.4743)	Acc@1 57.031 (53.330)	Acc@5 82.812 (83.069)
Epoch: [27][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.1389 (2.4801)	Acc@1 60.938 (52.993)	Acc@5 87.500 (83.121)
Epoch: [27][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5321 (2.4871)	Acc@1 54.688 (52.797)	Acc@5 81.250 (83.218)
Epoch: [27][90/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.5992 (2.4969)	Acc@1 51.562 (52.601)	Acc@5 83.594 (83.130)
Epoch: [27][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.3694 (2.4897)	Acc@1 57.812 (52.839)	Acc@5 85.938 (83.215)
Epoch: [27][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4380 (2.4926)	Acc@1 53.906 (52.653)	Acc@5 79.688 (83.171)
Epoch: [27][120/391]	Time 0.016 (0.013)	Data 0.001 (0.003)	Loss 2.6831 (2.4965)	Acc@1 41.406 (52.628)	Acc@5 80.469 (83.142)
Epoch: [27][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5899 (2.5018)	Acc@1 50.000 (52.469)	Acc@5 81.250 (82.997)
Epoch: [27][140/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.8643 (2.5040)	Acc@1 45.312 (52.455)	Acc@5 75.000 (82.879)
Epoch: [27][150/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5286 (2.5065)	Acc@1 53.125 (52.375)	Acc@5 82.812 (82.807)
Epoch: [27][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3745 (2.5066)	Acc@1 53.906 (52.426)	Acc@5 87.500 (82.827)
Epoch: [27][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5457 (2.5097)	Acc@1 49.219 (52.403)	Acc@5 83.594 (82.753)
Epoch: [27][180/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8913 (2.5157)	Acc@1 45.312 (52.288)	Acc@5 75.781 (82.558)
Epoch: [27][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7824 (2.5151)	Acc@1 41.406 (52.417)	Acc@5 75.000 (82.571)
Epoch: [27][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6551 (2.5174)	Acc@1 49.219 (52.410)	Acc@5 82.031 (82.525)
Epoch: [27][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3371 (2.5158)	Acc@1 57.031 (52.477)	Acc@5 84.375 (82.527)
Epoch: [27][220/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.3463 (2.5172)	Acc@1 55.469 (52.457)	Acc@5 84.375 (82.530)
Epoch: [27][230/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.7026 (2.5190)	Acc@1 46.094 (52.340)	Acc@5 78.125 (82.488)
Epoch: [27][240/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.8357 (2.5195)	Acc@1 47.656 (52.376)	Acc@5 80.469 (82.524)
Epoch: [27][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4695 (2.5200)	Acc@1 50.781 (52.341)	Acc@5 82.031 (82.501)
Epoch: [27][260/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5930 (2.5199)	Acc@1 57.031 (52.446)	Acc@5 82.031 (82.456)
Epoch: [27][270/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4993 (2.5227)	Acc@1 48.438 (52.315)	Acc@5 81.250 (82.420)
Epoch: [27][280/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5695 (2.5247)	Acc@1 55.469 (52.310)	Acc@5 79.688 (82.370)
Epoch: [27][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5499 (2.5254)	Acc@1 57.031 (52.384)	Acc@5 85.156 (82.361)
Epoch: [27][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7865 (2.5303)	Acc@1 45.312 (52.219)	Acc@5 75.781 (82.314)
Epoch: [27][310/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6688 (2.5307)	Acc@1 46.875 (52.223)	Acc@5 80.469 (82.323)
Epoch: [27][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4964 (2.5305)	Acc@1 56.250 (52.259)	Acc@5 82.812 (82.335)
Epoch: [27][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3875 (2.5309)	Acc@1 57.812 (52.259)	Acc@5 83.594 (82.281)
Epoch: [27][340/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6788 (2.5311)	Acc@1 46.094 (52.243)	Acc@5 80.469 (82.295)
Epoch: [27][350/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.7497 (2.5344)	Acc@1 52.344 (52.159)	Acc@5 78.906 (82.236)
Epoch: [27][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5502 (2.5370)	Acc@1 53.125 (52.151)	Acc@5 77.344 (82.161)
Epoch: [27][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5720 (2.5368)	Acc@1 55.469 (52.137)	Acc@5 81.250 (82.170)
Epoch: [27][380/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4286 (2.5382)	Acc@1 57.031 (52.100)	Acc@5 85.938 (82.130)
Epoch: [27][390/391]	Time 0.014 (0.013)	Data 0.002 (0.002)	Loss 2.4805 (2.5381)	Acc@1 48.750 (52.098)	Acc@5 83.750 (82.134)
num momentum params: 26
[0.1, 2.5380818411254884, 2.2491898453235626, 52.098, 43.54, tensor(0.3079, device='cuda:0', grad_fn=<DivBackward0>), 5.032071828842163, 0.37830018997192383]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [28 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [28][0/391]	Time 0.038 (0.038)	Data 0.151 (0.151)	Loss 2.4158 (2.4158)	Acc@1 53.906 (53.906)	Acc@5 82.031 (82.031)
Epoch: [28][10/391]	Time 0.013 (0.015)	Data 0.001 (0.015)	Loss 2.6986 (2.4988)	Acc@1 50.781 (53.409)	Acc@5 75.000 (81.108)
Epoch: [28][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.5448 (2.5113)	Acc@1 51.562 (53.534)	Acc@5 78.906 (81.213)
Epoch: [28][30/391]	Time 0.013 (0.014)	Data 0.001 (0.006)	Loss 2.3863 (2.4913)	Acc@1 57.031 (53.957)	Acc@5 86.719 (82.082)
Epoch: [28][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.3822 (2.5021)	Acc@1 51.562 (53.506)	Acc@5 82.812 (81.993)
Epoch: [28][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6157 (2.5077)	Acc@1 50.781 (53.309)	Acc@5 84.375 (81.939)
Epoch: [28][60/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6626 (2.4923)	Acc@1 49.219 (53.548)	Acc@5 77.344 (82.326)
Epoch: [28][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.5408 (2.4967)	Acc@1 50.781 (53.224)	Acc@5 83.594 (82.526)
Epoch: [28][80/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.5844 (2.4895)	Acc@1 47.656 (53.337)	Acc@5 82.812 (82.764)
Epoch: [28][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.3271 (2.4904)	Acc@1 59.375 (53.331)	Acc@5 85.156 (82.667)
Epoch: [28][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4582 (2.4867)	Acc@1 53.906 (53.380)	Acc@5 82.031 (82.843)
Epoch: [28][110/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.4500 (2.4876)	Acc@1 56.250 (53.456)	Acc@5 82.812 (82.721)
Epoch: [28][120/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5373 (2.4865)	Acc@1 47.656 (53.441)	Acc@5 85.938 (82.761)
Epoch: [28][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.3901 (2.4929)	Acc@1 53.125 (53.203)	Acc@5 83.594 (82.699)
Epoch: [28][140/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.7119 (2.5014)	Acc@1 49.219 (52.920)	Acc@5 76.562 (82.602)
Epoch: [28][150/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6122 (2.5068)	Acc@1 51.562 (52.711)	Acc@5 77.344 (82.590)
Epoch: [28][160/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6288 (2.5082)	Acc@1 45.312 (52.688)	Acc@5 82.031 (82.546)
Epoch: [28][170/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5291 (2.5106)	Acc@1 47.656 (52.609)	Acc@5 84.375 (82.534)
Epoch: [28][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3127 (2.5116)	Acc@1 56.250 (52.573)	Acc@5 85.156 (82.515)
Epoch: [28][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4430 (2.5088)	Acc@1 57.812 (52.671)	Acc@5 79.688 (82.620)
Epoch: [28][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4705 (2.5110)	Acc@1 50.781 (52.608)	Acc@5 80.469 (82.595)
Epoch: [28][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7595 (2.5118)	Acc@1 47.656 (52.566)	Acc@5 81.250 (82.557)
Epoch: [28][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5705 (2.5132)	Acc@1 47.656 (52.524)	Acc@5 82.812 (82.547)
Epoch: [28][230/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6533 (2.5138)	Acc@1 46.875 (52.513)	Acc@5 81.250 (82.545)
Epoch: [28][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6110 (2.5173)	Acc@1 51.562 (52.457)	Acc@5 82.031 (82.462)
Epoch: [28][250/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4982 (2.5181)	Acc@1 53.906 (52.450)	Acc@5 82.812 (82.427)
Epoch: [28][260/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6034 (2.5180)	Acc@1 55.469 (52.490)	Acc@5 82.031 (82.399)
Epoch: [28][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6833 (2.5183)	Acc@1 46.094 (52.485)	Acc@5 84.375 (82.412)
Epoch: [28][280/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.8202 (2.5198)	Acc@1 42.969 (52.499)	Acc@5 74.219 (82.373)
Epoch: [28][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7539 (2.5199)	Acc@1 47.656 (52.489)	Acc@5 80.469 (82.351)
Epoch: [28][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7594 (2.5219)	Acc@1 46.875 (52.442)	Acc@5 76.562 (82.319)
Epoch: [28][310/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.3731 (2.5257)	Acc@1 56.250 (52.351)	Acc@5 83.594 (82.250)
Epoch: [28][320/391]	Time 0.020 (0.013)	Data 0.001 (0.002)	Loss 2.5524 (2.5263)	Acc@1 50.000 (52.336)	Acc@5 82.812 (82.224)
Epoch: [28][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4048 (2.5249)	Acc@1 53.906 (52.412)	Acc@5 83.594 (82.237)
Epoch: [28][340/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6827 (2.5242)	Acc@1 50.781 (52.449)	Acc@5 80.469 (82.240)
Epoch: [28][350/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8599 (2.5249)	Acc@1 48.438 (52.422)	Acc@5 78.125 (82.232)
Epoch: [28][360/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.5278 (2.5281)	Acc@1 52.344 (52.372)	Acc@5 80.469 (82.172)
Epoch: [28][370/391]	Time 0.012 (0.013)	Data 0.002 (0.002)	Loss 2.5219 (2.5312)	Acc@1 56.250 (52.304)	Acc@5 82.031 (82.113)
Epoch: [28][380/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5528 (2.5324)	Acc@1 47.656 (52.272)	Acc@5 79.688 (82.078)
Epoch: [28][390/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.4988 (2.5338)	Acc@1 46.250 (52.220)	Acc@5 86.250 (82.064)
num momentum params: 26
[0.1, 2.5337835265350344, 2.0030315351486205, 52.22, 46.69, tensor(0.3086, device='cuda:0', grad_fn=<DivBackward0>), 5.051343679428101, 0.38629817962646484]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [29 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [29][0/391]	Time 0.040 (0.040)	Data 0.162 (0.162)	Loss 2.4091 (2.4091)	Acc@1 53.906 (53.906)	Acc@5 84.375 (84.375)
Epoch: [29][10/391]	Time 0.013 (0.015)	Data 0.001 (0.016)	Loss 2.4749 (2.4832)	Acc@1 50.781 (54.474)	Acc@5 83.594 (83.807)
Epoch: [29][20/391]	Time 0.015 (0.014)	Data 0.001 (0.009)	Loss 2.3285 (2.4323)	Acc@1 53.125 (55.022)	Acc@5 87.500 (84.189)
Epoch: [29][30/391]	Time 0.015 (0.014)	Data 0.001 (0.007)	Loss 2.4829 (2.4273)	Acc@1 53.906 (54.814)	Acc@5 81.250 (84.652)
Epoch: [29][40/391]	Time 0.014 (0.014)	Data 0.001 (0.005)	Loss 2.4673 (2.4342)	Acc@1 56.250 (54.573)	Acc@5 81.250 (84.165)
Epoch: [29][50/391]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 2.4769 (2.4542)	Acc@1 53.125 (54.059)	Acc@5 85.156 (84.007)
Epoch: [29][60/391]	Time 0.012 (0.014)	Data 0.001 (0.004)	Loss 2.4369 (2.4752)	Acc@1 54.688 (53.407)	Acc@5 85.938 (83.645)
Epoch: [29][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.4547 (2.4810)	Acc@1 57.812 (53.147)	Acc@5 83.594 (83.506)
Epoch: [29][80/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.3814 (2.4791)	Acc@1 55.469 (53.115)	Acc@5 83.594 (83.488)
Epoch: [29][90/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.3940 (2.4740)	Acc@1 57.812 (53.408)	Acc@5 88.281 (83.551)
Epoch: [29][100/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4221 (2.4844)	Acc@1 57.031 (53.233)	Acc@5 82.812 (83.246)
Epoch: [29][110/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.4730 (2.4871)	Acc@1 57.031 (53.132)	Acc@5 85.938 (83.186)
Epoch: [29][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6342 (2.4926)	Acc@1 51.562 (53.144)	Acc@5 78.125 (82.993)
Epoch: [29][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.3787 (2.4911)	Acc@1 54.688 (53.167)	Acc@5 82.812 (82.920)
Epoch: [29][140/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.2465 (2.4883)	Acc@1 57.812 (53.203)	Acc@5 87.500 (82.984)
Epoch: [29][150/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.4999 (2.4854)	Acc@1 54.688 (53.254)	Acc@5 84.375 (83.087)
Epoch: [29][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4506 (2.4838)	Acc@1 56.250 (53.174)	Acc@5 85.938 (83.172)
Epoch: [29][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5291 (2.4877)	Acc@1 49.219 (53.011)	Acc@5 84.375 (83.151)
Epoch: [29][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5204 (2.4908)	Acc@1 53.125 (52.866)	Acc@5 79.688 (83.123)
Epoch: [29][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3282 (2.4949)	Acc@1 54.688 (52.781)	Acc@5 84.375 (82.992)
Epoch: [29][200/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7956 (2.4978)	Acc@1 46.094 (52.736)	Acc@5 80.469 (82.968)
Epoch: [29][210/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7923 (2.5048)	Acc@1 49.219 (52.570)	Acc@5 82.812 (82.838)
Epoch: [29][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5033 (2.5045)	Acc@1 57.031 (52.658)	Acc@5 81.250 (82.858)
Epoch: [29][230/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6206 (2.5060)	Acc@1 52.344 (52.672)	Acc@5 78.125 (82.812)
Epoch: [29][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4929 (2.5044)	Acc@1 51.562 (52.697)	Acc@5 85.938 (82.822)
Epoch: [29][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5676 (2.5030)	Acc@1 49.219 (52.689)	Acc@5 85.156 (82.875)
Epoch: [29][260/391]	Time 0.018 (0.013)	Data 0.001 (0.002)	Loss 2.6045 (2.5039)	Acc@1 46.875 (52.631)	Acc@5 82.812 (82.845)
Epoch: [29][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7085 (2.5044)	Acc@1 38.281 (52.580)	Acc@5 83.594 (82.873)
Epoch: [29][280/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4102 (2.5053)	Acc@1 54.688 (52.566)	Acc@5 82.812 (82.868)
Epoch: [29][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6282 (2.5063)	Acc@1 53.125 (52.610)	Acc@5 84.375 (82.845)
Epoch: [29][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7637 (2.5080)	Acc@1 42.188 (52.525)	Acc@5 80.469 (82.815)
Epoch: [29][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5780 (2.5093)	Acc@1 49.219 (52.497)	Acc@5 81.250 (82.792)
Epoch: [29][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7454 (2.5122)	Acc@1 46.875 (52.405)	Acc@5 81.250 (82.754)
Epoch: [29][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7356 (2.5161)	Acc@1 51.562 (52.367)	Acc@5 78.125 (82.643)
Epoch: [29][340/391]	Time 0.013 (0.013)	Data 0.002 (0.002)	Loss 2.5443 (2.5195)	Acc@1 53.906 (52.229)	Acc@5 80.469 (82.583)
Epoch: [29][350/391]	Time 0.015 (0.013)	Data 0.000 (0.002)	Loss 2.6474 (2.5201)	Acc@1 49.219 (52.246)	Acc@5 81.250 (82.572)
Epoch: [29][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.3981 (2.5231)	Acc@1 54.688 (52.203)	Acc@5 83.594 (82.512)
Epoch: [29][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4141 (2.5227)	Acc@1 48.438 (52.220)	Acc@5 82.812 (82.513)
Epoch: [29][380/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6813 (2.5238)	Acc@1 53.125 (52.239)	Acc@5 80.469 (82.505)
Epoch: [29][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6603 (2.5249)	Acc@1 42.500 (52.204)	Acc@5 78.750 (82.488)
num momentum params: 26
[0.1, 2.52491871963501, 2.0911258351802826, 52.204, 45.68, tensor(0.3105, device='cuda:0', grad_fn=<DivBackward0>), 5.068248510360718, 0.3888576030731201]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [126, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [126]
Non Pruning Epoch - module.bn2.bias: [126]
Non Pruning Epoch - module.conv3.weight: [254, 126, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [255, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [493, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [493]
Non Pruning Epoch - module.bn5.bias: [493]
Non Pruning Epoch - module.conv6.weight: [471, 493, 3, 3]
Non Pruning Epoch - module.bn6.weight: [471]
Non Pruning Epoch - module.bn6.bias: [471]
Non Pruning Epoch - module.conv7.weight: [441, 471, 3, 3]
Non Pruning Epoch - module.bn7.weight: [441]
Non Pruning Epoch - module.bn7.bias: [441]
Non Pruning Epoch - module.conv8.weight: [299, 441, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [30 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [126, 56, 3, 3]
module.conv3.weight [254, 126, 3, 3]
module.conv4.weight [255, 254, 3, 3]
module.conv5.weight [493, 255, 3, 3]
module.conv6.weight [471, 493, 3, 3]
module.conv7.weight [441, 471, 3, 3]
module.conv8.weight [299, 441, 3, 3]
Epoch: [30][0/391]	Time 0.043 (0.043)	Data 0.153 (0.153)	Loss 2.2491 (2.2491)	Acc@1 57.812 (57.812)	Acc@5 87.500 (87.500)
Epoch: [30][10/391]	Time 0.013 (0.016)	Data 0.001 (0.015)	Loss 2.6347 (2.4251)	Acc@1 46.094 (54.545)	Acc@5 80.469 (83.736)
Epoch: [30][20/391]	Time 0.012 (0.014)	Data 0.001 (0.008)	Loss 2.3357 (2.3807)	Acc@1 53.906 (55.804)	Acc@5 85.938 (84.487)
Epoch: [30][30/391]	Time 0.013 (0.014)	Data 0.001 (0.006)	Loss 2.4270 (2.3940)	Acc@1 58.594 (54.662)	Acc@5 81.250 (84.425)
Epoch: [30][40/391]	Time 0.014 (0.014)	Data 0.001 (0.005)	Loss 2.3679 (2.4063)	Acc@1 59.375 (54.345)	Acc@5 87.500 (84.337)
Epoch: [30][50/391]	Time 0.014 (0.014)	Data 0.001 (0.004)	Loss 2.5137 (2.4138)	Acc@1 50.000 (54.136)	Acc@5 83.594 (84.298)
Epoch: [30][60/391]	Time 0.014 (0.014)	Data 0.001 (0.004)	Loss 2.4688 (2.4255)	Acc@1 55.469 (53.817)	Acc@5 81.250 (84.170)
Epoch: [30][70/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 2.4102 (2.4336)	Acc@1 53.906 (53.598)	Acc@5 80.469 (84.232)
Epoch: [30][80/391]	Time 0.012 (0.014)	Data 0.001 (0.003)	Loss 2.6920 (2.4574)	Acc@1 45.312 (53.202)	Acc@5 80.469 (83.883)
Epoch: [30][90/391]	Time 0.012 (0.014)	Data 0.001 (0.003)	Loss 2.5618 (2.4582)	Acc@1 51.562 (53.202)	Acc@5 85.156 (83.843)
Epoch: [30][100/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.2621 (2.4684)	Acc@1 60.938 (52.963)	Acc@5 84.375 (83.594)
Epoch: [30][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.4436 (2.4726)	Acc@1 57.031 (53.019)	Acc@5 82.812 (83.425)
Epoch: [30][120/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6786 (2.4781)	Acc@1 48.438 (52.970)	Acc@5 77.344 (83.232)
Epoch: [30][130/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5101 (2.4774)	Acc@1 46.094 (52.994)	Acc@5 85.156 (83.266)
Epoch: [30][140/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4896 (2.4780)	Acc@1 52.344 (52.909)	Acc@5 81.250 (83.239)
Epoch: [30][150/391]	Time 0.018 (0.013)	Data 0.001 (0.002)	Loss 2.4513 (2.4851)	Acc@1 53.906 (52.742)	Acc@5 81.250 (83.050)
Epoch: [30][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7549 (2.4877)	Acc@1 50.781 (52.814)	Acc@5 76.562 (83.007)
Epoch: [30][170/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.8936 (2.4905)	Acc@1 44.531 (52.755)	Acc@5 72.656 (82.968)
Epoch: [30][180/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6507 (2.5001)	Acc@1 41.406 (52.478)	Acc@5 82.812 (82.804)
Epoch: [30][190/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7058 (2.5033)	Acc@1 45.312 (52.393)	Acc@5 78.125 (82.694)
Epoch: [30][200/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.2614 (2.5012)	Acc@1 57.031 (52.480)	Acc@5 89.844 (82.727)
Epoch: [30][210/391]	Time 0.016 (0.013)	Data 0.001 (0.002)	Loss 2.5128 (2.5035)	Acc@1 55.469 (52.477)	Acc@5 84.375 (82.664)
Epoch: [30][220/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.7748 (2.5050)	Acc@1 47.656 (52.467)	Acc@5 77.344 (82.632)
Epoch: [30][230/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.6352 (2.5060)	Acc@1 46.875 (52.371)	Acc@5 81.250 (82.637)
Epoch: [30][240/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5356 (2.5013)	Acc@1 48.438 (52.470)	Acc@5 84.375 (82.770)
Epoch: [30][250/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4238 (2.5015)	Acc@1 50.781 (52.471)	Acc@5 85.938 (82.778)
Epoch: [30][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4800 (2.5039)	Acc@1 53.906 (52.437)	Acc@5 80.469 (82.735)
Epoch: [30][270/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.9286 (2.5068)	Acc@1 46.094 (52.419)	Acc@5 72.656 (82.663)
Epoch: [30][280/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.3090 (2.5089)	Acc@1 57.812 (52.410)	Acc@5 83.594 (82.582)
Epoch: [30][290/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3952 (2.5085)	Acc@1 57.031 (52.405)	Acc@5 83.594 (82.555)
Epoch: [30][300/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6269 (2.5065)	Acc@1 49.219 (52.455)	Acc@5 81.250 (82.597)
Epoch: [30][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4844 (2.5073)	Acc@1 50.781 (52.462)	Acc@5 80.469 (82.536)
Epoch: [30][320/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3345 (2.5085)	Acc@1 52.344 (52.402)	Acc@5 86.719 (82.523)
Epoch: [30][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6084 (2.5100)	Acc@1 49.219 (52.358)	Acc@5 81.250 (82.484)
Epoch: [30][340/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6604 (2.5088)	Acc@1 44.531 (52.396)	Acc@5 78.906 (82.494)
Epoch: [30][350/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.8200 (2.5121)	Acc@1 43.750 (52.293)	Acc@5 79.688 (82.445)
Epoch: [30][360/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6754 (2.5192)	Acc@1 47.656 (52.106)	Acc@5 82.031 (82.336)
Epoch: [30][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.6760 (2.5215)	Acc@1 53.906 (52.083)	Acc@5 82.031 (82.280)
Epoch: [30][380/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.2784 (2.5226)	Acc@1 61.719 (52.069)	Acc@5 85.938 (82.265)
Epoch: [30][390/391]	Time 0.014 (0.013)	Data 0.001 (0.002)	Loss 2.5663 (2.5243)	Acc@1 51.250 (52.050)	Acc@5 80.000 (82.242)
num momentum params: 26
[0.1, 2.524344208908081, 2.0081739008426664, 52.05, 47.09, tensor(0.3111, device='cuda:0', grad_fn=<DivBackward0>), 5.074350595474243, 0.3813283443450928]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [56, 3, 3, 3]
Before - module.bn1.weight: [56]
Before - module.bn1.bias: [56]
Before - module.conv2.weight: [126, 56, 3, 3]
Before - module.bn2.weight: [126]
Before - module.bn2.bias: [126]
Before - module.conv3.weight: [254, 126, 3, 3]
Before - module.bn3.weight: [254]
Before - module.bn3.bias: [254]
Before - module.conv4.weight: [255, 254, 3, 3]
Before - module.bn4.weight: [255]
Before - module.bn4.bias: [255]
Before - module.conv5.weight: [493, 255, 3, 3]
Before - module.bn5.weight: [493]
Before - module.bn5.bias: [493]
Before - module.conv6.weight: [471, 493, 3, 3]
Before - module.bn6.weight: [471]
Before - module.bn6.bias: [471]
Before - module.conv7.weight: [441, 471, 3, 3]
Before - module.bn7.weight: [441]
Before - module.bn7.bias: [441]
Before - module.conv8.weight: [299, 441, 3, 3]
Before - module.bn8.weight: [299]
Before - module.bn8.bias: [299]
Before - module.fc.weight: [100, 299]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [56, 3, 3, 3] >> [44, 3, 3, 3]
[module.bn1.weight]: 56 >> 44
running_mean [44]
running_var [44]
num_batches_tracked []
[module.conv2.weight]: [126, 56, 3, 3] >> [122, 44, 3, 3]
[module.bn2.weight]: 126 >> 122
running_mean [122]
running_var [122]
num_batches_tracked []
[module.conv3.weight]: [254, 126, 3, 3] >> [247, 122, 3, 3]
[module.bn3.weight]: 254 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv4.weight]: [255, 254, 3, 3] >> [255, 247, 3, 3]
[module.bn4.weight]: 255 >> 255
running_mean [255]
running_var [255]
num_batches_tracked []
[module.conv5.weight]: [493, 255, 3, 3] >> [441, 255, 3, 3]
[module.bn5.weight]: 493 >> 441
running_mean [441]
running_var [441]
num_batches_tracked []
[module.conv6.weight]: [471, 493, 3, 3] >> [411, 441, 3, 3]
[module.bn6.weight]: 471 >> 411
running_mean [411]
running_var [411]
num_batches_tracked []
[module.conv7.weight]: [441, 471, 3, 3] >> [346, 411, 3, 3]
[module.bn7.weight]: 441 >> 346
running_mean [346]
running_var [346]
num_batches_tracked []
[module.conv8.weight]: [299, 441, 3, 3] >> [250, 346, 3, 3]
[module.bn8.weight]: 299 >> 250
running_mean [250]
running_var [250]
num_batches_tracked []
[module.fc.weight]: [100, 299] >> [100, 250]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [44, 3, 3, 3]
After - module.bn1.weight: [44]
After - module.bn1.bias: [44]
After - module.conv2.weight: [122, 44, 3, 3]
After - module.bn2.weight: [122]
After - module.bn2.bias: [122]
After - module.conv3.weight: [247, 122, 3, 3]
After - module.bn3.weight: [247]
After - module.bn3.bias: [247]
After - module.conv4.weight: [255, 247, 3, 3]
After - module.bn4.weight: [255]
After - module.bn4.bias: [255]
After - module.conv5.weight: [441, 255, 3, 3]
After - module.bn5.weight: [441]
After - module.bn5.bias: [441]
After - module.conv6.weight: [411, 441, 3, 3]
After - module.bn6.weight: [411]
After - module.bn6.bias: [411]
After - module.conv7.weight: [346, 411, 3, 3]
After - module.bn7.weight: [346]
After - module.bn7.bias: [346]
After - module.conv8.weight: [250, 346, 3, 3]
After - module.bn8.weight: [250]
After - module.bn8.bias: [250]
After - module.fc.weight: [100, 250]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [44, 3, 3, 3]
conv2 --> [122, 44, 3, 3]
conv3 --> [247, 122, 3, 3]
conv4 --> [255, 247, 3, 3]
conv5 --> [441, 255, 3, 3]
conv6 --> [411, 441, 3, 3]
conv7 --> [346, 411, 3, 3]
conv8 --> [250, 346, 3, 3]
fc --> [250, 100]
1, 487213056, 1216512, 44
2, 5169770496, 12367872, 122
3, 7914875904, 17357184, 247
4, 16543388160, 36279360, 255
5, 8809274880, 16193520, 441
6, 14198478336, 26100144, 411
7, 3931711488, 5119416, 346
8, 2391552000, 3114000, 250
fc, 9600000, 25000, 0
===================
FLOP REPORT: 23224947000000.0 49772800000.0 117773008 124432 2116 10.712900161743164
[INFO] Storing checkpoint...

Epoch: [31 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [31][0/391]	Time 0.307 (0.307)	Data 0.152 (0.152)	Loss 2.2282 (2.2282)	Acc@1 57.812 (57.812)	Acc@5 90.625 (90.625)
Epoch: [31][10/391]	Time 0.012 (0.039)	Data 0.001 (0.015)	Loss 2.5487 (2.4098)	Acc@1 54.688 (55.824)	Acc@5 83.594 (84.517)
Epoch: [31][20/391]	Time 0.012 (0.026)	Data 0.001 (0.009)	Loss 2.6295 (2.4186)	Acc@1 49.219 (55.841)	Acc@5 79.688 (84.226)
Epoch: [31][30/391]	Time 0.012 (0.022)	Data 0.001 (0.006)	Loss 2.1113 (2.4300)	Acc@1 63.281 (55.242)	Acc@5 90.625 (83.972)
Epoch: [31][40/391]	Time 0.012 (0.019)	Data 0.001 (0.005)	Loss 2.3072 (2.4209)	Acc@1 57.031 (55.145)	Acc@5 85.156 (83.937)
Epoch: [31][50/391]	Time 0.016 (0.018)	Data 0.001 (0.004)	Loss 2.7553 (2.4506)	Acc@1 46.094 (54.396)	Acc@5 80.469 (83.532)
Epoch: [31][60/391]	Time 0.011 (0.017)	Data 0.001 (0.004)	Loss 2.3824 (2.4581)	Acc@1 53.906 (54.137)	Acc@5 84.375 (83.427)
Epoch: [31][70/391]	Time 0.012 (0.016)	Data 0.001 (0.004)	Loss 2.6943 (2.4666)	Acc@1 48.438 (53.774)	Acc@5 79.688 (83.319)
Epoch: [31][80/391]	Time 0.012 (0.016)	Data 0.001 (0.003)	Loss 2.3630 (2.4713)	Acc@1 57.812 (53.800)	Acc@5 85.156 (83.237)
Epoch: [31][90/391]	Time 0.013 (0.015)	Data 0.001 (0.003)	Loss 2.3970 (2.4819)	Acc@1 53.125 (53.511)	Acc@5 83.594 (82.924)
Epoch: [31][100/391]	Time 0.011 (0.015)	Data 0.001 (0.003)	Loss 2.3175 (2.4856)	Acc@1 50.781 (53.303)	Acc@5 85.938 (82.851)
Epoch: [31][110/391]	Time 0.012 (0.015)	Data 0.001 (0.003)	Loss 2.4988 (2.4835)	Acc@1 51.562 (53.378)	Acc@5 82.031 (82.890)
Epoch: [31][120/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.3459 (2.4882)	Acc@1 55.469 (53.222)	Acc@5 86.719 (82.800)
Epoch: [31][130/391]	Time 0.012 (0.014)	Data 0.001 (0.003)	Loss 2.4438 (2.4900)	Acc@1 54.688 (53.053)	Acc@5 84.375 (82.854)
Epoch: [31][140/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 2.7539 (2.4929)	Acc@1 49.219 (52.953)	Acc@5 77.344 (82.763)
Epoch: [31][150/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.5487 (2.4892)	Acc@1 51.562 (53.115)	Acc@5 82.812 (82.792)
Epoch: [31][160/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.4962 (2.4878)	Acc@1 52.344 (53.140)	Acc@5 85.938 (82.880)
Epoch: [31][170/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.3196 (2.4873)	Acc@1 60.156 (53.175)	Acc@5 84.375 (82.954)
Epoch: [31][180/391]	Time 0.012 (0.014)	Data 0.001 (0.002)	Loss 2.1489 (2.4838)	Acc@1 60.938 (53.280)	Acc@5 89.844 (83.102)
Epoch: [31][190/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4184 (2.4813)	Acc@1 53.906 (53.346)	Acc@5 83.594 (83.111)
Epoch: [31][200/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5936 (2.4865)	Acc@1 49.219 (53.152)	Acc@5 78.906 (83.065)
Epoch: [31][210/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5480 (2.4850)	Acc@1 49.219 (53.199)	Acc@5 79.688 (83.075)
Epoch: [31][220/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.2354 (2.4840)	Acc@1 61.719 (53.302)	Acc@5 91.406 (83.053)
Epoch: [31][230/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5088 (2.4880)	Acc@1 53.125 (53.210)	Acc@5 82.031 (82.897)
Epoch: [31][240/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5920 (2.4901)	Acc@1 53.906 (53.164)	Acc@5 81.250 (82.868)
Epoch: [31][250/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6515 (2.4933)	Acc@1 49.219 (53.057)	Acc@5 77.344 (82.812)
Epoch: [31][260/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4460 (2.4946)	Acc@1 52.344 (52.972)	Acc@5 85.938 (82.866)
Epoch: [31][270/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.3683 (2.4956)	Acc@1 57.031 (53.027)	Acc@5 86.719 (82.821)
Epoch: [31][280/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.5382 (2.4962)	Acc@1 50.781 (53.019)	Acc@5 82.812 (82.801)
Epoch: [31][290/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.6728 (2.4996)	Acc@1 48.438 (52.902)	Acc@5 82.031 (82.788)
Epoch: [31][300/391]	Time 0.015 (0.013)	Data 0.001 (0.002)	Loss 2.4711 (2.4988)	Acc@1 54.688 (52.954)	Acc@5 87.500 (82.794)
Epoch: [31][310/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5250 (2.4994)	Acc@1 56.250 (52.942)	Acc@5 78.125 (82.777)
Epoch: [31][320/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.1122 (2.4990)	Acc@1 65.625 (52.981)	Acc@5 90.625 (82.781)
Epoch: [31][330/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4665 (2.4988)	Acc@1 53.906 (53.035)	Acc@5 85.938 (82.770)
Epoch: [31][340/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5280 (2.4987)	Acc@1 53.906 (53.043)	Acc@5 82.812 (82.778)
Epoch: [31][350/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.5229 (2.4993)	Acc@1 46.094 (53.063)	Acc@5 85.938 (82.761)
Epoch: [31][360/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6489 (2.4986)	Acc@1 46.875 (53.058)	Acc@5 82.812 (82.771)
Epoch: [31][370/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.4269 (2.5023)	Acc@1 52.344 (52.900)	Acc@5 85.938 (82.711)
Epoch: [31][380/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.4390 (2.5062)	Acc@1 51.562 (52.791)	Acc@5 82.031 (82.624)
Epoch: [31][390/391]	Time 0.191 (0.013)	Data 0.001 (0.002)	Loss 2.3455 (2.5083)	Acc@1 53.750 (52.728)	Acc@5 88.750 (82.612)
num momentum params: 26
[0.1, 2.5082543058776854, 2.3253423953056336, 52.728, 42.69, tensor(0.3130, device='cuda:0', grad_fn=<DivBackward0>), 5.144697427749634, 0.43810105323791504]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [32 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [32][0/391]	Time 0.043 (0.043)	Data 0.154 (0.154)	Loss 2.3715 (2.3715)	Acc@1 55.469 (55.469)	Acc@5 85.938 (85.938)
Epoch: [32][10/391]	Time 0.012 (0.016)	Data 0.001 (0.015)	Loss 2.4148 (2.4431)	Acc@1 57.031 (53.693)	Acc@5 78.906 (82.884)
Epoch: [32][20/391]	Time 0.013 (0.015)	Data 0.001 (0.009)	Loss 2.4170 (2.4551)	Acc@1 57.031 (53.757)	Acc@5 82.812 (83.705)
Epoch: [32][30/391]	Time 0.012 (0.014)	Data 0.001 (0.006)	Loss 2.3371 (2.4516)	Acc@1 55.469 (53.478)	Acc@5 83.594 (83.795)
Epoch: [32][40/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 2.1600 (2.4515)	Acc@1 60.938 (53.868)	Acc@5 88.281 (83.251)
Epoch: [32][50/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.6342 (2.4451)	Acc@1 46.094 (53.952)	Acc@5 78.125 (83.287)
Epoch: [32][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.2402 (2.4433)	Acc@1 60.156 (54.470)	Acc@5 89.062 (83.376)
Epoch: [32][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.2929 (2.4539)	Acc@1 47.656 (53.939)	Acc@5 87.500 (83.385)
Epoch: [32][80/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.5305 (2.4568)	Acc@1 53.125 (53.993)	Acc@5 82.812 (83.410)
Epoch: [32][90/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.2450 (2.4593)	Acc@1 60.938 (54.009)	Acc@5 89.062 (83.328)
Epoch: [32][100/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5184 (2.4665)	Acc@1 50.781 (53.767)	Acc@5 83.594 (83.323)
Epoch: [32][110/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5679 (2.4723)	Acc@1 50.000 (53.723)	Acc@5 80.469 (83.066)
Epoch: [32][120/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5256 (2.4666)	Acc@1 47.656 (53.900)	Acc@5 85.156 (83.187)
Epoch: [32][130/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.6606 (2.4675)	Acc@1 49.219 (53.787)	Acc@5 80.469 (83.146)
Epoch: [32][140/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4271 (2.4719)	Acc@1 59.375 (53.662)	Acc@5 86.719 (83.150)
Epoch: [32][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3888 (2.4726)	Acc@1 59.375 (53.565)	Acc@5 81.250 (83.195)
Epoch: [32][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5513 (2.4713)	Acc@1 50.000 (53.562)	Acc@5 82.031 (83.186)
Epoch: [32][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3250 (2.4674)	Acc@1 55.469 (53.664)	Acc@5 84.375 (83.233)
Epoch: [32][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5674 (2.4692)	Acc@1 53.125 (53.630)	Acc@5 80.469 (83.136)
Epoch: [32][190/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4127 (2.4705)	Acc@1 53.906 (53.599)	Acc@5 86.719 (83.115)
Epoch: [32][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3605 (2.4745)	Acc@1 55.469 (53.506)	Acc@5 87.500 (83.053)
Epoch: [32][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5513 (2.4797)	Acc@1 51.562 (53.384)	Acc@5 80.469 (82.998)
Epoch: [32][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6792 (2.4805)	Acc@1 53.906 (53.415)	Acc@5 79.688 (82.961)
Epoch: [32][230/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 3.0487 (2.4824)	Acc@1 46.094 (53.429)	Acc@5 78.125 (82.958)
Epoch: [32][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5560 (2.4842)	Acc@1 45.312 (53.368)	Acc@5 85.938 (82.926)
Epoch: [32][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3835 (2.4849)	Acc@1 53.125 (53.343)	Acc@5 84.375 (82.928)
Epoch: [32][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5054 (2.4877)	Acc@1 59.375 (53.272)	Acc@5 82.031 (82.863)
Epoch: [32][270/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7426 (2.4924)	Acc@1 44.531 (53.122)	Acc@5 78.125 (82.778)
Epoch: [32][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7090 (2.4930)	Acc@1 46.094 (53.072)	Acc@5 79.688 (82.771)
Epoch: [32][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5384 (2.4972)	Acc@1 52.344 (52.961)	Acc@5 83.594 (82.753)
Epoch: [32][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5359 (2.4992)	Acc@1 52.344 (52.881)	Acc@5 81.250 (82.727)
Epoch: [32][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5779 (2.5015)	Acc@1 53.125 (52.861)	Acc@5 82.031 (82.642)
Epoch: [32][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6394 (2.5022)	Acc@1 50.000 (52.835)	Acc@5 82.031 (82.603)
Epoch: [32][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4489 (2.5044)	Acc@1 57.031 (52.778)	Acc@5 82.031 (82.558)
Epoch: [32][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7189 (2.5063)	Acc@1 46.875 (52.719)	Acc@5 78.125 (82.492)
Epoch: [32][350/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.7881 (2.5075)	Acc@1 52.344 (52.715)	Acc@5 75.000 (82.474)
Epoch: [32][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6597 (2.5097)	Acc@1 48.438 (52.684)	Acc@5 82.812 (82.449)
Epoch: [32][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5601 (2.5101)	Acc@1 49.219 (52.670)	Acc@5 82.031 (82.448)
Epoch: [32][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3399 (2.5083)	Acc@1 59.375 (52.686)	Acc@5 87.500 (82.484)
Epoch: [32][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4332 (2.5081)	Acc@1 58.750 (52.720)	Acc@5 80.000 (82.512)
num momentum params: 26
[0.1, 2.5081495961761475, 2.3586765921115873, 52.72, 41.7, tensor(0.3134, device='cuda:0', grad_fn=<DivBackward0>), 4.711036205291748, 0.3528327941894531]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [33 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [33][0/391]	Time 0.043 (0.043)	Data 0.166 (0.166)	Loss 2.5643 (2.5643)	Acc@1 52.344 (52.344)	Acc@5 87.500 (87.500)
Epoch: [33][10/391]	Time 0.013 (0.016)	Data 0.001 (0.016)	Loss 2.3049 (2.4410)	Acc@1 55.469 (53.977)	Acc@5 87.500 (84.517)
Epoch: [33][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.5770 (2.4448)	Acc@1 50.000 (53.311)	Acc@5 82.031 (84.263)
Epoch: [33][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.3210 (2.4245)	Acc@1 57.812 (53.881)	Acc@5 86.719 (84.501)
Epoch: [33][40/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 2.3824 (2.4228)	Acc@1 58.594 (53.811)	Acc@5 85.156 (84.546)
Epoch: [33][50/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 2.3260 (2.4078)	Acc@1 54.688 (54.075)	Acc@5 83.594 (84.835)
Epoch: [33][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.5532 (2.4227)	Acc@1 54.688 (53.765)	Acc@5 80.469 (84.413)
Epoch: [33][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2640 (2.4136)	Acc@1 60.156 (54.236)	Acc@5 84.375 (84.540)
Epoch: [33][80/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.5928 (2.4217)	Acc@1 53.125 (54.215)	Acc@5 82.812 (84.298)
Epoch: [33][90/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 2.5471 (2.4390)	Acc@1 52.344 (53.838)	Acc@5 81.250 (83.911)
Epoch: [33][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5759 (2.4419)	Acc@1 53.906 (53.929)	Acc@5 85.938 (83.903)
Epoch: [33][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5306 (2.4483)	Acc@1 50.000 (53.794)	Acc@5 81.250 (83.763)
Epoch: [33][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4241 (2.4524)	Acc@1 51.562 (53.732)	Acc@5 85.156 (83.755)
Epoch: [33][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3073 (2.4516)	Acc@1 59.375 (53.817)	Acc@5 87.500 (83.779)
Epoch: [33][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.7443 (2.4620)	Acc@1 50.781 (53.607)	Acc@5 77.344 (83.627)
Epoch: [33][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4740 (2.4631)	Acc@1 57.031 (53.627)	Acc@5 82.031 (83.630)
Epoch: [33][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6249 (2.4685)	Acc@1 43.750 (53.455)	Acc@5 78.125 (83.492)
Epoch: [33][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3005 (2.4678)	Acc@1 52.344 (53.586)	Acc@5 87.500 (83.466)
Epoch: [33][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2674 (2.4629)	Acc@1 57.031 (53.738)	Acc@5 85.156 (83.494)
Epoch: [33][190/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6257 (2.4661)	Acc@1 48.438 (53.604)	Acc@5 81.250 (83.442)
Epoch: [33][200/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.2027 (2.4691)	Acc@1 59.375 (53.529)	Acc@5 87.500 (83.349)
Epoch: [33][210/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5065 (2.4709)	Acc@1 53.125 (53.517)	Acc@5 82.031 (83.298)
Epoch: [33][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4668 (2.4703)	Acc@1 50.781 (53.563)	Acc@5 84.375 (83.304)
Epoch: [33][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5771 (2.4701)	Acc@1 46.875 (53.483)	Acc@5 82.031 (83.310)
Epoch: [33][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5711 (2.4716)	Acc@1 52.344 (53.433)	Acc@5 76.562 (83.283)
Epoch: [33][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4874 (2.4738)	Acc@1 54.688 (53.396)	Acc@5 81.250 (83.279)
Epoch: [33][260/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 2.6386 (2.4705)	Acc@1 50.781 (53.487)	Acc@5 75.781 (83.342)
Epoch: [33][270/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4727 (2.4744)	Acc@1 56.250 (53.425)	Acc@5 78.906 (83.239)
Epoch: [33][280/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4881 (2.4748)	Acc@1 53.125 (53.456)	Acc@5 82.812 (83.216)
Epoch: [33][290/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4919 (2.4769)	Acc@1 53.906 (53.369)	Acc@5 83.594 (83.218)
Epoch: [33][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6863 (2.4793)	Acc@1 44.531 (53.374)	Acc@5 78.906 (83.171)
Epoch: [33][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3446 (2.4805)	Acc@1 58.594 (53.351)	Acc@5 83.594 (83.162)
Epoch: [33][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2065 (2.4788)	Acc@1 57.812 (53.371)	Acc@5 87.500 (83.197)
Epoch: [33][330/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5208 (2.4797)	Acc@1 49.219 (53.370)	Acc@5 82.812 (83.178)
Epoch: [33][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6233 (2.4810)	Acc@1 51.562 (53.320)	Acc@5 81.250 (83.197)
Epoch: [33][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5923 (2.4821)	Acc@1 46.875 (53.259)	Acc@5 81.250 (83.178)
Epoch: [33][360/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6174 (2.4858)	Acc@1 53.906 (53.166)	Acc@5 80.469 (83.107)
Epoch: [33][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3568 (2.4881)	Acc@1 60.156 (53.142)	Acc@5 83.594 (83.082)
Epoch: [33][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7252 (2.4900)	Acc@1 50.781 (53.094)	Acc@5 80.469 (83.059)
Epoch: [33][390/391]	Time 0.014 (0.012)	Data 0.000 (0.002)	Loss 2.4587 (2.4922)	Acc@1 51.250 (53.030)	Acc@5 88.750 (83.046)
num momentum params: 26
[0.1, 2.492179914703369, 2.088336899280548, 53.03, 45.94, tensor(0.3158, device='cuda:0', grad_fn=<DivBackward0>), 4.729820251464844, 0.34835100173950195]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [34 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [34][0/391]	Time 0.041 (0.041)	Data 0.156 (0.156)	Loss 2.3360 (2.3360)	Acc@1 58.594 (58.594)	Acc@5 85.156 (85.156)
Epoch: [34][10/391]	Time 0.013 (0.016)	Data 0.001 (0.015)	Loss 2.6141 (2.4649)	Acc@1 47.656 (54.830)	Acc@5 81.250 (83.665)
Epoch: [34][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.5304 (2.4243)	Acc@1 53.125 (55.060)	Acc@5 82.031 (85.045)
Epoch: [34][30/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.3073 (2.4158)	Acc@1 53.906 (54.965)	Acc@5 85.156 (84.526)
Epoch: [34][40/391]	Time 0.016 (0.013)	Data 0.001 (0.005)	Loss 2.3969 (2.4269)	Acc@1 55.469 (55.030)	Acc@5 86.719 (84.432)
Epoch: [34][50/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.6404 (2.4313)	Acc@1 46.094 (54.688)	Acc@5 78.906 (84.176)
Epoch: [34][60/391]	Time 0.015 (0.013)	Data 0.001 (0.004)	Loss 2.4578 (2.4475)	Acc@1 50.000 (54.060)	Acc@5 84.375 (83.863)
Epoch: [34][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.3795 (2.4517)	Acc@1 55.469 (53.994)	Acc@5 85.156 (83.880)
Epoch: [34][80/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.5788 (2.4606)	Acc@1 53.906 (53.800)	Acc@5 78.125 (83.825)
Epoch: [34][90/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.6160 (2.4664)	Acc@1 50.000 (53.726)	Acc@5 82.031 (83.740)
Epoch: [34][100/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.4932 (2.4682)	Acc@1 50.781 (53.721)	Acc@5 84.375 (83.671)
Epoch: [34][110/391]	Time 0.013 (0.013)	Data 0.001 (0.003)	Loss 2.6820 (2.4774)	Acc@1 48.438 (53.477)	Acc@5 78.125 (83.432)
Epoch: [34][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4377 (2.4832)	Acc@1 56.250 (53.254)	Acc@5 81.250 (83.297)
Epoch: [34][130/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.6354 (2.4802)	Acc@1 48.438 (53.298)	Acc@5 79.688 (83.266)
Epoch: [34][140/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.3099 (2.4878)	Acc@1 57.031 (53.158)	Acc@5 89.062 (83.156)
Epoch: [34][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7648 (2.4897)	Acc@1 46.875 (53.172)	Acc@5 82.812 (83.113)
Epoch: [34][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3690 (2.4895)	Acc@1 56.250 (53.207)	Acc@5 83.594 (83.089)
Epoch: [34][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5013 (2.4929)	Acc@1 54.688 (53.157)	Acc@5 85.938 (83.041)
Epoch: [34][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4930 (2.4931)	Acc@1 53.125 (53.172)	Acc@5 82.812 (83.054)
Epoch: [34][190/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4376 (2.4930)	Acc@1 58.594 (53.207)	Acc@5 81.250 (82.997)
Epoch: [34][200/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.3548 (2.4918)	Acc@1 54.688 (53.211)	Acc@5 81.250 (82.929)
Epoch: [34][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3833 (2.4876)	Acc@1 58.594 (53.351)	Acc@5 80.469 (82.983)
Epoch: [34][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2934 (2.4847)	Acc@1 53.125 (53.365)	Acc@5 88.281 (83.025)
Epoch: [34][230/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 2.5265 (2.4879)	Acc@1 51.562 (53.318)	Acc@5 81.250 (82.985)
Epoch: [34][240/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6036 (2.4909)	Acc@1 47.656 (53.174)	Acc@5 77.344 (82.958)
Epoch: [34][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6740 (2.4960)	Acc@1 46.875 (52.976)	Acc@5 80.469 (82.925)
Epoch: [34][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4393 (2.4958)	Acc@1 55.469 (52.975)	Acc@5 85.938 (82.944)
Epoch: [34][270/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5211 (2.4998)	Acc@1 53.125 (52.868)	Acc@5 84.375 (82.913)
Epoch: [34][280/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5447 (2.5027)	Acc@1 51.562 (52.830)	Acc@5 82.812 (82.854)
Epoch: [34][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4069 (2.5014)	Acc@1 60.938 (52.857)	Acc@5 84.375 (82.880)
Epoch: [34][300/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4281 (2.5010)	Acc@1 51.562 (52.865)	Acc@5 80.469 (82.851)
Epoch: [34][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3062 (2.5016)	Acc@1 56.250 (52.839)	Acc@5 85.938 (82.815)
Epoch: [34][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6663 (2.5025)	Acc@1 53.125 (52.852)	Acc@5 78.906 (82.786)
Epoch: [34][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5997 (2.5064)	Acc@1 53.906 (52.806)	Acc@5 78.906 (82.694)
Epoch: [34][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5590 (2.5073)	Acc@1 50.000 (52.781)	Acc@5 79.688 (82.673)
Epoch: [34][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4161 (2.5065)	Acc@1 50.000 (52.798)	Acc@5 84.375 (82.657)
Epoch: [34][360/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4773 (2.5055)	Acc@1 54.688 (52.785)	Acc@5 82.031 (82.665)
Epoch: [34][370/391]	Time 0.020 (0.012)	Data 0.001 (0.002)	Loss 2.7144 (2.5066)	Acc@1 45.312 (52.792)	Acc@5 82.812 (82.615)
Epoch: [34][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4062 (2.5077)	Acc@1 55.469 (52.781)	Acc@5 82.812 (82.577)
Epoch: [34][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5937 (2.5097)	Acc@1 45.000 (52.766)	Acc@5 75.000 (82.538)
num momentum params: 26
[0.1, 2.509740267791748, 2.1526995205879214, 52.766, 44.94, tensor(0.3142, device='cuda:0', grad_fn=<DivBackward0>), 4.784787654876709, 0.34827733039855957]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [35 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [35][0/391]	Time 0.043 (0.043)	Data 0.151 (0.151)	Loss 2.6446 (2.6446)	Acc@1 44.531 (44.531)	Acc@5 82.812 (82.812)
Epoch: [35][10/391]	Time 0.012 (0.015)	Data 0.001 (0.015)	Loss 2.3535 (2.4829)	Acc@1 53.125 (52.841)	Acc@5 84.375 (84.233)
Epoch: [35][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.1886 (2.4261)	Acc@1 59.375 (54.390)	Acc@5 89.062 (84.487)
Epoch: [35][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.6404 (2.4095)	Acc@1 53.125 (54.889)	Acc@5 79.688 (84.173)
Epoch: [35][40/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 2.4100 (2.4112)	Acc@1 56.250 (55.164)	Acc@5 87.500 (84.280)
Epoch: [35][50/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.3443 (2.4208)	Acc@1 53.125 (55.009)	Acc@5 85.938 (84.115)
Epoch: [35][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.6519 (2.4321)	Acc@1 51.562 (54.777)	Acc@5 82.031 (83.824)
Epoch: [35][70/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.7743 (2.4355)	Acc@1 45.312 (54.445)	Acc@5 76.562 (83.902)
Epoch: [35][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4958 (2.4416)	Acc@1 55.469 (54.225)	Acc@5 82.031 (83.835)
Epoch: [35][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2934 (2.4474)	Acc@1 58.594 (54.104)	Acc@5 87.500 (83.817)
Epoch: [35][100/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 2.7666 (2.4539)	Acc@1 43.750 (53.844)	Acc@5 78.125 (83.803)
Epoch: [35][110/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 2.6550 (2.4635)	Acc@1 50.000 (53.491)	Acc@5 82.031 (83.664)
Epoch: [35][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4896 (2.4692)	Acc@1 53.906 (53.274)	Acc@5 83.594 (83.568)
Epoch: [35][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6503 (2.4747)	Acc@1 49.219 (53.047)	Acc@5 78.125 (83.486)
Epoch: [35][140/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5263 (2.4701)	Acc@1 51.562 (53.147)	Acc@5 82.031 (83.516)
Epoch: [35][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3940 (2.4727)	Acc@1 59.375 (53.125)	Acc@5 83.594 (83.423)
Epoch: [35][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.9591 (2.4856)	Acc@1 43.750 (52.863)	Acc@5 75.781 (83.167)
Epoch: [35][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4719 (2.4887)	Acc@1 55.469 (52.865)	Acc@5 82.812 (83.155)
Epoch: [35][180/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.7010 (2.4944)	Acc@1 40.625 (52.793)	Acc@5 81.250 (83.033)
Epoch: [35][190/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.3058 (2.4927)	Acc@1 55.469 (52.892)	Acc@5 88.281 (83.058)
Epoch: [35][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6696 (2.4954)	Acc@1 51.562 (52.896)	Acc@5 78.906 (82.991)
Epoch: [35][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5275 (2.4929)	Acc@1 48.438 (52.921)	Acc@5 80.469 (83.016)
Epoch: [35][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4926 (2.4928)	Acc@1 60.938 (52.959)	Acc@5 82.812 (83.053)
Epoch: [35][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6393 (2.4936)	Acc@1 52.344 (52.986)	Acc@5 78.125 (83.002)
Epoch: [35][240/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5690 (2.4964)	Acc@1 52.344 (52.930)	Acc@5 85.938 (82.975)
Epoch: [35][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7725 (2.4958)	Acc@1 46.875 (53.016)	Acc@5 76.562 (82.953)
Epoch: [35][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7673 (2.5000)	Acc@1 47.656 (52.898)	Acc@5 79.688 (82.872)
Epoch: [35][270/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2233 (2.5011)	Acc@1 60.156 (52.883)	Acc@5 86.719 (82.859)
Epoch: [35][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7960 (2.5056)	Acc@1 52.344 (52.777)	Acc@5 78.125 (82.799)
Epoch: [35][290/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4203 (2.5028)	Acc@1 46.094 (52.827)	Acc@5 87.500 (82.869)
Epoch: [35][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5437 (2.5022)	Acc@1 50.781 (52.816)	Acc@5 81.250 (82.883)
Epoch: [35][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6601 (2.5032)	Acc@1 49.219 (52.829)	Acc@5 80.469 (82.845)
Epoch: [35][320/391]	Time 0.015 (0.012)	Data 0.002 (0.002)	Loss 2.2547 (2.4996)	Acc@1 58.594 (52.952)	Acc@5 85.156 (82.886)
Epoch: [35][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5044 (2.4968)	Acc@1 56.250 (53.031)	Acc@5 81.250 (82.916)
Epoch: [35][340/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5814 (2.5000)	Acc@1 53.125 (52.965)	Acc@5 81.250 (82.872)
Epoch: [35][350/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.7364 (2.5015)	Acc@1 50.781 (53.020)	Acc@5 75.000 (82.815)
Epoch: [35][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6168 (2.5047)	Acc@1 50.000 (52.880)	Acc@5 81.250 (82.778)
Epoch: [35][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6871 (2.5073)	Acc@1 49.219 (52.839)	Acc@5 80.469 (82.711)
Epoch: [35][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4275 (2.5095)	Acc@1 52.344 (52.776)	Acc@5 85.938 (82.667)
Epoch: [35][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5242 (2.5096)	Acc@1 51.250 (52.762)	Acc@5 78.750 (82.658)
num momentum params: 26
[0.1, 2.5095518939971924, 1.9261772418022156, 52.762, 49.51, tensor(0.3146, device='cuda:0', grad_fn=<DivBackward0>), 4.733506679534912, 0.3579845428466797]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [36 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [36][0/391]	Time 0.044 (0.044)	Data 0.176 (0.176)	Loss 2.3002 (2.3002)	Acc@1 61.719 (61.719)	Acc@5 85.156 (85.156)
Epoch: [36][10/391]	Time 0.012 (0.015)	Data 0.001 (0.017)	Loss 2.2766 (2.3828)	Acc@1 58.594 (56.321)	Acc@5 87.500 (84.304)
Epoch: [36][20/391]	Time 0.013 (0.014)	Data 0.001 (0.010)	Loss 2.3681 (2.3710)	Acc@1 57.812 (56.138)	Acc@5 81.250 (84.710)
Epoch: [36][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.1954 (2.3880)	Acc@1 58.594 (55.796)	Acc@5 87.500 (84.551)
Epoch: [36][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.4122 (2.3890)	Acc@1 57.031 (55.716)	Acc@5 82.031 (84.299)
Epoch: [36][50/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.4932 (2.4021)	Acc@1 51.562 (55.193)	Acc@5 82.812 (84.298)
Epoch: [36][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.5874 (2.4168)	Acc@1 47.656 (54.803)	Acc@5 82.812 (84.221)
Epoch: [36][70/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.2064 (2.4204)	Acc@1 58.594 (54.721)	Acc@5 88.281 (84.155)
Epoch: [36][80/391]	Time 0.014 (0.013)	Data 0.001 (0.003)	Loss 2.6713 (2.4195)	Acc@1 50.000 (54.707)	Acc@5 75.000 (84.201)
Epoch: [36][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6457 (2.4320)	Acc@1 54.688 (54.327)	Acc@5 80.469 (83.980)
Epoch: [36][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3516 (2.4356)	Acc@1 55.469 (54.301)	Acc@5 87.500 (83.950)
Epoch: [36][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.8089 (2.4475)	Acc@1 47.656 (54.139)	Acc@5 85.156 (83.833)
Epoch: [36][120/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.4124 (2.4531)	Acc@1 53.125 (53.958)	Acc@5 84.375 (83.749)
Epoch: [36][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5844 (2.4570)	Acc@1 51.562 (53.930)	Acc@5 84.375 (83.719)
Epoch: [36][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4097 (2.4573)	Acc@1 53.906 (53.934)	Acc@5 86.719 (83.760)
Epoch: [36][150/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3742 (2.4565)	Acc@1 57.812 (53.989)	Acc@5 87.500 (83.801)
Epoch: [36][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5852 (2.4598)	Acc@1 47.656 (53.824)	Acc@5 82.031 (83.686)
Epoch: [36][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5121 (2.4619)	Acc@1 50.781 (53.692)	Acc@5 82.031 (83.607)
Epoch: [36][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3402 (2.4622)	Acc@1 56.250 (53.669)	Acc@5 86.719 (83.589)
Epoch: [36][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5349 (2.4620)	Acc@1 49.219 (53.644)	Acc@5 82.812 (83.565)
Epoch: [36][200/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5347 (2.4640)	Acc@1 50.781 (53.615)	Acc@5 83.594 (83.559)
Epoch: [36][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6045 (2.4656)	Acc@1 48.438 (53.562)	Acc@5 80.469 (83.520)
Epoch: [36][220/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5951 (2.4646)	Acc@1 48.438 (53.539)	Acc@5 82.031 (83.580)
Epoch: [36][230/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.3689 (2.4680)	Acc@1 50.781 (53.453)	Acc@5 86.719 (83.502)
Epoch: [36][240/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4173 (2.4696)	Acc@1 49.219 (53.417)	Acc@5 87.500 (83.493)
Epoch: [36][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5149 (2.4699)	Acc@1 52.344 (53.474)	Acc@5 85.156 (83.503)
Epoch: [36][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5954 (2.4712)	Acc@1 53.906 (53.472)	Acc@5 82.031 (83.456)
Epoch: [36][270/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4618 (2.4733)	Acc@1 56.250 (53.410)	Acc@5 85.938 (83.447)
Epoch: [36][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6613 (2.4736)	Acc@1 52.344 (53.434)	Acc@5 80.469 (83.416)
Epoch: [36][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5673 (2.4763)	Acc@1 52.344 (53.428)	Acc@5 75.781 (83.296)
Epoch: [36][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4117 (2.4783)	Acc@1 52.344 (53.403)	Acc@5 84.375 (83.251)
Epoch: [36][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4407 (2.4791)	Acc@1 58.594 (53.384)	Acc@5 82.031 (83.222)
Epoch: [36][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3964 (2.4827)	Acc@1 54.688 (53.273)	Acc@5 83.594 (83.153)
Epoch: [36][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3982 (2.4813)	Acc@1 57.031 (53.337)	Acc@5 84.375 (83.171)
Epoch: [36][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4445 (2.4831)	Acc@1 56.250 (53.320)	Acc@5 84.375 (83.154)
Epoch: [36][350/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6725 (2.4849)	Acc@1 47.656 (53.285)	Acc@5 78.906 (83.111)
Epoch: [36][360/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.8059 (2.4859)	Acc@1 46.094 (53.268)	Acc@5 79.688 (83.087)
Epoch: [36][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4675 (2.4872)	Acc@1 53.906 (53.296)	Acc@5 84.375 (83.046)
Epoch: [36][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5159 (2.4884)	Acc@1 52.344 (53.269)	Acc@5 84.375 (83.042)
Epoch: [36][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4982 (2.4902)	Acc@1 52.500 (53.260)	Acc@5 82.500 (83.008)
num momentum params: 26
[0.1, 2.490180654296875, 2.1847121894359587, 53.26, 43.82, tensor(0.3172, device='cuda:0', grad_fn=<DivBackward0>), 4.756133556365967, 0.3503532409667969]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [37 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [37][0/391]	Time 0.039 (0.039)	Data 0.176 (0.176)	Loss 2.4537 (2.4537)	Acc@1 55.469 (55.469)	Acc@5 81.250 (81.250)
Epoch: [37][10/391]	Time 0.012 (0.014)	Data 0.001 (0.017)	Loss 2.2989 (2.3784)	Acc@1 61.719 (56.250)	Acc@5 83.594 (85.582)
Epoch: [37][20/391]	Time 0.013 (0.013)	Data 0.001 (0.010)	Loss 2.2231 (2.3483)	Acc@1 62.500 (57.254)	Acc@5 88.281 (85.491)
Epoch: [37][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.5728 (2.3897)	Acc@1 52.344 (56.275)	Acc@5 83.594 (84.929)
Epoch: [37][40/391]	Time 0.012 (0.012)	Data 0.001 (0.006)	Loss 2.6870 (2.4116)	Acc@1 52.344 (55.736)	Acc@5 75.000 (84.375)
Epoch: [37][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5634 (2.4233)	Acc@1 51.562 (55.607)	Acc@5 78.125 (84.023)
Epoch: [37][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3547 (2.4225)	Acc@1 57.031 (55.533)	Acc@5 89.844 (84.170)
Epoch: [37][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5338 (2.4288)	Acc@1 56.250 (55.436)	Acc@5 80.469 (83.968)
Epoch: [37][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.7751 (2.4339)	Acc@1 42.188 (55.073)	Acc@5 83.594 (83.941)
Epoch: [37][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5522 (2.4415)	Acc@1 50.000 (54.842)	Acc@5 78.906 (83.740)
Epoch: [37][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5433 (2.4445)	Acc@1 53.125 (54.548)	Acc@5 82.812 (83.779)
Epoch: [37][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5124 (2.4481)	Acc@1 51.562 (54.483)	Acc@5 84.375 (83.713)
Epoch: [37][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5488 (2.4519)	Acc@1 53.906 (54.384)	Acc@5 81.250 (83.600)
Epoch: [37][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3264 (2.4535)	Acc@1 55.469 (54.300)	Acc@5 82.812 (83.528)
Epoch: [37][140/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.6549 (2.4563)	Acc@1 46.094 (54.255)	Acc@5 80.469 (83.511)
Epoch: [37][150/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 2.4372 (2.4590)	Acc@1 58.594 (54.118)	Acc@5 80.469 (83.464)
Epoch: [37][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5614 (2.4600)	Acc@1 48.438 (54.163)	Acc@5 81.250 (83.463)
Epoch: [37][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6668 (2.4644)	Acc@1 46.094 (53.943)	Acc@5 84.375 (83.448)
Epoch: [37][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.1824 (2.4605)	Acc@1 60.938 (54.014)	Acc@5 90.625 (83.512)
Epoch: [37][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4790 (2.4635)	Acc@1 49.219 (53.890)	Acc@5 85.156 (83.479)
Epoch: [37][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3060 (2.4617)	Acc@1 57.812 (53.992)	Acc@5 87.500 (83.520)
Epoch: [37][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3334 (2.4631)	Acc@1 54.688 (53.910)	Acc@5 86.719 (83.505)
Epoch: [37][220/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5103 (2.4648)	Acc@1 52.344 (53.853)	Acc@5 82.031 (83.459)
Epoch: [37][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7050 (2.4691)	Acc@1 52.344 (53.832)	Acc@5 76.562 (83.374)
Epoch: [37][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7675 (2.4751)	Acc@1 46.875 (53.683)	Acc@5 77.344 (83.263)
Epoch: [37][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5060 (2.4783)	Acc@1 53.906 (53.561)	Acc@5 82.031 (83.220)
Epoch: [37][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3775 (2.4798)	Acc@1 57.812 (53.541)	Acc@5 82.031 (83.193)
Epoch: [37][270/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4905 (2.4798)	Acc@1 54.688 (53.511)	Acc@5 83.594 (83.274)
Epoch: [37][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6820 (2.4815)	Acc@1 51.562 (53.470)	Acc@5 82.031 (83.230)
Epoch: [37][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.8513 (2.4865)	Acc@1 46.875 (53.367)	Acc@5 78.125 (83.137)
Epoch: [37][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3346 (2.4874)	Acc@1 54.688 (53.327)	Acc@5 85.938 (83.147)
Epoch: [37][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6923 (2.4908)	Acc@1 46.094 (53.228)	Acc@5 82.031 (83.096)
Epoch: [37][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2954 (2.4915)	Acc@1 53.906 (53.174)	Acc@5 88.281 (83.119)
Epoch: [37][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7708 (2.4910)	Acc@1 44.531 (53.193)	Acc@5 78.125 (83.119)
Epoch: [37][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.1601 (2.4903)	Acc@1 63.281 (53.233)	Acc@5 87.500 (83.099)
Epoch: [37][350/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 2.5858 (2.4926)	Acc@1 53.906 (53.225)	Acc@5 80.469 (83.037)
Epoch: [37][360/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4946 (2.4925)	Acc@1 46.875 (53.220)	Acc@5 84.375 (83.031)
Epoch: [37][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5317 (2.4932)	Acc@1 54.688 (53.251)	Acc@5 83.594 (83.027)
Epoch: [37][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5809 (2.4950)	Acc@1 53.125 (53.219)	Acc@5 82.031 (82.972)
Epoch: [37][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5742 (2.4936)	Acc@1 56.250 (53.278)	Acc@5 85.000 (82.968)
num momentum params: 26
[0.1, 2.493649191741943, 2.039388220310211, 53.278, 47.27, tensor(0.3166, device='cuda:0', grad_fn=<DivBackward0>), 4.65367579460144, 0.35051965713500977]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [38 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [38][0/391]	Time 0.039 (0.039)	Data 0.155 (0.155)	Loss 2.1657 (2.1657)	Acc@1 60.156 (60.156)	Acc@5 89.062 (89.062)
Epoch: [38][10/391]	Time 0.012 (0.014)	Data 0.001 (0.015)	Loss 2.2777 (2.3482)	Acc@1 59.375 (57.457)	Acc@5 83.594 (85.227)
Epoch: [38][20/391]	Time 0.012 (0.013)	Data 0.001 (0.009)	Loss 2.3948 (2.3797)	Acc@1 56.250 (55.915)	Acc@5 85.938 (84.896)
Epoch: [38][30/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.5496 (2.3877)	Acc@1 55.469 (56.401)	Acc@5 79.688 (84.551)
Epoch: [38][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.4658 (2.4005)	Acc@1 49.219 (55.678)	Acc@5 82.031 (84.146)
Epoch: [38][50/391]	Time 0.012 (0.012)	Data 0.002 (0.005)	Loss 2.3884 (2.4144)	Acc@1 53.125 (55.392)	Acc@5 86.719 (84.084)
Epoch: [38][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5377 (2.4330)	Acc@1 50.000 (54.636)	Acc@5 83.594 (83.888)
Epoch: [38][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3598 (2.4454)	Acc@1 53.125 (54.346)	Acc@5 89.062 (83.737)
Epoch: [38][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.7304 (2.4434)	Acc@1 46.875 (54.437)	Acc@5 75.000 (83.738)
Epoch: [38][90/391]	Time 0.015 (0.012)	Data 0.001 (0.003)	Loss 2.5593 (2.4431)	Acc@1 54.688 (54.464)	Acc@5 79.688 (83.800)
Epoch: [38][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6056 (2.4458)	Acc@1 46.875 (54.278)	Acc@5 84.375 (83.849)
Epoch: [38][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3555 (2.4524)	Acc@1 55.469 (54.153)	Acc@5 84.375 (83.770)
Epoch: [38][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2814 (2.4597)	Acc@1 57.812 (53.913)	Acc@5 84.375 (83.645)
Epoch: [38][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6620 (2.4606)	Acc@1 51.562 (53.930)	Acc@5 79.688 (83.516)
Epoch: [38][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.7329 (2.4624)	Acc@1 48.438 (53.901)	Acc@5 77.344 (83.466)
Epoch: [38][150/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4117 (2.4616)	Acc@1 54.688 (54.020)	Acc@5 82.812 (83.506)
Epoch: [38][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4176 (2.4609)	Acc@1 54.688 (54.066)	Acc@5 85.156 (83.550)
Epoch: [38][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3100 (2.4599)	Acc@1 54.688 (53.938)	Acc@5 85.938 (83.585)
Epoch: [38][180/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5781 (2.4576)	Acc@1 48.438 (53.958)	Acc@5 82.812 (83.702)
Epoch: [38][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.8023 (2.4593)	Acc@1 54.688 (53.906)	Acc@5 78.906 (83.712)
Epoch: [38][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6176 (2.4625)	Acc@1 49.219 (53.813)	Acc@5 78.906 (83.671)
Epoch: [38][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4345 (2.4653)	Acc@1 56.250 (53.740)	Acc@5 84.375 (83.638)
Epoch: [38][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4145 (2.4676)	Acc@1 52.344 (53.744)	Acc@5 86.719 (83.615)
Epoch: [38][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4719 (2.4700)	Acc@1 52.344 (53.693)	Acc@5 82.031 (83.557)
Epoch: [38][240/391]	Time 0.016 (0.012)	Data 0.001 (0.002)	Loss 2.5981 (2.4676)	Acc@1 51.562 (53.806)	Acc@5 80.469 (83.552)
Epoch: [38][250/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4282 (2.4699)	Acc@1 57.812 (53.757)	Acc@5 80.469 (83.507)
Epoch: [38][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5605 (2.4740)	Acc@1 55.469 (53.694)	Acc@5 80.469 (83.396)
Epoch: [38][270/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2143 (2.4741)	Acc@1 60.938 (53.704)	Acc@5 85.938 (83.369)
Epoch: [38][280/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.7719 (2.4757)	Acc@1 47.656 (53.659)	Acc@5 79.688 (83.346)
Epoch: [38][290/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6386 (2.4776)	Acc@1 49.219 (53.608)	Acc@5 82.031 (83.296)
Epoch: [38][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7281 (2.4793)	Acc@1 47.656 (53.571)	Acc@5 77.344 (83.267)
Epoch: [38][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4118 (2.4834)	Acc@1 52.344 (53.444)	Acc@5 85.156 (83.177)
Epoch: [38][320/391]	Time 0.015 (0.012)	Data 0.001 (0.002)	Loss 2.4433 (2.4844)	Acc@1 53.125 (53.437)	Acc@5 82.812 (83.134)
Epoch: [38][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6791 (2.4886)	Acc@1 45.312 (53.345)	Acc@5 78.125 (83.030)
Epoch: [38][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4821 (2.4900)	Acc@1 53.906 (53.336)	Acc@5 80.469 (83.014)
Epoch: [38][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4780 (2.4901)	Acc@1 56.250 (53.328)	Acc@5 81.250 (83.002)
Epoch: [38][360/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 2.6405 (2.4902)	Acc@1 53.125 (53.331)	Acc@5 77.344 (83.022)
Epoch: [38][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7142 (2.4896)	Acc@1 43.750 (53.327)	Acc@5 82.031 (83.044)
Epoch: [38][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4695 (2.4891)	Acc@1 57.031 (53.320)	Acc@5 83.594 (83.038)
Epoch: [38][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.3766 (2.4887)	Acc@1 53.750 (53.340)	Acc@5 80.000 (83.060)
num momentum params: 26
[0.1, 2.4886525211334227, 2.106335549354553, 53.34, 46.08, tensor(0.3177, device='cuda:0', grad_fn=<DivBackward0>), 4.701928615570068, 0.3485100269317627]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [39 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [39][0/391]	Time 0.045 (0.045)	Data 0.165 (0.165)	Loss 2.7343 (2.7343)	Acc@1 46.875 (46.875)	Acc@5 76.562 (76.562)
Epoch: [39][10/391]	Time 0.012 (0.016)	Data 0.001 (0.016)	Loss 2.4426 (2.4951)	Acc@1 57.812 (53.196)	Acc@5 82.031 (83.239)
Epoch: [39][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.3412 (2.4515)	Acc@1 60.938 (54.539)	Acc@5 84.375 (83.854)
Epoch: [39][30/391]	Time 0.011 (0.014)	Data 0.001 (0.007)	Loss 2.3991 (2.4056)	Acc@1 55.469 (55.544)	Acc@5 80.469 (84.173)
Epoch: [39][40/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.1775 (2.3937)	Acc@1 60.156 (55.869)	Acc@5 91.406 (84.680)
Epoch: [39][50/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 2.6668 (2.4178)	Acc@1 49.219 (55.285)	Acc@5 78.125 (84.390)
Epoch: [39][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.4704 (2.4155)	Acc@1 57.031 (55.110)	Acc@5 85.938 (84.541)
Epoch: [39][70/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 2.4598 (2.4180)	Acc@1 53.906 (55.183)	Acc@5 82.031 (84.496)
Epoch: [39][80/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.2420 (2.4177)	Acc@1 56.250 (54.948)	Acc@5 85.156 (84.568)
Epoch: [39][90/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5636 (2.4184)	Acc@1 50.781 (54.919)	Acc@5 86.719 (84.521)
Epoch: [39][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4185 (2.4221)	Acc@1 58.594 (54.858)	Acc@5 84.375 (84.406)
Epoch: [39][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5859 (2.4231)	Acc@1 51.562 (54.786)	Acc@5 80.469 (84.333)
Epoch: [39][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5147 (2.4244)	Acc@1 53.125 (54.830)	Acc@5 82.031 (84.272)
Epoch: [39][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3958 (2.4308)	Acc@1 58.594 (54.682)	Acc@5 84.375 (84.172)
Epoch: [39][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4927 (2.4417)	Acc@1 53.125 (54.477)	Acc@5 84.375 (83.959)
Epoch: [39][150/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 2.4619 (2.4480)	Acc@1 57.031 (54.362)	Acc@5 79.688 (83.811)
Epoch: [39][160/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.1675 (2.4472)	Acc@1 57.812 (54.299)	Acc@5 88.281 (83.807)
Epoch: [39][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4345 (2.4468)	Acc@1 57.031 (54.418)	Acc@5 81.250 (83.776)
Epoch: [39][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2696 (2.4463)	Acc@1 60.938 (54.498)	Acc@5 81.250 (83.736)
Epoch: [39][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4307 (2.4461)	Acc@1 52.344 (54.397)	Acc@5 84.375 (83.757)
Epoch: [39][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5864 (2.4468)	Acc@1 46.094 (54.275)	Acc@5 78.906 (83.675)
Epoch: [39][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5413 (2.4483)	Acc@1 52.344 (54.210)	Acc@5 79.688 (83.686)
Epoch: [39][220/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6343 (2.4535)	Acc@1 46.875 (54.101)	Acc@5 81.250 (83.580)
Epoch: [39][230/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5344 (2.4594)	Acc@1 53.906 (53.954)	Acc@5 80.469 (83.482)
Epoch: [39][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5103 (2.4613)	Acc@1 52.344 (53.942)	Acc@5 82.812 (83.480)
Epoch: [39][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5273 (2.4622)	Acc@1 53.906 (53.937)	Acc@5 80.469 (83.482)
Epoch: [39][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5046 (2.4620)	Acc@1 51.562 (53.912)	Acc@5 79.688 (83.480)
Epoch: [39][270/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4749 (2.4643)	Acc@1 50.781 (53.831)	Acc@5 82.812 (83.427)
Epoch: [39][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5421 (2.4631)	Acc@1 50.781 (53.878)	Acc@5 85.938 (83.399)
Epoch: [39][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6507 (2.4650)	Acc@1 45.312 (53.826)	Acc@5 81.250 (83.400)
Epoch: [39][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5719 (2.4676)	Acc@1 54.688 (53.748)	Acc@5 81.250 (83.373)
Epoch: [39][310/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.4889 (2.4696)	Acc@1 52.344 (53.708)	Acc@5 82.812 (83.388)
Epoch: [39][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3941 (2.4692)	Acc@1 55.469 (53.704)	Acc@5 87.500 (83.428)
Epoch: [39][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5952 (2.4697)	Acc@1 56.250 (53.699)	Acc@5 78.906 (83.405)
Epoch: [39][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6134 (2.4702)	Acc@1 53.125 (53.673)	Acc@5 82.812 (83.408)
Epoch: [39][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5927 (2.4701)	Acc@1 50.000 (53.704)	Acc@5 78.906 (83.407)
Epoch: [39][360/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.6335 (2.4750)	Acc@1 44.531 (53.547)	Acc@5 82.031 (83.345)
Epoch: [39][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3997 (2.4756)	Acc@1 55.469 (53.529)	Acc@5 84.375 (83.362)
Epoch: [39][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6683 (2.4781)	Acc@1 52.344 (53.490)	Acc@5 78.906 (83.303)
Epoch: [39][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5239 (2.4790)	Acc@1 47.500 (53.476)	Acc@5 86.250 (83.290)
num momentum params: 26
[0.1, 2.478984549865723, 1.9640459322929382, 53.476, 48.05, tensor(0.3188, device='cuda:0', grad_fn=<DivBackward0>), 4.725069761276245, 0.3521897792816162]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [122, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [122]
Non Pruning Epoch - module.bn2.bias: [122]
Non Pruning Epoch - module.conv3.weight: [247, 122, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [255, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [255]
Non Pruning Epoch - module.bn4.bias: [255]
Non Pruning Epoch - module.conv5.weight: [441, 255, 3, 3]
Non Pruning Epoch - module.bn5.weight: [441]
Non Pruning Epoch - module.bn5.bias: [441]
Non Pruning Epoch - module.conv6.weight: [411, 441, 3, 3]
Non Pruning Epoch - module.bn6.weight: [411]
Non Pruning Epoch - module.bn6.bias: [411]
Non Pruning Epoch - module.conv7.weight: [346, 411, 3, 3]
Non Pruning Epoch - module.bn7.weight: [346]
Non Pruning Epoch - module.bn7.bias: [346]
Non Pruning Epoch - module.conv8.weight: [250, 346, 3, 3]
Non Pruning Epoch - module.bn8.weight: [250]
Non Pruning Epoch - module.bn8.bias: [250]
Non Pruning Epoch - module.fc.weight: [100, 250]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [40 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [122, 44, 3, 3]
module.conv3.weight [247, 122, 3, 3]
module.conv4.weight [255, 247, 3, 3]
module.conv5.weight [441, 255, 3, 3]
module.conv6.weight [411, 441, 3, 3]
module.conv7.weight [346, 411, 3, 3]
module.conv8.weight [250, 346, 3, 3]
Epoch: [40][0/391]	Time 0.045 (0.045)	Data 0.150 (0.150)	Loss 2.2625 (2.2625)	Acc@1 63.281 (63.281)	Acc@5 83.594 (83.594)
Epoch: [40][10/391]	Time 0.012 (0.015)	Data 0.001 (0.015)	Loss 2.3711 (2.2618)	Acc@1 57.812 (58.807)	Acc@5 83.594 (86.080)
Epoch: [40][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.5249 (2.2860)	Acc@1 58.594 (58.929)	Acc@5 82.031 (86.496)
Epoch: [40][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.4299 (2.3251)	Acc@1 53.125 (57.737)	Acc@5 83.594 (85.811)
Epoch: [40][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.2771 (2.3358)	Acc@1 56.250 (57.450)	Acc@5 84.375 (85.404)
Epoch: [40][50/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.2219 (2.3373)	Acc@1 60.938 (57.169)	Acc@5 84.375 (85.417)
Epoch: [40][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4602 (2.3469)	Acc@1 56.250 (56.826)	Acc@5 85.156 (85.220)
Epoch: [40][70/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4907 (2.3591)	Acc@1 51.562 (56.448)	Acc@5 82.812 (85.112)
Epoch: [40][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4802 (2.3684)	Acc@1 47.656 (56.009)	Acc@5 85.156 (84.934)
Epoch: [40][90/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.1441 (2.3723)	Acc@1 60.938 (55.735)	Acc@5 89.062 (84.942)
Epoch: [40][100/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.1599 (2.3797)	Acc@1 60.938 (55.585)	Acc@5 87.500 (84.793)
Epoch: [40][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3988 (2.3889)	Acc@1 54.688 (55.419)	Acc@5 85.156 (84.692)
Epoch: [40][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5064 (2.4022)	Acc@1 50.781 (54.978)	Acc@5 85.156 (84.524)
Epoch: [40][130/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.5064 (2.4094)	Acc@1 53.906 (54.896)	Acc@5 86.719 (84.351)
Epoch: [40][140/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5296 (2.4174)	Acc@1 50.000 (54.710)	Acc@5 83.594 (84.170)
Epoch: [40][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5413 (2.4210)	Acc@1 53.906 (54.662)	Acc@5 82.031 (84.096)
Epoch: [40][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5271 (2.4254)	Acc@1 52.344 (54.455)	Acc@5 79.688 (84.050)
Epoch: [40][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5030 (2.4340)	Acc@1 59.375 (54.235)	Acc@5 78.906 (83.882)
Epoch: [40][180/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5643 (2.4360)	Acc@1 48.438 (54.131)	Acc@5 84.375 (83.913)
Epoch: [40][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6854 (2.4446)	Acc@1 51.562 (54.004)	Acc@5 77.344 (83.725)
Epoch: [40][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.8840 (2.4520)	Acc@1 47.656 (53.887)	Acc@5 76.562 (83.637)
Epoch: [40][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4887 (2.4536)	Acc@1 52.344 (53.862)	Acc@5 86.719 (83.579)
Epoch: [40][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6215 (2.4551)	Acc@1 53.906 (53.850)	Acc@5 78.125 (83.523)
Epoch: [40][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5379 (2.4571)	Acc@1 48.438 (53.788)	Acc@5 82.812 (83.550)
Epoch: [40][240/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.5278 (2.4603)	Acc@1 50.781 (53.692)	Acc@5 79.688 (83.464)
Epoch: [40][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5524 (2.4624)	Acc@1 54.688 (53.623)	Acc@5 82.031 (83.410)
Epoch: [40][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3585 (2.4643)	Acc@1 53.125 (53.583)	Acc@5 84.375 (83.420)
Epoch: [40][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5715 (2.4664)	Acc@1 50.000 (53.514)	Acc@5 82.031 (83.389)
Epoch: [40][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7047 (2.4698)	Acc@1 50.781 (53.498)	Acc@5 76.562 (83.305)
Epoch: [40][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6895 (2.4721)	Acc@1 49.219 (53.498)	Acc@5 76.562 (83.272)
Epoch: [40][300/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6110 (2.4725)	Acc@1 50.781 (53.525)	Acc@5 79.688 (83.223)
Epoch: [40][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5980 (2.4721)	Acc@1 48.438 (53.577)	Acc@5 80.469 (83.232)
Epoch: [40][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5732 (2.4709)	Acc@1 52.344 (53.675)	Acc@5 80.469 (83.221)
Epoch: [40][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6105 (2.4709)	Acc@1 45.312 (53.708)	Acc@5 82.812 (83.223)
Epoch: [40][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6510 (2.4744)	Acc@1 53.125 (53.640)	Acc@5 78.125 (83.163)
Epoch: [40][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6437 (2.4730)	Acc@1 44.531 (53.628)	Acc@5 83.594 (83.164)
Epoch: [40][360/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4158 (2.4736)	Acc@1 54.688 (53.597)	Acc@5 80.469 (83.148)
Epoch: [40][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4563 (2.4759)	Acc@1 55.469 (53.559)	Acc@5 84.375 (83.097)
Epoch: [40][380/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 2.3761 (2.4766)	Acc@1 57.812 (53.521)	Acc@5 85.938 (83.075)
Epoch: [40][390/391]	Time 0.015 (0.012)	Data 0.001 (0.002)	Loss 2.4954 (2.4796)	Acc@1 56.250 (53.442)	Acc@5 83.750 (83.028)
num momentum params: 26
[0.1, 2.479607726287842, 2.013327977657318, 53.442, 47.52, tensor(0.3189, device='cuda:0', grad_fn=<DivBackward0>), 4.713549375534058, 0.3524758815765381]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [44, 3, 3, 3]
Before - module.bn1.weight: [44]
Before - module.bn1.bias: [44]
Before - module.conv2.weight: [122, 44, 3, 3]
Before - module.bn2.weight: [122]
Before - module.bn2.bias: [122]
Before - module.conv3.weight: [247, 122, 3, 3]
Before - module.bn3.weight: [247]
Before - module.bn3.bias: [247]
Before - module.conv4.weight: [255, 247, 3, 3]
Before - module.bn4.weight: [255]
Before - module.bn4.bias: [255]
Before - module.conv5.weight: [441, 255, 3, 3]
Before - module.bn5.weight: [441]
Before - module.bn5.bias: [441]
Before - module.conv6.weight: [411, 441, 3, 3]
Before - module.bn6.weight: [411]
Before - module.bn6.bias: [411]
Before - module.conv7.weight: [346, 411, 3, 3]
Before - module.bn7.weight: [346]
Before - module.bn7.bias: [346]
Before - module.conv8.weight: [250, 346, 3, 3]
Before - module.bn8.weight: [250]
Before - module.bn8.bias: [250]
Before - module.fc.weight: [100, 250]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [44, 3, 3, 3] >> [37, 3, 3, 3]
[module.bn1.weight]: 44 >> 37
running_mean [37]
running_var [37]
num_batches_tracked []
[module.conv2.weight]: [122, 44, 3, 3] >> [119, 37, 3, 3]
[module.bn2.weight]: 122 >> 119
running_mean [119]
running_var [119]
num_batches_tracked []
[module.conv3.weight]: [247, 122, 3, 3] >> [239, 119, 3, 3]
[module.bn3.weight]: 247 >> 239
running_mean [239]
running_var [239]
num_batches_tracked []
[module.conv4.weight]: [255, 247, 3, 3] >> [253, 239, 3, 3]
[module.bn4.weight]: 255 >> 253
running_mean [253]
running_var [253]
num_batches_tracked []
[module.conv5.weight]: [441, 255, 3, 3] >> [414, 253, 3, 3]
[module.bn5.weight]: 441 >> 414
running_mean [414]
running_var [414]
num_batches_tracked []
[module.conv6.weight]: [411, 441, 3, 3] >> [371, 414, 3, 3]
[module.bn6.weight]: 411 >> 371
running_mean [371]
running_var [371]
num_batches_tracked []
[module.conv7.weight]: [346, 411, 3, 3] >> [287, 371, 3, 3]
[module.bn7.weight]: 346 >> 287
running_mean [287]
running_var [287]
num_batches_tracked []
[module.conv8.weight]: [250, 346, 3, 3] >> [235, 287, 3, 3]
[module.bn8.weight]: 250 >> 235
running_mean [235]
running_var [235]
num_batches_tracked []
[module.fc.weight]: [100, 250] >> [100, 235]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [37, 3, 3, 3]
After - module.bn1.weight: [37]
After - module.bn1.bias: [37]
After - module.conv2.weight: [119, 37, 3, 3]
After - module.bn2.weight: [119]
After - module.bn2.bias: [119]
After - module.conv3.weight: [239, 119, 3, 3]
After - module.bn3.weight: [239]
After - module.bn3.bias: [239]
After - module.conv4.weight: [253, 239, 3, 3]
After - module.bn4.weight: [253]
After - module.bn4.bias: [253]
After - module.conv5.weight: [414, 253, 3, 3]
After - module.bn5.weight: [414]
After - module.bn5.bias: [414]
After - module.conv6.weight: [371, 414, 3, 3]
After - module.bn6.weight: [371]
After - module.bn6.bias: [371]
After - module.conv7.weight: [287, 371, 3, 3]
After - module.bn7.weight: [287]
After - module.bn7.bias: [287]
After - module.conv8.weight: [235, 287, 3, 3]
After - module.bn8.weight: [235]
After - module.bn8.bias: [235]
After - module.fc.weight: [100, 235]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [37, 3, 3, 3]
conv2 --> [119, 37, 3, 3]
conv3 --> [239, 119, 3, 3]
conv4 --> [253, 239, 3, 3]
conv5 --> [414, 253, 3, 3]
conv6 --> [371, 414, 3, 3]
conv7 --> [287, 371, 3, 3]
conv8 --> [235, 287, 3, 3]
fc --> [235, 100]
1, 409701888, 1022976, 37
2, 4240406016, 10144512, 119
3, 7470199296, 16382016, 239
4, 15882020352, 34828992, 253
5, 8205069312, 15082848, 414
6, 12031939584, 22117536, 371
7, 2943876096, 3833172, 287
8, 1864719360, 2428020, 235
fc, 9024000, 23500, 0
===================
FLOP REPORT: 20725373400000.0 45795200000.0 105863572 114488 1955 9.072895050048828
[INFO] Storing checkpoint...

Epoch: [41 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [41][0/391]	Time 0.274 (0.274)	Data 0.169 (0.169)	Loss 2.5532 (2.5532)	Acc@1 48.438 (48.438)	Acc@5 83.594 (83.594)
Epoch: [41][10/391]	Time 0.012 (0.037)	Data 0.001 (0.017)	Loss 2.5747 (2.4240)	Acc@1 53.125 (54.830)	Acc@5 82.031 (85.440)
Epoch: [41][20/391]	Time 0.015 (0.025)	Data 0.002 (0.010)	Loss 2.5857 (2.4262)	Acc@1 50.000 (54.539)	Acc@5 82.031 (84.933)
Epoch: [41][30/391]	Time 0.013 (0.021)	Data 0.001 (0.007)	Loss 2.4526 (2.4332)	Acc@1 55.469 (54.309)	Acc@5 83.594 (84.703)
Epoch: [41][40/391]	Time 0.011 (0.018)	Data 0.001 (0.006)	Loss 2.3905 (2.4173)	Acc@1 58.594 (54.726)	Acc@5 85.938 (84.813)
Epoch: [41][50/391]	Time 0.011 (0.017)	Data 0.001 (0.005)	Loss 2.5940 (2.4119)	Acc@1 50.781 (55.055)	Acc@5 83.594 (84.896)
Epoch: [41][60/391]	Time 0.011 (0.016)	Data 0.001 (0.004)	Loss 2.3613 (2.3930)	Acc@1 51.562 (55.456)	Acc@5 86.719 (84.990)
Epoch: [41][70/391]	Time 0.010 (0.015)	Data 0.002 (0.004)	Loss 2.1577 (2.3767)	Acc@1 57.031 (55.766)	Acc@5 89.062 (85.266)
Epoch: [41][80/391]	Time 0.011 (0.015)	Data 0.001 (0.004)	Loss 2.4203 (2.3836)	Acc@1 55.469 (55.652)	Acc@5 84.375 (85.098)
Epoch: [41][90/391]	Time 0.012 (0.015)	Data 0.001 (0.003)	Loss 2.8635 (2.3940)	Acc@1 42.969 (55.108)	Acc@5 78.906 (84.959)
Epoch: [41][100/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.2206 (2.4015)	Acc@1 57.031 (54.981)	Acc@5 83.594 (84.677)
Epoch: [41][110/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.6191 (2.4136)	Acc@1 53.125 (54.835)	Acc@5 80.469 (84.431)
Epoch: [41][120/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.4566 (2.4197)	Acc@1 53.125 (54.558)	Acc@5 84.375 (84.278)
Epoch: [41][130/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 2.4005 (2.4301)	Acc@1 56.250 (54.324)	Acc@5 80.469 (84.101)
Epoch: [41][140/391]	Time 0.013 (0.014)	Data 0.001 (0.003)	Loss 2.5038 (2.4318)	Acc@1 48.438 (54.277)	Acc@5 87.500 (84.159)
Epoch: [41][150/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.4302 (2.4327)	Acc@1 53.906 (54.403)	Acc@5 83.594 (84.199)
Epoch: [41][160/391]	Time 0.013 (0.013)	Data 0.001 (0.002)	Loss 2.5487 (2.4345)	Acc@1 53.125 (54.440)	Acc@5 80.469 (84.132)
Epoch: [41][170/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.5969 (2.4427)	Acc@1 48.438 (54.244)	Acc@5 80.469 (83.991)
Epoch: [41][180/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.4908 (2.4506)	Acc@1 52.344 (53.988)	Acc@5 84.375 (83.917)
Epoch: [41][190/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.5115 (2.4536)	Acc@1 49.219 (53.882)	Acc@5 79.688 (83.929)
Epoch: [41][200/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.3162 (2.4536)	Acc@1 55.469 (53.891)	Acc@5 86.719 (83.975)
Epoch: [41][210/391]	Time 0.011 (0.013)	Data 0.002 (0.002)	Loss 2.2762 (2.4540)	Acc@1 60.938 (53.880)	Acc@5 82.031 (83.968)
Epoch: [41][220/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.6079 (2.4559)	Acc@1 49.219 (53.814)	Acc@5 78.906 (83.937)
Epoch: [41][230/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.6575 (2.4595)	Acc@1 45.312 (53.703)	Acc@5 79.688 (83.888)
Epoch: [41][240/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.3991 (2.4632)	Acc@1 56.250 (53.663)	Acc@5 83.594 (83.772)
Epoch: [41][250/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.5033 (2.4652)	Acc@1 47.656 (53.595)	Acc@5 81.250 (83.762)
Epoch: [41][260/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.3595 (2.4675)	Acc@1 56.250 (53.556)	Acc@5 83.594 (83.728)
Epoch: [41][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6741 (2.4683)	Acc@1 46.094 (53.526)	Acc@5 79.688 (83.677)
Epoch: [41][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3168 (2.4673)	Acc@1 53.906 (53.536)	Acc@5 83.594 (83.663)
Epoch: [41][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6022 (2.4682)	Acc@1 52.344 (53.581)	Acc@5 80.469 (83.639)
Epoch: [41][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6547 (2.4684)	Acc@1 50.781 (53.566)	Acc@5 77.344 (83.607)
Epoch: [41][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.0810 (2.4657)	Acc@1 63.281 (53.658)	Acc@5 91.406 (83.644)
Epoch: [41][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4883 (2.4683)	Acc@1 47.656 (53.575)	Acc@5 83.594 (83.565)
Epoch: [41][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5443 (2.4704)	Acc@1 51.562 (53.533)	Acc@5 85.156 (83.521)
Epoch: [41][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5332 (2.4719)	Acc@1 48.438 (53.480)	Acc@5 84.375 (83.486)
Epoch: [41][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4269 (2.4724)	Acc@1 53.906 (53.481)	Acc@5 83.594 (83.454)
Epoch: [41][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1333 (2.4696)	Acc@1 63.281 (53.582)	Acc@5 87.500 (83.483)
Epoch: [41][370/391]	Time 0.018 (0.012)	Data 0.001 (0.002)	Loss 2.4354 (2.4704)	Acc@1 56.250 (53.590)	Acc@5 82.812 (83.434)
Epoch: [41][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3952 (2.4712)	Acc@1 50.781 (53.539)	Acc@5 86.719 (83.438)
Epoch: [41][390/391]	Time 0.167 (0.013)	Data 0.001 (0.002)	Loss 2.7111 (2.4717)	Acc@1 48.750 (53.520)	Acc@5 76.250 (83.402)
num momentum params: 26
[0.1, 2.4717111575317383, 2.1857663869857786, 53.52, 44.86, tensor(0.3198, device='cuda:0', grad_fn=<DivBackward0>), 4.920339107513428, 0.4395174980163574]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [42 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [42][0/391]	Time 0.047 (0.047)	Data 0.159 (0.159)	Loss 2.4012 (2.4012)	Acc@1 58.594 (58.594)	Acc@5 85.938 (85.938)
Epoch: [42][10/391]	Time 0.012 (0.015)	Data 0.001 (0.016)	Loss 2.4326 (2.3660)	Acc@1 53.906 (55.753)	Acc@5 82.031 (85.014)
Epoch: [42][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.1008 (2.3681)	Acc@1 62.500 (55.580)	Acc@5 85.938 (84.747)
Epoch: [42][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.3916 (2.3698)	Acc@1 53.906 (55.796)	Acc@5 88.281 (84.955)
Epoch: [42][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4663 (2.3895)	Acc@1 57.031 (55.240)	Acc@5 85.156 (84.737)
Epoch: [42][50/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.6260 (2.3900)	Acc@1 49.219 (55.132)	Acc@5 79.688 (84.789)
Epoch: [42][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.7592 (2.4028)	Acc@1 49.219 (55.059)	Acc@5 78.906 (84.682)
Epoch: [42][70/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 2.4345 (2.3973)	Acc@1 59.375 (55.392)	Acc@5 84.375 (84.639)
Epoch: [42][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5238 (2.4079)	Acc@1 56.250 (55.228)	Acc@5 79.688 (84.520)
Epoch: [42][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3956 (2.4243)	Acc@1 50.000 (54.722)	Acc@5 86.719 (84.255)
Epoch: [42][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4384 (2.4286)	Acc@1 55.469 (54.579)	Acc@5 82.031 (84.197)
Epoch: [42][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2194 (2.4273)	Acc@1 57.031 (54.695)	Acc@5 87.500 (84.192)
Epoch: [42][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3468 (2.4201)	Acc@1 61.719 (54.849)	Acc@5 83.594 (84.252)
Epoch: [42][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.7084 (2.4232)	Acc@1 52.344 (54.866)	Acc@5 78.125 (84.142)
Epoch: [42][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5140 (2.4305)	Acc@1 54.688 (54.737)	Acc@5 82.812 (84.031)
Epoch: [42][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.7105 (2.4398)	Acc@1 50.781 (54.574)	Acc@5 72.656 (83.796)
Epoch: [42][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.1374 (2.4370)	Acc@1 60.938 (54.624)	Acc@5 85.938 (83.846)
Epoch: [42][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3059 (2.4333)	Acc@1 53.125 (54.619)	Acc@5 85.938 (83.918)
Epoch: [42][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5046 (2.4298)	Acc@1 51.562 (54.700)	Acc@5 84.375 (83.987)
Epoch: [42][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2318 (2.4298)	Acc@1 55.469 (54.708)	Acc@5 86.719 (84.031)
Epoch: [42][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5688 (2.4373)	Acc@1 55.469 (54.618)	Acc@5 78.125 (83.920)
Epoch: [42][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5213 (2.4425)	Acc@1 53.125 (54.458)	Acc@5 81.250 (83.820)
Epoch: [42][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4317 (2.4410)	Acc@1 54.688 (54.507)	Acc@5 85.938 (83.887)
Epoch: [42][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3834 (2.4416)	Acc@1 57.812 (54.515)	Acc@5 83.594 (83.874)
Epoch: [42][240/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.6359 (2.4453)	Acc@1 51.562 (54.431)	Acc@5 76.562 (83.795)
Epoch: [42][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5663 (2.4479)	Acc@1 53.125 (54.361)	Acc@5 82.031 (83.799)
Epoch: [42][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4236 (2.4494)	Acc@1 52.344 (54.352)	Acc@5 81.250 (83.770)
Epoch: [42][270/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.3634 (2.4516)	Acc@1 55.469 (54.295)	Acc@5 85.156 (83.755)
Epoch: [42][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2864 (2.4540)	Acc@1 57.812 (54.268)	Acc@5 88.281 (83.705)
Epoch: [42][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3816 (2.4572)	Acc@1 53.906 (54.196)	Acc@5 82.031 (83.650)
Epoch: [42][300/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.1451 (2.4541)	Acc@1 63.281 (54.298)	Acc@5 89.062 (83.708)
Epoch: [42][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5137 (2.4541)	Acc@1 53.906 (54.243)	Acc@5 85.156 (83.737)
Epoch: [42][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5177 (2.4540)	Acc@1 54.688 (54.276)	Acc@5 81.250 (83.747)
Epoch: [42][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5991 (2.4572)	Acc@1 53.125 (54.237)	Acc@5 82.812 (83.676)
Epoch: [42][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2877 (2.4576)	Acc@1 59.375 (54.241)	Acc@5 86.719 (83.679)
Epoch: [42][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3973 (2.4605)	Acc@1 60.156 (54.162)	Acc@5 84.375 (83.640)
Epoch: [42][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5108 (2.4645)	Acc@1 50.781 (54.045)	Acc@5 84.375 (83.557)
Epoch: [42][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.8923 (2.4661)	Acc@1 47.656 (54.052)	Acc@5 72.656 (83.539)
Epoch: [42][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5812 (2.4668)	Acc@1 53.125 (54.046)	Acc@5 81.250 (83.522)
Epoch: [42][390/391]	Time 0.014 (0.012)	Data 0.000 (0.002)	Loss 2.9592 (2.4702)	Acc@1 47.500 (53.970)	Acc@5 75.000 (83.476)
num momentum params: 26
[0.1, 2.470217628631592, 2.2702447736263274, 53.97, 42.41, tensor(0.3210, device='cuda:0', grad_fn=<DivBackward0>), 4.511012315750122, 0.3468170166015625]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [43 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [43][0/391]	Time 0.042 (0.042)	Data 0.164 (0.164)	Loss 2.4928 (2.4928)	Acc@1 57.812 (57.812)	Acc@5 82.812 (82.812)
Epoch: [43][10/391]	Time 0.012 (0.015)	Data 0.001 (0.016)	Loss 2.6525 (2.4377)	Acc@1 50.000 (54.830)	Acc@5 83.594 (83.878)
Epoch: [43][20/391]	Time 0.013 (0.014)	Data 0.001 (0.009)	Loss 2.2142 (2.4052)	Acc@1 57.812 (55.506)	Acc@5 89.062 (84.673)
Epoch: [43][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.5396 (2.3929)	Acc@1 50.781 (56.225)	Acc@5 86.719 (84.929)
Epoch: [43][40/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 2.6010 (2.4049)	Acc@1 53.906 (56.040)	Acc@5 77.344 (84.775)
Epoch: [43][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3196 (2.4092)	Acc@1 54.688 (55.836)	Acc@5 89.062 (84.835)
Epoch: [43][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3974 (2.4261)	Acc@1 56.250 (55.110)	Acc@5 84.375 (84.682)
Epoch: [43][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4489 (2.4279)	Acc@1 54.688 (55.172)	Acc@5 82.031 (84.408)
Epoch: [43][80/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.2703 (2.4142)	Acc@1 60.938 (55.517)	Acc@5 86.719 (84.578)
Epoch: [43][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5230 (2.4081)	Acc@1 57.031 (55.580)	Acc@5 79.688 (84.693)
Epoch: [43][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6847 (2.4068)	Acc@1 48.438 (55.446)	Acc@5 78.906 (84.708)
Epoch: [43][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2470 (2.4051)	Acc@1 60.156 (55.462)	Acc@5 90.625 (84.734)
Epoch: [43][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4879 (2.4087)	Acc@1 50.000 (55.391)	Acc@5 80.469 (84.601)
Epoch: [43][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3491 (2.4126)	Acc@1 54.688 (55.254)	Acc@5 84.375 (84.500)
Epoch: [43][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2328 (2.4190)	Acc@1 60.156 (55.109)	Acc@5 82.812 (84.403)
Epoch: [43][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2981 (2.4205)	Acc@1 60.156 (55.112)	Acc@5 84.375 (84.318)
Epoch: [43][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7348 (2.4275)	Acc@1 43.750 (54.896)	Acc@5 78.125 (84.249)
Epoch: [43][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5331 (2.4306)	Acc@1 52.344 (54.806)	Acc@5 80.469 (84.243)
Epoch: [43][180/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4343 (2.4331)	Acc@1 57.812 (54.765)	Acc@5 83.594 (84.198)
Epoch: [43][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3691 (2.4335)	Acc@1 57.812 (54.728)	Acc@5 83.594 (84.281)
Epoch: [43][200/391]	Time 0.012 (0.012)	Data 0.008 (0.002)	Loss 2.3846 (2.4317)	Acc@1 57.031 (54.761)	Acc@5 82.031 (84.297)
Epoch: [43][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3359 (2.4342)	Acc@1 56.250 (54.665)	Acc@5 85.938 (84.268)
Epoch: [43][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4343 (2.4357)	Acc@1 53.125 (54.585)	Acc@5 85.156 (84.219)
Epoch: [43][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3201 (2.4366)	Acc@1 57.031 (54.529)	Acc@5 87.500 (84.169)
Epoch: [43][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3630 (2.4409)	Acc@1 57.031 (54.405)	Acc@5 84.375 (84.096)
Epoch: [43][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1200 (2.4400)	Acc@1 62.500 (54.432)	Acc@5 89.062 (84.154)
Epoch: [43][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5403 (2.4425)	Acc@1 51.562 (54.301)	Acc@5 82.031 (84.067)
Epoch: [43][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3711 (2.4419)	Acc@1 57.031 (54.379)	Acc@5 79.688 (84.055)
Epoch: [43][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4650 (2.4438)	Acc@1 52.344 (54.351)	Acc@5 79.688 (84.027)
Epoch: [43][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6441 (2.4481)	Acc@1 50.000 (54.266)	Acc@5 82.031 (83.967)
Epoch: [43][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5413 (2.4499)	Acc@1 54.688 (54.257)	Acc@5 82.031 (83.921)
Epoch: [43][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5615 (2.4534)	Acc@1 51.562 (54.173)	Acc@5 80.469 (83.895)
Epoch: [43][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4308 (2.4545)	Acc@1 53.125 (54.108)	Acc@5 83.594 (83.866)
Epoch: [43][330/391]	Time 0.022 (0.012)	Data 0.001 (0.002)	Loss 2.5344 (2.4559)	Acc@1 53.906 (54.038)	Acc@5 84.375 (83.872)
Epoch: [43][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6784 (2.4599)	Acc@1 52.344 (53.952)	Acc@5 78.125 (83.791)
Epoch: [43][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2656 (2.4625)	Acc@1 60.938 (53.931)	Acc@5 89.062 (83.756)
Epoch: [43][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6114 (2.4640)	Acc@1 50.781 (53.919)	Acc@5 82.031 (83.711)
Epoch: [43][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4857 (2.4645)	Acc@1 50.781 (53.885)	Acc@5 84.375 (83.729)
Epoch: [43][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4131 (2.4682)	Acc@1 51.562 (53.818)	Acc@5 86.719 (83.657)
Epoch: [43][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.7498 (2.4706)	Acc@1 46.250 (53.800)	Acc@5 78.750 (83.580)
num momentum params: 26
[0.1, 2.4705678170776366, 2.305782355070114, 53.8, 42.38, tensor(0.3212, device='cuda:0', grad_fn=<DivBackward0>), 4.544545412063599, 0.35152220726013184]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [44 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [44][0/391]	Time 0.039 (0.039)	Data 0.156 (0.156)	Loss 2.6184 (2.6184)	Acc@1 54.688 (54.688)	Acc@5 82.031 (82.031)
Epoch: [44][10/391]	Time 0.013 (0.015)	Data 0.001 (0.015)	Loss 2.3959 (2.4228)	Acc@1 53.125 (53.693)	Acc@5 83.594 (85.582)
Epoch: [44][20/391]	Time 0.012 (0.013)	Data 0.001 (0.009)	Loss 2.1982 (2.3957)	Acc@1 60.156 (54.613)	Acc@5 86.719 (85.342)
Epoch: [44][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.5229 (2.3898)	Acc@1 52.344 (55.141)	Acc@5 82.031 (85.433)
Epoch: [44][40/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4541 (2.3804)	Acc@1 53.125 (55.316)	Acc@5 85.938 (85.728)
Epoch: [44][50/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3363 (2.3880)	Acc@1 55.469 (54.856)	Acc@5 89.062 (85.723)
Epoch: [44][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2454 (2.3728)	Acc@1 60.156 (55.020)	Acc@5 86.719 (85.899)
Epoch: [44][70/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 2.4055 (2.3739)	Acc@1 54.688 (55.084)	Acc@5 85.156 (85.783)
Epoch: [44][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4417 (2.3819)	Acc@1 53.906 (55.064)	Acc@5 87.500 (85.600)
Epoch: [44][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2135 (2.3804)	Acc@1 62.500 (55.237)	Acc@5 89.844 (85.482)
Epoch: [44][100/391]	Time 0.016 (0.012)	Data 0.003 (0.003)	Loss 2.1174 (2.3856)	Acc@1 60.938 (55.206)	Acc@5 91.406 (85.404)
Epoch: [44][110/391]	Time 0.011 (0.012)	Data 0.003 (0.003)	Loss 2.3658 (2.3906)	Acc@1 56.250 (55.032)	Acc@5 84.375 (85.325)
Epoch: [44][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.8956 (2.4008)	Acc@1 40.625 (54.759)	Acc@5 76.562 (85.169)
Epoch: [44][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3687 (2.4089)	Acc@1 51.562 (54.622)	Acc@5 85.938 (85.049)
Epoch: [44][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5320 (2.4117)	Acc@1 51.562 (54.660)	Acc@5 80.469 (84.946)
Epoch: [44][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.9627 (2.4236)	Acc@1 39.844 (54.470)	Acc@5 79.688 (84.753)
Epoch: [44][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3790 (2.4226)	Acc@1 55.469 (54.493)	Acc@5 83.594 (84.724)
Epoch: [44][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4959 (2.4194)	Acc@1 53.906 (54.633)	Acc@5 81.250 (84.768)
Epoch: [44][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7851 (2.4299)	Acc@1 47.656 (54.541)	Acc@5 76.562 (84.535)
Epoch: [44][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7527 (2.4382)	Acc@1 42.188 (54.336)	Acc@5 75.781 (84.404)
Epoch: [44][200/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.6117 (2.4400)	Acc@1 49.219 (54.314)	Acc@5 76.562 (84.348)
Epoch: [44][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4501 (2.4388)	Acc@1 51.562 (54.343)	Acc@5 80.469 (84.353)
Epoch: [44][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4792 (2.4403)	Acc@1 54.688 (54.256)	Acc@5 85.938 (84.347)
Epoch: [44][230/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4425 (2.4428)	Acc@1 51.562 (54.251)	Acc@5 85.938 (84.301)
Epoch: [44][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5200 (2.4435)	Acc@1 55.469 (54.217)	Acc@5 82.031 (84.278)
Epoch: [44][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5001 (2.4451)	Acc@1 49.219 (54.161)	Acc@5 79.688 (84.254)
Epoch: [44][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5308 (2.4470)	Acc@1 49.219 (54.095)	Acc@5 83.594 (84.219)
Epoch: [44][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2517 (2.4514)	Acc@1 56.250 (54.048)	Acc@5 89.844 (84.141)
Epoch: [44][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4663 (2.4519)	Acc@1 50.000 (53.967)	Acc@5 85.938 (84.128)
Epoch: [44][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2867 (2.4528)	Acc@1 59.375 (53.955)	Acc@5 86.719 (84.131)
Epoch: [44][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3468 (2.4524)	Acc@1 57.031 (53.992)	Acc@5 87.500 (84.131)
Epoch: [44][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5917 (2.4518)	Acc@1 54.688 (54.067)	Acc@5 82.031 (84.121)
Epoch: [44][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4349 (2.4547)	Acc@1 57.812 (54.038)	Acc@5 82.031 (84.059)
Epoch: [44][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7446 (2.4572)	Acc@1 44.531 (54.010)	Acc@5 79.688 (83.990)
Epoch: [44][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4787 (2.4576)	Acc@1 57.031 (54.019)	Acc@5 85.156 (83.983)
Epoch: [44][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5445 (2.4597)	Acc@1 53.125 (53.993)	Acc@5 78.125 (83.934)
Epoch: [44][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6021 (2.4639)	Acc@1 48.438 (53.900)	Acc@5 84.375 (83.856)
Epoch: [44][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3114 (2.4659)	Acc@1 63.281 (53.870)	Acc@5 85.938 (83.794)
Epoch: [44][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5833 (2.4691)	Acc@1 53.125 (53.773)	Acc@5 78.906 (83.737)
Epoch: [44][390/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4416 (2.4712)	Acc@1 53.750 (53.720)	Acc@5 85.000 (83.672)
num momentum params: 26
[0.1, 2.471225378036499, 1.936445505619049, 53.72, 49.4, tensor(0.3212, device='cuda:0', grad_fn=<DivBackward0>), 4.512543439865112, 0.34511470794677734]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [45 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [45][0/391]	Time 0.046 (0.046)	Data 0.189 (0.189)	Loss 2.2585 (2.2585)	Acc@1 60.938 (60.938)	Acc@5 89.062 (89.062)
Epoch: [45][10/391]	Time 0.013 (0.016)	Data 0.001 (0.018)	Loss 2.1546 (2.3979)	Acc@1 59.375 (53.906)	Acc@5 88.281 (84.659)
Epoch: [45][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.1284 (2.3400)	Acc@1 62.500 (56.213)	Acc@5 89.844 (85.342)
Epoch: [45][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.2664 (2.3210)	Acc@1 60.938 (56.527)	Acc@5 87.500 (85.433)
Epoch: [45][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.5599 (2.3454)	Acc@1 46.094 (55.926)	Acc@5 87.500 (85.309)
Epoch: [45][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5953 (2.3691)	Acc@1 52.344 (55.744)	Acc@5 78.906 (84.926)
Epoch: [45][60/391]	Time 0.012 (0.012)	Data 0.002 (0.004)	Loss 2.4652 (2.3874)	Acc@1 57.812 (55.494)	Acc@5 82.812 (84.682)
Epoch: [45][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5209 (2.3954)	Acc@1 53.906 (55.194)	Acc@5 84.375 (84.595)
Epoch: [45][80/391]	Time 0.013 (0.012)	Data 0.000 (0.004)	Loss 2.6280 (2.4084)	Acc@1 46.875 (54.996)	Acc@5 78.906 (84.375)
Epoch: [45][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3488 (2.4054)	Acc@1 53.125 (55.074)	Acc@5 84.375 (84.478)
Epoch: [45][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3243 (2.3993)	Acc@1 56.250 (55.306)	Acc@5 85.156 (84.568)
Epoch: [45][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4187 (2.4008)	Acc@1 56.250 (55.244)	Acc@5 81.250 (84.495)
Epoch: [45][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3769 (2.4047)	Acc@1 55.469 (55.139)	Acc@5 86.719 (84.407)
Epoch: [45][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4771 (2.4134)	Acc@1 53.125 (55.010)	Acc@5 83.594 (84.238)
Epoch: [45][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4147 (2.4204)	Acc@1 56.250 (54.721)	Acc@5 86.719 (84.181)
Epoch: [45][150/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4779 (2.4181)	Acc@1 56.250 (54.791)	Acc@5 80.469 (84.173)
Epoch: [45][160/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2359 (2.4215)	Acc@1 60.938 (54.770)	Acc@5 86.719 (84.113)
Epoch: [45][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.8119 (2.4237)	Acc@1 46.094 (54.747)	Acc@5 79.688 (84.105)
Epoch: [45][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6430 (2.4283)	Acc@1 50.000 (54.623)	Acc@5 81.250 (84.051)
Epoch: [45][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7023 (2.4299)	Acc@1 46.875 (54.667)	Acc@5 79.688 (84.011)
Epoch: [45][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3924 (2.4335)	Acc@1 55.469 (54.656)	Acc@5 85.938 (83.947)
Epoch: [45][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3764 (2.4344)	Acc@1 56.250 (54.647)	Acc@5 80.469 (83.923)
Epoch: [45][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6154 (2.4355)	Acc@1 53.125 (54.680)	Acc@5 81.250 (83.919)
Epoch: [45][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3533 (2.4400)	Acc@1 56.250 (54.593)	Acc@5 86.719 (83.871)
Epoch: [45][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5900 (2.4421)	Acc@1 50.781 (54.535)	Acc@5 83.594 (83.827)
Epoch: [45][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5103 (2.4450)	Acc@1 53.125 (54.501)	Acc@5 82.812 (83.771)
Epoch: [45][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7118 (2.4459)	Acc@1 50.781 (54.478)	Acc@5 82.812 (83.791)
Epoch: [45][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4097 (2.4503)	Acc@1 56.250 (54.365)	Acc@5 82.812 (83.729)
Epoch: [45][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3232 (2.4506)	Acc@1 57.031 (54.354)	Acc@5 85.156 (83.738)
Epoch: [45][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5758 (2.4505)	Acc@1 53.906 (54.363)	Acc@5 83.594 (83.771)
Epoch: [45][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.0582 (2.4489)	Acc@1 64.062 (54.425)	Acc@5 90.625 (83.822)
Epoch: [45][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5154 (2.4506)	Acc@1 47.656 (54.373)	Acc@5 85.156 (83.802)
Epoch: [45][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6599 (2.4519)	Acc@1 53.125 (54.327)	Acc@5 81.250 (83.818)
Epoch: [45][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5116 (2.4538)	Acc@1 53.125 (54.258)	Acc@5 81.250 (83.775)
Epoch: [45][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3491 (2.4552)	Acc@1 55.469 (54.229)	Acc@5 85.938 (83.770)
Epoch: [45][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5949 (2.4570)	Acc@1 53.906 (54.213)	Acc@5 79.688 (83.732)
Epoch: [45][360/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6346 (2.4568)	Acc@1 44.531 (54.179)	Acc@5 81.250 (83.737)
Epoch: [45][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5594 (2.4577)	Acc@1 52.344 (54.207)	Acc@5 77.344 (83.720)
Epoch: [45][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6284 (2.4592)	Acc@1 49.219 (54.154)	Acc@5 81.250 (83.668)
Epoch: [45][390/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4370 (2.4594)	Acc@1 51.250 (54.168)	Acc@5 82.500 (83.640)
num momentum params: 26
[0.1, 2.4593845095825198, 2.0145000064373018, 54.168, 47.83, tensor(0.3227, device='cuda:0', grad_fn=<DivBackward0>), 4.538224220275879, 0.34119129180908203]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [46 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [46][0/391]	Time 0.047 (0.047)	Data 0.156 (0.156)	Loss 2.3581 (2.3581)	Acc@1 62.500 (62.500)	Acc@5 83.594 (83.594)
Epoch: [46][10/391]	Time 0.012 (0.016)	Data 0.002 (0.015)	Loss 2.5015 (2.4439)	Acc@1 48.438 (53.977)	Acc@5 85.938 (83.310)
Epoch: [46][20/391]	Time 0.011 (0.014)	Data 0.001 (0.009)	Loss 2.2501 (2.3901)	Acc@1 60.156 (55.246)	Acc@5 86.719 (84.896)
Epoch: [46][30/391]	Time 0.012 (0.014)	Data 0.001 (0.006)	Loss 2.3608 (2.3697)	Acc@1 56.250 (55.393)	Acc@5 84.375 (85.156)
Epoch: [46][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.2690 (2.3719)	Acc@1 59.375 (55.469)	Acc@5 85.156 (85.156)
Epoch: [46][50/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 2.5523 (2.3991)	Acc@1 53.906 (54.841)	Acc@5 84.375 (84.743)
Epoch: [46][60/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 2.4295 (2.4004)	Acc@1 51.562 (54.828)	Acc@5 85.156 (84.823)
Epoch: [46][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3498 (2.4031)	Acc@1 61.719 (54.974)	Acc@5 84.375 (84.639)
Epoch: [46][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6384 (2.4037)	Acc@1 50.000 (55.044)	Acc@5 77.344 (84.539)
Epoch: [46][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5057 (2.4144)	Acc@1 49.219 (54.756)	Acc@5 81.250 (84.444)
Epoch: [46][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3891 (2.4140)	Acc@1 55.469 (54.842)	Acc@5 85.156 (84.499)
Epoch: [46][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1039 (2.4158)	Acc@1 60.156 (54.870)	Acc@5 89.062 (84.431)
Epoch: [46][120/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4518 (2.4150)	Acc@1 55.469 (54.720)	Acc@5 79.688 (84.440)
Epoch: [46][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2070 (2.4150)	Acc@1 60.938 (54.741)	Acc@5 85.938 (84.429)
Epoch: [46][140/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7707 (2.4203)	Acc@1 46.875 (54.660)	Acc@5 78.906 (84.381)
Epoch: [46][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5195 (2.4236)	Acc@1 50.781 (54.656)	Acc@5 82.031 (84.323)
Epoch: [46][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6345 (2.4213)	Acc@1 46.875 (54.770)	Acc@5 81.250 (84.312)
Epoch: [46][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4501 (2.4254)	Acc@1 55.469 (54.742)	Acc@5 86.719 (84.206)
Epoch: [46][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3294 (2.4280)	Acc@1 54.688 (54.593)	Acc@5 89.062 (84.189)
Epoch: [46][190/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4892 (2.4331)	Acc@1 50.000 (54.458)	Acc@5 83.594 (84.101)
Epoch: [46][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5277 (2.4322)	Acc@1 54.688 (54.427)	Acc@5 82.812 (84.169)
Epoch: [46][210/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.7322 (2.4395)	Acc@1 44.531 (54.273)	Acc@5 78.906 (84.042)
Epoch: [46][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3563 (2.4401)	Acc@1 53.906 (54.214)	Acc@5 85.938 (84.075)
Epoch: [46][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4100 (2.4389)	Acc@1 53.906 (54.275)	Acc@5 79.688 (84.088)
Epoch: [46][240/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.3224 (2.4427)	Acc@1 57.031 (54.247)	Acc@5 85.938 (83.960)
Epoch: [46][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3429 (2.4430)	Acc@1 55.469 (54.242)	Acc@5 85.938 (83.933)
Epoch: [46][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3444 (2.4436)	Acc@1 57.812 (54.203)	Acc@5 85.156 (83.962)
Epoch: [46][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3387 (2.4472)	Acc@1 57.031 (54.157)	Acc@5 83.594 (83.899)
Epoch: [46][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5452 (2.4487)	Acc@1 50.000 (54.112)	Acc@5 81.250 (83.858)
Epoch: [46][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4237 (2.4494)	Acc@1 51.562 (54.100)	Acc@5 82.812 (83.849)
Epoch: [46][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3385 (2.4530)	Acc@1 55.469 (54.002)	Acc@5 83.594 (83.788)
Epoch: [46][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6194 (2.4519)	Acc@1 50.781 (54.080)	Acc@5 85.156 (83.802)
Epoch: [46][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4181 (2.4514)	Acc@1 54.688 (54.099)	Acc@5 84.375 (83.837)
Epoch: [46][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2822 (2.4493)	Acc@1 62.500 (54.152)	Acc@5 85.938 (83.853)
Epoch: [46][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3733 (2.4515)	Acc@1 52.344 (54.055)	Acc@5 82.812 (83.818)
Epoch: [46][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4426 (2.4545)	Acc@1 57.031 (54.042)	Acc@5 85.156 (83.763)
Epoch: [46][360/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.3847 (2.4552)	Acc@1 59.375 (54.045)	Acc@5 85.938 (83.730)
Epoch: [46][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5358 (2.4574)	Acc@1 51.562 (53.990)	Acc@5 80.469 (83.663)
Epoch: [46][380/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6575 (2.4624)	Acc@1 47.656 (53.845)	Acc@5 85.156 (83.616)
Epoch: [46][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.2958 (2.4617)	Acc@1 57.500 (53.876)	Acc@5 83.750 (83.624)
num momentum params: 26
[0.1, 2.461673495864868, 2.002790246009827, 53.876, 47.87, tensor(0.3220, device='cuda:0', grad_fn=<DivBackward0>), 4.53350305557251, 0.33371806144714355]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [47 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [47][0/391]	Time 0.042 (0.042)	Data 0.157 (0.157)	Loss 2.2565 (2.2565)	Acc@1 57.812 (57.812)	Acc@5 86.719 (86.719)
Epoch: [47][10/391]	Time 0.011 (0.015)	Data 0.002 (0.016)	Loss 2.6147 (2.4155)	Acc@1 46.875 (54.759)	Acc@5 81.250 (84.801)
Epoch: [47][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.4317 (2.3932)	Acc@1 55.469 (55.134)	Acc@5 83.594 (85.342)
Epoch: [47][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.4807 (2.3939)	Acc@1 53.125 (54.914)	Acc@5 89.062 (85.610)
Epoch: [47][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5466 (2.3946)	Acc@1 49.219 (54.897)	Acc@5 82.031 (85.423)
Epoch: [47][50/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.6823 (2.4068)	Acc@1 51.562 (54.902)	Acc@5 82.031 (85.202)
Epoch: [47][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5190 (2.4121)	Acc@1 52.344 (54.892)	Acc@5 82.031 (84.887)
Epoch: [47][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2302 (2.3972)	Acc@1 58.594 (55.238)	Acc@5 89.062 (85.123)
Epoch: [47][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3819 (2.4046)	Acc@1 51.562 (54.977)	Acc@5 83.594 (84.954)
Epoch: [47][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3173 (2.4075)	Acc@1 57.031 (54.919)	Acc@5 85.938 (84.770)
Epoch: [47][100/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 2.3218 (2.4099)	Acc@1 60.938 (54.788)	Acc@5 85.156 (84.638)
Epoch: [47][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2572 (2.4137)	Acc@1 61.719 (54.652)	Acc@5 85.938 (84.466)
Epoch: [47][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2969 (2.4116)	Acc@1 55.469 (54.642)	Acc@5 86.719 (84.485)
Epoch: [47][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5558 (2.4181)	Acc@1 50.781 (54.676)	Acc@5 82.031 (84.381)
Epoch: [47][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3579 (2.4246)	Acc@1 53.125 (54.610)	Acc@5 84.375 (84.242)
Epoch: [47][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4743 (2.4262)	Acc@1 54.688 (54.615)	Acc@5 82.031 (84.209)
Epoch: [47][160/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5584 (2.4278)	Acc@1 57.031 (54.629)	Acc@5 85.938 (84.239)
Epoch: [47][170/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5031 (2.4268)	Acc@1 52.344 (54.669)	Acc@5 85.156 (84.233)
Epoch: [47][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.8386 (2.4319)	Acc@1 45.312 (54.541)	Acc@5 75.781 (84.168)
Epoch: [47][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4865 (2.4357)	Acc@1 56.250 (54.503)	Acc@5 82.812 (84.072)
Epoch: [47][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4430 (2.4389)	Acc@1 56.250 (54.427)	Acc@5 82.031 (84.006)
Epoch: [47][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5183 (2.4411)	Acc@1 55.469 (54.395)	Acc@5 81.250 (83.931)
Epoch: [47][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4389 (2.4411)	Acc@1 56.250 (54.451)	Acc@5 87.500 (83.965)
Epoch: [47][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4268 (2.4419)	Acc@1 54.688 (54.468)	Acc@5 85.938 (83.986)
Epoch: [47][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4958 (2.4448)	Acc@1 54.688 (54.405)	Acc@5 81.250 (83.944)
Epoch: [47][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4234 (2.4489)	Acc@1 58.594 (54.336)	Acc@5 83.594 (83.855)
Epoch: [47][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6256 (2.4544)	Acc@1 47.656 (54.197)	Acc@5 79.688 (83.767)
Epoch: [47][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3318 (2.4551)	Acc@1 56.250 (54.143)	Acc@5 90.625 (83.770)
Epoch: [47][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3540 (2.4536)	Acc@1 55.469 (54.229)	Acc@5 84.375 (83.736)
Epoch: [47][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3494 (2.4525)	Acc@1 54.688 (54.220)	Acc@5 85.938 (83.741)
Epoch: [47][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2590 (2.4524)	Acc@1 54.688 (54.251)	Acc@5 88.281 (83.734)
Epoch: [47][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4561 (2.4519)	Acc@1 53.125 (54.278)	Acc@5 82.031 (83.749)
Epoch: [47][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4374 (2.4506)	Acc@1 53.125 (54.291)	Acc@5 79.688 (83.774)
Epoch: [47][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5549 (2.4510)	Acc@1 51.562 (54.331)	Acc@5 84.375 (83.790)
Epoch: [47][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5634 (2.4523)	Acc@1 50.781 (54.323)	Acc@5 82.812 (83.770)
Epoch: [47][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2618 (2.4535)	Acc@1 58.594 (54.280)	Acc@5 89.062 (83.754)
Epoch: [47][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2133 (2.4525)	Acc@1 63.281 (54.307)	Acc@5 89.844 (83.771)
Epoch: [47][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5028 (2.4533)	Acc@1 50.000 (54.275)	Acc@5 86.719 (83.766)
Epoch: [47][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5745 (2.4543)	Acc@1 52.344 (54.245)	Acc@5 78.906 (83.748)
Epoch: [47][390/391]	Time 0.013 (0.012)	Data 0.000 (0.002)	Loss 2.6456 (2.4546)	Acc@1 57.500 (54.248)	Acc@5 75.000 (83.694)
num momentum params: 26
[0.1, 2.454647757568359, 2.300490711927414, 54.248, 43.53, tensor(0.3224, device='cuda:0', grad_fn=<DivBackward0>), 4.528528213500977, 0.33458638191223145]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [48 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [48][0/391]	Time 0.045 (0.045)	Data 0.151 (0.151)	Loss 2.5555 (2.5555)	Acc@1 48.438 (48.438)	Acc@5 81.250 (81.250)
Epoch: [48][10/391]	Time 0.011 (0.015)	Data 0.001 (0.015)	Loss 2.6807 (2.4114)	Acc@1 48.438 (53.835)	Acc@5 83.594 (84.446)
Epoch: [48][20/391]	Time 0.012 (0.013)	Data 0.001 (0.009)	Loss 2.1454 (2.3973)	Acc@1 64.844 (55.246)	Acc@5 86.719 (84.673)
Epoch: [48][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.5583 (2.3742)	Acc@1 46.875 (55.192)	Acc@5 82.812 (84.929)
Epoch: [48][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.2866 (2.3669)	Acc@1 58.594 (55.507)	Acc@5 86.719 (85.137)
Epoch: [48][50/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4640 (2.3729)	Acc@1 50.000 (55.362)	Acc@5 84.375 (85.080)
Epoch: [48][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2996 (2.3842)	Acc@1 56.250 (55.123)	Acc@5 85.938 (84.823)
Epoch: [48][70/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4994 (2.3891)	Acc@1 49.219 (54.820)	Acc@5 85.156 (84.914)
Epoch: [48][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2609 (2.3978)	Acc@1 53.906 (54.794)	Acc@5 85.938 (84.587)
Epoch: [48][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3801 (2.3995)	Acc@1 54.688 (54.859)	Acc@5 85.156 (84.461)
Epoch: [48][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4859 (2.4082)	Acc@1 58.594 (54.788)	Acc@5 83.594 (84.251)
Epoch: [48][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5042 (2.4156)	Acc@1 45.312 (54.575)	Acc@5 80.469 (84.143)
Epoch: [48][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3138 (2.4137)	Acc@1 60.156 (54.739)	Acc@5 82.031 (84.265)
Epoch: [48][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3638 (2.4136)	Acc@1 56.250 (54.878)	Acc@5 82.812 (84.268)
Epoch: [48][140/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4339 (2.4224)	Acc@1 57.031 (54.715)	Acc@5 81.250 (84.043)
Epoch: [48][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4571 (2.4212)	Acc@1 55.469 (54.750)	Acc@5 78.906 (84.085)
Epoch: [48][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2513 (2.4182)	Acc@1 56.250 (54.809)	Acc@5 87.500 (84.195)
Epoch: [48][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.1712 (2.4180)	Acc@1 59.375 (54.788)	Acc@5 88.281 (84.179)
Epoch: [48][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4700 (2.4223)	Acc@1 53.125 (54.640)	Acc@5 82.812 (84.103)
Epoch: [48][190/391]	Time 0.015 (0.012)	Data 0.001 (0.002)	Loss 2.4191 (2.4229)	Acc@1 58.594 (54.671)	Acc@5 83.594 (84.060)
Epoch: [48][200/391]	Time 0.017 (0.012)	Data 0.001 (0.002)	Loss 2.5110 (2.4215)	Acc@1 54.688 (54.726)	Acc@5 84.375 (84.118)
Epoch: [48][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7371 (2.4291)	Acc@1 46.875 (54.562)	Acc@5 82.031 (84.016)
Epoch: [48][220/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.6582 (2.4326)	Acc@1 49.219 (54.497)	Acc@5 82.031 (83.990)
Epoch: [48][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3940 (2.4349)	Acc@1 53.906 (54.430)	Acc@5 83.594 (83.969)
Epoch: [48][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4213 (2.4346)	Acc@1 57.031 (54.467)	Acc@5 85.156 (83.976)
Epoch: [48][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5392 (2.4351)	Acc@1 55.469 (54.460)	Acc@5 80.469 (83.952)
Epoch: [48][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5081 (2.4352)	Acc@1 57.031 (54.481)	Acc@5 82.031 (83.944)
Epoch: [48][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4208 (2.4352)	Acc@1 55.469 (54.471)	Acc@5 86.719 (83.983)
Epoch: [48][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6352 (2.4394)	Acc@1 53.125 (54.437)	Acc@5 81.250 (83.902)
Epoch: [48][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6100 (2.4421)	Acc@1 49.219 (54.384)	Acc@5 82.812 (83.830)
Epoch: [48][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4769 (2.4447)	Acc@1 53.125 (54.316)	Acc@5 82.812 (83.757)
Epoch: [48][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2925 (2.4432)	Acc@1 60.156 (54.363)	Acc@5 85.156 (83.800)
Epoch: [48][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4865 (2.4449)	Acc@1 54.688 (54.313)	Acc@5 82.812 (83.779)
Epoch: [48][330/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.1854 (2.4460)	Acc@1 60.938 (54.305)	Acc@5 89.844 (83.761)
Epoch: [48][340/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3870 (2.4488)	Acc@1 58.594 (54.282)	Acc@5 79.688 (83.692)
Epoch: [48][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4633 (2.4492)	Acc@1 53.906 (54.276)	Acc@5 80.469 (83.669)
Epoch: [48][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2968 (2.4495)	Acc@1 58.594 (54.300)	Acc@5 85.938 (83.626)
Epoch: [48][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2849 (2.4495)	Acc@1 64.062 (54.296)	Acc@5 85.938 (83.634)
Epoch: [48][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7228 (2.4546)	Acc@1 46.094 (54.173)	Acc@5 75.781 (83.526)
Epoch: [48][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5128 (2.4558)	Acc@1 60.000 (54.166)	Acc@5 85.000 (83.506)
num momentum params: 26
[0.1, 2.455836135635376, 1.9220984208583831, 54.166, 49.21, tensor(0.3225, device='cuda:0', grad_fn=<DivBackward0>), 4.489758729934692, 0.3417975902557373]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [49 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [49][0/391]	Time 0.047 (0.047)	Data 0.160 (0.160)	Loss 2.1599 (2.1599)	Acc@1 61.719 (61.719)	Acc@5 87.500 (87.500)
Epoch: [49][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.5854 (2.3340)	Acc@1 47.656 (57.528)	Acc@5 80.469 (85.298)
Epoch: [49][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.1860 (2.3440)	Acc@1 60.938 (57.329)	Acc@5 88.281 (84.933)
Epoch: [49][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.2755 (2.3530)	Acc@1 57.031 (56.678)	Acc@5 87.500 (85.156)
Epoch: [49][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.7560 (2.3682)	Acc@1 52.344 (56.326)	Acc@5 75.000 (84.813)
Epoch: [49][50/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 2.5159 (2.3960)	Acc@1 49.219 (55.836)	Acc@5 83.594 (84.697)
Epoch: [49][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5526 (2.4006)	Acc@1 52.344 (55.943)	Acc@5 78.906 (84.413)
Epoch: [49][70/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 2.5841 (2.4029)	Acc@1 50.000 (55.733)	Acc@5 83.594 (84.452)
Epoch: [49][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1961 (2.4062)	Acc@1 63.281 (55.835)	Acc@5 89.062 (84.394)
Epoch: [49][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2550 (2.3922)	Acc@1 63.281 (56.379)	Acc@5 85.156 (84.658)
Epoch: [49][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2830 (2.3968)	Acc@1 57.031 (56.126)	Acc@5 88.281 (84.576)
Epoch: [49][110/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.9671 (2.3948)	Acc@1 46.094 (56.166)	Acc@5 75.781 (84.664)
Epoch: [49][120/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.2624 (2.3952)	Acc@1 58.594 (56.089)	Acc@5 84.375 (84.562)
Epoch: [49][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2215 (2.3963)	Acc@1 54.688 (56.023)	Acc@5 88.281 (84.524)
Epoch: [49][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2599 (2.3997)	Acc@1 60.938 (55.884)	Acc@5 85.156 (84.541)
Epoch: [49][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4053 (2.3982)	Acc@1 57.031 (55.991)	Acc@5 84.375 (84.541)
Epoch: [49][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5692 (2.4037)	Acc@1 49.219 (55.852)	Acc@5 76.562 (84.409)
Epoch: [49][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5899 (2.4121)	Acc@1 52.344 (55.665)	Acc@5 82.812 (84.220)
Epoch: [49][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4041 (2.4161)	Acc@1 53.125 (55.521)	Acc@5 87.500 (84.168)
Epoch: [49][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3861 (2.4190)	Acc@1 57.031 (55.379)	Acc@5 82.031 (84.130)
Epoch: [49][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5081 (2.4218)	Acc@1 53.125 (55.290)	Acc@5 79.688 (84.103)
Epoch: [49][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4760 (2.4252)	Acc@1 50.781 (55.265)	Acc@5 81.250 (84.012)
Epoch: [49][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3382 (2.4269)	Acc@1 56.250 (55.225)	Acc@5 87.500 (83.972)
Epoch: [49][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2727 (2.4290)	Acc@1 54.688 (55.110)	Acc@5 89.062 (83.976)
Epoch: [49][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4915 (2.4315)	Acc@1 48.438 (54.924)	Acc@5 78.125 (83.950)
Epoch: [49][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3412 (2.4326)	Acc@1 55.469 (54.856)	Acc@5 82.812 (83.945)
Epoch: [49][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2714 (2.4341)	Acc@1 57.031 (54.852)	Acc@5 89.844 (83.953)
Epoch: [49][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4704 (2.4370)	Acc@1 53.125 (54.768)	Acc@5 82.812 (83.931)
Epoch: [49][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2680 (2.4389)	Acc@1 60.156 (54.790)	Acc@5 85.938 (83.933)
Epoch: [49][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3908 (2.4385)	Acc@1 56.250 (54.779)	Acc@5 83.594 (83.956)
Epoch: [49][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6508 (2.4405)	Acc@1 52.344 (54.760)	Acc@5 81.250 (83.892)
Epoch: [49][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6868 (2.4432)	Acc@1 52.344 (54.700)	Acc@5 76.562 (83.802)
Epoch: [49][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5555 (2.4472)	Acc@1 54.688 (54.622)	Acc@5 79.688 (83.750)
Epoch: [49][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5239 (2.4501)	Acc@1 51.562 (54.569)	Acc@5 81.250 (83.712)
Epoch: [49][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5544 (2.4498)	Acc@1 50.000 (54.534)	Acc@5 80.469 (83.727)
Epoch: [49][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5593 (2.4509)	Acc@1 49.219 (54.494)	Acc@5 83.594 (83.725)
Epoch: [49][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5347 (2.4541)	Acc@1 55.469 (54.430)	Acc@5 82.812 (83.674)
Epoch: [49][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2622 (2.4542)	Acc@1 59.375 (54.397)	Acc@5 85.938 (83.689)
Epoch: [49][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7070 (2.4565)	Acc@1 50.000 (54.355)	Acc@5 75.781 (83.661)
Epoch: [49][390/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3290 (2.4573)	Acc@1 53.750 (54.336)	Acc@5 88.750 (83.636)
num momentum params: 26
[0.1, 2.457307343826294, 2.071308767795563, 54.336, 46.03, tensor(0.3228, device='cuda:0', grad_fn=<DivBackward0>), 4.541147470474243, 0.3482799530029297]
Non Pruning Epoch - module.conv1.weight: [37, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [37]
Non Pruning Epoch - module.bn1.bias: [37]
Non Pruning Epoch - module.conv2.weight: [119, 37, 3, 3]
Non Pruning Epoch - module.bn2.weight: [119]
Non Pruning Epoch - module.bn2.bias: [119]
Non Pruning Epoch - module.conv3.weight: [239, 119, 3, 3]
Non Pruning Epoch - module.bn3.weight: [239]
Non Pruning Epoch - module.bn3.bias: [239]
Non Pruning Epoch - module.conv4.weight: [253, 239, 3, 3]
Non Pruning Epoch - module.bn4.weight: [253]
Non Pruning Epoch - module.bn4.bias: [253]
Non Pruning Epoch - module.conv5.weight: [414, 253, 3, 3]
Non Pruning Epoch - module.bn5.weight: [414]
Non Pruning Epoch - module.bn5.bias: [414]
Non Pruning Epoch - module.conv6.weight: [371, 414, 3, 3]
Non Pruning Epoch - module.bn6.weight: [371]
Non Pruning Epoch - module.bn6.bias: [371]
Non Pruning Epoch - module.conv7.weight: [287, 371, 3, 3]
Non Pruning Epoch - module.bn7.weight: [287]
Non Pruning Epoch - module.bn7.bias: [287]
Non Pruning Epoch - module.conv8.weight: [235, 287, 3, 3]
Non Pruning Epoch - module.bn8.weight: [235]
Non Pruning Epoch - module.bn8.bias: [235]
Non Pruning Epoch - module.fc.weight: [100, 235]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [50 | 180] LR: 0.100000
module.conv1.weight [37, 3, 3, 3]
module.conv2.weight [119, 37, 3, 3]
module.conv3.weight [239, 119, 3, 3]
module.conv4.weight [253, 239, 3, 3]
module.conv5.weight [414, 253, 3, 3]
module.conv6.weight [371, 414, 3, 3]
module.conv7.weight [287, 371, 3, 3]
module.conv8.weight [235, 287, 3, 3]
Epoch: [50][0/391]	Time 0.045 (0.045)	Data 0.150 (0.150)	Loss 2.4373 (2.4373)	Acc@1 53.125 (53.125)	Acc@5 82.031 (82.031)
Epoch: [50][10/391]	Time 0.020 (0.016)	Data 0.001 (0.015)	Loss 2.4765 (2.3922)	Acc@1 51.562 (54.119)	Acc@5 82.812 (84.872)
Epoch: [50][20/391]	Time 0.012 (0.015)	Data 0.001 (0.009)	Loss 2.2614 (2.3631)	Acc@1 57.812 (55.283)	Acc@5 85.156 (85.156)
Epoch: [50][30/391]	Time 0.011 (0.014)	Data 0.001 (0.006)	Loss 2.2118 (2.3504)	Acc@1 66.406 (56.149)	Acc@5 85.156 (85.307)
Epoch: [50][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.4154 (2.3551)	Acc@1 54.688 (56.079)	Acc@5 85.156 (85.347)
Epoch: [50][50/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.4486 (2.3809)	Acc@1 51.562 (55.607)	Acc@5 86.719 (84.957)
Epoch: [50][60/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 2.3971 (2.3844)	Acc@1 53.125 (55.546)	Acc@5 82.812 (84.721)
Epoch: [50][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4436 (2.3931)	Acc@1 52.344 (55.436)	Acc@5 83.594 (84.595)
Epoch: [50][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.0369 (2.3963)	Acc@1 65.625 (55.237)	Acc@5 86.719 (84.606)
Epoch: [50][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3423 (2.4006)	Acc@1 58.594 (55.203)	Acc@5 85.938 (84.650)
Epoch: [50][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4080 (2.4081)	Acc@1 57.031 (55.121)	Acc@5 84.375 (84.383)
Epoch: [50][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3181 (2.4031)	Acc@1 58.594 (55.173)	Acc@5 85.938 (84.509)
Epoch: [50][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4489 (2.4049)	Acc@1 51.562 (55.152)	Acc@5 85.156 (84.440)
Epoch: [50][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4632 (2.4038)	Acc@1 53.125 (55.290)	Acc@5 85.938 (84.441)
Epoch: [50][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5475 (2.4051)	Acc@1 51.562 (55.458)	Acc@5 83.594 (84.392)
Epoch: [50][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5506 (2.4107)	Acc@1 53.906 (55.324)	Acc@5 81.250 (84.303)
Epoch: [50][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5996 (2.4142)	Acc@1 45.312 (55.178)	Acc@5 80.469 (84.239)
Epoch: [50][170/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5130 (2.4183)	Acc@1 52.344 (55.076)	Acc@5 82.812 (84.156)
Epoch: [50][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3610 (2.4195)	Acc@1 54.688 (55.093)	Acc@5 80.469 (84.064)
Epoch: [50][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5845 (2.4200)	Acc@1 42.969 (55.113)	Acc@5 81.250 (83.991)
Epoch: [50][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6098 (2.4208)	Acc@1 51.562 (55.123)	Acc@5 78.125 (83.979)
Epoch: [50][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4398 (2.4207)	Acc@1 53.125 (55.106)	Acc@5 79.688 (83.990)
Epoch: [50][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.1686 (2.4204)	Acc@1 57.812 (55.151)	Acc@5 87.500 (84.053)
Epoch: [50][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6964 (2.4245)	Acc@1 48.438 (55.053)	Acc@5 76.562 (84.003)
Epoch: [50][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2124 (2.4248)	Acc@1 59.375 (55.060)	Acc@5 84.375 (83.970)
Epoch: [50][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2868 (2.4236)	Acc@1 56.250 (55.045)	Acc@5 88.281 (83.958)
Epoch: [50][260/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.3029 (2.4224)	Acc@1 55.469 (55.059)	Acc@5 82.812 (83.953)
Epoch: [50][270/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5118 (2.4273)	Acc@1 49.219 (54.915)	Acc@5 85.156 (83.856)
Epoch: [50][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7069 (2.4281)	Acc@1 49.219 (54.860)	Acc@5 79.688 (83.850)
Epoch: [50][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5659 (2.4306)	Acc@1 52.344 (54.830)	Acc@5 80.469 (83.819)
Epoch: [50][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4681 (2.4340)	Acc@1 53.906 (54.719)	Acc@5 82.812 (83.796)
Epoch: [50][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4973 (2.4363)	Acc@1 48.438 (54.662)	Acc@5 78.125 (83.777)
Epoch: [50][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4040 (2.4386)	Acc@1 54.688 (54.634)	Acc@5 87.500 (83.767)
Epoch: [50][330/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4799 (2.4402)	Acc@1 54.688 (54.607)	Acc@5 82.031 (83.726)
Epoch: [50][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5845 (2.4407)	Acc@1 57.031 (54.612)	Acc@5 78.125 (83.683)
Epoch: [50][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4350 (2.4412)	Acc@1 50.000 (54.565)	Acc@5 85.156 (83.694)
Epoch: [50][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6239 (2.4433)	Acc@1 46.094 (54.497)	Acc@5 79.688 (83.676)
Epoch: [50][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5909 (2.4458)	Acc@1 50.000 (54.407)	Acc@5 79.688 (83.634)
Epoch: [50][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6940 (2.4473)	Acc@1 50.781 (54.398)	Acc@5 78.906 (83.618)
Epoch: [50][390/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.7647 (2.4490)	Acc@1 43.750 (54.378)	Acc@5 76.250 (83.584)
num momentum params: 26
[0.1, 2.44904015335083, 2.0919055700302125, 54.378, 46.3, tensor(0.3234, device='cuda:0', grad_fn=<DivBackward0>), 4.521851301193237, 0.342879056930542]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [37, 3, 3, 3]
Before - module.bn1.weight: [37]
Before - module.bn1.bias: [37]
Before - module.conv2.weight: [119, 37, 3, 3]
Before - module.bn2.weight: [119]
Before - module.bn2.bias: [119]
Before - module.conv3.weight: [239, 119, 3, 3]
Before - module.bn3.weight: [239]
Before - module.bn3.bias: [239]
Before - module.conv4.weight: [253, 239, 3, 3]
Before - module.bn4.weight: [253]
Before - module.bn4.bias: [253]
Before - module.conv5.weight: [414, 253, 3, 3]
Before - module.bn5.weight: [414]
Before - module.bn5.bias: [414]
Before - module.conv6.weight: [371, 414, 3, 3]
Before - module.bn6.weight: [371]
Before - module.bn6.bias: [371]
Before - module.conv7.weight: [287, 371, 3, 3]
Before - module.bn7.weight: [287]
Before - module.bn7.bias: [287]
Before - module.conv8.weight: [235, 287, 3, 3]
Before - module.bn8.weight: [235]
Before - module.bn8.bias: [235]
Before - module.fc.weight: [100, 235]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [37, 3, 3, 3] >> [34, 3, 3, 3]
[module.bn1.weight]: 37 >> 34
running_mean [34]
running_var [34]
num_batches_tracked []
[module.conv2.weight]: [119, 37, 3, 3] >> [115, 34, 3, 3]
[module.bn2.weight]: 119 >> 115
running_mean [115]
running_var [115]
num_batches_tracked []
[module.conv3.weight]: [239, 119, 3, 3] >> [233, 115, 3, 3]
[module.bn3.weight]: 239 >> 233
running_mean [233]
running_var [233]
num_batches_tracked []
[module.conv4.weight]: [253, 239, 3, 3] >> [251, 233, 3, 3]
[module.bn4.weight]: 253 >> 251
running_mean [251]
running_var [251]
num_batches_tracked []
[module.conv5.weight]: [414, 253, 3, 3] >> [390, 251, 3, 3]
[module.bn5.weight]: 414 >> 390
running_mean [390]
running_var [390]
num_batches_tracked []
[module.conv6.weight]: [371, 414, 3, 3] >> [354, 390, 3, 3]
[module.bn6.weight]: 371 >> 354
running_mean [354]
running_var [354]
num_batches_tracked []
[module.conv7.weight]: [287, 371, 3, 3] >> [258, 354, 3, 3]
[module.bn7.weight]: 287 >> 258
running_mean [258]
running_var [258]
num_batches_tracked []
[module.conv8.weight]: [235, 287, 3, 3] >> [220, 258, 3, 3]
[module.bn8.weight]: 235 >> 220
running_mean [220]
running_var [220]
num_batches_tracked []
[module.fc.weight]: [100, 235] >> [100, 220]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [34, 3, 3, 3]
After - module.bn1.weight: [34]
After - module.bn1.bias: [34]
After - module.conv2.weight: [115, 34, 3, 3]
After - module.bn2.weight: [115]
After - module.bn2.bias: [115]
After - module.conv3.weight: [233, 115, 3, 3]
After - module.bn3.weight: [233]
After - module.bn3.bias: [233]
After - module.conv4.weight: [251, 233, 3, 3]
After - module.bn4.weight: [251]
After - module.bn4.bias: [251]
After - module.conv5.weight: [390, 251, 3, 3]
After - module.bn5.weight: [390]
After - module.bn5.bias: [390]
After - module.conv6.weight: [354, 390, 3, 3]
After - module.bn6.weight: [354]
After - module.bn6.bias: [354]
After - module.conv7.weight: [258, 354, 3, 3]
After - module.bn7.weight: [258]
After - module.bn7.bias: [258]
After - module.conv8.weight: [220, 258, 3, 3]
After - module.bn8.weight: [220]
After - module.bn8.bias: [220]
After - module.fc.weight: [100, 220]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [34, 3, 3, 3]
conv2 --> [115, 34, 3, 3]
conv3 --> [233, 115, 3, 3]
conv4 --> [251, 233, 3, 3]
conv5 --> [390, 251, 3, 3]
conv6 --> [354, 390, 3, 3]
conv7 --> [258, 354, 3, 3]
conv8 --> [220, 258, 3, 3]
fc --> [220, 100]
1, 376482816, 940032, 34
2, 3765611520, 9008640, 115
3, 7037867520, 15433920, 233
4, 15360910848, 33686208, 251
5, 7668311040, 14096160, 390
6, 10815068160, 19880640, 354
7, 2525147136, 3287952, 258
8, 1569300480, 2043360, 220
fc, 8448000, 22000, 0
===================
FLOP REPORT: 19190292000000.0 43619200000.0 98398912 109048 1855 8.171201705932617
[INFO] Storing checkpoint...

Epoch: [51 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [51][0/391]	Time 0.273 (0.273)	Data 0.179 (0.179)	Loss 2.0061 (2.0061)	Acc@1 64.844 (64.844)	Acc@5 92.188 (92.188)
Epoch: [51][10/391]	Time 0.014 (0.037)	Data 0.002 (0.017)	Loss 2.4764 (2.3408)	Acc@1 59.375 (57.812)	Acc@5 82.031 (84.162)
Epoch: [51][20/391]	Time 0.013 (0.025)	Data 0.001 (0.010)	Loss 2.3735 (2.3645)	Acc@1 55.469 (57.403)	Acc@5 82.031 (84.077)
Epoch: [51][30/391]	Time 0.011 (0.021)	Data 0.001 (0.007)	Loss 2.4949 (2.3783)	Acc@1 57.031 (57.233)	Acc@5 85.938 (84.098)
Epoch: [51][40/391]	Time 0.012 (0.019)	Data 0.001 (0.006)	Loss 2.3256 (2.3628)	Acc@1 59.375 (57.374)	Acc@5 83.594 (84.470)
Epoch: [51][50/391]	Time 0.011 (0.017)	Data 0.001 (0.005)	Loss 2.7932 (2.3929)	Acc@1 50.781 (56.357)	Acc@5 79.688 (84.069)
Epoch: [51][60/391]	Time 0.011 (0.016)	Data 0.001 (0.004)	Loss 2.4692 (2.4052)	Acc@1 55.469 (55.699)	Acc@5 84.375 (84.106)
Epoch: [51][70/391]	Time 0.012 (0.016)	Data 0.001 (0.004)	Loss 2.2443 (2.4105)	Acc@1 62.500 (55.744)	Acc@5 86.719 (84.067)
Epoch: [51][80/391]	Time 0.011 (0.015)	Data 0.001 (0.004)	Loss 2.5400 (2.4128)	Acc@1 53.906 (55.517)	Acc@5 78.906 (83.980)
Epoch: [51][90/391]	Time 0.011 (0.015)	Data 0.001 (0.003)	Loss 2.3588 (2.4057)	Acc@1 54.688 (55.495)	Acc@5 85.156 (84.160)
Epoch: [51][100/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.6013 (2.4133)	Acc@1 50.781 (55.244)	Acc@5 76.562 (84.019)
Epoch: [51][110/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.5225 (2.4155)	Acc@1 53.125 (55.089)	Acc@5 82.812 (84.009)
Epoch: [51][120/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.3169 (2.4199)	Acc@1 53.906 (54.965)	Acc@5 86.719 (84.013)
Epoch: [51][130/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.5223 (2.4231)	Acc@1 51.562 (54.866)	Acc@5 78.906 (83.850)
Epoch: [51][140/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.5927 (2.4270)	Acc@1 50.000 (54.760)	Acc@5 82.812 (83.810)
Epoch: [51][150/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.6236 (2.4301)	Acc@1 51.562 (54.739)	Acc@5 84.375 (83.723)
Epoch: [51][160/391]	Time 0.010 (0.013)	Data 0.001 (0.002)	Loss 2.5094 (2.4326)	Acc@1 52.344 (54.658)	Acc@5 83.594 (83.744)
Epoch: [51][170/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.3703 (2.4278)	Acc@1 60.938 (54.747)	Acc@5 84.375 (83.877)
Epoch: [51][180/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.6680 (2.4351)	Acc@1 51.562 (54.584)	Acc@5 78.125 (83.771)
Epoch: [51][190/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.6664 (2.4344)	Acc@1 50.000 (54.557)	Acc@5 77.344 (83.831)
Epoch: [51][200/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.3467 (2.4367)	Acc@1 51.562 (54.497)	Acc@5 89.062 (83.811)
Epoch: [51][210/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.4093 (2.4439)	Acc@1 57.031 (54.351)	Acc@5 80.469 (83.709)
Epoch: [51][220/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.6818 (2.4477)	Acc@1 51.562 (54.292)	Acc@5 76.562 (83.618)
Epoch: [51][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5663 (2.4478)	Acc@1 53.906 (54.302)	Acc@5 80.469 (83.655)
Epoch: [51][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5157 (2.4488)	Acc@1 51.562 (54.321)	Acc@5 82.031 (83.620)
Epoch: [51][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1852 (2.4502)	Acc@1 58.594 (54.314)	Acc@5 87.500 (83.603)
Epoch: [51][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7290 (2.4561)	Acc@1 49.219 (54.191)	Acc@5 76.562 (83.558)
Epoch: [51][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7097 (2.4584)	Acc@1 51.562 (54.108)	Acc@5 78.906 (83.519)
Epoch: [51][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4806 (2.4575)	Acc@1 53.125 (54.118)	Acc@5 85.938 (83.541)
Epoch: [51][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2617 (2.4562)	Acc@1 66.406 (54.188)	Acc@5 87.500 (83.583)
Epoch: [51][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3331 (2.4582)	Acc@1 58.594 (54.200)	Acc@5 89.062 (83.570)
Epoch: [51][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4358 (2.4595)	Acc@1 53.906 (54.095)	Acc@5 81.250 (83.581)
Epoch: [51][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3591 (2.4579)	Acc@1 55.469 (54.154)	Acc@5 83.594 (83.606)
Epoch: [51][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4377 (2.4569)	Acc@1 55.469 (54.114)	Acc@5 87.500 (83.662)
Epoch: [51][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4021 (2.4597)	Acc@1 56.250 (54.071)	Acc@5 83.594 (83.630)
Epoch: [51][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5434 (2.4621)	Acc@1 49.219 (54.020)	Acc@5 82.031 (83.605)
Epoch: [51][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3958 (2.4615)	Acc@1 55.469 (54.025)	Acc@5 89.062 (83.661)
Epoch: [51][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3043 (2.4605)	Acc@1 57.812 (54.049)	Acc@5 85.156 (83.665)
Epoch: [51][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5678 (2.4615)	Acc@1 52.344 (54.046)	Acc@5 82.812 (83.684)
Epoch: [51][390/391]	Time 0.156 (0.012)	Data 0.001 (0.002)	Loss 2.3694 (2.4627)	Acc@1 55.000 (54.012)	Acc@5 85.000 (83.670)
num momentum params: 26
[0.1, 2.4627390924072268, 1.9719118237495423, 54.012, 48.64, tensor(0.3224, device='cuda:0', grad_fn=<DivBackward0>), 4.801463603973389, 0.40848851203918457]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [52 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [52][0/391]	Time 0.051 (0.051)	Data 0.169 (0.169)	Loss 2.2134 (2.2134)	Acc@1 59.375 (59.375)	Acc@5 86.719 (86.719)
Epoch: [52][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.3609 (2.3743)	Acc@1 55.469 (55.753)	Acc@5 87.500 (84.233)
Epoch: [52][20/391]	Time 0.011 (0.013)	Data 0.002 (0.009)	Loss 2.6242 (2.3829)	Acc@1 45.312 (55.060)	Acc@5 83.594 (85.045)
Epoch: [52][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.2933 (2.3824)	Acc@1 55.469 (54.965)	Acc@5 84.375 (84.980)
Epoch: [52][40/391]	Time 0.010 (0.012)	Data 0.002 (0.005)	Loss 2.4066 (2.4120)	Acc@1 60.156 (54.916)	Acc@5 84.375 (84.432)
Epoch: [52][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4617 (2.4410)	Acc@1 50.781 (54.182)	Acc@5 84.375 (83.808)
Epoch: [52][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5170 (2.4459)	Acc@1 47.656 (53.804)	Acc@5 85.156 (83.709)
Epoch: [52][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5976 (2.4535)	Acc@1 53.125 (53.708)	Acc@5 82.031 (83.594)
Epoch: [52][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2930 (2.4452)	Acc@1 63.281 (54.003)	Acc@5 84.375 (83.845)
Epoch: [52][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3507 (2.4485)	Acc@1 59.375 (53.915)	Acc@5 85.156 (83.877)
Epoch: [52][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2816 (2.4454)	Acc@1 61.719 (54.169)	Acc@5 85.156 (83.888)
Epoch: [52][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3934 (2.4374)	Acc@1 61.719 (54.659)	Acc@5 82.031 (83.946)
Epoch: [52][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1326 (2.4311)	Acc@1 61.719 (54.855)	Acc@5 90.625 (83.975)
Epoch: [52][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2733 (2.4265)	Acc@1 66.406 (55.099)	Acc@5 85.938 (84.077)
Epoch: [52][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2135 (2.4246)	Acc@1 60.938 (55.142)	Acc@5 88.281 (84.076)
Epoch: [52][150/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.6929 (2.4284)	Acc@1 46.094 (54.905)	Acc@5 85.938 (84.127)
Epoch: [52][160/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4339 (2.4307)	Acc@1 54.688 (54.785)	Acc@5 81.250 (84.060)
Epoch: [52][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3084 (2.4302)	Acc@1 57.031 (54.811)	Acc@5 87.500 (84.078)
Epoch: [52][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5479 (2.4308)	Acc@1 54.688 (54.847)	Acc@5 86.719 (84.120)
Epoch: [52][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6414 (2.4347)	Acc@1 51.562 (54.790)	Acc@5 77.344 (84.040)
Epoch: [52][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5030 (2.4385)	Acc@1 52.344 (54.754)	Acc@5 85.156 (83.940)
Epoch: [52][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3804 (2.4393)	Acc@1 59.375 (54.736)	Acc@5 87.500 (83.960)
Epoch: [52][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3295 (2.4406)	Acc@1 56.250 (54.680)	Acc@5 84.375 (83.958)
Epoch: [52][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4576 (2.4428)	Acc@1 56.250 (54.637)	Acc@5 83.594 (83.902)
Epoch: [52][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6354 (2.4439)	Acc@1 53.125 (54.593)	Acc@5 83.594 (83.889)
Epoch: [52][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4692 (2.4426)	Acc@1 50.000 (54.638)	Acc@5 85.938 (83.927)
Epoch: [52][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4732 (2.4431)	Acc@1 59.375 (54.699)	Acc@5 85.156 (83.914)
Epoch: [52][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7554 (2.4449)	Acc@1 49.219 (54.630)	Acc@5 82.812 (83.911)
Epoch: [52][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4363 (2.4458)	Acc@1 54.688 (54.593)	Acc@5 84.375 (83.894)
Epoch: [52][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1731 (2.4454)	Acc@1 62.500 (54.596)	Acc@5 89.062 (83.878)
Epoch: [52][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4413 (2.4456)	Acc@1 55.469 (54.610)	Acc@5 86.719 (83.874)
Epoch: [52][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4869 (2.4481)	Acc@1 46.875 (54.617)	Acc@5 85.938 (83.855)
Epoch: [52][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3151 (2.4500)	Acc@1 60.156 (54.561)	Acc@5 86.719 (83.847)
Epoch: [52][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5239 (2.4520)	Acc@1 52.344 (54.492)	Acc@5 83.594 (83.820)
Epoch: [52][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.8331 (2.4526)	Acc@1 41.406 (54.509)	Acc@5 80.469 (83.795)
Epoch: [52][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5394 (2.4533)	Acc@1 52.344 (54.463)	Acc@5 81.250 (83.765)
Epoch: [52][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6764 (2.4542)	Acc@1 46.094 (54.417)	Acc@5 79.688 (83.750)
Epoch: [52][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4674 (2.4554)	Acc@1 55.469 (54.426)	Acc@5 80.469 (83.718)
Epoch: [52][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4743 (2.4555)	Acc@1 56.250 (54.468)	Acc@5 81.250 (83.719)
Epoch: [52][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.3295 (2.4567)	Acc@1 65.000 (54.446)	Acc@5 83.750 (83.712)
num momentum params: 26
[0.1, 2.4566631399536134, 1.9161315059661865, 54.446, 50.02, tensor(0.3237, device='cuda:0', grad_fn=<DivBackward0>), 4.454327821731567, 0.34357476234436035]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [53 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [53][0/391]	Time 0.046 (0.046)	Data 0.171 (0.171)	Loss 2.5022 (2.5022)	Acc@1 50.000 (50.000)	Acc@5 84.375 (84.375)
Epoch: [53][10/391]	Time 0.010 (0.015)	Data 0.001 (0.017)	Loss 2.4244 (2.3161)	Acc@1 57.031 (56.747)	Acc@5 82.812 (86.364)
Epoch: [53][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.1825 (2.2911)	Acc@1 61.719 (57.068)	Acc@5 85.938 (86.198)
Epoch: [53][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.1501 (2.2736)	Acc@1 59.375 (58.140)	Acc@5 87.500 (86.316)
Epoch: [53][40/391]	Time 0.012 (0.012)	Data 0.001 (0.006)	Loss 2.3472 (2.3116)	Acc@1 57.031 (57.546)	Acc@5 83.594 (86.109)
Epoch: [53][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4614 (2.3390)	Acc@1 51.562 (56.388)	Acc@5 85.938 (85.784)
Epoch: [53][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5469 (2.3610)	Acc@1 53.906 (56.135)	Acc@5 79.688 (85.412)
Epoch: [53][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2971 (2.3664)	Acc@1 58.594 (55.986)	Acc@5 88.281 (85.464)
Epoch: [53][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4964 (2.3678)	Acc@1 53.906 (55.961)	Acc@5 81.250 (85.340)
Epoch: [53][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4879 (2.3796)	Acc@1 51.562 (55.786)	Acc@5 84.375 (85.173)
Epoch: [53][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2471 (2.3786)	Acc@1 62.500 (56.002)	Acc@5 86.719 (85.272)
Epoch: [53][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4529 (2.3840)	Acc@1 47.656 (55.807)	Acc@5 86.719 (85.206)
Epoch: [53][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6257 (2.3863)	Acc@1 54.688 (55.792)	Acc@5 76.562 (85.137)
Epoch: [53][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3222 (2.3965)	Acc@1 56.250 (55.534)	Acc@5 83.594 (84.864)
Epoch: [53][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2867 (2.4017)	Acc@1 60.156 (55.502)	Acc@5 87.500 (84.835)
Epoch: [53][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3897 (2.4098)	Acc@1 53.906 (55.262)	Acc@5 85.156 (84.634)
Epoch: [53][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4576 (2.4095)	Acc@1 54.688 (55.236)	Acc@5 85.156 (84.676)
Epoch: [53][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5557 (2.4148)	Acc@1 55.469 (55.185)	Acc@5 82.812 (84.549)
Epoch: [53][180/391]	Time 0.012 (0.012)	Data 0.004 (0.002)	Loss 2.3247 (2.4200)	Acc@1 58.594 (55.167)	Acc@5 85.156 (84.401)
Epoch: [53][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3410 (2.4213)	Acc@1 58.594 (55.133)	Acc@5 86.719 (84.420)
Epoch: [53][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3282 (2.4179)	Acc@1 55.469 (55.177)	Acc@5 85.156 (84.457)
Epoch: [53][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5061 (2.4184)	Acc@1 51.562 (55.150)	Acc@5 82.812 (84.512)
Epoch: [53][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4499 (2.4192)	Acc@1 56.250 (55.115)	Acc@5 82.812 (84.478)
Epoch: [53][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4011 (2.4213)	Acc@1 59.375 (55.019)	Acc@5 82.812 (84.483)
Epoch: [53][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7719 (2.4264)	Acc@1 45.312 (54.976)	Acc@5 80.469 (84.401)
Epoch: [53][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2991 (2.4279)	Acc@1 64.062 (54.999)	Acc@5 84.375 (84.331)
Epoch: [53][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7946 (2.4322)	Acc@1 47.656 (54.897)	Acc@5 72.656 (84.246)
Epoch: [53][270/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 2.5677 (2.4359)	Acc@1 50.000 (54.791)	Acc@5 84.375 (84.162)
Epoch: [53][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3781 (2.4370)	Acc@1 57.812 (54.735)	Acc@5 81.250 (84.147)
Epoch: [53][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6160 (2.4420)	Acc@1 48.438 (54.631)	Acc@5 81.250 (84.066)
Epoch: [53][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4114 (2.4451)	Acc@1 54.688 (54.578)	Acc@5 85.156 (84.012)
Epoch: [53][310/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.2835 (2.4433)	Acc@1 61.719 (54.635)	Acc@5 84.375 (84.023)
Epoch: [53][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6208 (2.4445)	Acc@1 55.469 (54.624)	Acc@5 80.469 (84.000)
Epoch: [53][330/391]	Time 0.015 (0.012)	Data 0.001 (0.002)	Loss 2.2181 (2.4431)	Acc@1 66.406 (54.680)	Acc@5 85.938 (84.014)
Epoch: [53][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6261 (2.4424)	Acc@1 52.344 (54.706)	Acc@5 81.250 (84.020)
Epoch: [53][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4288 (2.4437)	Acc@1 57.031 (54.667)	Acc@5 80.469 (83.974)
Epoch: [53][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6583 (2.4476)	Acc@1 52.344 (54.560)	Acc@5 73.438 (83.879)
Epoch: [53][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4085 (2.4495)	Acc@1 57.031 (54.513)	Acc@5 84.375 (83.836)
Epoch: [53][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3156 (2.4482)	Acc@1 57.812 (54.593)	Acc@5 86.719 (83.830)
Epoch: [53][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.3943 (2.4478)	Acc@1 56.250 (54.570)	Acc@5 82.500 (83.868)
num momentum params: 26
[0.1, 2.4477551371765136, 1.8421037006378174, 54.57, 50.8, tensor(0.3246, device='cuda:0', grad_fn=<DivBackward0>), 4.469611644744873, 0.3321506977081299]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [54 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [54][0/391]	Time 0.044 (0.044)	Data 0.160 (0.160)	Loss 2.3184 (2.3184)	Acc@1 57.812 (57.812)	Acc@5 86.719 (86.719)
Epoch: [54][10/391]	Time 0.010 (0.015)	Data 0.001 (0.016)	Loss 2.2000 (2.3676)	Acc@1 58.594 (56.392)	Acc@5 89.844 (85.440)
Epoch: [54][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.6542 (2.3503)	Acc@1 48.438 (56.436)	Acc@5 84.375 (85.975)
Epoch: [54][30/391]	Time 0.011 (0.012)	Data 0.001 (0.007)	Loss 2.2189 (2.3280)	Acc@1 61.719 (57.182)	Acc@5 88.281 (86.265)
Epoch: [54][40/391]	Time 0.010 (0.012)	Data 0.002 (0.005)	Loss 2.6531 (2.3504)	Acc@1 51.562 (56.879)	Acc@5 78.906 (85.785)
Epoch: [54][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5254 (2.3547)	Acc@1 51.562 (56.587)	Acc@5 78.125 (85.432)
Epoch: [54][60/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 2.4658 (2.3632)	Acc@1 50.781 (56.365)	Acc@5 85.156 (85.374)
Epoch: [54][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5403 (2.3834)	Acc@1 51.562 (56.063)	Acc@5 83.594 (85.068)
Epoch: [54][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5017 (2.3800)	Acc@1 54.688 (56.125)	Acc@5 82.031 (84.944)
Epoch: [54][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.7243 (2.3929)	Acc@1 45.312 (55.786)	Acc@5 81.250 (84.839)
Epoch: [54][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.0869 (2.3967)	Acc@1 60.938 (55.685)	Acc@5 88.281 (84.824)
Epoch: [54][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.6813 (2.4008)	Acc@1 48.438 (55.539)	Acc@5 82.031 (84.797)
Epoch: [54][120/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3689 (2.4081)	Acc@1 53.906 (55.424)	Acc@5 89.062 (84.737)
Epoch: [54][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5130 (2.4062)	Acc@1 52.344 (55.433)	Acc@5 83.594 (84.751)
Epoch: [54][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4007 (2.4081)	Acc@1 54.688 (55.458)	Acc@5 85.156 (84.691)
Epoch: [54][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4961 (2.4140)	Acc@1 53.906 (55.257)	Acc@5 83.594 (84.613)
Epoch: [54][160/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2578 (2.4142)	Acc@1 57.812 (55.265)	Acc@5 87.500 (84.550)
Epoch: [54][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2484 (2.4229)	Acc@1 54.688 (55.112)	Acc@5 89.844 (84.398)
Epoch: [54][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6795 (2.4275)	Acc@1 49.219 (55.054)	Acc@5 79.688 (84.258)
Epoch: [54][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4577 (2.4277)	Acc@1 57.812 (54.998)	Acc@5 82.031 (84.236)
Epoch: [54][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4049 (2.4242)	Acc@1 54.688 (55.010)	Acc@5 84.375 (84.305)
Epoch: [54][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5228 (2.4228)	Acc@1 56.250 (55.106)	Acc@5 82.812 (84.279)
Epoch: [54][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5474 (2.4247)	Acc@1 53.906 (54.999)	Acc@5 79.688 (84.269)
Epoch: [54][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5592 (2.4274)	Acc@1 52.344 (54.897)	Acc@5 80.469 (84.226)
Epoch: [54][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4733 (2.4316)	Acc@1 57.031 (54.775)	Acc@5 86.719 (84.203)
Epoch: [54][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3666 (2.4290)	Acc@1 56.250 (54.812)	Acc@5 87.500 (84.254)
Epoch: [54][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8050 (2.4295)	Acc@1 51.562 (54.801)	Acc@5 75.781 (84.285)
Epoch: [54][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4109 (2.4295)	Acc@1 58.594 (54.814)	Acc@5 85.938 (84.257)
Epoch: [54][280/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.4943 (2.4328)	Acc@1 53.906 (54.676)	Acc@5 82.812 (84.189)
Epoch: [54][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3221 (2.4380)	Acc@1 59.375 (54.588)	Acc@5 85.938 (84.125)
Epoch: [54][300/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.5198 (2.4411)	Acc@1 50.000 (54.490)	Acc@5 78.906 (84.012)
Epoch: [54][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7877 (2.4444)	Acc@1 42.188 (54.466)	Acc@5 76.562 (83.955)
Epoch: [54][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3830 (2.4440)	Acc@1 49.219 (54.476)	Acc@5 89.844 (83.986)
Epoch: [54][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1733 (2.4406)	Acc@1 64.062 (54.569)	Acc@5 84.375 (84.002)
Epoch: [54][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5553 (2.4391)	Acc@1 51.562 (54.612)	Acc@5 77.344 (84.020)
Epoch: [54][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5109 (2.4392)	Acc@1 51.562 (54.576)	Acc@5 82.031 (84.023)
Epoch: [54][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5490 (2.4410)	Acc@1 49.219 (54.540)	Acc@5 87.500 (84.011)
Epoch: [54][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5614 (2.4441)	Acc@1 54.688 (54.418)	Acc@5 79.688 (83.952)
Epoch: [54][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6570 (2.4453)	Acc@1 53.125 (54.439)	Acc@5 78.125 (83.922)
Epoch: [54][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3290 (2.4463)	Acc@1 61.250 (54.418)	Acc@5 83.750 (83.908)
num momentum params: 26
[0.1, 2.446296314239502, 2.1458134758472442, 54.418, 45.48, tensor(0.3252, device='cuda:0', grad_fn=<DivBackward0>), 4.462766885757446, 0.33736109733581543]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [55 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [55][0/391]	Time 0.047 (0.047)	Data 0.153 (0.153)	Loss 2.4296 (2.4296)	Acc@1 51.562 (51.562)	Acc@5 83.594 (83.594)
Epoch: [55][10/391]	Time 0.012 (0.015)	Data 0.001 (0.015)	Loss 2.3036 (2.3795)	Acc@1 56.250 (56.392)	Acc@5 85.156 (84.162)
Epoch: [55][20/391]	Time 0.010 (0.013)	Data 0.001 (0.009)	Loss 2.2419 (2.3543)	Acc@1 57.812 (56.845)	Acc@5 86.719 (84.821)
Epoch: [55][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.3857 (2.3547)	Acc@1 57.812 (57.006)	Acc@5 85.938 (84.854)
Epoch: [55][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.4684 (2.3595)	Acc@1 57.812 (57.050)	Acc@5 79.688 (84.699)
Epoch: [55][50/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2858 (2.3616)	Acc@1 57.812 (57.031)	Acc@5 86.719 (84.972)
Epoch: [55][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3920 (2.3614)	Acc@1 55.469 (57.018)	Acc@5 86.719 (84.977)
Epoch: [55][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3634 (2.3713)	Acc@1 51.562 (56.778)	Acc@5 87.500 (84.870)
Epoch: [55][80/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.4444 (2.3677)	Acc@1 50.781 (56.713)	Acc@5 82.031 (84.925)
Epoch: [55][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5617 (2.3589)	Acc@1 46.875 (56.782)	Acc@5 82.031 (85.191)
Epoch: [55][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3285 (2.3667)	Acc@1 52.344 (56.629)	Acc@5 83.594 (84.862)
Epoch: [55][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.7511 (2.3750)	Acc@1 49.219 (56.426)	Acc@5 75.781 (84.741)
Epoch: [55][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3660 (2.3781)	Acc@1 57.812 (56.405)	Acc@5 85.156 (84.646)
Epoch: [55][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4033 (2.3853)	Acc@1 53.906 (56.178)	Acc@5 87.500 (84.542)
Epoch: [55][140/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3881 (2.3942)	Acc@1 54.688 (56.034)	Acc@5 89.062 (84.375)
Epoch: [55][150/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3489 (2.3968)	Acc@1 53.125 (55.914)	Acc@5 85.156 (84.354)
Epoch: [55][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8335 (2.4018)	Acc@1 48.438 (55.808)	Acc@5 73.438 (84.249)
Epoch: [55][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4071 (2.4085)	Acc@1 52.344 (55.674)	Acc@5 85.156 (84.179)
Epoch: [55][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4110 (2.4111)	Acc@1 53.125 (55.529)	Acc@5 85.938 (84.176)
Epoch: [55][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3360 (2.4157)	Acc@1 51.562 (55.452)	Acc@5 86.719 (84.093)
Epoch: [55][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2391 (2.4155)	Acc@1 60.156 (55.465)	Acc@5 87.500 (84.080)
Epoch: [55][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4545 (2.4111)	Acc@1 53.125 (55.580)	Acc@5 84.375 (84.194)
Epoch: [55][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2940 (2.4138)	Acc@1 57.031 (55.479)	Acc@5 91.406 (84.166)
Epoch: [55][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2734 (2.4151)	Acc@1 58.594 (55.404)	Acc@5 83.594 (84.165)
Epoch: [55][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3601 (2.4194)	Acc@1 61.719 (55.320)	Acc@5 82.031 (84.103)
Epoch: [55][250/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.5619 (2.4199)	Acc@1 51.562 (55.335)	Acc@5 85.156 (84.145)
Epoch: [55][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4218 (2.4253)	Acc@1 55.469 (55.241)	Acc@5 84.375 (84.040)
Epoch: [55][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4459 (2.4250)	Acc@1 49.219 (55.227)	Acc@5 80.469 (84.061)
Epoch: [55][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2710 (2.4260)	Acc@1 61.719 (55.132)	Acc@5 86.719 (84.058)
Epoch: [55][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5068 (2.4241)	Acc@1 60.156 (55.184)	Acc@5 80.469 (84.069)
Epoch: [55][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2988 (2.4268)	Acc@1 54.688 (55.100)	Acc@5 83.594 (84.019)
Epoch: [55][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5270 (2.4281)	Acc@1 53.906 (55.099)	Acc@5 82.812 (83.986)
Epoch: [55][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3473 (2.4277)	Acc@1 57.031 (55.143)	Acc@5 84.375 (83.969)
Epoch: [55][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6787 (2.4270)	Acc@1 48.438 (55.167)	Acc@5 82.031 (83.990)
Epoch: [55][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3367 (2.4289)	Acc@1 56.250 (55.141)	Acc@5 85.156 (83.965)
Epoch: [55][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4083 (2.4284)	Acc@1 53.906 (55.162)	Acc@5 78.125 (83.997)
Epoch: [55][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7897 (2.4308)	Acc@1 49.219 (55.086)	Acc@5 78.906 (83.968)
Epoch: [55][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3968 (2.4298)	Acc@1 56.250 (55.119)	Acc@5 82.812 (83.996)
Epoch: [55][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6226 (2.4321)	Acc@1 50.000 (55.073)	Acc@5 80.469 (83.953)
Epoch: [55][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4860 (2.4338)	Acc@1 53.750 (55.050)	Acc@5 80.000 (83.926)
num momentum params: 26
[0.1, 2.4337577454376222, 2.1895566999912264, 55.05, 43.97, tensor(0.3266, device='cuda:0', grad_fn=<DivBackward0>), 4.439136505126953, 0.3450436592102051]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [56 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [56][0/391]	Time 0.045 (0.045)	Data 0.166 (0.166)	Loss 2.5495 (2.5495)	Acc@1 48.438 (48.438)	Acc@5 82.031 (82.031)
Epoch: [56][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.5274 (2.4526)	Acc@1 53.125 (54.474)	Acc@5 77.344 (82.955)
Epoch: [56][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.2248 (2.3994)	Acc@1 62.500 (55.692)	Acc@5 91.406 (83.929)
Epoch: [56][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.2727 (2.3856)	Acc@1 60.938 (56.200)	Acc@5 85.938 (84.274)
Epoch: [56][40/391]	Time 0.014 (0.013)	Data 0.001 (0.005)	Loss 2.1365 (2.3806)	Acc@1 59.375 (55.774)	Acc@5 92.188 (84.356)
Epoch: [56][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5279 (2.3863)	Acc@1 48.438 (56.005)	Acc@5 82.812 (84.360)
Epoch: [56][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4947 (2.3968)	Acc@1 53.125 (55.635)	Acc@5 85.156 (84.375)
Epoch: [56][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4351 (2.4005)	Acc@1 60.156 (55.568)	Acc@5 82.812 (84.331)
Epoch: [56][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6056 (2.4109)	Acc@1 46.094 (55.122)	Acc@5 84.375 (84.288)
Epoch: [56][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4223 (2.4108)	Acc@1 49.219 (55.057)	Acc@5 86.719 (84.246)
Epoch: [56][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2388 (2.4120)	Acc@1 57.031 (54.989)	Acc@5 84.375 (84.321)
Epoch: [56][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4337 (2.4059)	Acc@1 58.594 (55.159)	Acc@5 80.469 (84.340)
Epoch: [56][120/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.3692 (2.4010)	Acc@1 58.594 (55.346)	Acc@5 85.156 (84.524)
Epoch: [56][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4142 (2.4062)	Acc@1 53.906 (55.278)	Acc@5 86.719 (84.381)
Epoch: [56][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3779 (2.4125)	Acc@1 50.000 (54.992)	Acc@5 85.938 (84.331)
Epoch: [56][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.9670 (2.4208)	Acc@1 40.625 (54.744)	Acc@5 73.438 (84.235)
Epoch: [56][160/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4423 (2.4211)	Acc@1 47.656 (54.775)	Acc@5 82.812 (84.254)
Epoch: [56][170/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4970 (2.4220)	Acc@1 55.469 (54.857)	Acc@5 85.938 (84.306)
Epoch: [56][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3265 (2.4182)	Acc@1 53.906 (54.998)	Acc@5 83.594 (84.353)
Epoch: [56][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6170 (2.4228)	Acc@1 50.781 (54.855)	Acc@5 78.906 (84.273)
Epoch: [56][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4154 (2.4252)	Acc@1 58.594 (54.847)	Acc@5 82.812 (84.227)
Epoch: [56][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7768 (2.4277)	Acc@1 45.312 (54.754)	Acc@5 75.000 (84.171)
Epoch: [56][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4415 (2.4304)	Acc@1 54.688 (54.709)	Acc@5 84.375 (84.113)
Epoch: [56][230/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 2.4626 (2.4331)	Acc@1 57.031 (54.650)	Acc@5 79.688 (84.101)
Epoch: [56][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4809 (2.4347)	Acc@1 57.031 (54.623)	Acc@5 83.594 (84.093)
Epoch: [56][250/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2525 (2.4299)	Acc@1 59.375 (54.753)	Acc@5 87.500 (84.173)
Epoch: [56][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3318 (2.4312)	Acc@1 60.156 (54.780)	Acc@5 86.719 (84.136)
Epoch: [56][270/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.3976 (2.4314)	Acc@1 59.375 (54.768)	Acc@5 82.812 (84.104)
Epoch: [56][280/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.4782 (2.4340)	Acc@1 55.469 (54.726)	Acc@5 82.031 (84.075)
Epoch: [56][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2935 (2.4351)	Acc@1 57.812 (54.661)	Acc@5 86.719 (84.037)
Epoch: [56][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6654 (2.4351)	Acc@1 53.125 (54.739)	Acc@5 82.812 (84.032)
Epoch: [56][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3801 (2.4364)	Acc@1 57.812 (54.695)	Acc@5 84.375 (83.996)
Epoch: [56][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4422 (2.4360)	Acc@1 53.125 (54.670)	Acc@5 86.719 (84.005)
Epoch: [56][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6237 (2.4399)	Acc@1 50.000 (54.586)	Acc@5 82.812 (83.969)
Epoch: [56][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3122 (2.4395)	Acc@1 57.031 (54.594)	Acc@5 83.594 (83.951)
Epoch: [56][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.9329 (2.4405)	Acc@1 46.094 (54.601)	Acc@5 77.344 (83.941)
Epoch: [56][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4231 (2.4400)	Acc@1 51.562 (54.586)	Acc@5 84.375 (83.966)
Epoch: [56][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5148 (2.4396)	Acc@1 54.688 (54.620)	Acc@5 83.594 (83.948)
Epoch: [56][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3006 (2.4431)	Acc@1 58.594 (54.564)	Acc@5 86.719 (83.885)
Epoch: [56][390/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6716 (2.4463)	Acc@1 48.750 (54.492)	Acc@5 82.500 (83.812)
num momentum params: 26
[0.1, 2.4463437895202635, 2.2117366802692415, 54.492, 44.34, tensor(0.3252, device='cuda:0', grad_fn=<DivBackward0>), 4.517189264297485, 0.33930468559265137]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [57 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [57][0/391]	Time 0.046 (0.046)	Data 0.167 (0.167)	Loss 2.3058 (2.3058)	Acc@1 57.812 (57.812)	Acc@5 84.375 (84.375)
Epoch: [57][10/391]	Time 0.013 (0.015)	Data 0.001 (0.016)	Loss 2.2610 (2.4155)	Acc@1 57.812 (56.108)	Acc@5 87.500 (82.955)
Epoch: [57][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.4132 (2.3590)	Acc@1 53.906 (57.366)	Acc@5 82.812 (83.854)
Epoch: [57][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.1247 (2.3408)	Acc@1 57.812 (57.182)	Acc@5 89.844 (84.476)
Epoch: [57][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.1729 (2.3545)	Acc@1 57.031 (56.574)	Acc@5 86.719 (84.566)
Epoch: [57][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2383 (2.3591)	Acc@1 64.062 (56.633)	Acc@5 89.062 (84.666)
Epoch: [57][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3586 (2.3579)	Acc@1 57.031 (56.596)	Acc@5 84.375 (84.670)
Epoch: [57][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.1548 (2.3531)	Acc@1 64.062 (56.756)	Acc@5 89.844 (84.738)
Epoch: [57][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2593 (2.3593)	Acc@1 60.938 (56.617)	Acc@5 85.938 (84.761)
Epoch: [57][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3133 (2.3593)	Acc@1 60.156 (56.525)	Acc@5 85.156 (84.796)
Epoch: [57][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3801 (2.3612)	Acc@1 56.250 (56.505)	Acc@5 87.500 (84.940)
Epoch: [57][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1967 (2.3584)	Acc@1 66.406 (56.609)	Acc@5 85.938 (85.030)
Epoch: [57][120/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.3783 (2.3641)	Acc@1 53.125 (56.373)	Acc@5 89.062 (85.059)
Epoch: [57][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4561 (2.3764)	Acc@1 54.688 (56.232)	Acc@5 83.594 (84.858)
Epoch: [57][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3255 (2.3801)	Acc@1 59.375 (56.161)	Acc@5 83.594 (84.840)
Epoch: [57][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3683 (2.3870)	Acc@1 54.688 (55.955)	Acc@5 85.938 (84.804)
Epoch: [57][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2960 (2.3900)	Acc@1 61.719 (55.886)	Acc@5 84.375 (84.690)
Epoch: [57][170/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.4600 (2.3958)	Acc@1 52.344 (55.674)	Acc@5 80.469 (84.599)
Epoch: [57][180/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 2.4792 (2.3998)	Acc@1 52.344 (55.581)	Acc@5 83.594 (84.569)
Epoch: [57][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4194 (2.4001)	Acc@1 54.688 (55.596)	Acc@5 82.031 (84.543)
Epoch: [57][200/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.3894 (2.4056)	Acc@1 61.719 (55.492)	Acc@5 83.594 (84.457)
Epoch: [57][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4181 (2.4054)	Acc@1 54.688 (55.506)	Acc@5 86.719 (84.527)
Epoch: [57][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.1834 (2.4039)	Acc@1 57.031 (55.515)	Acc@5 88.281 (84.502)
Epoch: [57][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4450 (2.4047)	Acc@1 58.594 (55.536)	Acc@5 83.594 (84.476)
Epoch: [57][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4982 (2.4076)	Acc@1 53.906 (55.472)	Acc@5 83.594 (84.427)
Epoch: [57][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4513 (2.4138)	Acc@1 60.156 (55.357)	Acc@5 85.156 (84.369)
Epoch: [57][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4058 (2.4127)	Acc@1 52.344 (55.385)	Acc@5 88.281 (84.405)
Epoch: [57][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5083 (2.4147)	Acc@1 55.469 (55.345)	Acc@5 82.812 (84.363)
Epoch: [57][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4834 (2.4157)	Acc@1 53.125 (55.316)	Acc@5 84.375 (84.353)
Epoch: [57][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4809 (2.4151)	Acc@1 53.906 (55.335)	Acc@5 79.688 (84.364)
Epoch: [57][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1739 (2.4129)	Acc@1 65.625 (55.406)	Acc@5 86.719 (84.414)
Epoch: [57][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5408 (2.4152)	Acc@1 57.812 (55.398)	Acc@5 80.469 (84.347)
Epoch: [57][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2668 (2.4172)	Acc@1 60.156 (55.364)	Acc@5 85.938 (84.295)
Epoch: [57][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3606 (2.4189)	Acc@1 59.375 (55.337)	Acc@5 88.281 (84.264)
Epoch: [57][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4624 (2.4200)	Acc@1 49.219 (55.256)	Acc@5 85.156 (84.260)
Epoch: [57][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2944 (2.4207)	Acc@1 56.250 (55.246)	Acc@5 85.156 (84.264)
Epoch: [57][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5759 (2.4238)	Acc@1 55.469 (55.213)	Acc@5 82.031 (84.195)
Epoch: [57][370/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4267 (2.4245)	Acc@1 59.375 (55.212)	Acc@5 85.938 (84.167)
Epoch: [57][380/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2981 (2.4244)	Acc@1 57.812 (55.171)	Acc@5 83.594 (84.168)
Epoch: [57][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3414 (2.4263)	Acc@1 61.250 (55.054)	Acc@5 85.000 (84.146)
num momentum params: 26
[0.1, 2.4262843798828126, 2.1983074820041657, 55.054, 44.89, tensor(0.3272, device='cuda:0', grad_fn=<DivBackward0>), 4.476799726486206, 0.3409407138824463]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [58 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [58][0/391]	Time 0.045 (0.045)	Data 0.163 (0.163)	Loss 2.5563 (2.5563)	Acc@1 57.031 (57.031)	Acc@5 79.688 (79.688)
Epoch: [58][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.4389 (2.3724)	Acc@1 54.688 (55.185)	Acc@5 85.938 (84.872)
Epoch: [58][20/391]	Time 0.011 (0.014)	Data 0.001 (0.009)	Loss 2.3227 (2.4078)	Acc@1 53.906 (54.911)	Acc@5 85.938 (84.263)
Epoch: [58][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.1764 (2.3906)	Acc@1 62.500 (55.721)	Acc@5 89.062 (84.551)
Epoch: [58][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4769 (2.3836)	Acc@1 54.688 (56.250)	Acc@5 83.594 (84.585)
Epoch: [58][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.2420 (2.3925)	Acc@1 57.031 (55.729)	Acc@5 89.062 (84.329)
Epoch: [58][60/391]	Time 0.020 (0.012)	Data 0.001 (0.004)	Loss 2.3325 (2.3940)	Acc@1 58.594 (55.635)	Acc@5 85.156 (84.247)
Epoch: [58][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4258 (2.3955)	Acc@1 56.250 (55.689)	Acc@5 84.375 (84.342)
Epoch: [58][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.1534 (2.4009)	Acc@1 61.719 (55.556)	Acc@5 86.719 (84.336)
Epoch: [58][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3819 (2.4090)	Acc@1 53.125 (55.228)	Acc@5 85.938 (84.375)
Epoch: [58][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2778 (2.4006)	Acc@1 59.375 (55.438)	Acc@5 85.938 (84.499)
Epoch: [58][110/391]	Time 0.010 (0.012)	Data 0.010 (0.003)	Loss 2.6659 (2.4065)	Acc@1 48.438 (55.258)	Acc@5 81.250 (84.368)
Epoch: [58][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3032 (2.4026)	Acc@1 60.938 (55.307)	Acc@5 87.500 (84.485)
Epoch: [58][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3948 (2.4046)	Acc@1 56.250 (55.194)	Acc@5 84.375 (84.458)
Epoch: [58][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6722 (2.4118)	Acc@1 50.781 (55.025)	Acc@5 83.594 (84.320)
Epoch: [58][150/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4349 (2.4132)	Acc@1 54.688 (55.003)	Acc@5 84.375 (84.318)
Epoch: [58][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4184 (2.4136)	Acc@1 57.031 (55.095)	Acc@5 84.375 (84.288)
Epoch: [58][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4480 (2.4145)	Acc@1 56.250 (55.062)	Acc@5 82.812 (84.243)
Epoch: [58][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5288 (2.4128)	Acc@1 46.094 (55.110)	Acc@5 85.156 (84.233)
Epoch: [58][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4815 (2.4139)	Acc@1 53.125 (55.154)	Acc@5 80.469 (84.191)
Epoch: [58][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4333 (2.4166)	Acc@1 50.781 (55.076)	Acc@5 82.812 (84.169)
Epoch: [58][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5314 (2.4200)	Acc@1 50.000 (54.958)	Acc@5 82.031 (84.120)
Epoch: [58][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2611 (2.4228)	Acc@1 58.594 (54.893)	Acc@5 85.938 (84.078)
Epoch: [58][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4914 (2.4270)	Acc@1 55.469 (54.775)	Acc@5 82.812 (83.976)
Epoch: [58][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4469 (2.4285)	Acc@1 54.688 (54.723)	Acc@5 81.250 (83.957)
Epoch: [58][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6517 (2.4323)	Acc@1 48.438 (54.619)	Acc@5 80.469 (83.886)
Epoch: [58][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4394 (2.4325)	Acc@1 54.688 (54.646)	Acc@5 81.250 (83.878)
Epoch: [58][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4466 (2.4310)	Acc@1 53.906 (54.702)	Acc@5 82.031 (83.934)
Epoch: [58][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6634 (2.4307)	Acc@1 54.688 (54.738)	Acc@5 79.688 (83.941)
Epoch: [58][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4521 (2.4312)	Acc@1 48.438 (54.741)	Acc@5 83.594 (83.951)
Epoch: [58][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5443 (2.4327)	Acc@1 51.562 (54.758)	Acc@5 82.031 (83.931)
Epoch: [58][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3932 (2.4328)	Acc@1 52.344 (54.753)	Acc@5 86.719 (83.925)
Epoch: [58][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7346 (2.4321)	Acc@1 42.188 (54.729)	Acc@5 78.906 (83.927)
Epoch: [58][330/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.4300 (2.4343)	Acc@1 54.688 (54.673)	Acc@5 86.719 (83.882)
Epoch: [58][340/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.1811 (2.4341)	Acc@1 59.375 (54.704)	Acc@5 90.625 (83.871)
Epoch: [58][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3336 (2.4372)	Acc@1 57.031 (54.614)	Acc@5 85.938 (83.830)
Epoch: [58][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3492 (2.4391)	Acc@1 53.125 (54.523)	Acc@5 89.844 (83.836)
Epoch: [58][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7844 (2.4414)	Acc@1 51.562 (54.509)	Acc@5 75.781 (83.798)
Epoch: [58][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2970 (2.4415)	Acc@1 57.812 (54.532)	Acc@5 92.188 (83.817)
Epoch: [58][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6317 (2.4436)	Acc@1 56.250 (54.490)	Acc@5 83.750 (83.804)
num momentum params: 26
[0.1, 2.443564016036987, 2.2383317399024962, 54.49, 44.19, tensor(0.3245, device='cuda:0', grad_fn=<DivBackward0>), 4.449748992919922, 0.3508596420288086]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [59 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [59][0/391]	Time 0.059 (0.059)	Data 0.171 (0.171)	Loss 2.3412 (2.3412)	Acc@1 52.344 (52.344)	Acc@5 83.594 (83.594)
Epoch: [59][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.4155 (2.3641)	Acc@1 51.562 (55.398)	Acc@5 86.719 (85.511)
Epoch: [59][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.1324 (2.3677)	Acc@1 59.375 (55.841)	Acc@5 86.719 (85.231)
Epoch: [59][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.1450 (2.3460)	Acc@1 61.719 (56.376)	Acc@5 86.719 (85.484)
Epoch: [59][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.5821 (2.3788)	Acc@1 52.344 (55.964)	Acc@5 81.250 (84.870)
Epoch: [59][50/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 2.4009 (2.3673)	Acc@1 56.250 (56.250)	Acc@5 82.031 (84.620)
Epoch: [59][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3710 (2.3600)	Acc@1 55.469 (56.378)	Acc@5 85.156 (84.798)
Epoch: [59][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.0646 (2.3601)	Acc@1 64.062 (56.503)	Acc@5 89.062 (84.958)
Epoch: [59][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4586 (2.3593)	Acc@1 58.594 (56.549)	Acc@5 82.812 (84.992)
Epoch: [59][90/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.2403 (2.3666)	Acc@1 64.844 (56.353)	Acc@5 84.375 (84.821)
Epoch: [59][100/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.5858 (2.3730)	Acc@1 50.000 (56.296)	Acc@5 79.688 (84.785)
Epoch: [59][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5618 (2.3739)	Acc@1 48.438 (56.180)	Acc@5 82.812 (84.776)
Epoch: [59][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4339 (2.3776)	Acc@1 53.906 (55.998)	Acc@5 83.594 (84.698)
Epoch: [59][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.5351 (2.3803)	Acc@1 51.562 (55.910)	Acc@5 83.594 (84.685)
Epoch: [59][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6378 (2.3855)	Acc@1 50.781 (55.790)	Acc@5 79.688 (84.630)
Epoch: [59][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5393 (2.3899)	Acc@1 52.344 (55.733)	Acc@5 80.469 (84.530)
Epoch: [59][160/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4657 (2.3905)	Acc@1 52.344 (55.707)	Acc@5 83.594 (84.521)
Epoch: [59][170/391]	Time 0.010 (0.012)	Data 0.003 (0.002)	Loss 2.5882 (2.3971)	Acc@1 51.562 (55.533)	Acc@5 76.562 (84.402)
Epoch: [59][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5168 (2.4000)	Acc@1 50.000 (55.456)	Acc@5 79.688 (84.340)
Epoch: [59][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1532 (2.4010)	Acc@1 60.938 (55.477)	Acc@5 87.500 (84.346)
Epoch: [59][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1118 (2.4035)	Acc@1 64.844 (55.465)	Acc@5 89.844 (84.282)
Epoch: [59][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4547 (2.4030)	Acc@1 57.031 (55.517)	Acc@5 81.250 (84.294)
Epoch: [59][220/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 2.5295 (2.4034)	Acc@1 53.906 (55.493)	Acc@5 85.938 (84.290)
Epoch: [59][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3676 (2.4054)	Acc@1 54.688 (55.391)	Acc@5 85.938 (84.301)
Epoch: [59][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1727 (2.4038)	Acc@1 58.594 (55.410)	Acc@5 89.062 (84.304)
Epoch: [59][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4428 (2.4039)	Acc@1 52.344 (55.366)	Acc@5 83.594 (84.294)
Epoch: [59][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4632 (2.4104)	Acc@1 52.344 (55.229)	Acc@5 85.156 (84.234)
Epoch: [59][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4198 (2.4158)	Acc@1 56.250 (55.149)	Acc@5 81.250 (84.075)
Epoch: [59][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6056 (2.4168)	Acc@1 50.781 (55.171)	Acc@5 80.469 (84.022)
Epoch: [59][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5805 (2.4171)	Acc@1 50.000 (55.152)	Acc@5 82.031 (84.080)
Epoch: [59][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4101 (2.4190)	Acc@1 61.719 (55.124)	Acc@5 82.812 (84.079)
Epoch: [59][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6949 (2.4227)	Acc@1 50.781 (55.049)	Acc@5 81.250 (83.996)
Epoch: [59][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3650 (2.4250)	Acc@1 48.438 (54.963)	Acc@5 85.938 (83.986)
Epoch: [59][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4086 (2.4263)	Acc@1 56.250 (54.935)	Acc@5 85.156 (83.997)
Epoch: [59][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6881 (2.4298)	Acc@1 50.781 (54.903)	Acc@5 79.688 (83.947)
Epoch: [59][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2550 (2.4316)	Acc@1 60.938 (54.877)	Acc@5 89.844 (83.941)
Epoch: [59][360/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 2.5909 (2.4353)	Acc@1 54.688 (54.798)	Acc@5 79.688 (83.921)
Epoch: [59][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4299 (2.4377)	Acc@1 51.562 (54.704)	Acc@5 83.594 (83.880)
Epoch: [59][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5557 (2.4393)	Acc@1 50.781 (54.640)	Acc@5 81.250 (83.846)
Epoch: [59][390/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6578 (2.4389)	Acc@1 45.000 (54.666)	Acc@5 77.500 (83.832)
num momentum params: 26
[0.1, 2.438881899185181, 2.047474113702774, 54.666, 46.71, tensor(0.3255, device='cuda:0', grad_fn=<DivBackward0>), 4.50510311126709, 0.3406069278717041]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [115, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [115]
Non Pruning Epoch - module.bn2.bias: [115]
Non Pruning Epoch - module.conv3.weight: [233, 115, 3, 3]
Non Pruning Epoch - module.bn3.weight: [233]
Non Pruning Epoch - module.bn3.bias: [233]
Non Pruning Epoch - module.conv4.weight: [251, 233, 3, 3]
Non Pruning Epoch - module.bn4.weight: [251]
Non Pruning Epoch - module.bn4.bias: [251]
Non Pruning Epoch - module.conv5.weight: [390, 251, 3, 3]
Non Pruning Epoch - module.bn5.weight: [390]
Non Pruning Epoch - module.bn5.bias: [390]
Non Pruning Epoch - module.conv6.weight: [354, 390, 3, 3]
Non Pruning Epoch - module.bn6.weight: [354]
Non Pruning Epoch - module.bn6.bias: [354]
Non Pruning Epoch - module.conv7.weight: [258, 354, 3, 3]
Non Pruning Epoch - module.bn7.weight: [258]
Non Pruning Epoch - module.bn7.bias: [258]
Non Pruning Epoch - module.conv8.weight: [220, 258, 3, 3]
Non Pruning Epoch - module.bn8.weight: [220]
Non Pruning Epoch - module.bn8.bias: [220]
Non Pruning Epoch - module.fc.weight: [100, 220]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [60 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [115, 34, 3, 3]
module.conv3.weight [233, 115, 3, 3]
module.conv4.weight [251, 233, 3, 3]
module.conv5.weight [390, 251, 3, 3]
module.conv6.weight [354, 390, 3, 3]
module.conv7.weight [258, 354, 3, 3]
module.conv8.weight [220, 258, 3, 3]
Epoch: [60][0/391]	Time 0.043 (0.043)	Data 0.174 (0.174)	Loss 2.4712 (2.4712)	Acc@1 51.562 (51.562)	Acc@5 82.031 (82.031)
Epoch: [60][10/391]	Time 0.012 (0.015)	Data 0.001 (0.017)	Loss 2.4690 (2.3509)	Acc@1 53.125 (56.250)	Acc@5 82.031 (85.085)
Epoch: [60][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.4436 (2.3695)	Acc@1 58.594 (56.399)	Acc@5 85.156 (84.673)
Epoch: [60][30/391]	Time 0.013 (0.013)	Data 0.001 (0.007)	Loss 2.2359 (2.3289)	Acc@1 53.125 (57.258)	Acc@5 89.062 (86.013)
Epoch: [60][40/391]	Time 0.012 (0.012)	Data 0.001 (0.006)	Loss 2.3948 (2.3362)	Acc@1 56.250 (57.203)	Acc@5 83.594 (85.671)
Epoch: [60][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.1655 (2.3405)	Acc@1 61.719 (57.154)	Acc@5 87.500 (85.340)
Epoch: [60][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4324 (2.3410)	Acc@1 57.812 (56.967)	Acc@5 83.594 (85.348)
Epoch: [60][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5786 (2.3443)	Acc@1 55.469 (56.679)	Acc@5 82.031 (85.376)
Epoch: [60][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3711 (2.3567)	Acc@1 54.688 (56.433)	Acc@5 85.156 (85.272)
Epoch: [60][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3448 (2.3680)	Acc@1 59.375 (56.087)	Acc@5 82.812 (85.130)
Epoch: [60][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4891 (2.3806)	Acc@1 50.000 (55.979)	Acc@5 85.938 (85.017)
Epoch: [60][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2057 (2.3836)	Acc@1 62.500 (56.137)	Acc@5 89.062 (84.945)
Epoch: [60][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1704 (2.3857)	Acc@1 57.031 (55.985)	Acc@5 86.719 (84.866)
Epoch: [60][130/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.5445 (2.3916)	Acc@1 49.219 (55.892)	Acc@5 83.594 (84.745)
Epoch: [60][140/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.4348 (2.3960)	Acc@1 56.250 (55.740)	Acc@5 83.594 (84.652)
Epoch: [60][150/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.1750 (2.3941)	Acc@1 64.062 (55.810)	Acc@5 87.500 (84.706)
Epoch: [60][160/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6370 (2.3986)	Acc@1 51.562 (55.760)	Acc@5 82.031 (84.627)
Epoch: [60][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5323 (2.4016)	Acc@1 54.688 (55.661)	Acc@5 82.812 (84.626)
Epoch: [60][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2692 (2.4019)	Acc@1 55.469 (55.654)	Acc@5 89.062 (84.586)
Epoch: [60][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3646 (2.4063)	Acc@1 54.688 (55.526)	Acc@5 85.938 (84.547)
Epoch: [60][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3827 (2.4091)	Acc@1 53.906 (55.465)	Acc@5 82.812 (84.472)
Epoch: [60][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1682 (2.4100)	Acc@1 60.156 (55.472)	Acc@5 86.719 (84.442)
Epoch: [60][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6237 (2.4087)	Acc@1 51.562 (55.575)	Acc@5 79.688 (84.428)
Epoch: [60][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3119 (2.4086)	Acc@1 62.500 (55.557)	Acc@5 85.156 (84.416)
Epoch: [60][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8624 (2.4149)	Acc@1 46.094 (55.388)	Acc@5 74.219 (84.284)
Epoch: [60][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5088 (2.4171)	Acc@1 48.438 (55.266)	Acc@5 82.031 (84.222)
Epoch: [60][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4312 (2.4156)	Acc@1 53.906 (55.280)	Acc@5 81.250 (84.222)
Epoch: [60][270/391]	Time 0.015 (0.011)	Data 0.001 (0.002)	Loss 2.6248 (2.4159)	Acc@1 53.906 (55.302)	Acc@5 78.906 (84.208)
Epoch: [60][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2243 (2.4166)	Acc@1 57.812 (55.210)	Acc@5 85.938 (84.228)
Epoch: [60][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5319 (2.4176)	Acc@1 52.344 (55.168)	Acc@5 85.156 (84.241)
Epoch: [60][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3787 (2.4199)	Acc@1 58.594 (55.079)	Acc@5 84.375 (84.235)
Epoch: [60][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6482 (2.4270)	Acc@1 51.562 (54.954)	Acc@5 81.250 (84.089)
Epoch: [60][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5352 (2.4297)	Acc@1 50.781 (54.890)	Acc@5 84.375 (84.071)
Epoch: [60][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2872 (2.4321)	Acc@1 58.594 (54.857)	Acc@5 85.156 (84.042)
Epoch: [60][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3628 (2.4345)	Acc@1 56.250 (54.841)	Acc@5 83.594 (83.990)
Epoch: [60][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4183 (2.4373)	Acc@1 55.469 (54.783)	Acc@5 83.594 (83.952)
Epoch: [60][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3279 (2.4346)	Acc@1 63.281 (54.854)	Acc@5 82.812 (84.003)
Epoch: [60][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1725 (2.4348)	Acc@1 60.156 (54.822)	Acc@5 87.500 (83.994)
Epoch: [60][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5247 (2.4355)	Acc@1 53.125 (54.798)	Acc@5 85.156 (83.953)
Epoch: [60][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.6303 (2.4383)	Acc@1 52.500 (54.750)	Acc@5 81.250 (83.896)
num momentum params: 26
[0.1, 2.4383353270721435, 2.204088479280472, 54.75, 44.86, tensor(0.3259, device='cuda:0', grad_fn=<DivBackward0>), 4.4652369022369385, 0.3348808288574219]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [34, 3, 3, 3]
Before - module.bn1.weight: [34]
Before - module.bn1.bias: [34]
Before - module.conv2.weight: [115, 34, 3, 3]
Before - module.bn2.weight: [115]
Before - module.bn2.bias: [115]
Before - module.conv3.weight: [233, 115, 3, 3]
Before - module.bn3.weight: [233]
Before - module.bn3.bias: [233]
Before - module.conv4.weight: [251, 233, 3, 3]
Before - module.bn4.weight: [251]
Before - module.bn4.bias: [251]
Before - module.conv5.weight: [390, 251, 3, 3]
Before - module.bn5.weight: [390]
Before - module.bn5.bias: [390]
Before - module.conv6.weight: [354, 390, 3, 3]
Before - module.bn6.weight: [354]
Before - module.bn6.bias: [354]
Before - module.conv7.weight: [258, 354, 3, 3]
Before - module.bn7.weight: [258]
Before - module.bn7.bias: [258]
Before - module.conv8.weight: [220, 258, 3, 3]
Before - module.bn8.weight: [220]
Before - module.bn8.bias: [220]
Before - module.fc.weight: [100, 220]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [34, 3, 3, 3] >> [34, 3, 3, 3]
[module.bn1.weight]: 34 >> 34
running_mean [34]
running_var [34]
num_batches_tracked []
[module.conv2.weight]: [115, 34, 3, 3] >> [112, 34, 3, 3]
[module.bn2.weight]: 115 >> 112
running_mean [112]
running_var [112]
num_batches_tracked []
[module.conv3.weight]: [233, 115, 3, 3] >> [227, 112, 3, 3]
[module.bn3.weight]: 233 >> 227
running_mean [227]
running_var [227]
num_batches_tracked []
[module.conv4.weight]: [251, 233, 3, 3] >> [249, 227, 3, 3]
[module.bn4.weight]: 251 >> 249
running_mean [249]
running_var [249]
num_batches_tracked []
[module.conv5.weight]: [390, 251, 3, 3] >> [381, 249, 3, 3]
[module.bn5.weight]: 390 >> 381
running_mean [381]
running_var [381]
num_batches_tracked []
[module.conv6.weight]: [354, 390, 3, 3] >> [339, 381, 3, 3]
[module.bn6.weight]: 354 >> 339
running_mean [339]
running_var [339]
num_batches_tracked []
[module.conv7.weight]: [258, 354, 3, 3] >> [239, 339, 3, 3]
[module.bn7.weight]: 258 >> 239
running_mean [239]
running_var [239]
num_batches_tracked []
[module.conv8.weight]: [220, 258, 3, 3] >> [214, 239, 3, 3]
[module.bn8.weight]: 220 >> 214
running_mean [214]
running_var [214]
num_batches_tracked []
[module.fc.weight]: [100, 220] >> [100, 214]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [34, 3, 3, 3]
After - module.bn1.weight: [34]
After - module.bn1.bias: [34]
After - module.conv2.weight: [112, 34, 3, 3]
After - module.bn2.weight: [112]
After - module.bn2.bias: [112]
After - module.conv3.weight: [227, 112, 3, 3]
After - module.bn3.weight: [227]
After - module.bn3.bias: [227]
After - module.conv4.weight: [249, 227, 3, 3]
After - module.bn4.weight: [249]
After - module.bn4.bias: [249]
After - module.conv5.weight: [381, 249, 3, 3]
After - module.bn5.weight: [381]
After - module.bn5.bias: [381]
After - module.conv6.weight: [339, 381, 3, 3]
After - module.bn6.weight: [339]
After - module.bn6.bias: [339]
After - module.conv7.weight: [239, 339, 3, 3]
After - module.bn7.weight: [239]
After - module.bn7.bias: [239]
After - module.conv8.weight: [214, 239, 3, 3]
After - module.bn8.weight: [214]
After - module.bn8.bias: [214]
After - module.fc.weight: [100, 214]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [34, 3, 3, 3]
conv2 --> [112, 34, 3, 3]
conv3 --> [227, 112, 3, 3]
conv4 --> [249, 227, 3, 3]
conv5 --> [381, 249, 3, 3]
conv6 --> [339, 381, 3, 3]
conv7 --> [239, 339, 3, 3]
conv8 --> [214, 239, 3, 3]
fc --> [214, 100]
1, 376482816, 940032, 34
2, 3667378176, 8773632, 112
3, 6677766144, 14644224, 227
4, 14846105088, 32557248, 249
5, 7431657984, 13661136, 381
6, 10117799424, 18598896, 339
7, 2240068608, 2916756, 239
8, 1414084608, 1841256, 214
fc, 8217600, 21400, 0
===================
FLOP REPORT: 18273265800000.0 42913600000.0 93954580 107284 1795 7.632974624633789
[INFO] Storing checkpoint...

Epoch: [61 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [61][0/391]	Time 0.248 (0.248)	Data 0.179 (0.179)	Loss 2.3204 (2.3204)	Acc@1 53.125 (53.125)	Acc@5 85.938 (85.938)
Epoch: [61][10/391]	Time 0.011 (0.034)	Data 0.001 (0.017)	Loss 2.2568 (2.4510)	Acc@1 58.594 (53.906)	Acc@5 88.281 (84.517)
Epoch: [61][20/391]	Time 0.011 (0.023)	Data 0.001 (0.010)	Loss 2.3907 (2.3906)	Acc@1 53.125 (55.580)	Acc@5 84.375 (84.710)
Epoch: [61][30/391]	Time 0.011 (0.019)	Data 0.001 (0.007)	Loss 2.5050 (2.3559)	Acc@1 57.812 (56.779)	Acc@5 85.156 (85.333)
Epoch: [61][40/391]	Time 0.011 (0.017)	Data 0.002 (0.006)	Loss 2.1275 (2.3364)	Acc@1 61.719 (57.069)	Acc@5 89.062 (85.823)
Epoch: [61][50/391]	Time 0.011 (0.016)	Data 0.001 (0.005)	Loss 2.4240 (2.3385)	Acc@1 53.125 (56.970)	Acc@5 85.156 (85.769)
Epoch: [61][60/391]	Time 0.010 (0.015)	Data 0.002 (0.004)	Loss 2.2035 (2.3524)	Acc@1 62.500 (56.775)	Acc@5 85.938 (85.438)
Epoch: [61][70/391]	Time 0.010 (0.015)	Data 0.001 (0.004)	Loss 2.5082 (2.3637)	Acc@1 53.125 (56.547)	Acc@5 85.156 (85.288)
Epoch: [61][80/391]	Time 0.012 (0.014)	Data 0.001 (0.004)	Loss 2.5833 (2.3734)	Acc@1 50.781 (56.182)	Acc@5 82.031 (84.992)
Epoch: [61][90/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.5570 (2.3820)	Acc@1 54.688 (56.190)	Acc@5 82.031 (84.959)
Epoch: [61][100/391]	Time 0.012 (0.014)	Data 0.001 (0.003)	Loss 2.0832 (2.3819)	Acc@1 59.375 (56.235)	Acc@5 90.625 (84.855)
Epoch: [61][110/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.5142 (2.3818)	Acc@1 52.344 (56.250)	Acc@5 82.812 (84.818)
Epoch: [61][120/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.2905 (2.3806)	Acc@1 57.812 (56.327)	Acc@5 83.594 (84.743)
Epoch: [61][130/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.5655 (2.3853)	Acc@1 53.125 (56.268)	Acc@5 83.594 (84.667)
Epoch: [61][140/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.5703 (2.3922)	Acc@1 53.125 (56.195)	Acc@5 84.375 (84.574)
Epoch: [61][150/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.2652 (2.3944)	Acc@1 55.469 (56.079)	Acc@5 85.156 (84.556)
Epoch: [61][160/391]	Time 0.010 (0.013)	Data 0.009 (0.003)	Loss 2.3020 (2.4016)	Acc@1 55.469 (55.905)	Acc@5 88.281 (84.467)
Epoch: [61][170/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.4075 (2.4054)	Acc@1 53.125 (55.747)	Acc@5 85.156 (84.352)
Epoch: [61][180/391]	Time 0.011 (0.013)	Data 0.001 (0.002)	Loss 2.3339 (2.4077)	Acc@1 57.812 (55.663)	Acc@5 88.281 (84.366)
Epoch: [61][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3970 (2.4065)	Acc@1 53.125 (55.751)	Acc@5 85.938 (84.395)
Epoch: [61][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4391 (2.4109)	Acc@1 55.469 (55.651)	Acc@5 81.250 (84.336)
Epoch: [61][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6501 (2.4133)	Acc@1 45.312 (55.513)	Acc@5 78.125 (84.275)
Epoch: [61][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2933 (2.4118)	Acc@1 57.031 (55.515)	Acc@5 84.375 (84.333)
Epoch: [61][230/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.5751 (2.4160)	Acc@1 51.562 (55.388)	Acc@5 81.250 (84.280)
Epoch: [61][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5912 (2.4198)	Acc@1 50.781 (55.349)	Acc@5 82.812 (84.190)
Epoch: [61][250/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.3350 (2.4217)	Acc@1 60.156 (55.326)	Acc@5 85.156 (84.170)
Epoch: [61][260/391]	Time 0.013 (0.012)	Data 0.000 (0.002)	Loss 2.4206 (2.4261)	Acc@1 47.656 (55.181)	Acc@5 87.500 (84.103)
Epoch: [61][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3148 (2.4270)	Acc@1 58.594 (55.111)	Acc@5 84.375 (84.087)
Epoch: [61][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3004 (2.4258)	Acc@1 57.812 (55.091)	Acc@5 84.375 (84.097)
Epoch: [61][290/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 2.7330 (2.4265)	Acc@1 50.781 (55.055)	Acc@5 79.688 (84.104)
Epoch: [61][300/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2589 (2.4258)	Acc@1 57.031 (55.072)	Acc@5 82.031 (84.095)
Epoch: [61][310/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4695 (2.4282)	Acc@1 53.125 (54.996)	Acc@5 84.375 (84.053)
Epoch: [61][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4475 (2.4275)	Acc@1 55.469 (55.014)	Acc@5 83.594 (84.059)
Epoch: [61][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6731 (2.4302)	Acc@1 51.562 (54.938)	Acc@5 79.688 (84.023)
Epoch: [61][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3095 (2.4313)	Acc@1 54.688 (54.903)	Acc@5 89.844 (84.011)
Epoch: [61][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6023 (2.4316)	Acc@1 53.906 (54.872)	Acc@5 83.594 (84.037)
Epoch: [61][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4291 (2.4328)	Acc@1 52.344 (54.832)	Acc@5 87.500 (84.031)
Epoch: [61][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5917 (2.4334)	Acc@1 50.000 (54.822)	Acc@5 79.688 (84.042)
Epoch: [61][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6554 (2.4377)	Acc@1 53.125 (54.712)	Acc@5 81.250 (83.971)
Epoch: [61][390/391]	Time 0.148 (0.012)	Data 0.001 (0.002)	Loss 2.4993 (2.4401)	Acc@1 53.750 (54.624)	Acc@5 81.250 (83.904)
num momentum params: 26
[0.1, 2.4401300865936277, 2.044260263442993, 54.624, 46.35, tensor(0.3255, device='cuda:0', grad_fn=<DivBackward0>), 4.779265642166138, 0.41550564765930176]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [62 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [62][0/391]	Time 0.052 (0.052)	Data 0.168 (0.168)	Loss 2.4863 (2.4863)	Acc@1 53.125 (53.125)	Acc@5 80.469 (80.469)
Epoch: [62][10/391]	Time 0.012 (0.015)	Data 0.003 (0.017)	Loss 2.3476 (2.3834)	Acc@1 60.156 (58.452)	Acc@5 85.156 (85.156)
Epoch: [62][20/391]	Time 0.012 (0.013)	Data 0.001 (0.009)	Loss 2.4769 (2.3836)	Acc@1 59.375 (57.812)	Acc@5 84.375 (85.119)
Epoch: [62][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.0047 (2.3515)	Acc@1 71.094 (58.619)	Acc@5 91.406 (85.333)
Epoch: [62][40/391]	Time 0.012 (0.012)	Data 0.001 (0.006)	Loss 2.6701 (2.3573)	Acc@1 52.344 (58.136)	Acc@5 82.812 (85.175)
Epoch: [62][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5014 (2.3729)	Acc@1 54.688 (57.445)	Acc@5 82.031 (84.957)
Epoch: [62][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4845 (2.3596)	Acc@1 53.125 (57.761)	Acc@5 81.250 (85.054)
Epoch: [62][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.6223 (2.3667)	Acc@1 46.875 (57.163)	Acc@5 81.250 (85.013)
Epoch: [62][80/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3110 (2.3761)	Acc@1 56.250 (56.761)	Acc@5 89.062 (84.896)
Epoch: [62][90/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 2.3830 (2.3821)	Acc@1 51.562 (56.430)	Acc@5 86.719 (84.864)
Epoch: [62][100/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.3536 (2.3814)	Acc@1 52.344 (56.327)	Acc@5 83.594 (84.971)
Epoch: [62][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2467 (2.3885)	Acc@1 57.812 (56.151)	Acc@5 88.281 (84.875)
Epoch: [62][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4537 (2.3887)	Acc@1 53.125 (56.134)	Acc@5 83.594 (84.737)
Epoch: [62][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.4745 (2.3937)	Acc@1 56.250 (56.125)	Acc@5 85.938 (84.709)
Epoch: [62][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.6986 (2.4056)	Acc@1 45.312 (55.884)	Acc@5 82.031 (84.497)
Epoch: [62][150/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3867 (2.4044)	Acc@1 55.469 (55.888)	Acc@5 81.250 (84.447)
Epoch: [62][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2380 (2.4078)	Acc@1 56.250 (55.745)	Acc@5 87.500 (84.394)
Epoch: [62][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3009 (2.4074)	Acc@1 53.906 (55.674)	Acc@5 89.062 (84.416)
Epoch: [62][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4330 (2.4077)	Acc@1 59.375 (55.667)	Acc@5 80.469 (84.397)
Epoch: [62][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2676 (2.4068)	Acc@1 58.594 (55.698)	Acc@5 85.156 (84.359)
Epoch: [62][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4550 (2.4086)	Acc@1 57.031 (55.679)	Acc@5 82.031 (84.309)
Epoch: [62][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4045 (2.4089)	Acc@1 52.344 (55.598)	Acc@5 89.062 (84.342)
Epoch: [62][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5484 (2.4144)	Acc@1 50.781 (55.444)	Acc@5 83.594 (84.258)
Epoch: [62][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3495 (2.4116)	Acc@1 53.906 (55.435)	Acc@5 87.500 (84.324)
Epoch: [62][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4290 (2.4146)	Acc@1 61.719 (55.401)	Acc@5 84.375 (84.294)
Epoch: [62][250/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.5038 (2.4177)	Acc@1 53.125 (55.270)	Acc@5 85.156 (84.254)
Epoch: [62][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4061 (2.4178)	Acc@1 51.562 (55.244)	Acc@5 84.375 (84.279)
Epoch: [62][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6663 (2.4205)	Acc@1 46.094 (55.157)	Acc@5 82.812 (84.240)
Epoch: [62][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4058 (2.4224)	Acc@1 54.688 (55.093)	Acc@5 80.469 (84.189)
Epoch: [62][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3566 (2.4207)	Acc@1 57.812 (55.101)	Acc@5 86.719 (84.235)
Epoch: [62][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5315 (2.4225)	Acc@1 53.906 (55.072)	Acc@5 81.250 (84.186)
Epoch: [62][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4775 (2.4221)	Acc@1 54.688 (55.102)	Acc@5 82.031 (84.177)
Epoch: [62][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.9566 (2.4242)	Acc@1 49.219 (55.067)	Acc@5 73.438 (84.132)
Epoch: [62][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4574 (2.4296)	Acc@1 53.906 (54.945)	Acc@5 83.594 (84.021)
Epoch: [62][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4710 (2.4314)	Acc@1 53.906 (54.912)	Acc@5 84.375 (83.986)
Epoch: [62][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4973 (2.4326)	Acc@1 48.438 (54.843)	Acc@5 85.156 (83.977)
Epoch: [62][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3231 (2.4341)	Acc@1 54.688 (54.826)	Acc@5 85.938 (83.947)
Epoch: [62][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5713 (2.4337)	Acc@1 54.688 (54.812)	Acc@5 81.250 (84.006)
Epoch: [62][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3890 (2.4327)	Acc@1 48.438 (54.784)	Acc@5 87.500 (84.047)
Epoch: [62][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4940 (2.4341)	Acc@1 56.250 (54.742)	Acc@5 87.500 (84.044)
num momentum params: 26
[0.1, 2.4341090441894533, 2.207337155342102, 54.742, 45.19, tensor(0.3261, device='cuda:0', grad_fn=<DivBackward0>), 4.3914477825164795, 0.3288271427154541]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [63 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [63][0/391]	Time 0.046 (0.046)	Data 0.166 (0.166)	Loss 2.2979 (2.2979)	Acc@1 57.812 (57.812)	Acc@5 85.938 (85.938)
Epoch: [63][10/391]	Time 0.013 (0.016)	Data 0.001 (0.016)	Loss 2.4667 (2.3137)	Acc@1 58.594 (58.097)	Acc@5 78.906 (85.369)
Epoch: [63][20/391]	Time 0.011 (0.014)	Data 0.001 (0.009)	Loss 2.2825 (2.3287)	Acc@1 63.281 (57.812)	Acc@5 87.500 (85.045)
Epoch: [63][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.1437 (2.3260)	Acc@1 63.281 (57.712)	Acc@5 87.500 (85.106)
Epoch: [63][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.3368 (2.3335)	Acc@1 60.156 (57.641)	Acc@5 81.250 (85.042)
Epoch: [63][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3518 (2.3452)	Acc@1 55.469 (56.909)	Acc@5 86.719 (85.187)
Epoch: [63][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3928 (2.3399)	Acc@1 57.812 (57.159)	Acc@5 82.812 (85.336)
Epoch: [63][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5004 (2.3404)	Acc@1 53.906 (57.174)	Acc@5 84.375 (85.288)
Epoch: [63][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3530 (2.3466)	Acc@1 59.375 (57.022)	Acc@5 87.500 (85.262)
Epoch: [63][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2706 (2.3536)	Acc@1 60.938 (56.834)	Acc@5 88.281 (85.328)
Epoch: [63][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3149 (2.3521)	Acc@1 64.062 (56.962)	Acc@5 85.156 (85.226)
Epoch: [63][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4872 (2.3499)	Acc@1 47.656 (56.848)	Acc@5 85.156 (85.276)
Epoch: [63][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1254 (2.3486)	Acc@1 61.719 (56.928)	Acc@5 91.406 (85.311)
Epoch: [63][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4399 (2.3545)	Acc@1 57.031 (56.840)	Acc@5 83.594 (85.240)
Epoch: [63][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5034 (2.3652)	Acc@1 50.781 (56.538)	Acc@5 85.938 (85.123)
Epoch: [63][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3109 (2.3748)	Acc@1 59.375 (56.333)	Acc@5 87.500 (84.986)
Epoch: [63][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3825 (2.3777)	Acc@1 57.812 (56.245)	Acc@5 87.500 (84.885)
Epoch: [63][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4862 (2.3808)	Acc@1 57.031 (56.168)	Acc@5 82.812 (84.800)
Epoch: [63][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5159 (2.3799)	Acc@1 49.219 (56.185)	Acc@5 85.156 (84.768)
Epoch: [63][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4939 (2.3844)	Acc@1 58.594 (56.164)	Acc@5 77.344 (84.702)
Epoch: [63][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6439 (2.3919)	Acc@1 46.875 (55.955)	Acc@5 78.125 (84.639)
Epoch: [63][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4871 (2.3953)	Acc@1 50.781 (55.917)	Acc@5 82.031 (84.612)
Epoch: [63][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4648 (2.3977)	Acc@1 56.250 (55.851)	Acc@5 82.812 (84.591)
Epoch: [63][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7146 (2.4008)	Acc@1 57.812 (55.807)	Acc@5 78.125 (84.514)
Epoch: [63][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6192 (2.4065)	Acc@1 48.438 (55.699)	Acc@5 77.344 (84.404)
Epoch: [63][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5129 (2.4074)	Acc@1 52.344 (55.624)	Acc@5 82.812 (84.422)
Epoch: [63][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4565 (2.4095)	Acc@1 53.906 (55.556)	Acc@5 82.812 (84.375)
Epoch: [63][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2935 (2.4110)	Acc@1 57.812 (55.492)	Acc@5 84.375 (84.346)
Epoch: [63][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6961 (2.4139)	Acc@1 51.562 (55.366)	Acc@5 77.344 (84.297)
Epoch: [63][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4203 (2.4124)	Acc@1 51.562 (55.367)	Acc@5 80.469 (84.313)
Epoch: [63][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5588 (2.4151)	Acc@1 52.344 (55.287)	Acc@5 81.250 (84.250)
Epoch: [63][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4084 (2.4167)	Acc@1 57.031 (55.245)	Acc@5 85.938 (84.269)
Epoch: [63][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6156 (2.4182)	Acc@1 52.344 (55.223)	Acc@5 78.125 (84.224)
Epoch: [63][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4292 (2.4165)	Acc@1 53.125 (55.254)	Acc@5 85.938 (84.266)
Epoch: [63][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3366 (2.4164)	Acc@1 57.812 (55.203)	Acc@5 85.156 (84.256)
Epoch: [63][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5382 (2.4186)	Acc@1 50.000 (55.130)	Acc@5 85.156 (84.206)
Epoch: [63][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5195 (2.4193)	Acc@1 53.125 (55.101)	Acc@5 82.812 (84.213)
Epoch: [63][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7520 (2.4218)	Acc@1 49.219 (55.029)	Acc@5 75.781 (84.156)
Epoch: [63][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3889 (2.4225)	Acc@1 55.469 (54.981)	Acc@5 85.156 (84.162)
Epoch: [63][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.7359 (2.4248)	Acc@1 50.000 (54.916)	Acc@5 81.250 (84.108)
num momentum params: 26
[0.1, 2.42484267578125, 2.0799736261367796, 54.916, 46.3, tensor(0.3272, device='cuda:0', grad_fn=<DivBackward0>), 4.4229655265808105, 0.3219900131225586]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [64 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [64][0/391]	Time 0.043 (0.043)	Data 0.156 (0.156)	Loss 2.4074 (2.4074)	Acc@1 57.812 (57.812)	Acc@5 85.156 (85.156)
Epoch: [64][10/391]	Time 0.012 (0.015)	Data 0.001 (0.015)	Loss 2.5221 (2.3635)	Acc@1 55.469 (57.102)	Acc@5 85.938 (86.009)
Epoch: [64][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.4503 (2.3692)	Acc@1 50.781 (55.841)	Acc@5 89.062 (86.086)
Epoch: [64][30/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.4227 (2.3735)	Acc@1 52.344 (55.872)	Acc@5 84.375 (85.509)
Epoch: [64][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2880 (2.3710)	Acc@1 57.812 (56.098)	Acc@5 85.156 (85.518)
Epoch: [64][50/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.0584 (2.3622)	Acc@1 64.844 (56.097)	Acc@5 89.844 (85.754)
Epoch: [64][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2636 (2.3583)	Acc@1 61.719 (56.378)	Acc@5 87.500 (85.745)
Epoch: [64][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3265 (2.3517)	Acc@1 58.594 (56.481)	Acc@5 84.375 (85.750)
Epoch: [64][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2735 (2.3614)	Acc@1 53.125 (56.182)	Acc@5 85.156 (85.503)
Epoch: [64][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4866 (2.3647)	Acc@1 57.031 (56.070)	Acc@5 83.594 (85.371)
Epoch: [64][100/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.2925 (2.3676)	Acc@1 61.719 (56.111)	Acc@5 89.062 (85.435)
Epoch: [64][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.7169 (2.3783)	Acc@1 46.875 (55.835)	Acc@5 79.688 (85.163)
Epoch: [64][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5425 (2.3898)	Acc@1 57.031 (55.495)	Acc@5 83.594 (84.892)
Epoch: [64][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3264 (2.3952)	Acc@1 58.594 (55.326)	Acc@5 83.594 (84.870)
Epoch: [64][140/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5005 (2.3990)	Acc@1 55.469 (55.341)	Acc@5 83.594 (84.802)
Epoch: [64][150/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6659 (2.4015)	Acc@1 50.000 (55.288)	Acc@5 80.469 (84.789)
Epoch: [64][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4338 (2.4028)	Acc@1 55.469 (55.284)	Acc@5 82.812 (84.778)
Epoch: [64][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5608 (2.4077)	Acc@1 47.656 (55.117)	Acc@5 83.594 (84.704)
Epoch: [64][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4049 (2.4066)	Acc@1 58.594 (55.162)	Acc@5 83.594 (84.720)
Epoch: [64][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3764 (2.4084)	Acc@1 59.375 (55.166)	Acc@5 85.938 (84.694)
Epoch: [64][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3007 (2.4094)	Acc@1 60.156 (55.154)	Acc@5 89.062 (84.713)
Epoch: [64][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4365 (2.4095)	Acc@1 50.000 (55.143)	Acc@5 83.594 (84.723)
Epoch: [64][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3393 (2.4119)	Acc@1 59.375 (55.144)	Acc@5 85.156 (84.651)
Epoch: [64][230/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.4878 (2.4150)	Acc@1 53.125 (55.090)	Acc@5 79.688 (84.551)
Epoch: [64][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4335 (2.4158)	Acc@1 52.344 (55.025)	Acc@5 85.156 (84.550)
Epoch: [64][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6213 (2.4185)	Acc@1 53.906 (55.005)	Acc@5 77.344 (84.518)
Epoch: [64][260/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3118 (2.4189)	Acc@1 59.375 (54.996)	Acc@5 87.500 (84.495)
Epoch: [64][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3489 (2.4186)	Acc@1 57.812 (55.007)	Acc@5 87.500 (84.499)
Epoch: [64][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3333 (2.4172)	Acc@1 60.156 (55.054)	Acc@5 82.812 (84.525)
Epoch: [64][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4490 (2.4215)	Acc@1 54.688 (54.986)	Acc@5 83.594 (84.429)
Epoch: [64][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4993 (2.4250)	Acc@1 53.906 (54.882)	Acc@5 80.469 (84.383)
Epoch: [64][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3383 (2.4242)	Acc@1 56.250 (54.911)	Acc@5 84.375 (84.378)
Epoch: [64][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3365 (2.4233)	Acc@1 53.906 (54.907)	Acc@5 82.812 (84.397)
Epoch: [64][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6801 (2.4226)	Acc@1 50.000 (54.949)	Acc@5 78.906 (84.413)
Epoch: [64][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7169 (2.4259)	Acc@1 50.781 (54.891)	Acc@5 77.344 (84.311)
Epoch: [64][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3732 (2.4275)	Acc@1 52.344 (54.857)	Acc@5 81.250 (84.277)
Epoch: [64][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8026 (2.4300)	Acc@1 46.094 (54.783)	Acc@5 82.031 (84.254)
Epoch: [64][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3534 (2.4318)	Acc@1 53.125 (54.742)	Acc@5 89.062 (84.263)
Epoch: [64][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7149 (2.4340)	Acc@1 50.781 (54.694)	Acc@5 82.031 (84.240)
Epoch: [64][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3346 (2.4342)	Acc@1 57.500 (54.650)	Acc@5 82.500 (84.234)
num momentum params: 26
[0.1, 2.434201020278931, 1.9713449573516846, 54.65, 47.98, tensor(0.3269, device='cuda:0', grad_fn=<DivBackward0>), 4.423025608062744, 0.3202667236328125]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [65 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [65][0/391]	Time 0.048 (0.048)	Data 0.161 (0.161)	Loss 2.5299 (2.5299)	Acc@1 57.031 (57.031)	Acc@5 78.125 (78.125)
Epoch: [65][10/391]	Time 0.011 (0.016)	Data 0.001 (0.016)	Loss 2.6292 (2.4894)	Acc@1 50.781 (54.119)	Acc@5 79.688 (81.889)
Epoch: [65][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.1070 (2.4059)	Acc@1 59.375 (55.283)	Acc@5 91.406 (83.743)
Epoch: [65][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.2578 (2.3846)	Acc@1 57.031 (55.822)	Acc@5 90.625 (84.829)
Epoch: [65][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.5345 (2.3633)	Acc@1 50.781 (56.555)	Acc@5 81.250 (85.137)
Epoch: [65][50/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2083 (2.3613)	Acc@1 54.688 (56.464)	Acc@5 88.281 (85.263)
Epoch: [65][60/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 2.2934 (2.3478)	Acc@1 58.594 (56.826)	Acc@5 87.500 (85.745)
Epoch: [65][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.1557 (2.3464)	Acc@1 61.719 (56.899)	Acc@5 89.062 (85.662)
Epoch: [65][80/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.1647 (2.3470)	Acc@1 60.156 (56.877)	Acc@5 91.406 (85.619)
Epoch: [65][90/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 2.6715 (2.3551)	Acc@1 48.438 (56.585)	Acc@5 78.125 (85.362)
Epoch: [65][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.0980 (2.3566)	Acc@1 62.500 (56.660)	Acc@5 90.625 (85.350)
Epoch: [65][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3836 (2.3675)	Acc@1 54.688 (56.320)	Acc@5 86.719 (85.184)
Epoch: [65][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2113 (2.3728)	Acc@1 57.812 (56.179)	Acc@5 89.062 (85.092)
Epoch: [65][130/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 2.4790 (2.3790)	Acc@1 51.562 (56.017)	Acc@5 84.375 (85.055)
Epoch: [65][140/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5046 (2.3815)	Acc@1 52.344 (55.901)	Acc@5 82.812 (84.996)
Epoch: [65][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4138 (2.3828)	Acc@1 59.375 (55.945)	Acc@5 85.156 (84.965)
Epoch: [65][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4482 (2.3875)	Acc@1 51.562 (55.838)	Acc@5 85.156 (84.904)
Epoch: [65][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3280 (2.3903)	Acc@1 56.250 (55.706)	Acc@5 86.719 (84.855)
Epoch: [65][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6285 (2.3883)	Acc@1 46.094 (55.788)	Acc@5 85.156 (84.966)
Epoch: [65][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4057 (2.3909)	Acc@1 58.594 (55.751)	Acc@5 84.375 (84.849)
Epoch: [65][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3410 (2.3970)	Acc@1 57.031 (55.601)	Acc@5 85.938 (84.713)
Epoch: [65][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7481 (2.4038)	Acc@1 46.875 (55.424)	Acc@5 78.125 (84.619)
Epoch: [65][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3887 (2.4089)	Acc@1 51.562 (55.264)	Acc@5 84.375 (84.587)
Epoch: [65][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5233 (2.4133)	Acc@1 53.906 (55.147)	Acc@5 82.812 (84.507)
Epoch: [65][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3938 (2.4159)	Acc@1 55.469 (55.102)	Acc@5 82.812 (84.450)
Epoch: [65][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5911 (2.4212)	Acc@1 52.344 (55.011)	Acc@5 78.125 (84.369)
Epoch: [65][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6032 (2.4228)	Acc@1 50.000 (54.951)	Acc@5 79.688 (84.318)
Epoch: [65][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5029 (2.4251)	Acc@1 50.781 (54.930)	Acc@5 85.156 (84.280)
Epoch: [65][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6269 (2.4282)	Acc@1 52.344 (54.877)	Acc@5 84.375 (84.242)
Epoch: [65][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4703 (2.4299)	Acc@1 53.906 (54.830)	Acc@5 82.812 (84.187)
Epoch: [65][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4395 (2.4311)	Acc@1 57.031 (54.794)	Acc@5 85.938 (84.115)
Epoch: [65][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4692 (2.4306)	Acc@1 56.250 (54.828)	Acc@5 81.250 (84.139)
Epoch: [65][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6116 (2.4292)	Acc@1 47.656 (54.877)	Acc@5 81.250 (84.197)
Epoch: [65][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6049 (2.4320)	Acc@1 52.344 (54.803)	Acc@5 79.688 (84.167)
Epoch: [65][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4950 (2.4333)	Acc@1 54.688 (54.759)	Acc@5 81.250 (84.169)
Epoch: [65][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4993 (2.4358)	Acc@1 56.250 (54.763)	Acc@5 82.031 (84.110)
Epoch: [65][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5209 (2.4343)	Acc@1 53.125 (54.835)	Acc@5 84.375 (84.135)
Epoch: [65][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4746 (2.4344)	Acc@1 50.781 (54.833)	Acc@5 80.469 (84.129)
Epoch: [65][380/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3167 (2.4326)	Acc@1 61.719 (54.847)	Acc@5 85.938 (84.151)
Epoch: [65][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.2352 (2.4330)	Acc@1 55.000 (54.816)	Acc@5 88.750 (84.140)
num momentum params: 26
[0.1, 2.4330152549743653, 2.161488070487976, 54.816, 45.46, tensor(0.3274, device='cuda:0', grad_fn=<DivBackward0>), 4.441426515579224, 0.3410518169403076]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [66 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [66][0/391]	Time 0.041 (0.041)	Data 0.165 (0.165)	Loss 2.4165 (2.4165)	Acc@1 50.000 (50.000)	Acc@5 84.375 (84.375)
Epoch: [66][10/391]	Time 0.012 (0.014)	Data 0.002 (0.016)	Loss 2.6507 (2.3799)	Acc@1 50.000 (55.469)	Acc@5 80.469 (85.298)
Epoch: [66][20/391]	Time 0.012 (0.013)	Data 0.001 (0.009)	Loss 2.3129 (2.3524)	Acc@1 54.688 (56.399)	Acc@5 87.500 (85.900)
Epoch: [66][30/391]	Time 0.011 (0.012)	Data 0.001 (0.007)	Loss 2.4980 (2.3546)	Acc@1 50.781 (56.502)	Acc@5 84.375 (85.761)
Epoch: [66][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.1370 (2.3406)	Acc@1 60.156 (57.203)	Acc@5 89.844 (85.575)
Epoch: [66][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.6160 (2.3289)	Acc@1 51.562 (57.230)	Acc@5 78.906 (85.723)
Epoch: [66][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3616 (2.3382)	Acc@1 55.469 (57.185)	Acc@5 83.594 (85.515)
Epoch: [66][70/391]	Time 0.013 (0.012)	Data 0.007 (0.004)	Loss 2.5037 (2.3579)	Acc@1 53.906 (56.668)	Acc@5 86.719 (85.156)
Epoch: [66][80/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2169 (2.3477)	Acc@1 57.812 (56.838)	Acc@5 85.938 (85.426)
Epoch: [66][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2752 (2.3471)	Acc@1 57.812 (56.791)	Acc@5 85.938 (85.448)
Epoch: [66][100/391]	Time 0.013 (0.011)	Data 0.001 (0.003)	Loss 2.3546 (2.3539)	Acc@1 53.906 (56.521)	Acc@5 85.156 (85.288)
Epoch: [66][110/391]	Time 0.010 (0.011)	Data 0.003 (0.003)	Loss 2.6320 (2.3566)	Acc@1 52.344 (56.581)	Acc@5 78.906 (85.241)
Epoch: [66][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4796 (2.3620)	Acc@1 50.000 (56.534)	Acc@5 80.469 (85.059)
Epoch: [66][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3015 (2.3688)	Acc@1 63.281 (56.292)	Acc@5 86.719 (84.989)
Epoch: [66][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.8792 (2.3765)	Acc@1 45.312 (56.012)	Acc@5 72.656 (84.907)
Epoch: [66][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3931 (2.3844)	Acc@1 59.375 (55.846)	Acc@5 84.375 (84.753)
Epoch: [66][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3727 (2.3866)	Acc@1 55.469 (55.847)	Acc@5 84.375 (84.729)
Epoch: [66][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4676 (2.3873)	Acc@1 55.469 (55.770)	Acc@5 82.031 (84.699)
Epoch: [66][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.8576 (2.3920)	Acc@1 45.312 (55.672)	Acc@5 80.469 (84.716)
Epoch: [66][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2590 (2.3944)	Acc@1 57.812 (55.579)	Acc@5 87.500 (84.727)
Epoch: [66][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5156 (2.4004)	Acc@1 52.344 (55.473)	Acc@5 81.250 (84.554)
Epoch: [66][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3192 (2.4016)	Acc@1 62.500 (55.432)	Acc@5 83.594 (84.553)
Epoch: [66][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3456 (2.4028)	Acc@1 53.906 (55.366)	Acc@5 81.250 (84.513)
Epoch: [66][230/391]	Time 0.013 (0.011)	Data 0.002 (0.002)	Loss 2.6992 (2.4033)	Acc@1 57.812 (55.357)	Acc@5 80.469 (84.544)
Epoch: [66][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4635 (2.4048)	Acc@1 52.344 (55.261)	Acc@5 87.500 (84.527)
Epoch: [66][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3374 (2.4030)	Acc@1 60.156 (55.251)	Acc@5 86.719 (84.562)
Epoch: [66][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4265 (2.4050)	Acc@1 53.125 (55.223)	Acc@5 86.719 (84.534)
Epoch: [66][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4483 (2.4070)	Acc@1 51.562 (55.212)	Acc@5 85.156 (84.496)
Epoch: [66][280/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.2520 (2.4104)	Acc@1 62.500 (55.138)	Acc@5 88.281 (84.408)
Epoch: [66][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1412 (2.4123)	Acc@1 60.938 (55.136)	Acc@5 87.500 (84.407)
Epoch: [66][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5026 (2.4159)	Acc@1 53.125 (55.028)	Acc@5 82.031 (84.326)
Epoch: [66][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6175 (2.4178)	Acc@1 48.438 (54.949)	Acc@5 82.031 (84.280)
Epoch: [66][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4517 (2.4173)	Acc@1 56.250 (54.980)	Acc@5 85.156 (84.331)
Epoch: [66][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4393 (2.4171)	Acc@1 55.469 (54.983)	Acc@5 83.594 (84.328)
Epoch: [66][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6616 (2.4210)	Acc@1 46.875 (54.907)	Acc@5 79.688 (84.240)
Epoch: [66][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4284 (2.4199)	Acc@1 50.781 (54.899)	Acc@5 82.031 (84.244)
Epoch: [66][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3689 (2.4216)	Acc@1 57.031 (54.889)	Acc@5 85.156 (84.206)
Epoch: [66][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4412 (2.4243)	Acc@1 48.438 (54.869)	Acc@5 84.375 (84.139)
Epoch: [66][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7296 (2.4267)	Acc@1 47.656 (54.802)	Acc@5 75.781 (84.110)
Epoch: [66][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3565 (2.4271)	Acc@1 55.000 (54.764)	Acc@5 87.500 (84.134)
num momentum params: 26
[0.1, 2.4271119289398193, 2.0688439774513245, 54.764, 46.39, tensor(0.3278, device='cuda:0', grad_fn=<DivBackward0>), 4.404842376708984, 0.331866979598999]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [67 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [67][0/391]	Time 0.053 (0.053)	Data 0.170 (0.170)	Loss 2.4499 (2.4499)	Acc@1 57.812 (57.812)	Acc@5 81.250 (81.250)
Epoch: [67][10/391]	Time 0.012 (0.016)	Data 0.001 (0.017)	Loss 2.2895 (2.3183)	Acc@1 60.938 (58.381)	Acc@5 86.719 (85.298)
Epoch: [67][20/391]	Time 0.011 (0.014)	Data 0.001 (0.009)	Loss 2.2817 (2.3166)	Acc@1 59.375 (58.036)	Acc@5 82.812 (85.826)
Epoch: [67][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.0542 (2.3105)	Acc@1 64.062 (57.888)	Acc@5 88.281 (86.064)
Epoch: [67][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3718 (2.3155)	Acc@1 53.906 (57.984)	Acc@5 84.375 (85.938)
Epoch: [67][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2868 (2.3192)	Acc@1 61.719 (57.721)	Acc@5 88.281 (86.075)
Epoch: [67][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3258 (2.3325)	Acc@1 53.906 (57.287)	Acc@5 84.375 (85.873)
Epoch: [67][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.6296 (2.3414)	Acc@1 48.438 (57.229)	Acc@5 83.594 (85.618)
Epoch: [67][80/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2989 (2.3467)	Acc@1 60.156 (57.157)	Acc@5 85.156 (85.590)
Epoch: [67][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2852 (2.3535)	Acc@1 56.250 (56.911)	Acc@5 87.500 (85.491)
Epoch: [67][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6112 (2.3619)	Acc@1 48.438 (56.830)	Acc@5 80.469 (85.357)
Epoch: [67][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3527 (2.3650)	Acc@1 61.719 (56.905)	Acc@5 85.938 (85.170)
Epoch: [67][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4891 (2.3660)	Acc@1 53.906 (56.928)	Acc@5 83.594 (85.189)
Epoch: [67][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3562 (2.3701)	Acc@1 57.031 (56.721)	Acc@5 85.156 (85.120)
Epoch: [67][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3127 (2.3752)	Acc@1 50.781 (56.544)	Acc@5 85.938 (85.040)
Epoch: [67][150/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4698 (2.3815)	Acc@1 53.125 (56.436)	Acc@5 79.688 (84.856)
Epoch: [67][160/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3484 (2.3853)	Acc@1 53.906 (56.231)	Acc@5 85.156 (84.836)
Epoch: [67][170/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.5632 (2.3905)	Acc@1 53.906 (56.095)	Acc@5 82.812 (84.786)
Epoch: [67][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2686 (2.3920)	Acc@1 59.375 (55.995)	Acc@5 88.281 (84.751)
Epoch: [67][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4457 (2.3944)	Acc@1 58.594 (55.964)	Acc@5 82.812 (84.686)
Epoch: [67][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4853 (2.3950)	Acc@1 57.812 (55.951)	Acc@5 81.250 (84.698)
Epoch: [67][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4405 (2.3944)	Acc@1 50.781 (55.983)	Acc@5 86.719 (84.697)
Epoch: [67][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4059 (2.3985)	Acc@1 56.250 (55.882)	Acc@5 82.812 (84.594)
Epoch: [67][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3012 (2.3951)	Acc@1 61.719 (55.956)	Acc@5 79.688 (84.669)
Epoch: [67][240/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4780 (2.3970)	Acc@1 51.562 (55.936)	Acc@5 83.594 (84.615)
Epoch: [67][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4907 (2.4019)	Acc@1 50.781 (55.836)	Acc@5 85.938 (84.540)
Epoch: [67][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4944 (2.4081)	Acc@1 52.344 (55.672)	Acc@5 83.594 (84.444)
Epoch: [67][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8517 (2.4122)	Acc@1 42.188 (55.587)	Acc@5 75.000 (84.361)
Epoch: [67][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6603 (2.4144)	Acc@1 54.688 (55.527)	Acc@5 75.781 (84.353)
Epoch: [67][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3259 (2.4149)	Acc@1 60.156 (55.504)	Acc@5 88.281 (84.375)
Epoch: [67][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3814 (2.4126)	Acc@1 54.688 (55.562)	Acc@5 82.812 (84.409)
Epoch: [67][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4119 (2.4133)	Acc@1 57.031 (55.569)	Acc@5 85.938 (84.420)
Epoch: [67][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4956 (2.4133)	Acc@1 52.344 (55.595)	Acc@5 82.031 (84.397)
Epoch: [67][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3472 (2.4153)	Acc@1 54.688 (55.530)	Acc@5 85.156 (84.368)
Epoch: [67][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6235 (2.4194)	Acc@1 51.562 (55.384)	Acc@5 82.812 (84.352)
Epoch: [67][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3093 (2.4215)	Acc@1 55.469 (55.320)	Acc@5 87.500 (84.319)
Epoch: [67][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4307 (2.4235)	Acc@1 57.812 (55.293)	Acc@5 82.812 (84.262)
Epoch: [67][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5959 (2.4247)	Acc@1 53.125 (55.248)	Acc@5 81.250 (84.215)
Epoch: [67][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6753 (2.4274)	Acc@1 52.344 (55.190)	Acc@5 80.469 (84.172)
Epoch: [67][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6025 (2.4277)	Acc@1 52.500 (55.194)	Acc@5 81.250 (84.154)
num momentum params: 26
[0.1, 2.4276751331329347, 2.490293904542923, 55.194, 41.36, tensor(0.3276, device='cuda:0', grad_fn=<DivBackward0>), 4.374716758728027, 0.3411703109741211]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [68 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [68][0/391]	Time 0.048 (0.048)	Data 0.167 (0.167)	Loss 2.0918 (2.0918)	Acc@1 61.719 (61.719)	Acc@5 89.844 (89.844)
Epoch: [68][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.4577 (2.3397)	Acc@1 55.469 (58.026)	Acc@5 87.500 (86.222)
Epoch: [68][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.2265 (2.3318)	Acc@1 61.719 (57.887)	Acc@5 85.938 (86.272)
Epoch: [68][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.4751 (2.3463)	Acc@1 53.125 (57.510)	Acc@5 89.062 (86.114)
Epoch: [68][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2849 (2.3503)	Acc@1 60.938 (57.107)	Acc@5 87.500 (85.899)
Epoch: [68][50/391]	Time 0.014 (0.012)	Data 0.001 (0.005)	Loss 2.5028 (2.3615)	Acc@1 56.250 (56.985)	Acc@5 78.906 (85.141)
Epoch: [68][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3648 (2.3670)	Acc@1 57.031 (56.481)	Acc@5 84.375 (85.195)
Epoch: [68][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5375 (2.3759)	Acc@1 55.469 (56.195)	Acc@5 84.375 (85.090)
Epoch: [68][80/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.3649 (2.3816)	Acc@1 54.688 (55.970)	Acc@5 84.375 (84.963)
Epoch: [68][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2492 (2.3864)	Acc@1 63.281 (55.838)	Acc@5 85.938 (84.727)
Epoch: [68][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3717 (2.3895)	Acc@1 60.938 (55.763)	Acc@5 79.688 (84.646)
Epoch: [68][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.7129 (2.3980)	Acc@1 47.656 (55.476)	Acc@5 80.469 (84.523)
Epoch: [68][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5931 (2.3982)	Acc@1 50.781 (55.378)	Acc@5 82.812 (84.569)
Epoch: [68][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5313 (2.4012)	Acc@1 50.000 (55.367)	Acc@5 86.719 (84.494)
Epoch: [68][140/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.2677 (2.4045)	Acc@1 60.156 (55.402)	Acc@5 85.156 (84.381)
Epoch: [68][150/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2408 (2.4046)	Acc@1 58.594 (55.412)	Acc@5 86.719 (84.396)
Epoch: [68][160/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4752 (2.4032)	Acc@1 47.656 (55.386)	Acc@5 82.031 (84.491)
Epoch: [68][170/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 2.4053 (2.4028)	Acc@1 55.469 (55.364)	Acc@5 86.719 (84.507)
Epoch: [68][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4058 (2.4056)	Acc@1 57.031 (55.275)	Acc@5 82.031 (84.435)
Epoch: [68][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4423 (2.4047)	Acc@1 55.469 (55.321)	Acc@5 85.156 (84.498)
Epoch: [68][200/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3718 (2.4060)	Acc@1 59.375 (55.329)	Acc@5 86.719 (84.461)
Epoch: [68][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4581 (2.4124)	Acc@1 51.562 (55.161)	Acc@5 84.375 (84.327)
Epoch: [68][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6520 (2.4153)	Acc@1 47.656 (55.087)	Acc@5 78.906 (84.311)
Epoch: [68][230/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6637 (2.4181)	Acc@1 50.781 (54.989)	Acc@5 78.906 (84.257)
Epoch: [68][240/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3626 (2.4213)	Acc@1 52.344 (54.898)	Acc@5 86.719 (84.180)
Epoch: [68][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3440 (2.4213)	Acc@1 58.594 (54.993)	Acc@5 87.500 (84.179)
Epoch: [68][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4395 (2.4201)	Acc@1 55.469 (55.026)	Acc@5 82.031 (84.186)
Epoch: [68][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2828 (2.4203)	Acc@1 57.812 (55.036)	Acc@5 82.812 (84.185)
Epoch: [68][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5548 (2.4212)	Acc@1 54.688 (55.032)	Acc@5 81.250 (84.139)
Epoch: [68][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4393 (2.4230)	Acc@1 50.781 (55.010)	Acc@5 84.375 (84.104)
Epoch: [68][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3345 (2.4227)	Acc@1 60.938 (55.035)	Acc@5 84.375 (84.121)
Epoch: [68][310/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.2188 (2.4218)	Acc@1 64.062 (55.102)	Acc@5 86.719 (84.149)
Epoch: [68][320/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.1631 (2.4238)	Acc@1 57.812 (55.021)	Acc@5 89.844 (84.141)
Epoch: [68][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1683 (2.4267)	Acc@1 61.719 (54.968)	Acc@5 89.062 (84.106)
Epoch: [68][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3526 (2.4273)	Acc@1 58.594 (54.988)	Acc@5 85.156 (84.102)
Epoch: [68][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3021 (2.4280)	Acc@1 58.594 (54.979)	Acc@5 85.938 (84.090)
Epoch: [68][360/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 2.4628 (2.4289)	Acc@1 56.250 (54.965)	Acc@5 82.812 (84.091)
Epoch: [68][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3591 (2.4297)	Acc@1 59.375 (54.928)	Acc@5 86.719 (84.065)
Epoch: [68][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.4991 (2.4336)	Acc@1 52.344 (54.856)	Acc@5 86.719 (84.018)
Epoch: [68][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.4628 (2.4385)	Acc@1 53.750 (54.748)	Acc@5 86.250 (83.936)
num momentum params: 26
[0.1, 2.4385141845703124, 2.177848205566406, 54.748, 44.88, tensor(0.3267, device='cuda:0', grad_fn=<DivBackward0>), 4.521430492401123, 0.33606910705566406]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [69 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [69][0/391]	Time 0.051 (0.051)	Data 0.155 (0.155)	Loss 2.3480 (2.3480)	Acc@1 58.594 (58.594)	Acc@5 89.062 (89.062)
Epoch: [69][10/391]	Time 0.012 (0.016)	Data 0.001 (0.015)	Loss 2.1908 (2.3444)	Acc@1 63.281 (56.250)	Acc@5 87.500 (85.938)
Epoch: [69][20/391]	Time 0.011 (0.014)	Data 0.007 (0.009)	Loss 2.3429 (2.3739)	Acc@1 53.906 (56.176)	Acc@5 89.062 (85.938)
Epoch: [69][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.4260 (2.3519)	Acc@1 55.469 (57.056)	Acc@5 85.938 (85.938)
Epoch: [69][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.5056 (2.3558)	Acc@1 53.906 (56.803)	Acc@5 81.250 (85.861)
Epoch: [69][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.1605 (2.3317)	Acc@1 60.156 (57.200)	Acc@5 88.281 (86.366)
Epoch: [69][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2424 (2.3483)	Acc@1 64.062 (56.852)	Acc@5 85.938 (85.912)
Epoch: [69][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4842 (2.3470)	Acc@1 56.250 (56.855)	Acc@5 80.469 (85.938)
Epoch: [69][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1439 (2.3439)	Acc@1 65.625 (56.983)	Acc@5 89.062 (85.947)
Epoch: [69][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3996 (2.3372)	Acc@1 55.469 (57.117)	Acc@5 84.375 (86.023)
Epoch: [69][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1563 (2.3404)	Acc@1 57.812 (56.969)	Acc@5 87.500 (85.976)
Epoch: [69][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1246 (2.3457)	Acc@1 65.625 (56.905)	Acc@5 88.281 (85.755)
Epoch: [69][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2167 (2.3491)	Acc@1 59.375 (56.831)	Acc@5 86.719 (85.621)
Epoch: [69][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 2.1673 (2.3559)	Acc@1 59.375 (56.703)	Acc@5 88.281 (85.586)
Epoch: [69][140/391]	Time 0.013 (0.011)	Data 0.002 (0.003)	Loss 2.5906 (2.3619)	Acc@1 45.312 (56.577)	Acc@5 82.812 (85.478)
Epoch: [69][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4728 (2.3656)	Acc@1 50.000 (56.447)	Acc@5 85.938 (85.441)
Epoch: [69][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5443 (2.3723)	Acc@1 47.656 (56.250)	Acc@5 82.031 (85.273)
Epoch: [69][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6384 (2.3793)	Acc@1 49.219 (56.067)	Acc@5 81.250 (85.175)
Epoch: [69][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4094 (2.3811)	Acc@1 53.906 (56.039)	Acc@5 89.062 (85.182)
Epoch: [69][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3582 (2.3846)	Acc@1 54.688 (55.919)	Acc@5 85.156 (85.173)
Epoch: [69][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3308 (2.3864)	Acc@1 56.250 (55.826)	Acc@5 85.156 (85.106)
Epoch: [69][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2520 (2.3863)	Acc@1 57.031 (55.820)	Acc@5 86.719 (85.130)
Epoch: [69][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1226 (2.3859)	Acc@1 63.281 (55.840)	Acc@5 90.625 (85.064)
Epoch: [69][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4564 (2.3899)	Acc@1 54.688 (55.712)	Acc@5 81.250 (84.987)
Epoch: [69][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4995 (2.3918)	Acc@1 54.688 (55.686)	Acc@5 82.812 (84.913)
Epoch: [69][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2643 (2.3949)	Acc@1 56.250 (55.659)	Acc@5 86.719 (84.845)
Epoch: [69][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5891 (2.3985)	Acc@1 49.219 (55.582)	Acc@5 81.250 (84.785)
Epoch: [69][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2985 (2.3982)	Acc@1 58.594 (55.616)	Acc@5 84.375 (84.773)
Epoch: [69][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2393 (2.3984)	Acc@1 57.031 (55.613)	Acc@5 89.844 (84.756)
Epoch: [69][290/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.3400 (2.4028)	Acc@1 56.250 (55.498)	Acc@5 83.594 (84.692)
Epoch: [69][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5899 (2.4042)	Acc@1 51.562 (55.448)	Acc@5 81.250 (84.697)
Epoch: [69][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5115 (2.4071)	Acc@1 51.562 (55.401)	Acc@5 82.812 (84.646)
Epoch: [69][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7616 (2.4119)	Acc@1 47.656 (55.276)	Acc@5 80.469 (84.528)
Epoch: [69][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6237 (2.4145)	Acc@1 53.906 (55.273)	Acc@5 83.594 (84.484)
Epoch: [69][340/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.7640 (2.4176)	Acc@1 48.438 (55.212)	Acc@5 75.781 (84.407)
Epoch: [69][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5774 (2.4187)	Acc@1 47.656 (55.177)	Acc@5 85.156 (84.388)
Epoch: [69][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5347 (2.4216)	Acc@1 50.781 (55.097)	Acc@5 85.938 (84.338)
Epoch: [69][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2337 (2.4196)	Acc@1 62.500 (55.151)	Acc@5 88.281 (84.396)
Epoch: [69][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4149 (2.4219)	Acc@1 55.469 (55.137)	Acc@5 85.938 (84.367)
Epoch: [69][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7634 (2.4240)	Acc@1 47.500 (55.080)	Acc@5 80.000 (84.328)
num momentum params: 26
[0.1, 2.4240499520874024, 1.9836664378643036, 55.08, 48.32, tensor(0.3288, device='cuda:0', grad_fn=<DivBackward0>), 4.415522813796997, 0.33439087867736816]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [227, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [227]
Non Pruning Epoch - module.bn3.bias: [227]
Non Pruning Epoch - module.conv4.weight: [249, 227, 3, 3]
Non Pruning Epoch - module.bn4.weight: [249]
Non Pruning Epoch - module.bn4.bias: [249]
Non Pruning Epoch - module.conv5.weight: [381, 249, 3, 3]
Non Pruning Epoch - module.bn5.weight: [381]
Non Pruning Epoch - module.bn5.bias: [381]
Non Pruning Epoch - module.conv6.weight: [339, 381, 3, 3]
Non Pruning Epoch - module.bn6.weight: [339]
Non Pruning Epoch - module.bn6.bias: [339]
Non Pruning Epoch - module.conv7.weight: [239, 339, 3, 3]
Non Pruning Epoch - module.bn7.weight: [239]
Non Pruning Epoch - module.bn7.bias: [239]
Non Pruning Epoch - module.conv8.weight: [214, 239, 3, 3]
Non Pruning Epoch - module.bn8.weight: [214]
Non Pruning Epoch - module.bn8.bias: [214]
Non Pruning Epoch - module.fc.weight: [100, 214]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [70 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [227, 112, 3, 3]
module.conv4.weight [249, 227, 3, 3]
module.conv5.weight [381, 249, 3, 3]
module.conv6.weight [339, 381, 3, 3]
module.conv7.weight [239, 339, 3, 3]
module.conv8.weight [214, 239, 3, 3]
Epoch: [70][0/391]	Time 0.043 (0.043)	Data 0.152 (0.152)	Loss 2.3520 (2.3520)	Acc@1 58.594 (58.594)	Acc@5 86.719 (86.719)
Epoch: [70][10/391]	Time 0.011 (0.015)	Data 0.001 (0.015)	Loss 2.4139 (2.2733)	Acc@1 53.125 (59.091)	Acc@5 84.375 (87.145)
Epoch: [70][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.2061 (2.3208)	Acc@1 58.594 (57.254)	Acc@5 88.281 (86.607)
Epoch: [70][30/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.3196 (2.3394)	Acc@1 57.031 (57.082)	Acc@5 87.500 (86.013)
Epoch: [70][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.3450 (2.3547)	Acc@1 57.031 (56.764)	Acc@5 83.594 (85.309)
Epoch: [70][50/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 2.4458 (2.3702)	Acc@1 54.688 (56.158)	Acc@5 85.156 (85.110)
Epoch: [70][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2732 (2.3614)	Acc@1 57.812 (56.365)	Acc@5 87.500 (85.207)
Epoch: [70][70/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 2.0174 (2.3597)	Acc@1 69.531 (56.712)	Acc@5 89.062 (85.222)
Epoch: [70][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4104 (2.3649)	Acc@1 53.125 (56.375)	Acc@5 85.938 (85.243)
Epoch: [70][90/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.5595 (2.3752)	Acc@1 50.000 (56.027)	Acc@5 84.375 (85.182)
Epoch: [70][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.1185 (2.3743)	Acc@1 61.719 (56.134)	Acc@5 86.719 (85.063)
Epoch: [70][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4681 (2.3791)	Acc@1 55.469 (55.961)	Acc@5 82.812 (85.051)
Epoch: [70][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3449 (2.3797)	Acc@1 55.469 (55.921)	Acc@5 82.031 (85.008)
Epoch: [70][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5820 (2.3823)	Acc@1 50.000 (55.844)	Acc@5 78.125 (85.037)
Epoch: [70][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3991 (2.3829)	Acc@1 57.812 (55.951)	Acc@5 81.250 (85.034)
Epoch: [70][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4885 (2.3796)	Acc@1 57.031 (56.105)	Acc@5 81.250 (85.037)
Epoch: [70][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5464 (2.3830)	Acc@1 51.562 (56.027)	Acc@5 80.469 (84.991)
Epoch: [70][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6142 (2.3847)	Acc@1 49.219 (56.113)	Acc@5 85.156 (85.019)
Epoch: [70][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1984 (2.3853)	Acc@1 58.594 (56.008)	Acc@5 88.281 (84.979)
Epoch: [70][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4507 (2.3879)	Acc@1 60.156 (56.005)	Acc@5 82.812 (84.980)
Epoch: [70][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5233 (2.3891)	Acc@1 55.469 (55.997)	Acc@5 83.594 (84.950)
Epoch: [70][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5482 (2.3927)	Acc@1 50.781 (55.846)	Acc@5 84.375 (84.886)
Epoch: [70][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3165 (2.3943)	Acc@1 54.688 (55.769)	Acc@5 85.156 (84.895)
Epoch: [70][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5671 (2.3967)	Acc@1 49.219 (55.739)	Acc@5 82.031 (84.855)
Epoch: [70][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4333 (2.3982)	Acc@1 56.250 (55.692)	Acc@5 85.156 (84.858)
Epoch: [70][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4069 (2.3972)	Acc@1 55.469 (55.764)	Acc@5 82.812 (84.854)
Epoch: [70][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4502 (2.3975)	Acc@1 54.688 (55.768)	Acc@5 86.719 (84.875)
Epoch: [70][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4003 (2.3971)	Acc@1 54.688 (55.760)	Acc@5 83.594 (84.900)
Epoch: [70][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8136 (2.3980)	Acc@1 50.000 (55.786)	Acc@5 78.906 (84.859)
Epoch: [70][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5582 (2.4028)	Acc@1 54.688 (55.692)	Acc@5 79.688 (84.783)
Epoch: [70][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7407 (2.4070)	Acc@1 49.219 (55.599)	Acc@5 76.562 (84.731)
Epoch: [70][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4633 (2.4098)	Acc@1 53.906 (55.557)	Acc@5 82.031 (84.674)
Epoch: [70][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2903 (2.4132)	Acc@1 56.250 (55.464)	Acc@5 88.281 (84.635)
Epoch: [70][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4004 (2.4152)	Acc@1 53.906 (55.436)	Acc@5 87.500 (84.606)
Epoch: [70][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4608 (2.4169)	Acc@1 53.906 (55.377)	Acc@5 81.250 (84.588)
Epoch: [70][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4956 (2.4190)	Acc@1 54.688 (55.324)	Acc@5 81.250 (84.531)
Epoch: [70][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7957 (2.4213)	Acc@1 46.875 (55.278)	Acc@5 78.906 (84.498)
Epoch: [70][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3050 (2.4221)	Acc@1 60.156 (55.256)	Acc@5 84.375 (84.468)
Epoch: [70][380/391]	Time 0.011 (0.011)	Data 0.009 (0.002)	Loss 2.5653 (2.4232)	Acc@1 52.344 (55.243)	Acc@5 83.594 (84.475)
Epoch: [70][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.3312 (2.4240)	Acc@1 50.000 (55.198)	Acc@5 87.500 (84.442)
num momentum params: 26
[0.1, 2.4240271910095217, 2.0483766555786134, 55.198, 46.37, tensor(0.3285, device='cuda:0', grad_fn=<DivBackward0>), 4.420180082321167, 0.32666635513305664]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [34, 3, 3, 3]
Before - module.bn1.weight: [34]
Before - module.bn1.bias: [34]
Before - module.conv2.weight: [112, 34, 3, 3]
Before - module.bn2.weight: [112]
Before - module.bn2.bias: [112]
Before - module.conv3.weight: [227, 112, 3, 3]
Before - module.bn3.weight: [227]
Before - module.bn3.bias: [227]
Before - module.conv4.weight: [249, 227, 3, 3]
Before - module.bn4.weight: [249]
Before - module.bn4.bias: [249]
Before - module.conv5.weight: [381, 249, 3, 3]
Before - module.bn5.weight: [381]
Before - module.bn5.bias: [381]
Before - module.conv6.weight: [339, 381, 3, 3]
Before - module.bn6.weight: [339]
Before - module.bn6.bias: [339]
Before - module.conv7.weight: [239, 339, 3, 3]
Before - module.bn7.weight: [239]
Before - module.bn7.bias: [239]
Before - module.conv8.weight: [214, 239, 3, 3]
Before - module.bn8.weight: [214]
Before - module.bn8.bias: [214]
Before - module.fc.weight: [100, 214]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [34, 3, 3, 3] >> [34, 3, 3, 3]
[module.bn1.weight]: 34 >> 34
running_mean [34]
running_var [34]
num_batches_tracked []
[module.conv2.weight]: [112, 34, 3, 3] >> [112, 34, 3, 3]
[module.bn2.weight]: 112 >> 112
running_mean [112]
running_var [112]
num_batches_tracked []
[module.conv3.weight]: [227, 112, 3, 3] >> [223, 112, 3, 3]
[module.bn3.weight]: 227 >> 223
running_mean [223]
running_var [223]
num_batches_tracked []
[module.conv4.weight]: [249, 227, 3, 3] >> [248, 223, 3, 3]
[module.bn4.weight]: 249 >> 248
running_mean [248]
running_var [248]
num_batches_tracked []
[module.conv5.weight]: [381, 249, 3, 3] >> [378, 248, 3, 3]
[module.bn5.weight]: 381 >> 378
running_mean [378]
running_var [378]
num_batches_tracked []
[module.conv6.weight]: [339, 381, 3, 3] >> [332, 378, 3, 3]
[module.bn6.weight]: 339 >> 332
running_mean [332]
running_var [332]
num_batches_tracked []
[module.conv7.weight]: [239, 339, 3, 3] >> [222, 332, 3, 3]
[module.bn7.weight]: 239 >> 222
running_mean [222]
running_var [222]
num_batches_tracked []
[module.conv8.weight]: [214, 239, 3, 3] >> [206, 222, 3, 3]
[module.bn8.weight]: 214 >> 206
running_mean [206]
running_var [206]
num_batches_tracked []
[module.fc.weight]: [100, 214] >> [100, 206]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [34, 3, 3, 3]
After - module.bn1.weight: [34]
After - module.bn1.bias: [34]
After - module.conv2.weight: [112, 34, 3, 3]
After - module.bn2.weight: [112]
After - module.bn2.bias: [112]
After - module.conv3.weight: [223, 112, 3, 3]
After - module.bn3.weight: [223]
After - module.bn3.bias: [223]
After - module.conv4.weight: [248, 223, 3, 3]
After - module.bn4.weight: [248]
After - module.bn4.bias: [248]
After - module.conv5.weight: [378, 248, 3, 3]
After - module.bn5.weight: [378]
After - module.bn5.bias: [378]
After - module.conv6.weight: [332, 378, 3, 3]
After - module.bn6.weight: [332]
After - module.bn6.bias: [332]
After - module.conv7.weight: [222, 332, 3, 3]
After - module.bn7.weight: [222]
After - module.bn7.bias: [222]
After - module.conv8.weight: [206, 222, 3, 3]
After - module.bn8.weight: [206]
After - module.bn8.bias: [206]
After - module.fc.weight: [100, 206]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [34, 3, 3, 3]
conv2 --> [112, 34, 3, 3]
conv3 --> [223, 112, 3, 3]
conv4 --> [248, 223, 3, 3]
conv5 --> [378, 248, 3, 3]
conv6 --> [332, 378, 3, 3]
conv7 --> [222, 332, 3, 3]
conv8 --> [206, 222, 3, 3]
fc --> [206, 100]
1, 376482816, 940032, 34
2, 3667378176, 8773632, 112
3, 6560096256, 14386176, 223
4, 14525927424, 31855104, 248
5, 7343529984, 13499136, 378
6, 9830854656, 18071424, 332
7, 2037768192, 2653344, 222
8, 1264398336, 1646352, 206
fc, 7910400, 20600, 0
===================
FLOP REPORT: 17818104000000.0 42681600000.0 91845800 106704 1755 7.302007675170898
[INFO] Storing checkpoint...

Epoch: [71 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [71][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 2.4772 (2.4772)	Acc@1 53.906 (53.906)	Acc@5 84.375 (84.375)
Epoch: [71][10/391]	Time 0.013 (0.031)	Data 0.004 (0.018)	Loss 2.2847 (2.3702)	Acc@1 61.719 (56.321)	Acc@5 83.594 (84.375)
Epoch: [71][20/391]	Time 0.011 (0.021)	Data 0.001 (0.010)	Loss 2.2038 (2.3346)	Acc@1 62.500 (57.329)	Acc@5 87.500 (85.082)
Epoch: [71][30/391]	Time 0.012 (0.018)	Data 0.001 (0.007)	Loss 2.4115 (2.3562)	Acc@1 54.688 (56.880)	Acc@5 84.375 (84.904)
Epoch: [71][40/391]	Time 0.011 (0.016)	Data 0.001 (0.006)	Loss 2.2391 (2.3614)	Acc@1 58.594 (56.860)	Acc@5 86.719 (84.985)
Epoch: [71][50/391]	Time 0.011 (0.015)	Data 0.001 (0.005)	Loss 2.2397 (2.3587)	Acc@1 64.062 (57.016)	Acc@5 85.938 (84.850)
Epoch: [71][60/391]	Time 0.012 (0.015)	Data 0.002 (0.004)	Loss 2.2303 (2.3604)	Acc@1 62.500 (57.006)	Acc@5 85.156 (84.887)
Epoch: [71][70/391]	Time 0.012 (0.014)	Data 0.001 (0.004)	Loss 2.3346 (2.3565)	Acc@1 61.719 (57.108)	Acc@5 86.719 (84.958)
Epoch: [71][80/391]	Time 0.010 (0.014)	Data 0.001 (0.004)	Loss 2.5029 (2.3633)	Acc@1 59.375 (57.060)	Acc@5 82.812 (84.848)
Epoch: [71][90/391]	Time 0.010 (0.014)	Data 0.001 (0.003)	Loss 2.3031 (2.3593)	Acc@1 54.688 (57.005)	Acc@5 86.719 (84.959)
Epoch: [71][100/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.3350 (2.3611)	Acc@1 57.812 (56.915)	Acc@5 88.281 (84.971)
Epoch: [71][110/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.3566 (2.3657)	Acc@1 57.812 (56.743)	Acc@5 84.375 (84.882)
Epoch: [71][120/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.3873 (2.3636)	Acc@1 57.812 (56.767)	Acc@5 85.156 (84.969)
Epoch: [71][130/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.5677 (2.3719)	Acc@1 55.469 (56.489)	Acc@5 82.031 (84.971)
Epoch: [71][140/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.5746 (2.3797)	Acc@1 54.688 (56.305)	Acc@5 78.125 (84.791)
Epoch: [71][150/391]	Time 0.015 (0.013)	Data 0.001 (0.003)	Loss 2.4789 (2.3899)	Acc@1 55.469 (56.110)	Acc@5 82.031 (84.670)
Epoch: [71][160/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4333 (2.3941)	Acc@1 55.469 (55.964)	Acc@5 86.719 (84.608)
Epoch: [71][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2905 (2.3975)	Acc@1 61.719 (56.012)	Acc@5 81.250 (84.512)
Epoch: [71][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5208 (2.3945)	Acc@1 53.906 (56.047)	Acc@5 80.469 (84.578)
Epoch: [71][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2775 (2.3925)	Acc@1 60.156 (56.054)	Acc@5 86.719 (84.625)
Epoch: [71][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5685 (2.3950)	Acc@1 50.781 (55.993)	Acc@5 82.812 (84.608)
Epoch: [71][210/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2637 (2.3927)	Acc@1 53.906 (56.006)	Acc@5 87.500 (84.623)
Epoch: [71][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7202 (2.3980)	Acc@1 53.906 (55.900)	Acc@5 75.000 (84.523)
Epoch: [71][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6878 (2.4000)	Acc@1 49.219 (55.841)	Acc@5 82.031 (84.504)
Epoch: [71][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2289 (2.3986)	Acc@1 60.156 (55.848)	Acc@5 82.812 (84.537)
Epoch: [71][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3424 (2.3980)	Acc@1 57.812 (55.848)	Acc@5 82.031 (84.540)
Epoch: [71][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3324 (2.3972)	Acc@1 53.125 (55.864)	Acc@5 84.375 (84.588)
Epoch: [71][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2776 (2.3948)	Acc@1 53.906 (55.881)	Acc@5 85.156 (84.588)
Epoch: [71][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6817 (2.3964)	Acc@1 53.125 (55.880)	Acc@5 76.562 (84.539)
Epoch: [71][290/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.7389 (2.4005)	Acc@1 48.438 (55.743)	Acc@5 80.469 (84.488)
Epoch: [71][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6908 (2.4066)	Acc@1 50.000 (55.617)	Acc@5 77.344 (84.333)
Epoch: [71][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4728 (2.4089)	Acc@1 55.469 (55.557)	Acc@5 80.469 (84.300)
Epoch: [71][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2455 (2.4092)	Acc@1 59.375 (55.566)	Acc@5 87.500 (84.326)
Epoch: [71][330/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5711 (2.4117)	Acc@1 51.562 (55.473)	Acc@5 85.938 (84.325)
Epoch: [71][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4200 (2.4147)	Acc@1 55.469 (55.373)	Acc@5 84.375 (84.297)
Epoch: [71][350/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5247 (2.4175)	Acc@1 49.219 (55.297)	Acc@5 77.344 (84.268)
Epoch: [71][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5950 (2.4200)	Acc@1 57.031 (55.242)	Acc@5 78.906 (84.247)
Epoch: [71][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4665 (2.4229)	Acc@1 54.688 (55.193)	Acc@5 82.031 (84.185)
Epoch: [71][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5322 (2.4263)	Acc@1 52.344 (55.110)	Acc@5 82.812 (84.100)
Epoch: [71][390/391]	Time 0.121 (0.012)	Data 0.001 (0.002)	Loss 2.5676 (2.4283)	Acc@1 51.250 (55.082)	Acc@5 86.250 (84.096)
num momentum params: 26
[0.1, 2.4282718869018556, 1.9084069502353669, 55.082, 50.19, tensor(0.3284, device='cuda:0', grad_fn=<DivBackward0>), 4.594534397125244, 0.3902435302734375]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [72 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [72][0/391]	Time 0.054 (0.054)	Data 0.164 (0.164)	Loss 2.2581 (2.2581)	Acc@1 57.031 (57.031)	Acc@5 82.812 (82.812)
Epoch: [72][10/391]	Time 0.010 (0.016)	Data 0.001 (0.016)	Loss 2.2234 (2.3712)	Acc@1 59.375 (55.043)	Acc@5 87.500 (84.162)
Epoch: [72][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.3571 (2.3705)	Acc@1 53.125 (55.022)	Acc@5 83.594 (84.524)
Epoch: [72][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.1900 (2.3473)	Acc@1 55.469 (55.519)	Acc@5 88.281 (84.904)
Epoch: [72][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2675 (2.3529)	Acc@1 58.594 (55.373)	Acc@5 85.156 (84.909)
Epoch: [72][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.7120 (2.3516)	Acc@1 48.438 (55.469)	Acc@5 82.812 (85.263)
Epoch: [72][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3934 (2.3655)	Acc@1 53.125 (55.328)	Acc@5 85.156 (84.926)
Epoch: [72][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.3614 (2.3776)	Acc@1 54.688 (55.260)	Acc@5 84.375 (84.584)
Epoch: [72][80/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2584 (2.3745)	Acc@1 59.375 (55.575)	Acc@5 88.281 (84.597)
Epoch: [72][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5061 (2.3677)	Acc@1 50.781 (55.761)	Acc@5 80.469 (84.830)
Epoch: [72][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3922 (2.3675)	Acc@1 54.688 (55.794)	Acc@5 85.156 (84.870)
Epoch: [72][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3100 (2.3710)	Acc@1 56.250 (55.870)	Acc@5 88.281 (84.790)
Epoch: [72][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3771 (2.3723)	Acc@1 58.594 (55.966)	Acc@5 83.594 (84.749)
Epoch: [72][130/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3736 (2.3775)	Acc@1 60.938 (55.934)	Acc@5 87.500 (84.709)
Epoch: [72][140/391]	Time 0.014 (0.011)	Data 0.001 (0.003)	Loss 2.4265 (2.3791)	Acc@1 50.000 (55.945)	Acc@5 88.281 (84.768)
Epoch: [72][150/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2576 (2.3793)	Acc@1 60.156 (55.945)	Acc@5 83.594 (84.815)
Epoch: [72][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3556 (2.3814)	Acc@1 55.469 (55.910)	Acc@5 83.594 (84.749)
Epoch: [72][170/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4025 (2.3848)	Acc@1 53.906 (55.747)	Acc@5 85.156 (84.654)
Epoch: [72][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4620 (2.3846)	Acc@1 58.594 (55.827)	Acc@5 81.250 (84.608)
Epoch: [72][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5150 (2.3865)	Acc@1 47.656 (55.714)	Acc@5 80.469 (84.633)
Epoch: [72][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5480 (2.3896)	Acc@1 50.000 (55.616)	Acc@5 80.469 (84.554)
Epoch: [72][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5423 (2.3914)	Acc@1 56.250 (55.587)	Acc@5 83.594 (84.586)
Epoch: [72][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4263 (2.3927)	Acc@1 56.250 (55.596)	Acc@5 83.594 (84.576)
Epoch: [72][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3570 (2.3955)	Acc@1 56.250 (55.540)	Acc@5 88.281 (84.558)
Epoch: [72][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5285 (2.3992)	Acc@1 52.344 (55.472)	Acc@5 83.594 (84.511)
Epoch: [72][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3099 (2.4063)	Acc@1 56.250 (55.301)	Acc@5 85.938 (84.406)
Epoch: [72][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6167 (2.4116)	Acc@1 48.438 (55.181)	Acc@5 81.250 (84.303)
Epoch: [72][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.8495 (2.4151)	Acc@1 47.656 (55.129)	Acc@5 74.219 (84.254)
Epoch: [72][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5538 (2.4203)	Acc@1 53.906 (55.010)	Acc@5 83.594 (84.164)
Epoch: [72][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2372 (2.4188)	Acc@1 62.500 (55.096)	Acc@5 88.281 (84.163)
Epoch: [72][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3524 (2.4163)	Acc@1 54.688 (55.134)	Acc@5 85.938 (84.224)
Epoch: [72][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4990 (2.4143)	Acc@1 46.875 (55.167)	Acc@5 85.156 (84.252)
Epoch: [72][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3291 (2.4152)	Acc@1 54.688 (55.177)	Acc@5 85.938 (84.222)
Epoch: [72][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3704 (2.4182)	Acc@1 53.125 (55.077)	Acc@5 83.594 (84.165)
Epoch: [72][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4479 (2.4200)	Acc@1 54.688 (55.022)	Acc@5 79.688 (84.132)
Epoch: [72][350/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 2.5623 (2.4199)	Acc@1 50.000 (55.041)	Acc@5 80.469 (84.137)
Epoch: [72][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6706 (2.4204)	Acc@1 49.219 (55.019)	Acc@5 82.031 (84.172)
Epoch: [72][370/391]	Time 0.018 (0.011)	Data 0.001 (0.002)	Loss 2.4993 (2.4180)	Acc@1 52.344 (55.067)	Acc@5 85.938 (84.230)
Epoch: [72][380/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.4646 (2.4192)	Acc@1 56.250 (55.075)	Acc@5 85.938 (84.209)
Epoch: [72][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.9865 (2.4228)	Acc@1 48.750 (54.980)	Acc@5 72.500 (84.158)
num momentum params: 26
[0.1, 2.42276522895813, 2.699143500328064, 54.98, 39.12, tensor(0.3288, device='cuda:0', grad_fn=<DivBackward0>), 4.308293104171753, 0.332261323928833]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [73 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [73][0/391]	Time 0.047 (0.047)	Data 0.181 (0.181)	Loss 2.1526 (2.1526)	Acc@1 59.375 (59.375)	Acc@5 86.719 (86.719)
Epoch: [73][10/391]	Time 0.010 (0.015)	Data 0.001 (0.018)	Loss 2.4536 (2.3882)	Acc@1 52.344 (55.256)	Acc@5 85.156 (85.085)
Epoch: [73][20/391]	Time 0.011 (0.013)	Data 0.001 (0.010)	Loss 2.5447 (2.3545)	Acc@1 50.781 (55.878)	Acc@5 82.812 (85.677)
Epoch: [73][30/391]	Time 0.012 (0.012)	Data 0.001 (0.007)	Loss 2.1911 (2.3507)	Acc@1 59.375 (56.124)	Acc@5 87.500 (85.736)
Epoch: [73][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.2932 (2.3442)	Acc@1 58.594 (56.498)	Acc@5 87.500 (85.614)
Epoch: [73][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4222 (2.3380)	Acc@1 55.469 (57.047)	Acc@5 85.938 (85.846)
Epoch: [73][60/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.6571 (2.3398)	Acc@1 45.312 (56.801)	Acc@5 79.688 (85.720)
Epoch: [73][70/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.5204 (2.3471)	Acc@1 53.125 (56.591)	Acc@5 82.031 (85.431)
Epoch: [73][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.4510 (2.3520)	Acc@1 55.469 (56.385)	Acc@5 85.156 (85.301)
Epoch: [73][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5664 (2.3519)	Acc@1 52.344 (56.379)	Acc@5 78.125 (85.242)
Epoch: [73][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2665 (2.3544)	Acc@1 59.375 (56.304)	Acc@5 86.719 (85.226)
Epoch: [73][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4898 (2.3651)	Acc@1 52.344 (56.116)	Acc@5 86.719 (85.128)
Epoch: [73][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1387 (2.3641)	Acc@1 60.156 (56.140)	Acc@5 89.062 (85.105)
Epoch: [73][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4950 (2.3746)	Acc@1 58.594 (55.797)	Acc@5 82.031 (84.995)
Epoch: [73][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5534 (2.3848)	Acc@1 50.000 (55.663)	Acc@5 85.938 (84.813)
Epoch: [73][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5780 (2.3903)	Acc@1 53.906 (55.588)	Acc@5 81.250 (84.680)
Epoch: [73][160/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1206 (2.3975)	Acc@1 61.719 (55.386)	Acc@5 86.719 (84.501)
Epoch: [73][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0247 (2.3983)	Acc@1 60.938 (55.300)	Acc@5 89.062 (84.485)
Epoch: [73][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2753 (2.3956)	Acc@1 57.031 (55.400)	Acc@5 89.062 (84.561)
Epoch: [73][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3893 (2.4006)	Acc@1 55.469 (55.358)	Acc@5 83.594 (84.490)
Epoch: [73][200/391]	Time 0.010 (0.011)	Data 0.005 (0.002)	Loss 2.4964 (2.4016)	Acc@1 53.906 (55.442)	Acc@5 82.812 (84.437)
Epoch: [73][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3011 (2.3985)	Acc@1 59.375 (55.472)	Acc@5 89.062 (84.523)
Epoch: [73][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2800 (2.4012)	Acc@1 56.250 (55.462)	Acc@5 85.938 (84.478)
Epoch: [73][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3615 (2.4059)	Acc@1 60.938 (55.364)	Acc@5 85.938 (84.412)
Epoch: [73][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7037 (2.4132)	Acc@1 50.000 (55.154)	Acc@5 82.031 (84.275)
Epoch: [73][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2103 (2.4125)	Acc@1 57.812 (55.192)	Acc@5 87.500 (84.307)
Epoch: [73][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6900 (2.4134)	Acc@1 50.000 (55.178)	Acc@5 80.469 (84.297)
Epoch: [73][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4198 (2.4154)	Acc@1 57.031 (55.146)	Acc@5 82.031 (84.251)
Epoch: [73][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0232 (2.4141)	Acc@1 64.844 (55.216)	Acc@5 89.062 (84.264)
Epoch: [73][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4514 (2.4151)	Acc@1 53.906 (55.165)	Acc@5 82.812 (84.243)
Epoch: [73][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3769 (2.4180)	Acc@1 52.344 (55.124)	Acc@5 84.375 (84.217)
Epoch: [73][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5588 (2.4227)	Acc@1 50.781 (55.044)	Acc@5 80.469 (84.136)
Epoch: [73][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2307 (2.4215)	Acc@1 60.156 (55.053)	Acc@5 85.938 (84.134)
Epoch: [73][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3297 (2.4217)	Acc@1 57.031 (55.013)	Acc@5 82.031 (84.122)
Epoch: [73][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2828 (2.4234)	Acc@1 60.938 (54.953)	Acc@5 85.156 (84.114)
Epoch: [73][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6709 (2.4233)	Acc@1 51.562 (54.977)	Acc@5 79.688 (84.110)
Epoch: [73][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4512 (2.4249)	Acc@1 60.156 (54.926)	Acc@5 81.250 (84.074)
Epoch: [73][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2685 (2.4264)	Acc@1 56.250 (54.873)	Acc@5 90.625 (84.068)
Epoch: [73][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3499 (2.4271)	Acc@1 56.250 (54.835)	Acc@5 89.844 (84.078)
Epoch: [73][390/391]	Time 0.011 (0.011)	Data 0.000 (0.002)	Loss 2.6451 (2.4281)	Acc@1 47.500 (54.810)	Acc@5 82.500 (84.092)
num momentum params: 26
[0.1, 2.4280656438446044, 2.0470913314819335, 54.81, 47.23, tensor(0.3278, device='cuda:0', grad_fn=<DivBackward0>), 4.275014400482178, 0.3245363235473633]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [74 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [74][0/391]	Time 0.047 (0.047)	Data 0.157 (0.157)	Loss 2.2997 (2.2997)	Acc@1 54.688 (54.688)	Acc@5 85.156 (85.156)
Epoch: [74][10/391]	Time 0.011 (0.014)	Data 0.001 (0.016)	Loss 2.3333 (2.3781)	Acc@1 54.688 (55.398)	Acc@5 88.281 (83.949)
Epoch: [74][20/391]	Time 0.010 (0.013)	Data 0.001 (0.009)	Loss 2.4017 (2.3605)	Acc@1 57.812 (56.176)	Acc@5 85.156 (85.156)
Epoch: [74][30/391]	Time 0.010 (0.012)	Data 0.001 (0.007)	Loss 2.5571 (2.3554)	Acc@1 52.344 (56.250)	Acc@5 82.812 (85.181)
Epoch: [74][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4462 (2.3663)	Acc@1 53.906 (56.193)	Acc@5 80.469 (84.985)
Epoch: [74][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.1149 (2.3694)	Acc@1 64.062 (56.373)	Acc@5 87.500 (84.972)
Epoch: [74][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2704 (2.3652)	Acc@1 53.906 (56.545)	Acc@5 87.500 (85.054)
Epoch: [74][70/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2826 (2.3712)	Acc@1 61.719 (56.338)	Acc@5 84.375 (84.969)
Epoch: [74][80/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5352 (2.3645)	Acc@1 50.000 (56.568)	Acc@5 78.906 (85.108)
Epoch: [74][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2050 (2.3577)	Acc@1 56.250 (56.628)	Acc@5 85.156 (85.199)
Epoch: [74][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4203 (2.3664)	Acc@1 54.688 (56.405)	Acc@5 83.594 (85.056)
Epoch: [74][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3908 (2.3699)	Acc@1 53.125 (56.250)	Acc@5 87.500 (85.058)
Epoch: [74][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5790 (2.3809)	Acc@1 50.781 (56.043)	Acc@5 82.031 (84.859)
Epoch: [74][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5855 (2.3847)	Acc@1 55.469 (55.934)	Acc@5 75.781 (84.727)
Epoch: [74][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1183 (2.3818)	Acc@1 61.719 (56.006)	Acc@5 92.969 (84.885)
Epoch: [74][150/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4048 (2.3793)	Acc@1 49.219 (56.110)	Acc@5 83.594 (84.856)
Epoch: [74][160/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.7104 (2.3818)	Acc@1 51.562 (56.075)	Acc@5 81.250 (84.783)
Epoch: [74][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4470 (2.3872)	Acc@1 57.812 (56.026)	Acc@5 79.688 (84.709)
Epoch: [74][180/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 2.3031 (2.3932)	Acc@1 60.156 (55.995)	Acc@5 89.844 (84.669)
Epoch: [74][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3778 (2.3938)	Acc@1 61.719 (56.033)	Acc@5 85.156 (84.653)
Epoch: [74][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4487 (2.3969)	Acc@1 50.781 (55.885)	Acc@5 81.250 (84.600)
Epoch: [74][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3864 (2.4006)	Acc@1 57.031 (55.835)	Acc@5 86.719 (84.556)
Epoch: [74][220/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3583 (2.4059)	Acc@1 55.469 (55.709)	Acc@5 85.156 (84.460)
Epoch: [74][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3672 (2.4070)	Acc@1 56.250 (55.682)	Acc@5 84.375 (84.446)
Epoch: [74][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1792 (2.4050)	Acc@1 60.938 (55.683)	Acc@5 88.281 (84.453)
Epoch: [74][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2336 (2.4043)	Acc@1 58.594 (55.696)	Acc@5 85.938 (84.481)
Epoch: [74][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4351 (2.4082)	Acc@1 54.688 (55.562)	Acc@5 82.812 (84.447)
Epoch: [74][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6385 (2.4101)	Acc@1 46.875 (55.526)	Acc@5 81.250 (84.392)
Epoch: [74][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1520 (2.4136)	Acc@1 64.844 (55.458)	Acc@5 92.188 (84.358)
Epoch: [74][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4733 (2.4156)	Acc@1 52.344 (55.348)	Acc@5 86.719 (84.356)
Epoch: [74][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1089 (2.4163)	Acc@1 61.719 (55.316)	Acc@5 87.500 (84.357)
Epoch: [74][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4094 (2.4169)	Acc@1 55.469 (55.308)	Acc@5 84.375 (84.307)
Epoch: [74][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5927 (2.4189)	Acc@1 50.781 (55.218)	Acc@5 83.594 (84.319)
Epoch: [74][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4229 (2.4163)	Acc@1 55.469 (55.242)	Acc@5 82.812 (84.354)
Epoch: [74][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3883 (2.4166)	Acc@1 51.562 (55.233)	Acc@5 82.031 (84.366)
Epoch: [74][350/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3238 (2.4200)	Acc@1 60.938 (55.115)	Acc@5 83.594 (84.333)
Epoch: [74][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5607 (2.4205)	Acc@1 53.906 (55.118)	Acc@5 79.688 (84.325)
Epoch: [74][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4419 (2.4199)	Acc@1 57.812 (55.130)	Acc@5 80.469 (84.331)
Epoch: [74][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3790 (2.4207)	Acc@1 50.781 (55.077)	Acc@5 86.719 (84.340)
Epoch: [74][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4271 (2.4219)	Acc@1 50.000 (55.026)	Acc@5 82.500 (84.324)
num momentum params: 26
[0.1, 2.421884111328125, 2.1287883245944976, 55.026, 45.8, tensor(0.3290, device='cuda:0', grad_fn=<DivBackward0>), 4.241814136505127, 0.3400859832763672]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [75 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [75][0/391]	Time 0.048 (0.048)	Data 0.156 (0.156)	Loss 2.2533 (2.2533)	Acc@1 57.812 (57.812)	Acc@5 89.844 (89.844)
Epoch: [75][10/391]	Time 0.010 (0.015)	Data 0.001 (0.016)	Loss 2.1469 (2.3259)	Acc@1 57.812 (56.534)	Acc@5 93.750 (87.642)
Epoch: [75][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.1146 (2.3532)	Acc@1 67.969 (56.324)	Acc@5 85.938 (85.751)
Epoch: [75][30/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.6870 (2.3716)	Acc@1 55.469 (56.326)	Acc@5 79.688 (85.862)
Epoch: [75][40/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1922 (2.3329)	Acc@1 57.812 (57.012)	Acc@5 86.719 (86.014)
Epoch: [75][50/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4140 (2.3330)	Acc@1 54.688 (57.047)	Acc@5 85.938 (86.075)
Epoch: [75][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4731 (2.3616)	Acc@1 56.250 (56.711)	Acc@5 81.250 (85.528)
Epoch: [75][70/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.3080 (2.3782)	Acc@1 61.719 (56.316)	Acc@5 86.719 (85.420)
Epoch: [75][80/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2220 (2.3744)	Acc@1 58.594 (56.472)	Acc@5 89.062 (85.311)
Epoch: [75][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4659 (2.3734)	Acc@1 59.375 (56.559)	Acc@5 81.250 (85.294)
Epoch: [75][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4082 (2.3756)	Acc@1 53.125 (56.521)	Acc@5 86.719 (85.249)
Epoch: [75][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0752 (2.3699)	Acc@1 62.500 (56.616)	Acc@5 92.188 (85.367)
Epoch: [75][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2315 (2.3704)	Acc@1 59.375 (56.586)	Acc@5 90.625 (85.318)
Epoch: [75][130/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.4827 (2.3702)	Acc@1 53.125 (56.548)	Acc@5 85.938 (85.407)
Epoch: [75][140/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4462 (2.3759)	Acc@1 57.031 (56.427)	Acc@5 82.031 (85.273)
Epoch: [75][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2450 (2.3801)	Acc@1 63.281 (56.343)	Acc@5 89.844 (85.229)
Epoch: [75][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1821 (2.3801)	Acc@1 67.188 (56.483)	Acc@5 85.938 (85.151)
Epoch: [75][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4254 (2.3827)	Acc@1 51.562 (56.437)	Acc@5 88.281 (85.124)
Epoch: [75][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3819 (2.3867)	Acc@1 54.688 (56.315)	Acc@5 82.031 (85.005)
Epoch: [75][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2855 (2.3903)	Acc@1 56.250 (56.307)	Acc@5 85.938 (84.899)
Epoch: [75][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5040 (2.3947)	Acc@1 55.469 (56.227)	Acc@5 79.688 (84.768)
Epoch: [75][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6617 (2.3991)	Acc@1 49.219 (56.143)	Acc@5 78.125 (84.727)
Epoch: [75][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5341 (2.4016)	Acc@1 51.562 (56.070)	Acc@5 78.125 (84.668)
Epoch: [75][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3412 (2.4045)	Acc@1 58.594 (55.946)	Acc@5 85.156 (84.602)
Epoch: [75][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5808 (2.4077)	Acc@1 50.000 (55.806)	Acc@5 83.594 (84.547)
Epoch: [75][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4541 (2.4131)	Acc@1 51.562 (55.599)	Acc@5 83.594 (84.456)
Epoch: [75][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4045 (2.4146)	Acc@1 56.250 (55.532)	Acc@5 84.375 (84.378)
Epoch: [75][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5680 (2.4153)	Acc@1 53.125 (55.489)	Acc@5 79.688 (84.352)
Epoch: [75][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7384 (2.4157)	Acc@1 45.312 (55.522)	Acc@5 78.906 (84.361)
Epoch: [75][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5723 (2.4178)	Acc@1 50.781 (55.463)	Acc@5 83.594 (84.348)
Epoch: [75][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3437 (2.4171)	Acc@1 53.906 (55.469)	Acc@5 85.938 (84.388)
Epoch: [75][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6912 (2.4196)	Acc@1 44.531 (55.376)	Acc@5 78.906 (84.335)
Epoch: [75][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4049 (2.4198)	Acc@1 53.906 (55.342)	Acc@5 85.156 (84.343)
Epoch: [75][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2543 (2.4220)	Acc@1 57.031 (55.311)	Acc@5 85.938 (84.316)
Epoch: [75][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7903 (2.4260)	Acc@1 44.531 (55.201)	Acc@5 77.344 (84.267)
Epoch: [75][350/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.5218 (2.4265)	Acc@1 47.656 (55.222)	Acc@5 83.594 (84.244)
Epoch: [75][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8054 (2.4279)	Acc@1 42.188 (55.183)	Acc@5 75.781 (84.215)
Epoch: [75][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6948 (2.4294)	Acc@1 50.000 (55.163)	Acc@5 80.469 (84.192)
Epoch: [75][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4802 (2.4293)	Acc@1 56.250 (55.147)	Acc@5 84.375 (84.201)
Epoch: [75][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3988 (2.4313)	Acc@1 53.750 (55.066)	Acc@5 80.000 (84.178)
num momentum params: 26
[0.1, 2.431320688934326, 2.132043662071228, 55.066, 45.15, tensor(0.3272, device='cuda:0', grad_fn=<DivBackward0>), 4.2581467628479, 0.33924245834350586]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [76 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [76][0/391]	Time 0.050 (0.050)	Data 0.160 (0.160)	Loss 2.5631 (2.5631)	Acc@1 50.781 (50.781)	Acc@5 80.469 (80.469)
Epoch: [76][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.4645 (2.4206)	Acc@1 52.344 (54.830)	Acc@5 85.156 (84.304)
Epoch: [76][20/391]	Time 0.012 (0.014)	Data 0.001 (0.009)	Loss 2.3140 (2.3677)	Acc@1 57.031 (56.548)	Acc@5 82.812 (85.082)
Epoch: [76][30/391]	Time 0.014 (0.013)	Data 0.001 (0.007)	Loss 2.3650 (2.3626)	Acc@1 51.562 (56.074)	Acc@5 85.156 (85.408)
Epoch: [76][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.3723 (2.3462)	Acc@1 54.688 (56.212)	Acc@5 79.688 (85.480)
Epoch: [76][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1207 (2.3393)	Acc@1 65.625 (56.495)	Acc@5 87.500 (85.585)
Epoch: [76][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4191 (2.3489)	Acc@1 53.125 (56.352)	Acc@5 82.812 (85.476)
Epoch: [76][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3043 (2.3632)	Acc@1 60.156 (55.997)	Acc@5 86.719 (85.211)
Epoch: [76][80/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 2.5422 (2.3662)	Acc@1 51.562 (56.028)	Acc@5 84.375 (85.137)
Epoch: [76][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.7527 (2.3743)	Acc@1 48.438 (55.941)	Acc@5 76.562 (84.933)
Epoch: [76][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4800 (2.3837)	Acc@1 57.812 (55.794)	Acc@5 86.719 (84.793)
Epoch: [76][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4922 (2.3881)	Acc@1 55.469 (55.673)	Acc@5 85.156 (84.734)
Epoch: [76][120/391]	Time 0.010 (0.011)	Data 0.006 (0.003)	Loss 2.5473 (2.3901)	Acc@1 53.906 (55.695)	Acc@5 82.812 (84.730)
Epoch: [76][130/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.6831 (2.3970)	Acc@1 48.438 (55.445)	Acc@5 78.906 (84.625)
Epoch: [76][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5895 (2.4034)	Acc@1 51.562 (55.275)	Acc@5 79.688 (84.486)
Epoch: [76][150/391]	Time 0.013 (0.011)	Data 0.001 (0.003)	Loss 2.5651 (2.4098)	Acc@1 43.750 (55.039)	Acc@5 88.281 (84.401)
Epoch: [76][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4536 (2.4165)	Acc@1 57.812 (54.988)	Acc@5 81.250 (84.263)
Epoch: [76][170/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3687 (2.4179)	Acc@1 56.250 (54.989)	Acc@5 85.938 (84.288)
Epoch: [76][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2603 (2.4142)	Acc@1 54.688 (55.046)	Acc@5 85.156 (84.358)
Epoch: [76][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3501 (2.4168)	Acc@1 54.688 (54.974)	Acc@5 89.844 (84.301)
Epoch: [76][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2975 (2.4146)	Acc@1 54.688 (54.991)	Acc@5 89.062 (84.398)
Epoch: [76][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2762 (2.4127)	Acc@1 60.156 (55.043)	Acc@5 83.594 (84.427)
Epoch: [76][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4377 (2.4089)	Acc@1 53.906 (55.105)	Acc@5 82.812 (84.499)
Epoch: [76][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3936 (2.4113)	Acc@1 54.688 (55.066)	Acc@5 85.156 (84.480)
Epoch: [76][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1901 (2.4134)	Acc@1 60.938 (55.051)	Acc@5 88.281 (84.466)
Epoch: [76][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7308 (2.4160)	Acc@1 45.312 (54.977)	Acc@5 81.250 (84.422)
Epoch: [76][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4024 (2.4166)	Acc@1 56.250 (55.005)	Acc@5 82.812 (84.432)
Epoch: [76][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3365 (2.4184)	Acc@1 56.250 (54.967)	Acc@5 83.594 (84.384)
Epoch: [76][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2801 (2.4206)	Acc@1 57.031 (54.913)	Acc@5 88.281 (84.361)
Epoch: [76][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5742 (2.4250)	Acc@1 53.125 (54.798)	Acc@5 80.469 (84.286)
Epoch: [76][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5054 (2.4251)	Acc@1 53.906 (54.771)	Acc@5 81.250 (84.266)
Epoch: [76][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8135 (2.4272)	Acc@1 39.062 (54.698)	Acc@5 80.469 (84.254)
Epoch: [76][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1748 (2.4252)	Acc@1 64.062 (54.758)	Acc@5 86.719 (84.270)
Epoch: [76][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4247 (2.4267)	Acc@1 58.594 (54.780)	Acc@5 82.031 (84.240)
Epoch: [76][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3403 (2.4248)	Acc@1 54.688 (54.862)	Acc@5 89.062 (84.263)
Epoch: [76][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2319 (2.4247)	Acc@1 58.594 (54.854)	Acc@5 90.625 (84.270)
Epoch: [76][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3813 (2.4250)	Acc@1 59.375 (54.874)	Acc@5 85.938 (84.262)
Epoch: [76][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5886 (2.4268)	Acc@1 50.000 (54.822)	Acc@5 82.812 (84.232)
Epoch: [76][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3671 (2.4259)	Acc@1 59.375 (54.870)	Acc@5 83.594 (84.281)
Epoch: [76][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.9739 (2.4289)	Acc@1 43.750 (54.830)	Acc@5 72.500 (84.206)
num momentum params: 26
[0.1, 2.428949603729248, 2.15741290807724, 54.83, 45.16, tensor(0.3279, device='cuda:0', grad_fn=<DivBackward0>), 4.294623136520386, 0.32948875427246094]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [77 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [77][0/391]	Time 0.047 (0.047)	Data 0.163 (0.163)	Loss 2.2744 (2.2744)	Acc@1 56.250 (56.250)	Acc@5 85.156 (85.156)
Epoch: [77][10/391]	Time 0.012 (0.016)	Data 0.001 (0.017)	Loss 2.3554 (2.3586)	Acc@1 53.125 (56.534)	Acc@5 89.062 (84.801)
Epoch: [77][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.1666 (2.3451)	Acc@1 60.938 (56.287)	Acc@5 89.844 (85.268)
Epoch: [77][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.2888 (2.3491)	Acc@1 53.906 (56.351)	Acc@5 87.500 (85.307)
Epoch: [77][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.1511 (2.3447)	Acc@1 64.844 (56.593)	Acc@5 88.281 (85.537)
Epoch: [77][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.4222 (2.3485)	Acc@1 57.812 (56.725)	Acc@5 85.156 (85.325)
Epoch: [77][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4220 (2.3424)	Acc@1 52.344 (56.814)	Acc@5 83.594 (85.438)
Epoch: [77][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5476 (2.3423)	Acc@1 47.656 (56.833)	Acc@5 84.375 (85.453)
Epoch: [77][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5711 (2.3642)	Acc@1 49.219 (56.414)	Acc@5 82.031 (84.944)
Epoch: [77][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3604 (2.3617)	Acc@1 55.469 (56.490)	Acc@5 82.812 (85.053)
Epoch: [77][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5221 (2.3761)	Acc@1 55.469 (56.250)	Acc@5 82.812 (84.754)
Epoch: [77][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3789 (2.3773)	Acc@1 58.594 (56.327)	Acc@5 83.594 (84.685)
Epoch: [77][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3346 (2.3742)	Acc@1 54.688 (56.444)	Acc@5 82.031 (84.756)
Epoch: [77][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2489 (2.3768)	Acc@1 57.031 (56.369)	Acc@5 88.281 (84.828)
Epoch: [77][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5506 (2.3802)	Acc@1 46.875 (56.217)	Acc@5 85.156 (84.768)
Epoch: [77][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4803 (2.3822)	Acc@1 50.781 (56.121)	Acc@5 85.156 (84.742)
Epoch: [77][160/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3140 (2.3863)	Acc@1 55.469 (56.027)	Acc@5 85.938 (84.715)
Epoch: [77][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1383 (2.3822)	Acc@1 64.844 (56.140)	Acc@5 88.281 (84.772)
Epoch: [77][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5486 (2.3839)	Acc@1 52.344 (56.017)	Acc@5 82.812 (84.772)
Epoch: [77][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4291 (2.3884)	Acc@1 56.250 (55.861)	Acc@5 83.594 (84.731)
Epoch: [77][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5137 (2.3917)	Acc@1 53.125 (55.741)	Acc@5 84.375 (84.667)
Epoch: [77][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6270 (2.3952)	Acc@1 44.531 (55.687)	Acc@5 82.031 (84.590)
Epoch: [77][220/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.6397 (2.3988)	Acc@1 50.781 (55.571)	Acc@5 83.594 (84.523)
Epoch: [77][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1764 (2.3990)	Acc@1 60.938 (55.628)	Acc@5 89.062 (84.541)
Epoch: [77][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4223 (2.4014)	Acc@1 57.031 (55.569)	Acc@5 85.156 (84.557)
Epoch: [77][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2696 (2.4025)	Acc@1 62.500 (55.571)	Acc@5 85.156 (84.556)
Epoch: [77][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3898 (2.4068)	Acc@1 53.125 (55.511)	Acc@5 89.844 (84.519)
Epoch: [77][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4473 (2.4092)	Acc@1 52.344 (55.460)	Acc@5 85.938 (84.508)
Epoch: [77][280/391]	Time 0.017 (0.011)	Data 0.001 (0.002)	Loss 2.2882 (2.4082)	Acc@1 59.375 (55.505)	Acc@5 82.812 (84.520)
Epoch: [77][290/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4754 (2.4088)	Acc@1 52.344 (55.482)	Acc@5 78.906 (84.509)
Epoch: [77][300/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.5847 (2.4107)	Acc@1 52.344 (55.360)	Acc@5 78.906 (84.479)
Epoch: [77][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4941 (2.4117)	Acc@1 53.125 (55.348)	Acc@5 84.375 (84.465)
Epoch: [77][320/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.5575 (2.4128)	Acc@1 54.688 (55.354)	Acc@5 78.906 (84.463)
Epoch: [77][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4143 (2.4147)	Acc@1 55.469 (55.280)	Acc@5 82.812 (84.415)
Epoch: [77][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3148 (2.4152)	Acc@1 54.688 (55.283)	Acc@5 89.062 (84.423)
Epoch: [77][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3685 (2.4166)	Acc@1 55.469 (55.257)	Acc@5 85.156 (84.420)
Epoch: [77][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4280 (2.4171)	Acc@1 57.812 (55.270)	Acc@5 84.375 (84.403)
Epoch: [77][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5385 (2.4185)	Acc@1 54.688 (55.258)	Acc@5 81.250 (84.371)
Epoch: [77][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5635 (2.4209)	Acc@1 49.219 (55.176)	Acc@5 82.031 (84.334)
Epoch: [77][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.6590 (2.4228)	Acc@1 51.250 (55.138)	Acc@5 78.750 (84.296)
num momentum params: 26
[0.1, 2.422750386428833, 2.0882258892059324, 55.138, 46.61, tensor(0.3284, device='cuda:0', grad_fn=<DivBackward0>), 4.333861351013184, 0.3446848392486572]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [78 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [78][0/391]	Time 0.045 (0.045)	Data 0.166 (0.166)	Loss 2.3229 (2.3229)	Acc@1 58.594 (58.594)	Acc@5 85.156 (85.156)
Epoch: [78][10/391]	Time 0.012 (0.015)	Data 0.002 (0.016)	Loss 2.3427 (2.3763)	Acc@1 57.812 (55.753)	Acc@5 89.062 (85.724)
Epoch: [78][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.2599 (2.3355)	Acc@1 57.031 (57.068)	Acc@5 90.625 (86.124)
Epoch: [78][30/391]	Time 0.011 (0.012)	Data 0.001 (0.007)	Loss 2.2204 (2.3072)	Acc@1 67.188 (58.291)	Acc@5 83.594 (86.467)
Epoch: [78][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.3698 (2.3131)	Acc@1 55.469 (58.270)	Acc@5 85.938 (86.128)
Epoch: [78][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1661 (2.3059)	Acc@1 62.500 (58.257)	Acc@5 87.500 (86.259)
Epoch: [78][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.1515 (2.3137)	Acc@1 61.719 (58.005)	Acc@5 89.062 (86.181)
Epoch: [78][70/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 2.2152 (2.3203)	Acc@1 63.281 (57.989)	Acc@5 87.500 (85.849)
Epoch: [78][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5256 (2.3357)	Acc@1 48.438 (57.427)	Acc@5 84.375 (85.754)
Epoch: [78][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5977 (2.3509)	Acc@1 53.125 (57.083)	Acc@5 81.250 (85.379)
Epoch: [78][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5682 (2.3505)	Acc@1 50.000 (56.938)	Acc@5 82.812 (85.404)
Epoch: [78][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1756 (2.3568)	Acc@1 56.250 (56.707)	Acc@5 90.625 (85.304)
Epoch: [78][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4640 (2.3643)	Acc@1 54.688 (56.553)	Acc@5 78.125 (85.111)
Epoch: [78][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2627 (2.3672)	Acc@1 55.469 (56.333)	Acc@5 88.281 (85.138)
Epoch: [78][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3772 (2.3729)	Acc@1 60.938 (56.250)	Acc@5 82.031 (85.029)
Epoch: [78][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3360 (2.3771)	Acc@1 55.469 (56.317)	Acc@5 86.719 (84.975)
Epoch: [78][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3976 (2.3762)	Acc@1 57.812 (56.323)	Acc@5 87.500 (85.054)
Epoch: [78][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5133 (2.3806)	Acc@1 51.562 (56.154)	Acc@5 83.594 (85.024)
Epoch: [78][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3402 (2.3868)	Acc@1 53.125 (55.922)	Acc@5 89.844 (84.880)
Epoch: [78][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5706 (2.3926)	Acc@1 52.344 (55.804)	Acc@5 83.594 (84.809)
Epoch: [78][200/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.6120 (2.3978)	Acc@1 47.656 (55.741)	Acc@5 81.250 (84.744)
Epoch: [78][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1629 (2.3983)	Acc@1 58.594 (55.687)	Acc@5 88.281 (84.745)
Epoch: [78][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4888 (2.4031)	Acc@1 49.219 (55.536)	Acc@5 82.031 (84.690)
Epoch: [78][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6919 (2.4027)	Acc@1 49.219 (55.594)	Acc@5 78.906 (84.679)
Epoch: [78][240/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4285 (2.4070)	Acc@1 52.344 (55.501)	Acc@5 86.719 (84.599)
Epoch: [78][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3085 (2.4071)	Acc@1 56.250 (55.494)	Acc@5 87.500 (84.565)
Epoch: [78][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7370 (2.4100)	Acc@1 47.656 (55.433)	Acc@5 79.688 (84.528)
Epoch: [78][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5651 (2.4137)	Acc@1 53.906 (55.327)	Acc@5 80.469 (84.459)
Epoch: [78][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7109 (2.4162)	Acc@1 47.656 (55.260)	Acc@5 80.469 (84.392)
Epoch: [78][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4506 (2.4179)	Acc@1 57.031 (55.227)	Acc@5 81.250 (84.340)
Epoch: [78][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3914 (2.4185)	Acc@1 57.031 (55.207)	Acc@5 86.719 (84.328)
Epoch: [78][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4348 (2.4216)	Acc@1 57.812 (55.170)	Acc@5 81.250 (84.282)
Epoch: [78][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5257 (2.4225)	Acc@1 57.031 (55.147)	Acc@5 81.250 (84.231)
Epoch: [78][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1086 (2.4234)	Acc@1 60.156 (55.112)	Acc@5 89.062 (84.219)
Epoch: [78][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3495 (2.4263)	Acc@1 53.906 (55.066)	Acc@5 83.594 (84.139)
Epoch: [78][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4302 (2.4269)	Acc@1 57.031 (55.032)	Acc@5 82.812 (84.152)
Epoch: [78][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4891 (2.4265)	Acc@1 50.781 (55.047)	Acc@5 79.688 (84.148)
Epoch: [78][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3490 (2.4267)	Acc@1 58.594 (55.064)	Acc@5 84.375 (84.103)
Epoch: [78][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6484 (2.4278)	Acc@1 49.219 (55.026)	Acc@5 78.125 (84.092)
Epoch: [78][390/391]	Time 0.011 (0.011)	Data 0.000 (0.002)	Loss 2.3583 (2.4283)	Acc@1 66.250 (55.036)	Acc@5 82.500 (84.078)
num momentum params: 26
[0.1, 2.428349466018677, 2.1938020849227904, 55.036, 44.69, tensor(0.3279, device='cuda:0', grad_fn=<DivBackward0>), 4.288314580917358, 0.3333396911621094]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [79 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [79][0/391]	Time 0.052 (0.052)	Data 0.153 (0.153)	Loss 2.3021 (2.3021)	Acc@1 55.469 (55.469)	Acc@5 88.281 (88.281)
Epoch: [79][10/391]	Time 0.011 (0.016)	Data 0.001 (0.015)	Loss 2.2491 (2.3646)	Acc@1 60.938 (55.966)	Acc@5 89.844 (86.790)
Epoch: [79][20/391]	Time 0.010 (0.013)	Data 0.001 (0.009)	Loss 2.3262 (2.3586)	Acc@1 61.719 (57.106)	Acc@5 85.156 (85.565)
Epoch: [79][30/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.2444 (2.3600)	Acc@1 59.375 (56.729)	Acc@5 89.844 (85.711)
Epoch: [79][40/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1916 (2.3563)	Acc@1 60.938 (56.764)	Acc@5 88.281 (85.652)
Epoch: [79][50/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2737 (2.3474)	Acc@1 60.156 (56.817)	Acc@5 84.375 (85.738)
Epoch: [79][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.0917 (2.3391)	Acc@1 65.625 (56.993)	Acc@5 89.062 (85.784)
Epoch: [79][70/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.4643 (2.3533)	Acc@1 51.562 (56.734)	Acc@5 84.375 (85.475)
Epoch: [79][80/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5746 (2.3697)	Acc@1 48.438 (56.260)	Acc@5 81.250 (85.079)
Epoch: [79][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2754 (2.3832)	Acc@1 59.375 (55.924)	Acc@5 89.844 (84.976)
Epoch: [79][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4991 (2.3847)	Acc@1 53.906 (55.840)	Acc@5 83.594 (84.893)
Epoch: [79][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5690 (2.3881)	Acc@1 53.906 (55.757)	Acc@5 83.594 (84.832)
Epoch: [79][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3206 (2.3818)	Acc@1 59.375 (56.121)	Acc@5 83.594 (84.833)
Epoch: [79][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5590 (2.3895)	Acc@1 51.562 (55.904)	Acc@5 82.812 (84.816)
Epoch: [79][140/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3621 (2.3915)	Acc@1 53.125 (55.751)	Acc@5 87.500 (84.752)
Epoch: [79][150/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4213 (2.3947)	Acc@1 54.688 (55.764)	Acc@5 85.938 (84.685)
Epoch: [79][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0591 (2.3947)	Acc@1 60.156 (55.668)	Acc@5 92.188 (84.724)
Epoch: [79][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6151 (2.3985)	Acc@1 48.438 (55.633)	Acc@5 84.375 (84.699)
Epoch: [79][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2937 (2.3975)	Acc@1 58.594 (55.672)	Acc@5 85.938 (84.651)
Epoch: [79][190/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2962 (2.3938)	Acc@1 55.469 (55.780)	Acc@5 89.844 (84.747)
Epoch: [79][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4908 (2.3918)	Acc@1 53.125 (55.764)	Acc@5 83.594 (84.830)
Epoch: [79][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1500 (2.3907)	Acc@1 60.156 (55.791)	Acc@5 87.500 (84.882)
Epoch: [79][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3703 (2.3897)	Acc@1 56.250 (55.787)	Acc@5 87.500 (84.909)
Epoch: [79][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5236 (2.3908)	Acc@1 52.344 (55.746)	Acc@5 81.250 (84.862)
Epoch: [79][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6617 (2.3956)	Acc@1 48.438 (55.576)	Acc@5 81.250 (84.796)
Epoch: [79][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4733 (2.4002)	Acc@1 49.219 (55.487)	Acc@5 86.719 (84.730)
Epoch: [79][260/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3530 (2.4041)	Acc@1 55.469 (55.451)	Acc@5 88.281 (84.659)
Epoch: [79][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1707 (2.4040)	Acc@1 58.594 (55.472)	Acc@5 88.281 (84.663)
Epoch: [79][280/391]	Time 0.009 (0.011)	Data 0.002 (0.002)	Loss 2.3484 (2.4041)	Acc@1 56.250 (55.466)	Acc@5 82.812 (84.611)
Epoch: [79][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1891 (2.4020)	Acc@1 57.031 (55.482)	Acc@5 83.594 (84.633)
Epoch: [79][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3290 (2.4056)	Acc@1 55.469 (55.373)	Acc@5 88.281 (84.580)
Epoch: [79][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5809 (2.4100)	Acc@1 53.125 (55.270)	Acc@5 84.375 (84.496)
Epoch: [79][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3745 (2.4126)	Acc@1 58.594 (55.201)	Acc@5 86.719 (84.465)
Epoch: [79][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2815 (2.4156)	Acc@1 57.812 (55.155)	Acc@5 86.719 (84.427)
Epoch: [79][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3433 (2.4168)	Acc@1 56.250 (55.116)	Acc@5 82.031 (84.380)
Epoch: [79][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5379 (2.4174)	Acc@1 48.438 (55.153)	Acc@5 83.594 (84.346)
Epoch: [79][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7989 (2.4174)	Acc@1 44.531 (55.161)	Acc@5 77.344 (84.338)
Epoch: [79][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2681 (2.4177)	Acc@1 57.812 (55.166)	Acc@5 82.031 (84.341)
Epoch: [79][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3141 (2.4171)	Acc@1 53.906 (55.165)	Acc@5 87.500 (84.342)
Epoch: [79][390/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5327 (2.4190)	Acc@1 60.000 (55.120)	Acc@5 82.500 (84.322)
num momentum params: 26
[0.1, 2.4189505908203124, 1.9871398317813873, 55.12, 48.73, tensor(0.3296, device='cuda:0', grad_fn=<DivBackward0>), 4.265694618225098, 0.3373229503631592]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [112, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [112]
Non Pruning Epoch - module.bn2.bias: [112]
Non Pruning Epoch - module.conv3.weight: [223, 112, 3, 3]
Non Pruning Epoch - module.bn3.weight: [223]
Non Pruning Epoch - module.bn3.bias: [223]
Non Pruning Epoch - module.conv4.weight: [248, 223, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [378, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [378]
Non Pruning Epoch - module.bn5.bias: [378]
Non Pruning Epoch - module.conv6.weight: [332, 378, 3, 3]
Non Pruning Epoch - module.bn6.weight: [332]
Non Pruning Epoch - module.bn6.bias: [332]
Non Pruning Epoch - module.conv7.weight: [222, 332, 3, 3]
Non Pruning Epoch - module.bn7.weight: [222]
Non Pruning Epoch - module.bn7.bias: [222]
Non Pruning Epoch - module.conv8.weight: [206, 222, 3, 3]
Non Pruning Epoch - module.bn8.weight: [206]
Non Pruning Epoch - module.bn8.bias: [206]
Non Pruning Epoch - module.fc.weight: [100, 206]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [80 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [112, 34, 3, 3]
module.conv3.weight [223, 112, 3, 3]
module.conv4.weight [248, 223, 3, 3]
module.conv5.weight [378, 248, 3, 3]
module.conv6.weight [332, 378, 3, 3]
module.conv7.weight [222, 332, 3, 3]
module.conv8.weight [206, 222, 3, 3]
Epoch: [80][0/391]	Time 0.046 (0.046)	Data 0.164 (0.164)	Loss 2.5799 (2.5799)	Acc@1 53.125 (53.125)	Acc@5 79.688 (79.688)
Epoch: [80][10/391]	Time 0.013 (0.015)	Data 0.001 (0.016)	Loss 2.6474 (2.4686)	Acc@1 51.562 (53.906)	Acc@5 76.562 (82.599)
Epoch: [80][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.3742 (2.4379)	Acc@1 55.469 (54.390)	Acc@5 86.719 (83.594)
Epoch: [80][30/391]	Time 0.012 (0.012)	Data 0.001 (0.007)	Loss 1.9947 (2.3857)	Acc@1 64.062 (55.519)	Acc@5 91.406 (84.476)
Epoch: [80][40/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.5137 (2.3810)	Acc@1 50.000 (55.640)	Acc@5 84.375 (84.623)
Epoch: [80][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.3252 (2.3684)	Acc@1 57.031 (56.204)	Acc@5 82.031 (84.804)
Epoch: [80][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4827 (2.3597)	Acc@1 53.125 (56.468)	Acc@5 82.031 (84.810)
Epoch: [80][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.1448 (2.3614)	Acc@1 60.938 (56.547)	Acc@5 85.156 (84.705)
Epoch: [80][80/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0879 (2.3502)	Acc@1 62.500 (56.790)	Acc@5 90.625 (85.021)
Epoch: [80][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.0840 (2.3378)	Acc@1 61.719 (57.048)	Acc@5 92.969 (85.242)
Epoch: [80][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3131 (2.3490)	Acc@1 57.031 (56.660)	Acc@5 84.375 (85.133)
Epoch: [80][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4356 (2.3589)	Acc@1 53.125 (56.482)	Acc@5 82.031 (84.910)
Epoch: [80][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5525 (2.3641)	Acc@1 51.562 (56.302)	Acc@5 80.469 (84.892)
Epoch: [80][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3540 (2.3689)	Acc@1 55.469 (56.286)	Acc@5 87.500 (84.900)
Epoch: [80][140/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.4125 (2.3712)	Acc@1 57.812 (56.300)	Acc@5 85.156 (84.840)
Epoch: [80][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4929 (2.3788)	Acc@1 53.125 (56.136)	Acc@5 85.156 (84.758)
Epoch: [80][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4610 (2.3797)	Acc@1 59.375 (56.197)	Acc@5 82.031 (84.773)
Epoch: [80][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4011 (2.3797)	Acc@1 53.125 (56.223)	Acc@5 85.938 (84.800)
Epoch: [80][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2590 (2.3823)	Acc@1 57.812 (56.103)	Acc@5 89.062 (84.794)
Epoch: [80][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3574 (2.3861)	Acc@1 51.562 (56.029)	Acc@5 85.156 (84.706)
Epoch: [80][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6180 (2.3887)	Acc@1 46.094 (55.892)	Acc@5 82.031 (84.705)
Epoch: [80][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3680 (2.3936)	Acc@1 53.125 (55.687)	Acc@5 87.500 (84.642)
Epoch: [80][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5873 (2.3957)	Acc@1 48.438 (55.649)	Acc@5 79.688 (84.587)
Epoch: [80][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2909 (2.3985)	Acc@1 58.594 (55.648)	Acc@5 90.625 (84.544)
Epoch: [80][240/391]	Time 0.010 (0.011)	Data 0.004 (0.002)	Loss 2.6596 (2.4012)	Acc@1 53.906 (55.605)	Acc@5 79.688 (84.492)
Epoch: [80][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5771 (2.4040)	Acc@1 53.125 (55.531)	Acc@5 80.469 (84.434)
Epoch: [80][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5492 (2.4062)	Acc@1 57.812 (55.433)	Acc@5 84.375 (84.396)
Epoch: [80][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3709 (2.4082)	Acc@1 53.906 (55.388)	Acc@5 85.156 (84.407)
Epoch: [80][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3524 (2.4108)	Acc@1 57.812 (55.307)	Acc@5 89.062 (84.392)
Epoch: [80][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4161 (2.4127)	Acc@1 58.594 (55.300)	Acc@5 83.594 (84.351)
Epoch: [80][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5601 (2.4144)	Acc@1 52.344 (55.274)	Acc@5 79.688 (84.315)
Epoch: [80][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5398 (2.4169)	Acc@1 51.562 (55.248)	Acc@5 85.938 (84.310)
Epoch: [80][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5000 (2.4175)	Acc@1 53.906 (55.235)	Acc@5 82.812 (84.283)
Epoch: [80][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2703 (2.4194)	Acc@1 58.594 (55.162)	Acc@5 85.156 (84.257)
Epoch: [80][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6835 (2.4207)	Acc@1 50.781 (55.180)	Acc@5 82.812 (84.221)
Epoch: [80][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2826 (2.4177)	Acc@1 60.156 (55.264)	Acc@5 86.719 (84.266)
Epoch: [80][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4040 (2.4178)	Acc@1 59.375 (55.237)	Acc@5 85.156 (84.286)
Epoch: [80][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3996 (2.4210)	Acc@1 57.031 (55.182)	Acc@5 87.500 (84.282)
Epoch: [80][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6278 (2.4216)	Acc@1 50.781 (55.196)	Acc@5 77.344 (84.244)
Epoch: [80][390/391]	Time 0.011 (0.011)	Data 0.000 (0.002)	Loss 2.4977 (2.4230)	Acc@1 50.000 (55.174)	Acc@5 78.750 (84.202)
num momentum params: 26
[0.1, 2.423037702636719, 1.9402598857879638, 55.174, 48.54, tensor(0.3286, device='cuda:0', grad_fn=<DivBackward0>), 4.244486093521118, 0.3286457061767578]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [34, 3, 3, 3]
Before - module.bn1.weight: [34]
Before - module.bn1.bias: [34]
Before - module.conv2.weight: [112, 34, 3, 3]
Before - module.bn2.weight: [112]
Before - module.bn2.bias: [112]
Before - module.conv3.weight: [223, 112, 3, 3]
Before - module.bn3.weight: [223]
Before - module.bn3.bias: [223]
Before - module.conv4.weight: [248, 223, 3, 3]
Before - module.bn4.weight: [248]
Before - module.bn4.bias: [248]
Before - module.conv5.weight: [378, 248, 3, 3]
Before - module.bn5.weight: [378]
Before - module.bn5.bias: [378]
Before - module.conv6.weight: [332, 378, 3, 3]
Before - module.bn6.weight: [332]
Before - module.bn6.bias: [332]
Before - module.conv7.weight: [222, 332, 3, 3]
Before - module.bn7.weight: [222]
Before - module.bn7.bias: [222]
Before - module.conv8.weight: [206, 222, 3, 3]
Before - module.bn8.weight: [206]
Before - module.bn8.bias: [206]
Before - module.fc.weight: [100, 206]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [34, 3, 3, 3] >> [34, 3, 3, 3]
[module.bn1.weight]: 34 >> 34
running_mean [34]
running_var [34]
num_batches_tracked []
[module.conv2.weight]: [112, 34, 3, 3] >> [111, 34, 3, 3]
[module.bn2.weight]: 112 >> 111
running_mean [111]
running_var [111]
num_batches_tracked []
[module.conv3.weight]: [223, 112, 3, 3] >> [219, 111, 3, 3]
[module.bn3.weight]: 223 >> 219
running_mean [219]
running_var [219]
num_batches_tracked []
[module.conv4.weight]: [248, 223, 3, 3] >> [248, 219, 3, 3]
[module.bn4.weight]: 248 >> 248
running_mean [248]
running_var [248]
num_batches_tracked []
[module.conv5.weight]: [378, 248, 3, 3] >> [375, 248, 3, 3]
[module.bn5.weight]: 378 >> 375
running_mean [375]
running_var [375]
num_batches_tracked []
[module.conv6.weight]: [332, 378, 3, 3] >> [324, 375, 3, 3]
[module.bn6.weight]: 332 >> 324
running_mean [324]
running_var [324]
num_batches_tracked []
[module.conv7.weight]: [222, 332, 3, 3] >> [217, 324, 3, 3]
[module.bn7.weight]: 222 >> 217
running_mean [217]
running_var [217]
num_batches_tracked []
[module.conv8.weight]: [206, 222, 3, 3] >> [203, 217, 3, 3]
[module.bn8.weight]: 206 >> 203
running_mean [203]
running_var [203]
num_batches_tracked []
[module.fc.weight]: [100, 206] >> [100, 203]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [34, 3, 3, 3]
After - module.bn1.weight: [34]
After - module.bn1.bias: [34]
After - module.conv2.weight: [111, 34, 3, 3]
After - module.bn2.weight: [111]
After - module.bn2.bias: [111]
After - module.conv3.weight: [219, 111, 3, 3]
After - module.bn3.weight: [219]
After - module.bn3.bias: [219]
After - module.conv4.weight: [248, 219, 3, 3]
After - module.bn4.weight: [248]
After - module.bn4.bias: [248]
After - module.conv5.weight: [375, 248, 3, 3]
After - module.bn5.weight: [375]
After - module.bn5.bias: [375]
After - module.conv6.weight: [324, 375, 3, 3]
After - module.bn6.weight: [324]
After - module.bn6.bias: [324]
After - module.conv7.weight: [217, 324, 3, 3]
After - module.bn7.weight: [217]
After - module.bn7.bias: [217]
After - module.conv8.weight: [203, 217, 3, 3]
After - module.bn8.weight: [203]
After - module.bn8.bias: [203]
After - module.fc.weight: [100, 203]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [34, 3, 3, 3]
conv2 --> [111, 34, 3, 3]
conv3 --> [219, 111, 3, 3]
conv4 --> [248, 219, 3, 3]
conv5 --> [375, 248, 3, 3]
conv6 --> [324, 375, 3, 3]
conv7 --> [217, 324, 3, 3]
conv8 --> [203, 217, 3, 3]
fc --> [203, 100]
1, 376482816, 940032, 34
2, 3634633728, 8695296, 111
3, 6384904704, 14001984, 219
4, 14265372672, 31283712, 248
5, 7285248000, 13392000, 375
6, 9517824000, 17496000, 324
7, 1943875584, 2531088, 217
8, 1217922048, 1585836, 203
fc, 7795200, 20300, 0
===================
FLOP REPORT: 17435179200000.0 42393600000.0 89946248 105984 1731 7.103801727294922
[INFO] Storing checkpoint...

Epoch: [81 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [81][0/391]	Time 0.227 (0.227)	Data 0.178 (0.178)	Loss 2.1409 (2.1409)	Acc@1 64.844 (64.844)	Acc@5 88.281 (88.281)
Epoch: [81][10/391]	Time 0.012 (0.032)	Data 0.001 (0.017)	Loss 2.6431 (2.3968)	Acc@1 50.781 (55.682)	Acc@5 80.469 (83.949)
Epoch: [81][20/391]	Time 0.011 (0.022)	Data 0.001 (0.010)	Loss 2.3547 (2.4009)	Acc@1 57.031 (55.804)	Acc@5 86.719 (84.375)
Epoch: [81][30/391]	Time 0.012 (0.018)	Data 0.001 (0.007)	Loss 2.4538 (2.3865)	Acc@1 54.688 (56.149)	Acc@5 80.469 (84.501)
Epoch: [81][40/391]	Time 0.012 (0.017)	Data 0.001 (0.006)	Loss 2.2565 (2.3807)	Acc@1 57.031 (55.945)	Acc@5 87.500 (84.718)
Epoch: [81][50/391]	Time 0.012 (0.016)	Data 0.001 (0.005)	Loss 2.2327 (2.3781)	Acc@1 58.594 (55.806)	Acc@5 88.281 (85.003)
Epoch: [81][60/391]	Time 0.010 (0.015)	Data 0.001 (0.004)	Loss 2.3073 (2.3697)	Acc@1 53.906 (56.160)	Acc@5 85.938 (85.067)
Epoch: [81][70/391]	Time 0.010 (0.014)	Data 0.001 (0.004)	Loss 2.3997 (2.3764)	Acc@1 53.906 (55.854)	Acc@5 85.938 (84.925)
Epoch: [81][80/391]	Time 0.011 (0.014)	Data 0.001 (0.004)	Loss 2.5098 (2.3743)	Acc@1 55.469 (56.086)	Acc@5 81.250 (85.002)
Epoch: [81][90/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.2316 (2.3747)	Acc@1 53.906 (56.070)	Acc@5 89.844 (84.959)
Epoch: [81][100/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.3799 (2.3808)	Acc@1 55.469 (55.995)	Acc@5 88.281 (84.855)
Epoch: [81][110/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.2546 (2.3819)	Acc@1 60.938 (56.032)	Acc@5 86.719 (84.952)
Epoch: [81][120/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.5585 (2.3796)	Acc@1 48.438 (56.050)	Acc@5 82.031 (84.956)
Epoch: [81][130/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.5089 (2.3791)	Acc@1 57.031 (56.184)	Acc@5 82.812 (84.942)
Epoch: [81][140/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 2.1819 (2.3804)	Acc@1 58.594 (56.078)	Acc@5 90.625 (84.973)
Epoch: [81][150/391]	Time 0.012 (0.013)	Data 0.001 (0.003)	Loss 2.4657 (2.3870)	Acc@1 51.562 (55.986)	Acc@5 87.500 (84.825)
Epoch: [81][160/391]	Time 0.012 (0.013)	Data 0.001 (0.002)	Loss 2.3879 (2.3934)	Acc@1 53.906 (55.813)	Acc@5 89.844 (84.729)
Epoch: [81][170/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.5851 (2.4005)	Acc@1 50.781 (55.693)	Acc@5 82.031 (84.585)
Epoch: [81][180/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.1882 (2.3981)	Acc@1 62.500 (55.827)	Acc@5 89.062 (84.565)
Epoch: [81][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2106 (2.3967)	Acc@1 54.688 (55.857)	Acc@5 89.062 (84.580)
Epoch: [81][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4580 (2.4002)	Acc@1 58.594 (55.791)	Acc@5 78.125 (84.527)
Epoch: [81][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4428 (2.3972)	Acc@1 53.906 (55.802)	Acc@5 85.156 (84.645)
Epoch: [81][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.9359 (2.3987)	Acc@1 43.750 (55.794)	Acc@5 78.906 (84.584)
Epoch: [81][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4505 (2.3993)	Acc@1 56.250 (55.797)	Acc@5 82.812 (84.598)
Epoch: [81][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4730 (2.3987)	Acc@1 52.344 (55.783)	Acc@5 84.375 (84.595)
Epoch: [81][250/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2864 (2.4025)	Acc@1 58.594 (55.755)	Acc@5 85.938 (84.465)
Epoch: [81][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6922 (2.4050)	Acc@1 45.312 (55.669)	Acc@5 81.250 (84.441)
Epoch: [81][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5097 (2.4077)	Acc@1 53.125 (55.610)	Acc@5 82.812 (84.335)
Epoch: [81][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4965 (2.4068)	Acc@1 54.688 (55.630)	Acc@5 88.281 (84.367)
Epoch: [81][290/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5358 (2.4106)	Acc@1 53.125 (55.584)	Acc@5 82.812 (84.305)
Epoch: [81][300/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3591 (2.4112)	Acc@1 58.594 (55.586)	Acc@5 83.594 (84.308)
Epoch: [81][310/391]	Time 0.019 (0.012)	Data 0.001 (0.002)	Loss 2.2279 (2.4120)	Acc@1 64.062 (55.604)	Acc@5 84.375 (84.259)
Epoch: [81][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5063 (2.4136)	Acc@1 53.906 (55.576)	Acc@5 85.938 (84.227)
Epoch: [81][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2640 (2.4122)	Acc@1 60.156 (55.568)	Acc@5 86.719 (84.276)
Epoch: [81][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2263 (2.4130)	Acc@1 57.031 (55.547)	Acc@5 86.719 (84.247)
Epoch: [81][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7719 (2.4143)	Acc@1 45.312 (55.511)	Acc@5 79.688 (84.244)
Epoch: [81][360/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5357 (2.4162)	Acc@1 52.344 (55.432)	Acc@5 84.375 (84.241)
Epoch: [81][370/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3006 (2.4140)	Acc@1 55.469 (55.431)	Acc@5 82.812 (84.249)
Epoch: [81][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5346 (2.4149)	Acc@1 53.125 (55.407)	Acc@5 83.594 (84.234)
Epoch: [81][390/391]	Time 0.146 (0.012)	Data 0.001 (0.002)	Loss 2.3656 (2.4164)	Acc@1 60.000 (55.368)	Acc@5 88.750 (84.218)
num momentum params: 26
[0.1, 2.416402384338379, 2.225902978181839, 55.368, 45.46, tensor(0.3298, device='cuda:0', grad_fn=<DivBackward0>), 4.650441646575928, 0.40636706352233887]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [82 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [82][0/391]	Time 0.045 (0.045)	Data 0.154 (0.154)	Loss 2.4572 (2.4572)	Acc@1 53.125 (53.125)	Acc@5 84.375 (84.375)
Epoch: [82][10/391]	Time 0.013 (0.015)	Data 0.001 (0.016)	Loss 2.1383 (2.2979)	Acc@1 59.375 (57.955)	Acc@5 89.844 (86.577)
Epoch: [82][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.3199 (2.2678)	Acc@1 59.375 (58.743)	Acc@5 85.938 (86.644)
Epoch: [82][30/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.2197 (2.2681)	Acc@1 60.938 (58.745)	Acc@5 85.156 (86.542)
Epoch: [82][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.3750 (2.3064)	Acc@1 52.344 (57.736)	Acc@5 83.594 (86.014)
Epoch: [82][50/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 2.3529 (2.3188)	Acc@1 63.281 (57.721)	Acc@5 85.938 (85.784)
Epoch: [82][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.1784 (2.3186)	Acc@1 67.188 (57.659)	Acc@5 86.719 (85.579)
Epoch: [82][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2775 (2.3271)	Acc@1 60.156 (57.273)	Acc@5 85.938 (85.508)
Epoch: [82][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5390 (2.3364)	Acc@1 50.000 (57.041)	Acc@5 79.688 (85.320)
Epoch: [82][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.5297 (2.3374)	Acc@1 54.688 (56.980)	Acc@5 83.594 (85.388)
Epoch: [82][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6283 (2.3459)	Acc@1 51.562 (56.861)	Acc@5 78.906 (85.288)
Epoch: [82][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4556 (2.3544)	Acc@1 53.125 (56.658)	Acc@5 82.812 (85.191)
Epoch: [82][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1908 (2.3543)	Acc@1 64.844 (56.715)	Acc@5 89.062 (85.182)
Epoch: [82][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2731 (2.3552)	Acc@1 57.812 (56.751)	Acc@5 89.062 (85.198)
Epoch: [82][140/391]	Time 0.013 (0.011)	Data 0.001 (0.003)	Loss 2.5546 (2.3559)	Acc@1 49.219 (56.693)	Acc@5 83.594 (85.322)
Epoch: [82][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3292 (2.3585)	Acc@1 57.031 (56.623)	Acc@5 84.375 (85.379)
Epoch: [82][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4188 (2.3558)	Acc@1 59.375 (56.692)	Acc@5 81.250 (85.409)
Epoch: [82][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2956 (2.3624)	Acc@1 57.031 (56.506)	Acc@5 86.719 (85.284)
Epoch: [82][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4796 (2.3661)	Acc@1 49.219 (56.332)	Acc@5 81.250 (85.230)
Epoch: [82][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6231 (2.3682)	Acc@1 49.219 (56.336)	Acc@5 82.812 (85.185)
Epoch: [82][200/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3028 (2.3661)	Acc@1 57.031 (56.394)	Acc@5 85.938 (85.257)
Epoch: [82][210/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2422 (2.3683)	Acc@1 62.500 (56.328)	Acc@5 82.812 (85.156)
Epoch: [82][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6356 (2.3697)	Acc@1 50.000 (56.211)	Acc@5 82.031 (85.199)
Epoch: [82][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7031 (2.3765)	Acc@1 46.875 (56.057)	Acc@5 79.688 (85.099)
Epoch: [82][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8445 (2.3802)	Acc@1 46.094 (55.994)	Acc@5 75.000 (85.036)
Epoch: [82][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4903 (2.3846)	Acc@1 51.562 (55.926)	Acc@5 81.250 (84.904)
Epoch: [82][260/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.4713 (2.3894)	Acc@1 53.125 (55.807)	Acc@5 90.625 (84.818)
Epoch: [82][270/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4981 (2.3924)	Acc@1 53.125 (55.792)	Acc@5 83.594 (84.802)
Epoch: [82][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7249 (2.3956)	Acc@1 50.000 (55.738)	Acc@5 76.562 (84.720)
Epoch: [82][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5139 (2.4001)	Acc@1 53.906 (55.579)	Acc@5 82.812 (84.660)
Epoch: [82][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5669 (2.4042)	Acc@1 51.562 (55.474)	Acc@5 78.906 (84.598)
Epoch: [82][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2888 (2.4061)	Acc@1 54.688 (55.429)	Acc@5 91.406 (84.586)
Epoch: [82][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5132 (2.4083)	Acc@1 51.562 (55.349)	Acc@5 86.719 (84.577)
Epoch: [82][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4250 (2.4103)	Acc@1 52.344 (55.325)	Acc@5 82.812 (84.550)
Epoch: [82][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5848 (2.4118)	Acc@1 50.781 (55.274)	Acc@5 81.250 (84.508)
Epoch: [82][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3405 (2.4135)	Acc@1 58.594 (55.193)	Acc@5 83.594 (84.473)
Epoch: [82][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4837 (2.4149)	Acc@1 50.781 (55.179)	Acc@5 81.250 (84.442)
Epoch: [82][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6763 (2.4162)	Acc@1 47.656 (55.157)	Acc@5 79.688 (84.409)
Epoch: [82][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6311 (2.4167)	Acc@1 47.656 (55.165)	Acc@5 78.125 (84.410)
Epoch: [82][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3443 (2.4163)	Acc@1 60.000 (55.178)	Acc@5 86.250 (84.394)
num momentum params: 26
[0.1, 2.4162555358886717, 2.0322732806205748, 55.178, 47.62, tensor(0.3293, device='cuda:0', grad_fn=<DivBackward0>), 4.303467750549316, 0.33513450622558594]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [83 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [83][0/391]	Time 0.043 (0.043)	Data 0.168 (0.168)	Loss 2.2593 (2.2593)	Acc@1 60.938 (60.938)	Acc@5 82.031 (82.031)
Epoch: [83][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.4535 (2.2985)	Acc@1 59.375 (59.446)	Acc@5 85.938 (85.511)
Epoch: [83][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.3349 (2.2718)	Acc@1 61.719 (59.635)	Acc@5 85.938 (86.198)
Epoch: [83][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.3232 (2.2740)	Acc@1 60.156 (59.375)	Acc@5 88.281 (86.341)
Epoch: [83][40/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.6953 (2.2947)	Acc@1 50.781 (58.880)	Acc@5 79.688 (86.109)
Epoch: [83][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3880 (2.3055)	Acc@1 56.250 (58.640)	Acc@5 84.375 (86.106)
Epoch: [83][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5102 (2.3084)	Acc@1 52.344 (58.491)	Acc@5 85.938 (86.104)
Epoch: [83][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4246 (2.3167)	Acc@1 51.562 (58.319)	Acc@5 85.156 (86.070)
Epoch: [83][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3769 (2.3258)	Acc@1 57.031 (57.957)	Acc@5 84.375 (85.841)
Epoch: [83][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.5860 (2.3363)	Acc@1 53.906 (57.649)	Acc@5 82.812 (85.663)
Epoch: [83][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5677 (2.3485)	Acc@1 52.344 (57.310)	Acc@5 81.250 (85.311)
Epoch: [83][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5198 (2.3625)	Acc@1 48.438 (56.996)	Acc@5 84.375 (85.156)
Epoch: [83][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3968 (2.3604)	Acc@1 54.688 (57.025)	Acc@5 83.594 (85.176)
Epoch: [83][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4139 (2.3626)	Acc@1 57.031 (57.049)	Acc@5 85.156 (85.067)
Epoch: [83][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1861 (2.3620)	Acc@1 62.500 (56.954)	Acc@5 85.156 (85.045)
Epoch: [83][150/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5772 (2.3644)	Acc@1 52.344 (56.767)	Acc@5 82.812 (85.017)
Epoch: [83][160/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3853 (2.3652)	Acc@1 53.125 (56.774)	Acc@5 85.156 (84.986)
Epoch: [83][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2593 (2.3698)	Acc@1 60.938 (56.579)	Acc@5 85.156 (84.987)
Epoch: [83][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3820 (2.3712)	Acc@1 52.344 (56.496)	Acc@5 83.594 (85.009)
Epoch: [83][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3965 (2.3751)	Acc@1 58.594 (56.405)	Acc@5 84.375 (84.894)
Epoch: [83][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4407 (2.3772)	Acc@1 55.469 (56.390)	Acc@5 85.938 (84.845)
Epoch: [83][210/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.5488 (2.3793)	Acc@1 52.344 (56.324)	Acc@5 78.906 (84.830)
Epoch: [83][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5684 (2.3825)	Acc@1 53.125 (56.282)	Acc@5 83.594 (84.796)
Epoch: [83][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2883 (2.3840)	Acc@1 54.688 (56.223)	Acc@5 87.500 (84.730)
Epoch: [83][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1542 (2.3858)	Acc@1 59.375 (56.162)	Acc@5 86.719 (84.735)
Epoch: [83][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3395 (2.3864)	Acc@1 57.812 (56.116)	Acc@5 84.375 (84.699)
Epoch: [83][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3913 (2.3896)	Acc@1 53.125 (56.037)	Acc@5 85.156 (84.653)
Epoch: [83][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4131 (2.3902)	Acc@1 51.562 (56.022)	Acc@5 84.375 (84.629)
Epoch: [83][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4060 (2.3885)	Acc@1 56.250 (56.067)	Acc@5 84.375 (84.620)
Epoch: [83][290/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.5474 (2.3909)	Acc@1 50.781 (55.987)	Acc@5 79.688 (84.576)
Epoch: [83][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5032 (2.3930)	Acc@1 58.594 (55.985)	Acc@5 79.688 (84.507)
Epoch: [83][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6172 (2.3954)	Acc@1 45.312 (55.923)	Acc@5 83.594 (84.486)
Epoch: [83][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7280 (2.3970)	Acc@1 53.125 (55.902)	Acc@5 75.781 (84.431)
Epoch: [83][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1489 (2.3965)	Acc@1 61.719 (55.908)	Acc@5 88.281 (84.368)
Epoch: [83][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5086 (2.4005)	Acc@1 50.781 (55.792)	Acc@5 82.812 (84.311)
Epoch: [83][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3791 (2.4030)	Acc@1 52.344 (55.702)	Acc@5 82.031 (84.290)
Epoch: [83][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3944 (2.4030)	Acc@1 55.469 (55.709)	Acc@5 85.938 (84.306)
Epoch: [83][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5535 (2.4043)	Acc@1 50.781 (55.684)	Acc@5 83.594 (84.299)
Epoch: [83][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3400 (2.4054)	Acc@1 56.250 (55.670)	Acc@5 83.594 (84.262)
Epoch: [83][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4463 (2.4075)	Acc@1 53.750 (55.632)	Acc@5 83.750 (84.220)
num momentum params: 26
[0.1, 2.4075047580718993, 2.2210555469989774, 55.632, 43.95, tensor(0.3300, device='cuda:0', grad_fn=<DivBackward0>), 4.354969501495361, 0.3242669105529785]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [84 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [84][0/391]	Time 0.052 (0.052)	Data 0.172 (0.172)	Loss 2.3518 (2.3518)	Acc@1 54.688 (54.688)	Acc@5 85.156 (85.156)
Epoch: [84][10/391]	Time 0.012 (0.017)	Data 0.001 (0.017)	Loss 2.2643 (2.3178)	Acc@1 55.469 (58.168)	Acc@5 88.281 (85.511)
Epoch: [84][20/391]	Time 0.012 (0.015)	Data 0.001 (0.009)	Loss 2.5276 (2.3502)	Acc@1 56.250 (56.064)	Acc@5 81.250 (85.082)
Epoch: [84][30/391]	Time 0.015 (0.014)	Data 0.001 (0.007)	Loss 2.4932 (2.3918)	Acc@1 53.125 (54.688)	Acc@5 84.375 (84.829)
Epoch: [84][40/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 2.6580 (2.4030)	Acc@1 53.125 (54.973)	Acc@5 80.469 (84.375)
Epoch: [84][50/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.1871 (2.3879)	Acc@1 56.250 (55.224)	Acc@5 89.062 (84.697)
Epoch: [84][60/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 2.4823 (2.3808)	Acc@1 50.781 (55.277)	Acc@5 85.156 (84.734)
Epoch: [84][70/391]	Time 0.013 (0.013)	Data 0.001 (0.004)	Loss 2.2127 (2.3707)	Acc@1 59.375 (55.546)	Acc@5 89.844 (85.090)
Epoch: [84][80/391]	Time 0.010 (0.012)	Data 0.004 (0.004)	Loss 2.6253 (2.3717)	Acc@1 47.656 (55.575)	Acc@5 84.375 (85.108)
Epoch: [84][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.6961 (2.3687)	Acc@1 47.656 (55.735)	Acc@5 79.688 (85.010)
Epoch: [84][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3781 (2.3661)	Acc@1 58.594 (56.018)	Acc@5 85.938 (85.009)
Epoch: [84][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3706 (2.3685)	Acc@1 58.594 (56.025)	Acc@5 83.594 (84.945)
Epoch: [84][120/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3704 (2.3633)	Acc@1 59.375 (56.185)	Acc@5 86.719 (85.034)
Epoch: [84][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.6388 (2.3557)	Acc@1 48.438 (56.411)	Acc@5 81.250 (85.180)
Epoch: [84][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2753 (2.3574)	Acc@1 58.594 (56.455)	Acc@5 88.281 (85.112)
Epoch: [84][150/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3895 (2.3621)	Acc@1 55.469 (56.364)	Acc@5 88.281 (85.053)
Epoch: [84][160/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2463 (2.3672)	Acc@1 64.062 (56.357)	Acc@5 85.156 (84.889)
Epoch: [84][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5407 (2.3732)	Acc@1 53.125 (56.227)	Acc@5 80.469 (84.818)
Epoch: [84][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5171 (2.3731)	Acc@1 50.000 (56.259)	Acc@5 81.250 (84.858)
Epoch: [84][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6771 (2.3803)	Acc@1 49.219 (56.111)	Acc@5 78.125 (84.768)
Epoch: [84][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4184 (2.3855)	Acc@1 47.656 (55.904)	Acc@5 84.375 (84.729)
Epoch: [84][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4332 (2.3846)	Acc@1 57.031 (55.928)	Acc@5 86.719 (84.797)
Epoch: [84][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5102 (2.3884)	Acc@1 50.000 (55.819)	Acc@5 82.031 (84.760)
Epoch: [84][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3604 (2.3901)	Acc@1 60.938 (55.760)	Acc@5 85.938 (84.710)
Epoch: [84][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4872 (2.3921)	Acc@1 56.250 (55.731)	Acc@5 84.375 (84.696)
Epoch: [84][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4235 (2.3948)	Acc@1 56.250 (55.662)	Acc@5 82.812 (84.627)
Epoch: [84][260/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.4689 (2.3957)	Acc@1 55.469 (55.654)	Acc@5 85.156 (84.635)
Epoch: [84][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4221 (2.3982)	Acc@1 53.125 (55.526)	Acc@5 85.156 (84.600)
Epoch: [84][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3888 (2.4000)	Acc@1 55.469 (55.399)	Acc@5 78.125 (84.545)
Epoch: [84][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3996 (2.4029)	Acc@1 57.031 (55.361)	Acc@5 80.469 (84.482)
Epoch: [84][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7499 (2.4039)	Acc@1 50.000 (55.368)	Acc@5 78.125 (84.471)
Epoch: [84][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5106 (2.4071)	Acc@1 51.562 (55.290)	Acc@5 79.688 (84.435)
Epoch: [84][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4404 (2.4094)	Acc@1 53.906 (55.250)	Acc@5 87.500 (84.416)
Epoch: [84][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1753 (2.4080)	Acc@1 59.375 (55.294)	Acc@5 87.500 (84.469)
Epoch: [84][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4543 (2.4093)	Acc@1 59.375 (55.320)	Acc@5 80.469 (84.414)
Epoch: [84][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4220 (2.4117)	Acc@1 58.594 (55.277)	Acc@5 86.719 (84.384)
Epoch: [84][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7132 (2.4133)	Acc@1 46.875 (55.222)	Acc@5 80.469 (84.360)
Epoch: [84][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5790 (2.4130)	Acc@1 49.219 (55.231)	Acc@5 83.594 (84.379)
Epoch: [84][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5662 (2.4157)	Acc@1 47.656 (55.204)	Acc@5 80.469 (84.332)
Epoch: [84][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6135 (2.4163)	Acc@1 48.750 (55.212)	Acc@5 81.250 (84.340)
num momentum params: 26
[0.1, 2.4163112600708008, 2.0079724061489106, 55.212, 47.77, tensor(0.3286, device='cuda:0', grad_fn=<DivBackward0>), 4.393678188323975, 0.3297996520996094]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [85 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [85][0/391]	Time 0.045 (0.045)	Data 0.161 (0.161)	Loss 2.3999 (2.3999)	Acc@1 54.688 (54.688)	Acc@5 85.938 (85.938)
Epoch: [85][10/391]	Time 0.010 (0.015)	Data 0.001 (0.016)	Loss 2.1110 (2.3280)	Acc@1 56.250 (57.031)	Acc@5 90.625 (86.506)
Epoch: [85][20/391]	Time 0.013 (0.013)	Data 0.001 (0.009)	Loss 2.3554 (2.3470)	Acc@1 51.562 (56.399)	Acc@5 84.375 (85.789)
Epoch: [85][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.1234 (2.3400)	Acc@1 61.719 (56.401)	Acc@5 88.281 (85.635)
Epoch: [85][40/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2088 (2.3182)	Acc@1 66.406 (57.279)	Acc@5 86.719 (86.014)
Epoch: [85][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4976 (2.3167)	Acc@1 57.031 (57.276)	Acc@5 82.812 (85.968)
Epoch: [85][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3476 (2.3239)	Acc@1 60.938 (57.121)	Acc@5 86.719 (85.899)
Epoch: [85][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5696 (2.3496)	Acc@1 52.344 (56.668)	Acc@5 81.250 (85.409)
Epoch: [85][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4215 (2.3537)	Acc@1 57.031 (56.607)	Acc@5 83.594 (85.291)
Epoch: [85][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3851 (2.3566)	Acc@1 56.250 (56.654)	Acc@5 86.719 (85.251)
Epoch: [85][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.5363 (2.3564)	Acc@1 52.344 (56.614)	Acc@5 80.469 (85.311)
Epoch: [85][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3727 (2.3632)	Acc@1 58.594 (56.215)	Acc@5 82.812 (85.255)
Epoch: [85][120/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4293 (2.3623)	Acc@1 53.125 (56.308)	Acc@5 84.375 (85.221)
Epoch: [85][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4328 (2.3618)	Acc@1 53.125 (56.292)	Acc@5 83.594 (85.258)
Epoch: [85][140/391]	Time 0.013 (0.011)	Data 0.001 (0.003)	Loss 2.3335 (2.3613)	Acc@1 60.156 (56.283)	Acc@5 87.500 (85.278)
Epoch: [85][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2768 (2.3626)	Acc@1 60.156 (56.291)	Acc@5 85.938 (85.172)
Epoch: [85][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2903 (2.3652)	Acc@1 57.031 (56.337)	Acc@5 87.500 (85.016)
Epoch: [85][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3680 (2.3710)	Acc@1 55.469 (56.241)	Acc@5 83.594 (84.873)
Epoch: [85][180/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3433 (2.3772)	Acc@1 52.344 (56.021)	Acc@5 85.938 (84.798)
Epoch: [85][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3054 (2.3773)	Acc@1 61.719 (56.025)	Acc@5 88.281 (84.886)
Epoch: [85][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2190 (2.3752)	Acc@1 58.594 (55.958)	Acc@5 86.719 (84.919)
Epoch: [85][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3795 (2.3807)	Acc@1 55.469 (55.858)	Acc@5 83.594 (84.897)
Epoch: [85][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2680 (2.3895)	Acc@1 60.156 (55.727)	Acc@5 91.406 (84.824)
Epoch: [85][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5573 (2.3947)	Acc@1 52.344 (55.574)	Acc@5 85.156 (84.791)
Epoch: [85][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3285 (2.3967)	Acc@1 50.000 (55.524)	Acc@5 86.719 (84.738)
Epoch: [85][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5145 (2.3979)	Acc@1 53.125 (55.475)	Acc@5 81.250 (84.714)
Epoch: [85][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6048 (2.3994)	Acc@1 55.469 (55.484)	Acc@5 78.906 (84.686)
Epoch: [85][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5176 (2.4031)	Acc@1 53.125 (55.463)	Acc@5 89.062 (84.600)
Epoch: [85][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4290 (2.4046)	Acc@1 53.125 (55.497)	Acc@5 84.375 (84.575)
Epoch: [85][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6349 (2.4073)	Acc@1 45.312 (55.447)	Acc@5 81.250 (84.517)
Epoch: [85][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5667 (2.4070)	Acc@1 53.906 (55.461)	Acc@5 81.250 (84.520)
Epoch: [85][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8059 (2.4092)	Acc@1 44.531 (55.419)	Acc@5 78.125 (84.498)
Epoch: [85][320/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.5011 (2.4132)	Acc@1 49.219 (55.311)	Acc@5 85.938 (84.455)
Epoch: [85][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5101 (2.4149)	Acc@1 54.688 (55.261)	Acc@5 80.469 (84.406)
Epoch: [85][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5376 (2.4172)	Acc@1 46.875 (55.157)	Acc@5 85.156 (84.393)
Epoch: [85][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3331 (2.4173)	Acc@1 60.156 (55.157)	Acc@5 83.594 (84.368)
Epoch: [85][360/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.2655 (2.4188)	Acc@1 54.688 (55.127)	Acc@5 86.719 (84.330)
Epoch: [85][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1836 (2.4185)	Acc@1 60.156 (55.153)	Acc@5 90.625 (84.310)
Epoch: [85][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2157 (2.4191)	Acc@1 64.844 (55.130)	Acc@5 86.719 (84.322)
Epoch: [85][390/391]	Time 0.011 (0.011)	Data 0.000 (0.002)	Loss 2.5354 (2.4199)	Acc@1 51.250 (55.124)	Acc@5 80.000 (84.314)
num momentum params: 26
[0.1, 2.4199273348999024, 1.984744348526001, 55.124, 49.88, tensor(0.3294, device='cuda:0', grad_fn=<DivBackward0>), 4.339340925216675, 0.33293700218200684]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [86 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [86][0/391]	Time 0.049 (0.049)	Data 0.157 (0.157)	Loss 1.9488 (1.9488)	Acc@1 69.531 (69.531)	Acc@5 90.625 (90.625)
Epoch: [86][10/391]	Time 0.010 (0.015)	Data 0.001 (0.016)	Loss 2.1087 (2.2946)	Acc@1 65.625 (59.091)	Acc@5 89.844 (86.719)
Epoch: [86][20/391]	Time 0.010 (0.013)	Data 0.001 (0.009)	Loss 2.2908 (2.2900)	Acc@1 58.594 (58.929)	Acc@5 85.938 (86.496)
Epoch: [86][30/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.3792 (2.2783)	Acc@1 55.469 (59.501)	Acc@5 83.594 (86.593)
Epoch: [86][40/391]	Time 0.021 (0.012)	Data 0.001 (0.005)	Loss 2.0615 (2.3009)	Acc@1 67.969 (58.918)	Acc@5 90.625 (86.261)
Epoch: [86][50/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 2.2148 (2.3192)	Acc@1 56.250 (58.134)	Acc@5 89.062 (85.968)
Epoch: [86][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5225 (2.3336)	Acc@1 54.688 (57.902)	Acc@5 81.250 (85.784)
Epoch: [86][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.1507 (2.3272)	Acc@1 63.281 (58.022)	Acc@5 86.719 (85.805)
Epoch: [86][80/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 2.0321 (2.3282)	Acc@1 60.938 (57.957)	Acc@5 90.625 (85.774)
Epoch: [86][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2526 (2.3260)	Acc@1 59.375 (58.019)	Acc@5 87.500 (85.731)
Epoch: [86][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4340 (2.3375)	Acc@1 54.688 (57.812)	Acc@5 85.156 (85.520)
Epoch: [86][110/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 2.2263 (2.3393)	Acc@1 59.375 (57.693)	Acc@5 85.156 (85.431)
Epoch: [86][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6970 (2.3545)	Acc@1 53.906 (57.277)	Acc@5 82.031 (85.260)
Epoch: [86][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1194 (2.3591)	Acc@1 61.719 (57.061)	Acc@5 93.750 (85.270)
Epoch: [86][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4022 (2.3627)	Acc@1 55.469 (56.915)	Acc@5 85.938 (85.256)
Epoch: [86][150/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5354 (2.3698)	Acc@1 53.125 (56.757)	Acc@5 84.375 (85.151)
Epoch: [86][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3250 (2.3737)	Acc@1 57.031 (56.619)	Acc@5 84.375 (85.108)
Epoch: [86][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5968 (2.3791)	Acc@1 52.344 (56.515)	Acc@5 82.031 (84.996)
Epoch: [86][180/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.1744 (2.3808)	Acc@1 60.938 (56.474)	Acc@5 90.625 (84.971)
Epoch: [86][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2949 (2.3854)	Acc@1 53.906 (56.324)	Acc@5 90.625 (84.866)
Epoch: [86][200/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.6163 (2.3878)	Acc@1 52.344 (56.262)	Acc@5 79.688 (84.880)
Epoch: [86][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5904 (2.3885)	Acc@1 53.125 (56.269)	Acc@5 84.375 (84.916)
Epoch: [86][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3868 (2.3866)	Acc@1 57.031 (56.296)	Acc@5 84.375 (84.944)
Epoch: [86][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4888 (2.3899)	Acc@1 54.688 (56.196)	Acc@5 82.031 (84.896)
Epoch: [86][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5625 (2.3921)	Acc@1 52.344 (56.124)	Acc@5 78.906 (84.861)
Epoch: [86][250/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.5221 (2.3927)	Acc@1 53.906 (56.110)	Acc@5 82.031 (84.848)
Epoch: [86][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4713 (2.3971)	Acc@1 58.594 (56.073)	Acc@5 81.250 (84.749)
Epoch: [86][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5846 (2.3969)	Acc@1 50.000 (56.045)	Acc@5 84.375 (84.787)
Epoch: [86][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3548 (2.3971)	Acc@1 55.469 (56.061)	Acc@5 84.375 (84.753)
Epoch: [86][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3942 (2.4000)	Acc@1 58.594 (55.998)	Acc@5 82.031 (84.705)
Epoch: [86][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4947 (2.4048)	Acc@1 51.562 (55.868)	Acc@5 83.594 (84.658)
Epoch: [86][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5454 (2.4044)	Acc@1 54.688 (55.888)	Acc@5 82.031 (84.656)
Epoch: [86][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5145 (2.4057)	Acc@1 50.781 (55.836)	Acc@5 82.031 (84.621)
Epoch: [86][330/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2598 (2.4068)	Acc@1 60.156 (55.813)	Acc@5 84.375 (84.616)
Epoch: [86][340/391]	Time 0.010 (0.011)	Data 0.011 (0.002)	Loss 2.4666 (2.4104)	Acc@1 57.812 (55.732)	Acc@5 78.906 (84.515)
Epoch: [86][350/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.6053 (2.4138)	Acc@1 52.344 (55.700)	Acc@5 78.125 (84.446)
Epoch: [86][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5194 (2.4156)	Acc@1 55.469 (55.629)	Acc@5 81.250 (84.427)
Epoch: [86][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4887 (2.4151)	Acc@1 49.219 (55.561)	Acc@5 86.719 (84.417)
Epoch: [86][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4735 (2.4169)	Acc@1 55.469 (55.547)	Acc@5 82.031 (84.389)
Epoch: [86][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.4254 (2.4173)	Acc@1 58.750 (55.524)	Acc@5 83.750 (84.404)
num momentum params: 26
[0.1, 2.4172504259490966, 2.0484174048900603, 55.524, 47.45, tensor(0.3297, device='cuda:0', grad_fn=<DivBackward0>), 4.328185558319092, 0.3286163806915283]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [87 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [87][0/391]	Time 0.052 (0.052)	Data 0.158 (0.158)	Loss 2.6707 (2.6707)	Acc@1 52.344 (52.344)	Acc@5 76.562 (76.562)
Epoch: [87][10/391]	Time 0.011 (0.016)	Data 0.001 (0.016)	Loss 2.3969 (2.3846)	Acc@1 53.906 (56.037)	Acc@5 86.719 (84.659)
Epoch: [87][20/391]	Time 0.010 (0.014)	Data 0.001 (0.009)	Loss 2.1962 (2.3160)	Acc@1 62.500 (57.887)	Acc@5 89.844 (85.751)
Epoch: [87][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.0917 (2.3088)	Acc@1 65.625 (58.518)	Acc@5 86.719 (86.290)
Epoch: [87][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.0090 (2.3057)	Acc@1 71.094 (58.956)	Acc@5 88.281 (85.938)
Epoch: [87][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.5149 (2.3311)	Acc@1 53.125 (58.349)	Acc@5 82.812 (85.509)
Epoch: [87][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.5545 (2.3366)	Acc@1 50.781 (57.992)	Acc@5 82.812 (85.566)
Epoch: [87][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.7596 (2.3545)	Acc@1 43.750 (57.405)	Acc@5 80.469 (85.299)
Epoch: [87][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3220 (2.3567)	Acc@1 52.344 (57.041)	Acc@5 85.938 (85.253)
Epoch: [87][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2920 (2.3595)	Acc@1 53.125 (56.834)	Acc@5 84.375 (85.182)
Epoch: [87][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2550 (2.3621)	Acc@1 61.719 (56.691)	Acc@5 83.594 (85.087)
Epoch: [87][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3628 (2.3653)	Acc@1 56.250 (56.595)	Acc@5 85.156 (85.051)
Epoch: [87][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3494 (2.3705)	Acc@1 57.812 (56.605)	Acc@5 88.281 (84.943)
Epoch: [87][130/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.5102 (2.3793)	Acc@1 50.000 (56.536)	Acc@5 85.938 (84.840)
Epoch: [87][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5932 (2.3855)	Acc@1 53.906 (56.466)	Acc@5 82.812 (84.735)
Epoch: [87][150/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3756 (2.3862)	Acc@1 56.250 (56.509)	Acc@5 86.719 (84.727)
Epoch: [87][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4461 (2.3882)	Acc@1 53.906 (56.512)	Acc@5 82.812 (84.647)
Epoch: [87][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4202 (2.3875)	Acc@1 57.812 (56.483)	Acc@5 82.812 (84.663)
Epoch: [87][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5036 (2.3856)	Acc@1 55.469 (56.405)	Acc@5 84.375 (84.729)
Epoch: [87][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4148 (2.3899)	Acc@1 57.031 (56.275)	Acc@5 85.156 (84.735)
Epoch: [87][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5172 (2.3973)	Acc@1 51.562 (56.095)	Acc@5 83.594 (84.616)
Epoch: [87][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2448 (2.4010)	Acc@1 55.469 (55.995)	Acc@5 88.281 (84.564)
Epoch: [87][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.8496 (2.4067)	Acc@1 49.219 (55.861)	Acc@5 73.438 (84.435)
Epoch: [87][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3434 (2.4078)	Acc@1 59.375 (55.851)	Acc@5 83.594 (84.432)
Epoch: [87][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3248 (2.4079)	Acc@1 56.250 (55.816)	Acc@5 85.156 (84.411)
Epoch: [87][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4403 (2.4084)	Acc@1 52.344 (55.817)	Acc@5 82.812 (84.403)
Epoch: [87][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2634 (2.4080)	Acc@1 57.812 (55.774)	Acc@5 82.031 (84.414)
Epoch: [87][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2923 (2.4073)	Acc@1 52.344 (55.780)	Acc@5 83.594 (84.433)
Epoch: [87][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5478 (2.4082)	Acc@1 50.000 (55.705)	Acc@5 84.375 (84.456)
Epoch: [87][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3142 (2.4101)	Acc@1 57.031 (55.643)	Acc@5 90.625 (84.456)
Epoch: [87][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6647 (2.4144)	Acc@1 46.094 (55.487)	Acc@5 78.906 (84.409)
Epoch: [87][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5803 (2.4161)	Acc@1 49.219 (55.398)	Acc@5 83.594 (84.445)
Epoch: [87][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6797 (2.4217)	Acc@1 46.094 (55.315)	Acc@5 75.781 (84.321)
Epoch: [87][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3543 (2.4243)	Acc@1 55.469 (55.193)	Acc@5 84.375 (84.285)
Epoch: [87][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3463 (2.4228)	Acc@1 58.594 (55.235)	Acc@5 82.812 (84.283)
Epoch: [87][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2950 (2.4219)	Acc@1 55.469 (55.244)	Acc@5 87.500 (84.261)
Epoch: [87][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4720 (2.4243)	Acc@1 51.562 (55.185)	Acc@5 83.594 (84.230)
Epoch: [87][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2601 (2.4235)	Acc@1 59.375 (55.206)	Acc@5 87.500 (84.270)
Epoch: [87][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5183 (2.4220)	Acc@1 53.125 (55.249)	Acc@5 79.688 (84.279)
Epoch: [87][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.2674 (2.4210)	Acc@1 56.250 (55.268)	Acc@5 88.750 (84.304)
num momentum params: 26
[0.1, 2.4210009881591796, 1.9197132313251495, 55.268, 49.98, tensor(0.3297, device='cuda:0', grad_fn=<DivBackward0>), 4.323317527770996, 0.3268096446990967]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [88 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [88][0/391]	Time 0.047 (0.047)	Data 0.159 (0.159)	Loss 2.0182 (2.0182)	Acc@1 64.062 (64.062)	Acc@5 90.625 (90.625)
Epoch: [88][10/391]	Time 0.014 (0.015)	Data 0.001 (0.016)	Loss 2.1948 (2.3190)	Acc@1 59.375 (56.250)	Acc@5 89.844 (86.151)
Epoch: [88][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.3802 (2.2800)	Acc@1 56.250 (58.705)	Acc@5 84.375 (86.607)
Epoch: [88][30/391]	Time 0.011 (0.013)	Data 0.002 (0.007)	Loss 2.3139 (2.3023)	Acc@1 56.250 (58.241)	Acc@5 85.938 (86.416)
Epoch: [88][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4619 (2.3096)	Acc@1 53.125 (58.155)	Acc@5 84.375 (86.433)
Epoch: [88][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.1804 (2.3133)	Acc@1 62.500 (57.858)	Acc@5 88.281 (86.489)
Epoch: [88][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4597 (2.3297)	Acc@1 53.906 (57.339)	Acc@5 82.031 (86.296)
Epoch: [88][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3264 (2.3424)	Acc@1 51.562 (56.877)	Acc@5 87.500 (86.026)
Epoch: [88][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.9628 (2.3308)	Acc@1 64.062 (56.993)	Acc@5 92.188 (86.188)
Epoch: [88][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3337 (2.3354)	Acc@1 55.469 (56.988)	Acc@5 86.719 (86.075)
Epoch: [88][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2170 (2.3334)	Acc@1 58.594 (56.877)	Acc@5 86.719 (86.115)
Epoch: [88][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3176 (2.3412)	Acc@1 54.688 (56.637)	Acc@5 86.719 (85.952)
Epoch: [88][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.4875 (2.3461)	Acc@1 55.469 (56.437)	Acc@5 82.812 (85.847)
Epoch: [88][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4116 (2.3464)	Acc@1 64.062 (56.483)	Acc@5 85.938 (85.842)
Epoch: [88][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4027 (2.3475)	Acc@1 58.594 (56.577)	Acc@5 82.812 (85.760)
Epoch: [88][150/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6099 (2.3449)	Acc@1 50.781 (56.674)	Acc@5 83.594 (85.813)
Epoch: [88][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3283 (2.3542)	Acc@1 57.812 (56.502)	Acc@5 87.500 (85.608)
Epoch: [88][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6111 (2.3604)	Acc@1 50.000 (56.414)	Acc@5 81.250 (85.462)
Epoch: [88][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2749 (2.3648)	Acc@1 60.156 (56.267)	Acc@5 87.500 (85.385)
Epoch: [88][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3324 (2.3629)	Acc@1 57.812 (56.258)	Acc@5 85.156 (85.418)
Epoch: [88][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4033 (2.3669)	Acc@1 54.688 (56.211)	Acc@5 81.250 (85.304)
Epoch: [88][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2989 (2.3686)	Acc@1 57.812 (56.102)	Acc@5 86.719 (85.312)
Epoch: [88][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5626 (2.3706)	Acc@1 53.906 (56.066)	Acc@5 84.375 (85.298)
Epoch: [88][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5164 (2.3745)	Acc@1 54.688 (55.993)	Acc@5 82.812 (85.237)
Epoch: [88][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.0828 (2.3768)	Acc@1 60.938 (55.978)	Acc@5 89.844 (85.182)
Epoch: [88][250/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4865 (2.3803)	Acc@1 53.906 (55.933)	Acc@5 85.938 (85.060)
Epoch: [88][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4511 (2.3826)	Acc@1 58.594 (55.900)	Acc@5 82.812 (85.037)
Epoch: [88][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4188 (2.3887)	Acc@1 51.562 (55.740)	Acc@5 83.594 (84.926)
Epoch: [88][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4999 (2.3941)	Acc@1 49.219 (55.641)	Acc@5 88.281 (84.831)
Epoch: [88][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3923 (2.3973)	Acc@1 58.594 (55.576)	Acc@5 85.938 (84.826)
Epoch: [88][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2178 (2.3996)	Acc@1 58.594 (55.554)	Acc@5 87.500 (84.803)
Epoch: [88][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4004 (2.4007)	Acc@1 57.812 (55.514)	Acc@5 83.594 (84.789)
Epoch: [88][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4149 (2.4047)	Acc@1 53.906 (55.437)	Acc@5 82.031 (84.684)
Epoch: [88][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3077 (2.4070)	Acc@1 56.250 (55.334)	Acc@5 86.719 (84.689)
Epoch: [88][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2115 (2.4069)	Acc@1 64.844 (55.329)	Acc@5 84.375 (84.652)
Epoch: [88][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3661 (2.4074)	Acc@1 57.031 (55.311)	Acc@5 83.594 (84.647)
Epoch: [88][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3469 (2.4085)	Acc@1 60.156 (55.319)	Acc@5 85.938 (84.637)
Epoch: [88][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4563 (2.4082)	Acc@1 57.812 (55.336)	Acc@5 84.375 (84.651)
Epoch: [88][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5536 (2.4083)	Acc@1 57.031 (55.356)	Acc@5 80.469 (84.625)
Epoch: [88][390/391]	Time 0.011 (0.011)	Data 0.000 (0.002)	Loss 2.3923 (2.4094)	Acc@1 48.750 (55.334)	Acc@5 85.000 (84.596)
num momentum params: 26
[0.1, 2.409377071304321, 2.029833678007126, 55.334, 47.74, tensor(0.3309, device='cuda:0', grad_fn=<DivBackward0>), 4.273509502410889, 0.3320133686065674]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [89 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [89][0/391]	Time 0.054 (0.054)	Data 0.165 (0.165)	Loss 2.2909 (2.2909)	Acc@1 59.375 (59.375)	Acc@5 86.719 (86.719)
Epoch: [89][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.1009 (2.2885)	Acc@1 64.062 (58.239)	Acc@5 91.406 (86.435)
Epoch: [89][20/391]	Time 0.010 (0.013)	Data 0.001 (0.009)	Loss 2.3428 (2.3453)	Acc@1 59.375 (56.882)	Acc@5 84.375 (85.491)
Epoch: [89][30/391]	Time 0.011 (0.012)	Data 0.001 (0.007)	Loss 2.0245 (2.3280)	Acc@1 64.062 (56.855)	Acc@5 92.188 (85.912)
Epoch: [89][40/391]	Time 0.012 (0.012)	Data 0.002 (0.005)	Loss 2.6098 (2.3272)	Acc@1 47.656 (56.879)	Acc@5 84.375 (85.842)
Epoch: [89][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2048 (2.3279)	Acc@1 61.719 (56.970)	Acc@5 87.500 (86.014)
Epoch: [89][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3363 (2.3206)	Acc@1 54.688 (57.467)	Acc@5 90.625 (86.168)
Epoch: [89][70/391]	Time 0.012 (0.011)	Data 0.001 (0.004)	Loss 2.3597 (2.3260)	Acc@1 57.031 (57.240)	Acc@5 85.156 (86.059)
Epoch: [89][80/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1341 (2.3352)	Acc@1 61.719 (57.099)	Acc@5 89.062 (85.629)
Epoch: [89][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5363 (2.3473)	Acc@1 53.906 (56.902)	Acc@5 85.938 (85.362)
Epoch: [89][100/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3300 (2.3505)	Acc@1 58.594 (56.938)	Acc@5 85.156 (85.303)
Epoch: [89][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3227 (2.3555)	Acc@1 59.375 (56.792)	Acc@5 84.375 (85.241)
Epoch: [89][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2544 (2.3516)	Acc@1 61.719 (56.876)	Acc@5 88.281 (85.343)
Epoch: [89][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1515 (2.3470)	Acc@1 64.844 (56.972)	Acc@5 88.281 (85.407)
Epoch: [89][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3161 (2.3547)	Acc@1 56.250 (56.793)	Acc@5 85.938 (85.311)
Epoch: [89][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3911 (2.3626)	Acc@1 58.594 (56.695)	Acc@5 85.938 (85.136)
Epoch: [89][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4037 (2.3670)	Acc@1 57.031 (56.483)	Acc@5 81.250 (85.103)
Epoch: [89][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4399 (2.3678)	Acc@1 54.688 (56.419)	Acc@5 87.500 (85.133)
Epoch: [89][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4983 (2.3678)	Acc@1 54.688 (56.401)	Acc@5 85.938 (85.195)
Epoch: [89][190/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3372 (2.3677)	Acc@1 57.812 (56.348)	Acc@5 87.500 (85.230)
Epoch: [89][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3390 (2.3687)	Acc@1 58.594 (56.320)	Acc@5 84.375 (85.176)
Epoch: [89][210/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.5111 (2.3714)	Acc@1 56.250 (56.228)	Acc@5 81.250 (85.153)
Epoch: [89][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4492 (2.3759)	Acc@1 54.688 (56.137)	Acc@5 82.031 (85.036)
Epoch: [89][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6685 (2.3805)	Acc@1 55.469 (56.037)	Acc@5 78.906 (84.963)
Epoch: [89][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1599 (2.3799)	Acc@1 61.719 (56.065)	Acc@5 89.844 (84.952)
Epoch: [89][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3327 (2.3811)	Acc@1 57.031 (56.017)	Acc@5 85.156 (84.913)
Epoch: [89][260/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3822 (2.3808)	Acc@1 56.250 (55.960)	Acc@5 86.719 (84.941)
Epoch: [89][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4589 (2.3832)	Acc@1 59.375 (55.916)	Acc@5 85.156 (84.934)
Epoch: [89][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5640 (2.3848)	Acc@1 53.906 (55.891)	Acc@5 81.250 (84.892)
Epoch: [89][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4348 (2.3847)	Acc@1 57.031 (55.866)	Acc@5 84.375 (84.904)
Epoch: [89][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3462 (2.3860)	Acc@1 59.375 (55.920)	Acc@5 84.375 (84.847)
Epoch: [89][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8021 (2.3899)	Acc@1 47.656 (55.841)	Acc@5 79.688 (84.810)
Epoch: [89][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4086 (2.3908)	Acc@1 56.250 (55.783)	Acc@5 78.906 (84.791)
Epoch: [89][330/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3443 (2.3923)	Acc@1 60.156 (55.773)	Acc@5 86.719 (84.781)
Epoch: [89][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.8945 (2.3965)	Acc@1 46.875 (55.663)	Acc@5 73.438 (84.691)
Epoch: [89][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4505 (2.3996)	Acc@1 55.469 (55.607)	Acc@5 84.375 (84.675)
Epoch: [89][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3727 (2.4036)	Acc@1 56.250 (55.534)	Acc@5 83.594 (84.596)
Epoch: [89][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2735 (2.4054)	Acc@1 56.250 (55.490)	Acc@5 88.281 (84.592)
Epoch: [89][380/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.1069 (2.4072)	Acc@1 62.500 (55.471)	Acc@5 87.500 (84.549)
Epoch: [89][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5557 (2.4073)	Acc@1 51.250 (55.450)	Acc@5 78.750 (84.554)
num momentum params: 26
[0.1, 2.407329459991455, 2.0653591108322145, 55.45, 46.37, tensor(0.3310, device='cuda:0', grad_fn=<DivBackward0>), 4.366143703460693, 0.320176362991333]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [111, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [111]
Non Pruning Epoch - module.bn2.bias: [111]
Non Pruning Epoch - module.conv3.weight: [219, 111, 3, 3]
Non Pruning Epoch - module.bn3.weight: [219]
Non Pruning Epoch - module.bn3.bias: [219]
Non Pruning Epoch - module.conv4.weight: [248, 219, 3, 3]
Non Pruning Epoch - module.bn4.weight: [248]
Non Pruning Epoch - module.bn4.bias: [248]
Non Pruning Epoch - module.conv5.weight: [375, 248, 3, 3]
Non Pruning Epoch - module.bn5.weight: [375]
Non Pruning Epoch - module.bn5.bias: [375]
Non Pruning Epoch - module.conv6.weight: [324, 375, 3, 3]
Non Pruning Epoch - module.bn6.weight: [324]
Non Pruning Epoch - module.bn6.bias: [324]
Non Pruning Epoch - module.conv7.weight: [217, 324, 3, 3]
Non Pruning Epoch - module.bn7.weight: [217]
Non Pruning Epoch - module.bn7.bias: [217]
Non Pruning Epoch - module.conv8.weight: [203, 217, 3, 3]
Non Pruning Epoch - module.bn8.weight: [203]
Non Pruning Epoch - module.bn8.bias: [203]
Non Pruning Epoch - module.fc.weight: [100, 203]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [90 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [111, 34, 3, 3]
module.conv3.weight [219, 111, 3, 3]
module.conv4.weight [248, 219, 3, 3]
module.conv5.weight [375, 248, 3, 3]
module.conv6.weight [324, 375, 3, 3]
module.conv7.weight [217, 324, 3, 3]
module.conv8.weight [203, 217, 3, 3]
Epoch: [90][0/391]	Time 0.047 (0.047)	Data 0.168 (0.168)	Loss 2.2488 (2.2488)	Acc@1 59.375 (59.375)	Acc@5 89.062 (89.062)
Epoch: [90][10/391]	Time 0.010 (0.015)	Data 0.001 (0.016)	Loss 2.6169 (2.2978)	Acc@1 50.000 (57.457)	Acc@5 80.469 (86.577)
Epoch: [90][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.3449 (2.2780)	Acc@1 56.250 (58.408)	Acc@5 83.594 (86.905)
Epoch: [90][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.1395 (2.2771)	Acc@1 66.406 (58.569)	Acc@5 89.062 (86.895)
Epoch: [90][40/391]	Time 0.010 (0.012)	Data 0.002 (0.005)	Loss 2.5716 (2.3077)	Acc@1 50.781 (57.832)	Acc@5 81.250 (86.300)
Epoch: [90][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5314 (2.3235)	Acc@1 53.906 (57.629)	Acc@5 83.594 (86.336)
Epoch: [90][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4897 (2.3332)	Acc@1 48.438 (57.249)	Acc@5 87.500 (86.270)
Epoch: [90][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3468 (2.3366)	Acc@1 58.594 (57.383)	Acc@5 85.938 (86.048)
Epoch: [90][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4796 (2.3449)	Acc@1 53.906 (57.157)	Acc@5 85.156 (85.793)
Epoch: [90][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4430 (2.3477)	Acc@1 57.812 (57.194)	Acc@5 82.031 (85.646)
Epoch: [90][100/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 2.1330 (2.3508)	Acc@1 67.969 (57.116)	Acc@5 85.938 (85.520)
Epoch: [90][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1754 (2.3466)	Acc@1 60.938 (57.095)	Acc@5 88.281 (85.642)
Epoch: [90][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4637 (2.3431)	Acc@1 58.594 (57.180)	Acc@5 83.594 (85.647)
Epoch: [90][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4662 (2.3479)	Acc@1 52.344 (57.115)	Acc@5 77.344 (85.520)
Epoch: [90][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2971 (2.3506)	Acc@1 60.938 (57.031)	Acc@5 85.156 (85.472)
Epoch: [90][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2307 (2.3514)	Acc@1 56.250 (57.016)	Acc@5 91.406 (85.508)
Epoch: [90][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3721 (2.3566)	Acc@1 58.594 (56.842)	Acc@5 83.594 (85.394)
Epoch: [90][170/391]	Time 0.017 (0.011)	Data 0.001 (0.002)	Loss 2.2560 (2.3553)	Acc@1 60.938 (56.903)	Acc@5 86.719 (85.403)
Epoch: [90][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4308 (2.3611)	Acc@1 52.344 (56.777)	Acc@5 82.031 (85.251)
Epoch: [90][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4064 (2.3664)	Acc@1 52.344 (56.618)	Acc@5 87.500 (85.156)
Epoch: [90][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5948 (2.3711)	Acc@1 52.344 (56.530)	Acc@5 78.125 (85.024)
Epoch: [90][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7036 (2.3756)	Acc@1 47.656 (56.402)	Acc@5 82.812 (84.975)
Epoch: [90][220/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.2562 (2.3755)	Acc@1 58.594 (56.413)	Acc@5 86.719 (84.958)
Epoch: [90][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3739 (2.3775)	Acc@1 57.812 (56.368)	Acc@5 86.719 (84.947)
Epoch: [90][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3482 (2.3795)	Acc@1 59.375 (56.289)	Acc@5 85.156 (84.916)
Epoch: [90][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3052 (2.3821)	Acc@1 56.250 (56.210)	Acc@5 88.281 (84.889)
Epoch: [90][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3286 (2.3828)	Acc@1 57.812 (56.169)	Acc@5 87.500 (84.824)
Epoch: [90][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6803 (2.3869)	Acc@1 40.625 (56.031)	Acc@5 85.156 (84.805)
Epoch: [90][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4187 (2.3854)	Acc@1 53.125 (56.061)	Acc@5 80.469 (84.781)
Epoch: [90][290/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 2.4078 (2.3897)	Acc@1 56.250 (55.914)	Acc@5 87.500 (84.713)
Epoch: [90][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3441 (2.3900)	Acc@1 56.250 (55.915)	Acc@5 85.156 (84.718)
Epoch: [90][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4353 (2.3905)	Acc@1 56.250 (55.901)	Acc@5 80.469 (84.729)
Epoch: [90][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3783 (2.3904)	Acc@1 61.719 (55.980)	Acc@5 88.281 (84.730)
Epoch: [90][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2417 (2.3926)	Acc@1 56.250 (55.948)	Acc@5 87.500 (84.691)
Epoch: [90][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5454 (2.3941)	Acc@1 55.469 (55.941)	Acc@5 84.375 (84.696)
Epoch: [90][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5190 (2.3966)	Acc@1 53.906 (55.881)	Acc@5 82.031 (84.606)
Epoch: [90][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3320 (2.3985)	Acc@1 62.500 (55.845)	Acc@5 85.938 (84.559)
Epoch: [90][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6949 (2.4008)	Acc@1 53.125 (55.835)	Acc@5 80.469 (84.531)
Epoch: [90][380/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.2122 (2.4020)	Acc@1 57.812 (55.787)	Acc@5 89.844 (84.531)
Epoch: [90][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.6537 (2.4017)	Acc@1 52.500 (55.798)	Acc@5 82.500 (84.514)
num momentum params: 26
[0.1, 2.4016738346862794, 1.8920599627494812, 55.798, 49.92, tensor(0.3311, device='cuda:0', grad_fn=<DivBackward0>), 4.296377658843994, 0.326371431350708]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [34, 3, 3, 3]
Before - module.bn1.weight: [34]
Before - module.bn1.bias: [34]
Before - module.conv2.weight: [111, 34, 3, 3]
Before - module.bn2.weight: [111]
Before - module.bn2.bias: [111]
Before - module.conv3.weight: [219, 111, 3, 3]
Before - module.bn3.weight: [219]
Before - module.bn3.bias: [219]
Before - module.conv4.weight: [248, 219, 3, 3]
Before - module.bn4.weight: [248]
Before - module.bn4.bias: [248]
Before - module.conv5.weight: [375, 248, 3, 3]
Before - module.bn5.weight: [375]
Before - module.bn5.bias: [375]
Before - module.conv6.weight: [324, 375, 3, 3]
Before - module.bn6.weight: [324]
Before - module.bn6.bias: [324]
Before - module.conv7.weight: [217, 324, 3, 3]
Before - module.bn7.weight: [217]
Before - module.bn7.bias: [217]
Before - module.conv8.weight: [203, 217, 3, 3]
Before - module.bn8.weight: [203]
Before - module.bn8.bias: [203]
Before - module.fc.weight: [100, 203]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [34, 3, 3, 3] >> [34, 3, 3, 3]
[module.bn1.weight]: 34 >> 34
running_mean [34]
running_var [34]
num_batches_tracked []
[module.conv2.weight]: [111, 34, 3, 3] >> [110, 34, 3, 3]
[module.bn2.weight]: 111 >> 110
running_mean [110]
running_var [110]
num_batches_tracked []
[module.conv3.weight]: [219, 111, 3, 3] >> [216, 110, 3, 3]
[module.bn3.weight]: 219 >> 216
running_mean [216]
running_var [216]
num_batches_tracked []
[module.conv4.weight]: [248, 219, 3, 3] >> [247, 216, 3, 3]
[module.bn4.weight]: 248 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [375, 248, 3, 3] >> [366, 247, 3, 3]
[module.bn5.weight]: 375 >> 366
running_mean [366]
running_var [366]
num_batches_tracked []
[module.conv6.weight]: [324, 375, 3, 3] >> [322, 366, 3, 3]
[module.bn6.weight]: 324 >> 322
running_mean [322]
running_var [322]
num_batches_tracked []
[module.conv7.weight]: [217, 324, 3, 3] >> [208, 322, 3, 3]
[module.bn7.weight]: 217 >> 208
running_mean [208]
running_var [208]
num_batches_tracked []
[module.conv8.weight]: [203, 217, 3, 3] >> [200, 208, 3, 3]
[module.bn8.weight]: 203 >> 200
running_mean [200]
running_var [200]
num_batches_tracked []
[module.fc.weight]: [100, 203] >> [100, 200]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [34, 3, 3, 3]
After - module.bn1.weight: [34]
After - module.bn1.bias: [34]
After - module.conv2.weight: [110, 34, 3, 3]
After - module.bn2.weight: [110]
After - module.bn2.bias: [110]
After - module.conv3.weight: [216, 110, 3, 3]
After - module.bn3.weight: [216]
After - module.bn3.bias: [216]
After - module.conv4.weight: [247, 216, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [366, 247, 3, 3]
After - module.bn5.weight: [366]
After - module.bn5.bias: [366]
After - module.conv6.weight: [322, 366, 3, 3]
After - module.bn6.weight: [322]
After - module.bn6.bias: [322]
After - module.conv7.weight: [208, 322, 3, 3]
After - module.bn7.weight: [208]
After - module.bn7.bias: [208]
After - module.conv8.weight: [200, 208, 3, 3]
After - module.bn8.weight: [200]
After - module.bn8.bias: [200]
After - module.fc.weight: [100, 200]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [34, 3, 3, 3]
conv2 --> [110, 34, 3, 3]
conv3 --> [216, 110, 3, 3]
conv4 --> [247, 216, 3, 3]
conv5 --> [366, 247, 3, 3]
conv6 --> [322, 366, 3, 3]
conv7 --> [208, 322, 3, 3]
conv8 --> [200, 208, 3, 3]
fc --> [200, 100]
1, 376482816, 940032, 34
2, 3601889280, 8616960, 110
3, 6240706560, 13685760, 216
4, 14013222912, 30730752, 247
5, 7081731072, 13017888, 366
6, 9232054272, 16970688, 322
7, 1851752448, 2411136, 208
8, 1150156800, 1497600, 200
fc, 7680000, 20000, 0
===================
FLOP REPORT: 17013936000000.0 42099200000.0 87890816 105248 1703 6.87019157409668
[INFO] Storing checkpoint...

Epoch: [91 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [91][0/391]	Time 0.236 (0.236)	Data 0.176 (0.176)	Loss 2.4035 (2.4035)	Acc@1 55.469 (55.469)	Acc@5 84.375 (84.375)
Epoch: [91][10/391]	Time 0.011 (0.033)	Data 0.001 (0.017)	Loss 2.0029 (2.2172)	Acc@1 63.281 (60.156)	Acc@5 93.750 (87.855)
Epoch: [91][20/391]	Time 0.013 (0.023)	Data 0.001 (0.010)	Loss 2.2217 (2.2757)	Acc@1 61.719 (58.929)	Acc@5 82.812 (86.607)
Epoch: [91][30/391]	Time 0.012 (0.019)	Data 0.001 (0.007)	Loss 2.1660 (2.2877)	Acc@1 60.156 (58.569)	Acc@5 89.844 (86.517)
Epoch: [91][40/391]	Time 0.011 (0.017)	Data 0.001 (0.006)	Loss 2.1948 (2.3040)	Acc@1 64.062 (57.870)	Acc@5 87.500 (86.147)
Epoch: [91][50/391]	Time 0.011 (0.016)	Data 0.001 (0.005)	Loss 2.5569 (2.3105)	Acc@1 53.125 (57.751)	Acc@5 81.250 (86.060)
Epoch: [91][60/391]	Time 0.010 (0.015)	Data 0.001 (0.004)	Loss 2.3159 (2.3235)	Acc@1 58.594 (57.569)	Acc@5 84.375 (86.014)
Epoch: [91][70/391]	Time 0.010 (0.015)	Data 0.001 (0.004)	Loss 2.4953 (2.3327)	Acc@1 52.344 (57.086)	Acc@5 84.375 (85.971)
Epoch: [91][80/391]	Time 0.010 (0.014)	Data 0.001 (0.004)	Loss 2.3409 (2.3291)	Acc@1 56.250 (57.099)	Acc@5 85.156 (85.793)
Epoch: [91][90/391]	Time 0.011 (0.014)	Data 0.001 (0.003)	Loss 2.1988 (2.3346)	Acc@1 65.625 (57.023)	Acc@5 87.500 (85.646)
Epoch: [91][100/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.3469 (2.3384)	Acc@1 55.469 (56.946)	Acc@5 85.156 (85.528)
Epoch: [91][110/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.4782 (2.3416)	Acc@1 52.344 (56.841)	Acc@5 82.031 (85.501)
Epoch: [91][120/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.5420 (2.3496)	Acc@1 53.125 (56.657)	Acc@5 82.812 (85.356)
Epoch: [91][130/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.3716 (2.3517)	Acc@1 50.781 (56.667)	Acc@5 85.156 (85.270)
Epoch: [91][140/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.5703 (2.3513)	Acc@1 51.562 (56.727)	Acc@5 84.375 (85.295)
Epoch: [91][150/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4282 (2.3532)	Acc@1 53.125 (56.700)	Acc@5 84.375 (85.327)
Epoch: [91][160/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.1815 (2.3493)	Acc@1 65.625 (56.798)	Acc@5 89.062 (85.413)
Epoch: [91][170/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4630 (2.3568)	Acc@1 53.906 (56.570)	Acc@5 78.906 (85.225)
Epoch: [91][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2321 (2.3614)	Acc@1 56.250 (56.401)	Acc@5 85.156 (85.148)
Epoch: [91][190/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.7057 (2.3659)	Acc@1 50.781 (56.422)	Acc@5 78.906 (85.066)
Epoch: [91][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5132 (2.3752)	Acc@1 54.688 (56.273)	Acc@5 84.375 (84.927)
Epoch: [91][210/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2733 (2.3723)	Acc@1 55.469 (56.383)	Acc@5 85.938 (84.979)
Epoch: [91][220/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6528 (2.3741)	Acc@1 51.562 (56.282)	Acc@5 87.500 (84.990)
Epoch: [91][230/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2276 (2.3780)	Acc@1 61.719 (56.135)	Acc@5 85.938 (84.903)
Epoch: [91][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2642 (2.3812)	Acc@1 60.156 (56.091)	Acc@5 85.938 (84.855)
Epoch: [91][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2731 (2.3833)	Acc@1 54.688 (56.010)	Acc@5 84.375 (84.764)
Epoch: [91][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5811 (2.3844)	Acc@1 53.125 (56.043)	Acc@5 82.812 (84.797)
Epoch: [91][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2596 (2.3869)	Acc@1 60.156 (55.953)	Acc@5 85.156 (84.776)
Epoch: [91][280/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3072 (2.3928)	Acc@1 60.156 (55.816)	Acc@5 87.500 (84.670)
Epoch: [91][290/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5059 (2.3972)	Acc@1 54.688 (55.721)	Acc@5 81.250 (84.617)
Epoch: [91][300/391]	Time 0.012 (0.012)	Data 0.007 (0.002)	Loss 2.4466 (2.4016)	Acc@1 54.688 (55.645)	Acc@5 83.594 (84.570)
Epoch: [91][310/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.4965 (2.4021)	Acc@1 51.562 (55.645)	Acc@5 84.375 (84.563)
Epoch: [91][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.0331 (2.4022)	Acc@1 64.062 (55.605)	Acc@5 90.625 (84.567)
Epoch: [91][330/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 2.3458 (2.4017)	Acc@1 57.812 (55.615)	Acc@5 84.375 (84.578)
Epoch: [91][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6332 (2.4008)	Acc@1 53.125 (55.668)	Acc@5 85.156 (84.616)
Epoch: [91][350/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4958 (2.4008)	Acc@1 53.125 (55.660)	Acc@5 84.375 (84.635)
Epoch: [91][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6350 (2.4032)	Acc@1 49.219 (55.594)	Acc@5 82.812 (84.591)
Epoch: [91][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5006 (2.4060)	Acc@1 51.562 (55.536)	Acc@5 82.812 (84.550)
Epoch: [91][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4460 (2.4079)	Acc@1 57.031 (55.481)	Acc@5 82.812 (84.500)
Epoch: [91][390/391]	Time 0.139 (0.012)	Data 0.000 (0.002)	Loss 2.4115 (2.4098)	Acc@1 53.750 (55.454)	Acc@5 83.750 (84.432)
num momentum params: 26
[0.1, 2.4097936360168455, 1.9883242523670197, 55.454, 48.46, tensor(0.3304, device='cuda:0', grad_fn=<DivBackward0>), 4.616126775741577, 0.41097450256347656]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [92 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [92][0/391]	Time 0.050 (0.050)	Data 0.167 (0.167)	Loss 2.3891 (2.3891)	Acc@1 54.688 (54.688)	Acc@5 88.281 (88.281)
Epoch: [92][10/391]	Time 0.012 (0.016)	Data 0.001 (0.016)	Loss 2.4887 (2.3836)	Acc@1 52.344 (55.824)	Acc@5 79.688 (85.085)
Epoch: [92][20/391]	Time 0.014 (0.014)	Data 0.001 (0.009)	Loss 2.5852 (2.3594)	Acc@1 54.688 (57.366)	Acc@5 82.031 (84.896)
Epoch: [92][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.3170 (2.3313)	Acc@1 57.812 (57.308)	Acc@5 85.938 (85.736)
Epoch: [92][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.2554 (2.3238)	Acc@1 56.250 (57.393)	Acc@5 84.375 (85.671)
Epoch: [92][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1648 (2.3275)	Acc@1 57.812 (57.031)	Acc@5 88.281 (85.738)
Epoch: [92][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4799 (2.3447)	Acc@1 50.000 (56.660)	Acc@5 85.156 (85.336)
Epoch: [92][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.5062 (2.3498)	Acc@1 54.688 (56.635)	Acc@5 82.812 (85.233)
Epoch: [92][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2734 (2.3506)	Acc@1 60.938 (56.674)	Acc@5 87.500 (85.397)
Epoch: [92][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2874 (2.3483)	Acc@1 60.156 (56.731)	Acc@5 85.938 (85.517)
Epoch: [92][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1166 (2.3524)	Acc@1 62.500 (56.683)	Acc@5 87.500 (85.381)
Epoch: [92][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4409 (2.3560)	Acc@1 53.906 (56.588)	Acc@5 85.938 (85.283)
Epoch: [92][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4786 (2.3532)	Acc@1 53.906 (56.683)	Acc@5 81.250 (85.356)
Epoch: [92][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0972 (2.3565)	Acc@1 67.188 (56.638)	Acc@5 86.719 (85.305)
Epoch: [92][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6004 (2.3626)	Acc@1 51.562 (56.411)	Acc@5 78.125 (85.184)
Epoch: [92][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5417 (2.3680)	Acc@1 51.562 (56.343)	Acc@5 86.719 (85.105)
Epoch: [92][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5756 (2.3721)	Acc@1 53.125 (56.323)	Acc@5 82.031 (85.113)
Epoch: [92][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5165 (2.3817)	Acc@1 55.469 (56.186)	Acc@5 83.594 (84.955)
Epoch: [92][180/391]	Time 0.010 (0.011)	Data 0.012 (0.002)	Loss 2.5192 (2.3884)	Acc@1 53.125 (56.021)	Acc@5 80.469 (84.854)
Epoch: [92][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5071 (2.3919)	Acc@1 49.219 (55.861)	Acc@5 82.812 (84.817)
Epoch: [92][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2359 (2.3927)	Acc@1 60.156 (55.865)	Acc@5 89.062 (84.838)
Epoch: [92][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4468 (2.3928)	Acc@1 50.000 (55.865)	Acc@5 88.281 (84.849)
Epoch: [92][220/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.4796 (2.3954)	Acc@1 54.688 (55.773)	Acc@5 82.812 (84.820)
Epoch: [92][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5386 (2.3936)	Acc@1 58.594 (55.797)	Acc@5 80.469 (84.811)
Epoch: [92][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5140 (2.3926)	Acc@1 54.688 (55.842)	Acc@5 84.375 (84.816)
Epoch: [92][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3421 (2.3899)	Acc@1 57.031 (55.867)	Acc@5 83.594 (84.901)
Epoch: [92][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5263 (2.3890)	Acc@1 50.781 (55.891)	Acc@5 77.344 (84.902)
Epoch: [92][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4573 (2.3959)	Acc@1 53.906 (55.731)	Acc@5 85.938 (84.758)
Epoch: [92][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2774 (2.4001)	Acc@1 56.250 (55.605)	Acc@5 86.719 (84.725)
Epoch: [92][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5167 (2.4048)	Acc@1 55.469 (55.506)	Acc@5 80.469 (84.614)
Epoch: [92][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4883 (2.4096)	Acc@1 54.688 (55.386)	Acc@5 85.938 (84.557)
Epoch: [92][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2791 (2.4124)	Acc@1 56.250 (55.398)	Acc@5 90.625 (84.496)
Epoch: [92][320/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3793 (2.4135)	Acc@1 53.906 (55.393)	Acc@5 85.156 (84.487)
Epoch: [92][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4581 (2.4143)	Acc@1 50.781 (55.370)	Acc@5 85.156 (84.486)
Epoch: [92][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6434 (2.4159)	Acc@1 51.562 (55.345)	Acc@5 78.906 (84.446)
Epoch: [92][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4168 (2.4160)	Acc@1 56.250 (55.342)	Acc@5 85.156 (84.437)
Epoch: [92][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3704 (2.4153)	Acc@1 57.812 (55.348)	Acc@5 84.375 (84.457)
Epoch: [92][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2349 (2.4137)	Acc@1 58.594 (55.363)	Acc@5 90.625 (84.503)
Epoch: [92][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4937 (2.4148)	Acc@1 57.031 (55.352)	Acc@5 82.031 (84.500)
Epoch: [92][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.1528 (2.4151)	Acc@1 62.500 (55.344)	Acc@5 86.250 (84.486)
num momentum params: 26
[0.1, 2.415070297393799, 2.027481887340546, 55.344, 47.71, tensor(0.3300, device='cuda:0', grad_fn=<DivBackward0>), 4.28086256980896, 0.33719778060913086]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [93 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [93][0/391]	Time 0.052 (0.052)	Data 0.158 (0.158)	Loss 2.3114 (2.3114)	Acc@1 53.906 (53.906)	Acc@5 88.281 (88.281)
Epoch: [93][10/391]	Time 0.011 (0.015)	Data 0.001 (0.016)	Loss 2.2776 (2.2928)	Acc@1 57.812 (57.812)	Acc@5 87.500 (86.648)
Epoch: [93][20/391]	Time 0.011 (0.013)	Data 0.001 (0.009)	Loss 2.2347 (2.3155)	Acc@1 62.500 (57.440)	Acc@5 90.625 (86.235)
Epoch: [93][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.4102 (2.2987)	Acc@1 52.344 (57.636)	Acc@5 85.156 (86.517)
Epoch: [93][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.0500 (2.3058)	Acc@1 70.312 (57.641)	Acc@5 89.844 (85.918)
Epoch: [93][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.3593 (2.3103)	Acc@1 60.938 (57.384)	Acc@5 86.719 (85.968)
Epoch: [93][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3046 (2.3291)	Acc@1 59.375 (56.839)	Acc@5 86.719 (85.745)
Epoch: [93][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4784 (2.3431)	Acc@1 54.688 (56.602)	Acc@5 82.812 (85.365)
Epoch: [93][80/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2110 (2.3482)	Acc@1 64.844 (56.549)	Acc@5 88.281 (85.272)
Epoch: [93][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3963 (2.3497)	Acc@1 54.688 (56.482)	Acc@5 85.156 (85.268)
Epoch: [93][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3780 (2.3423)	Acc@1 57.812 (56.567)	Acc@5 85.156 (85.404)
Epoch: [93][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3972 (2.3454)	Acc@1 51.562 (56.482)	Acc@5 85.156 (85.403)
Epoch: [93][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1967 (2.3463)	Acc@1 64.062 (56.579)	Acc@5 88.281 (85.337)
Epoch: [93][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3998 (2.3504)	Acc@1 57.812 (56.459)	Acc@5 84.375 (85.287)
Epoch: [93][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3792 (2.3498)	Acc@1 52.344 (56.405)	Acc@5 82.812 (85.295)
Epoch: [93][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4366 (2.3518)	Acc@1 53.906 (56.410)	Acc@5 85.938 (85.239)
Epoch: [93][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3906 (2.3609)	Acc@1 57.031 (56.274)	Acc@5 85.156 (85.137)
Epoch: [93][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5264 (2.3669)	Acc@1 52.344 (56.095)	Acc@5 82.031 (84.992)
Epoch: [93][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6343 (2.3722)	Acc@1 50.000 (55.982)	Acc@5 82.031 (84.897)
Epoch: [93][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2252 (2.3742)	Acc@1 60.156 (56.017)	Acc@5 87.500 (84.899)
Epoch: [93][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2395 (2.3726)	Acc@1 62.500 (56.122)	Acc@5 89.062 (84.985)
Epoch: [93][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3170 (2.3756)	Acc@1 55.469 (56.065)	Acc@5 87.500 (84.930)
Epoch: [93][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3972 (2.3787)	Acc@1 57.812 (56.045)	Acc@5 82.031 (84.831)
Epoch: [93][230/391]	Time 0.011 (0.011)	Data 0.003 (0.002)	Loss 2.3225 (2.3844)	Acc@1 57.031 (55.946)	Acc@5 85.156 (84.706)
Epoch: [93][240/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2437 (2.3834)	Acc@1 55.469 (55.945)	Acc@5 88.281 (84.715)
Epoch: [93][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1230 (2.3802)	Acc@1 61.719 (56.032)	Acc@5 87.500 (84.795)
Epoch: [93][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3369 (2.3774)	Acc@1 55.469 (56.112)	Acc@5 84.375 (84.824)
Epoch: [93][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3965 (2.3805)	Acc@1 54.688 (56.080)	Acc@5 81.250 (84.770)
Epoch: [93][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5862 (2.3817)	Acc@1 50.000 (56.036)	Acc@5 83.594 (84.781)
Epoch: [93][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4754 (2.3858)	Acc@1 48.438 (55.920)	Acc@5 81.250 (84.708)
Epoch: [93][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6392 (2.3862)	Acc@1 38.281 (55.843)	Acc@5 84.375 (84.718)
Epoch: [93][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5594 (2.3891)	Acc@1 46.875 (55.748)	Acc@5 80.469 (84.709)
Epoch: [93][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6162 (2.3896)	Acc@1 50.000 (55.797)	Acc@5 80.469 (84.687)
Epoch: [93][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4662 (2.3920)	Acc@1 53.125 (55.724)	Acc@5 85.938 (84.656)
Epoch: [93][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4721 (2.3962)	Acc@1 49.219 (55.618)	Acc@5 86.719 (84.606)
Epoch: [93][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6813 (2.3982)	Acc@1 52.344 (55.580)	Acc@5 78.906 (84.602)
Epoch: [93][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3344 (2.4011)	Acc@1 57.812 (55.540)	Acc@5 85.156 (84.526)
Epoch: [93][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6590 (2.4034)	Acc@1 48.438 (55.494)	Acc@5 79.688 (84.476)
Epoch: [93][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2927 (2.4045)	Acc@1 61.719 (55.454)	Acc@5 85.156 (84.475)
Epoch: [93][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.2741 (2.4035)	Acc@1 60.000 (55.476)	Acc@5 87.500 (84.498)
num momentum params: 26
[0.1, 2.4034720684814452, 1.996774923801422, 55.476, 47.92, tensor(0.3307, device='cuda:0', grad_fn=<DivBackward0>), 4.332961559295654, 0.3343219757080078]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [94 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [94][0/391]	Time 0.062 (0.062)	Data 0.185 (0.185)	Loss 2.4428 (2.4428)	Acc@1 58.594 (58.594)	Acc@5 82.031 (82.031)
Epoch: [94][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 2.5595 (2.2837)	Acc@1 53.125 (58.239)	Acc@5 79.688 (85.227)
Epoch: [94][20/391]	Time 0.011 (0.014)	Data 0.002 (0.010)	Loss 2.3712 (2.2937)	Acc@1 57.812 (58.036)	Acc@5 86.719 (85.640)
Epoch: [94][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.0998 (2.3052)	Acc@1 61.719 (57.485)	Acc@5 90.625 (85.711)
Epoch: [94][40/391]	Time 0.012 (0.012)	Data 0.001 (0.006)	Loss 2.4340 (2.3150)	Acc@1 48.438 (57.088)	Acc@5 87.500 (85.690)
Epoch: [94][50/391]	Time 0.013 (0.012)	Data 0.001 (0.005)	Loss 2.4833 (2.3276)	Acc@1 53.125 (56.863)	Acc@5 82.031 (85.493)
Epoch: [94][60/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 2.5315 (2.3405)	Acc@1 53.125 (56.481)	Acc@5 85.938 (85.387)
Epoch: [94][70/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 2.5571 (2.3476)	Acc@1 51.562 (56.250)	Acc@5 82.031 (85.343)
Epoch: [94][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.6424 (2.3486)	Acc@1 53.125 (56.404)	Acc@5 82.031 (85.311)
Epoch: [94][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2181 (2.3520)	Acc@1 60.156 (56.413)	Acc@5 85.938 (85.302)
Epoch: [94][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3695 (2.3556)	Acc@1 54.688 (56.389)	Acc@5 89.062 (85.102)
Epoch: [94][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2253 (2.3498)	Acc@1 60.156 (56.595)	Acc@5 84.375 (85.213)
Epoch: [94][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4142 (2.3504)	Acc@1 50.781 (56.521)	Acc@5 84.375 (85.298)
Epoch: [94][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3654 (2.3546)	Acc@1 55.469 (56.351)	Acc@5 86.719 (85.293)
Epoch: [94][140/391]	Time 0.016 (0.011)	Data 0.001 (0.003)	Loss 2.5091 (2.3635)	Acc@1 53.125 (56.134)	Acc@5 81.250 (85.162)
Epoch: [94][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4719 (2.3662)	Acc@1 54.688 (56.090)	Acc@5 83.594 (85.084)
Epoch: [94][160/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 2.1576 (2.3720)	Acc@1 64.844 (55.978)	Acc@5 85.938 (84.967)
Epoch: [94][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5817 (2.3731)	Acc@1 48.438 (55.967)	Acc@5 82.031 (84.964)
Epoch: [94][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4057 (2.3784)	Acc@1 55.469 (55.818)	Acc@5 86.719 (84.936)
Epoch: [94][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3677 (2.3841)	Acc@1 53.125 (55.632)	Acc@5 89.062 (84.899)
Epoch: [94][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3005 (2.3822)	Acc@1 58.594 (55.663)	Acc@5 85.938 (84.989)
Epoch: [94][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5392 (2.3857)	Acc@1 50.000 (55.565)	Acc@5 79.688 (84.964)
Epoch: [94][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4511 (2.3873)	Acc@1 52.344 (55.522)	Acc@5 82.031 (84.979)
Epoch: [94][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2736 (2.3877)	Acc@1 63.281 (55.621)	Acc@5 81.250 (84.916)
Epoch: [94][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2655 (2.3904)	Acc@1 57.812 (55.550)	Acc@5 87.500 (84.913)
Epoch: [94][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6077 (2.3965)	Acc@1 46.094 (55.469)	Acc@5 82.812 (84.789)
Epoch: [94][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2754 (2.3988)	Acc@1 60.938 (55.463)	Acc@5 86.719 (84.746)
Epoch: [94][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2616 (2.3979)	Acc@1 60.156 (55.492)	Acc@5 85.938 (84.747)
Epoch: [94][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6224 (2.4016)	Acc@1 53.906 (55.416)	Acc@5 78.906 (84.661)
Epoch: [94][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3666 (2.4023)	Acc@1 60.156 (55.372)	Acc@5 85.156 (84.657)
Epoch: [94][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4617 (2.4033)	Acc@1 55.469 (55.378)	Acc@5 86.719 (84.616)
Epoch: [94][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6004 (2.4052)	Acc@1 49.219 (55.305)	Acc@5 82.031 (84.591)
Epoch: [94][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4316 (2.4053)	Acc@1 55.469 (55.323)	Acc@5 84.375 (84.594)
Epoch: [94][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4645 (2.4093)	Acc@1 57.031 (55.266)	Acc@5 82.031 (84.517)
Epoch: [94][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5805 (2.4101)	Acc@1 52.344 (55.244)	Acc@5 81.250 (84.508)
Epoch: [94][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3650 (2.4105)	Acc@1 60.156 (55.242)	Acc@5 84.375 (84.497)
Epoch: [94][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2774 (2.4107)	Acc@1 56.250 (55.239)	Acc@5 85.938 (84.494)
Epoch: [94][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3680 (2.4106)	Acc@1 57.812 (55.231)	Acc@5 87.500 (84.495)
Epoch: [94][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8304 (2.4106)	Acc@1 41.406 (55.210)	Acc@5 78.125 (84.521)
Epoch: [94][390/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4479 (2.4119)	Acc@1 56.250 (55.154)	Acc@5 83.750 (84.504)
num momentum params: 26
[0.1, 2.4119331883239745, 2.274406487941742, 55.154, 43.46, tensor(0.3301, device='cuda:0', grad_fn=<DivBackward0>), 4.354483366012573, 0.3548593521118164]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [95 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [95][0/391]	Time 0.053 (0.053)	Data 0.173 (0.173)	Loss 2.5371 (2.5371)	Acc@1 54.688 (54.688)	Acc@5 80.469 (80.469)
Epoch: [95][10/391]	Time 0.013 (0.016)	Data 0.001 (0.017)	Loss 2.3337 (2.4778)	Acc@1 57.812 (53.267)	Acc@5 84.375 (82.670)
Epoch: [95][20/391]	Time 0.010 (0.013)	Data 0.001 (0.010)	Loss 2.3380 (2.3931)	Acc@1 55.469 (56.213)	Acc@5 86.719 (84.524)
Epoch: [95][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.3175 (2.3632)	Acc@1 61.719 (56.981)	Acc@5 84.375 (85.055)
Epoch: [95][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.5117 (2.3367)	Acc@1 51.562 (57.546)	Acc@5 85.156 (85.652)
Epoch: [95][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5597 (2.3359)	Acc@1 50.000 (57.552)	Acc@5 81.250 (85.555)
Epoch: [95][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3436 (2.3262)	Acc@1 56.250 (57.697)	Acc@5 86.719 (85.579)
Epoch: [95][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.5307 (2.3347)	Acc@1 50.781 (57.625)	Acc@5 84.375 (85.442)
Epoch: [95][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5103 (2.3439)	Acc@1 52.344 (57.253)	Acc@5 84.375 (85.214)
Epoch: [95][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3584 (2.3446)	Acc@1 57.031 (57.272)	Acc@5 86.719 (85.388)
Epoch: [95][100/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2304 (2.3482)	Acc@1 62.500 (57.232)	Acc@5 87.500 (85.334)
Epoch: [95][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5914 (2.3496)	Acc@1 49.219 (57.158)	Acc@5 81.250 (85.304)
Epoch: [95][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3885 (2.3553)	Acc@1 50.000 (56.947)	Acc@5 85.938 (85.318)
Epoch: [95][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1796 (2.3602)	Acc@1 56.250 (56.727)	Acc@5 91.406 (85.305)
Epoch: [95][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6097 (2.3656)	Acc@1 52.344 (56.538)	Acc@5 79.688 (85.189)
Epoch: [95][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4794 (2.3729)	Acc@1 47.656 (56.317)	Acc@5 84.375 (85.068)
Epoch: [95][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3405 (2.3782)	Acc@1 55.469 (56.172)	Acc@5 84.375 (85.006)
Epoch: [95][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2915 (2.3768)	Acc@1 59.375 (56.277)	Acc@5 84.375 (85.015)
Epoch: [95][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2511 (2.3746)	Acc@1 61.719 (56.418)	Acc@5 84.375 (85.001)
Epoch: [95][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0748 (2.3730)	Acc@1 62.500 (56.365)	Acc@5 90.625 (85.038)
Epoch: [95][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4422 (2.3800)	Acc@1 50.000 (56.126)	Acc@5 86.719 (84.962)
Epoch: [95][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3323 (2.3837)	Acc@1 54.688 (56.143)	Acc@5 86.719 (84.886)
Epoch: [95][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3560 (2.3829)	Acc@1 53.906 (56.137)	Acc@5 88.281 (84.909)
Epoch: [95][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1484 (2.3830)	Acc@1 60.938 (56.135)	Acc@5 86.719 (84.886)
Epoch: [95][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5335 (2.3835)	Acc@1 56.250 (56.169)	Acc@5 84.375 (84.877)
Epoch: [95][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5086 (2.3864)	Acc@1 48.438 (56.060)	Acc@5 82.031 (84.826)
Epoch: [95][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4716 (2.3886)	Acc@1 49.219 (55.960)	Acc@5 85.938 (84.806)
Epoch: [95][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2239 (2.3880)	Acc@1 57.812 (55.973)	Acc@5 87.500 (84.836)
Epoch: [95][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5471 (2.3905)	Acc@1 53.125 (55.922)	Acc@5 84.375 (84.767)
Epoch: [95][290/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 2.3908 (2.3910)	Acc@1 55.469 (55.949)	Acc@5 85.156 (84.729)
Epoch: [95][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4312 (2.3922)	Acc@1 58.594 (55.933)	Acc@5 83.594 (84.710)
Epoch: [95][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4899 (2.3961)	Acc@1 54.688 (55.841)	Acc@5 81.250 (84.644)
Epoch: [95][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7408 (2.4006)	Acc@1 50.781 (55.817)	Acc@5 76.562 (84.555)
Epoch: [95][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3351 (2.4046)	Acc@1 60.156 (55.733)	Acc@5 82.812 (84.502)
Epoch: [95][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5209 (2.4086)	Acc@1 50.781 (55.631)	Acc@5 82.812 (84.451)
Epoch: [95][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2783 (2.4094)	Acc@1 62.500 (55.609)	Acc@5 85.938 (84.451)
Epoch: [95][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3932 (2.4105)	Acc@1 52.344 (55.557)	Acc@5 83.594 (84.423)
Epoch: [95][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4583 (2.4130)	Acc@1 56.250 (55.462)	Acc@5 85.938 (84.419)
Epoch: [95][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4212 (2.4145)	Acc@1 54.688 (55.422)	Acc@5 83.594 (84.371)
Epoch: [95][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4888 (2.4150)	Acc@1 55.000 (55.400)	Acc@5 83.750 (84.388)
num momentum params: 26
[0.1, 2.4149557052612303, 1.971024923324585, 55.4, 48.76, tensor(0.3294, device='cuda:0', grad_fn=<DivBackward0>), 4.327451944351196, 0.33710432052612305]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [96 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [96][0/391]	Time 0.053 (0.053)	Data 0.176 (0.176)	Loss 2.3916 (2.3916)	Acc@1 55.469 (55.469)	Acc@5 82.031 (82.031)
Epoch: [96][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.1425 (2.3250)	Acc@1 61.719 (58.026)	Acc@5 88.281 (85.582)
Epoch: [96][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.2787 (2.2538)	Acc@1 60.156 (59.226)	Acc@5 87.500 (86.905)
Epoch: [96][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.4080 (2.2768)	Acc@1 58.594 (58.795)	Acc@5 84.375 (86.215)
Epoch: [96][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.4263 (2.2715)	Acc@1 53.125 (58.594)	Acc@5 85.938 (86.433)
Epoch: [96][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4503 (2.2972)	Acc@1 52.344 (57.935)	Acc@5 81.250 (86.029)
Epoch: [96][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.1100 (2.3004)	Acc@1 66.406 (57.979)	Acc@5 92.188 (86.014)
Epoch: [96][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3172 (2.2973)	Acc@1 55.469 (58.121)	Acc@5 85.938 (86.015)
Epoch: [96][80/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.3525 (2.3069)	Acc@1 53.125 (57.822)	Acc@5 85.938 (85.851)
Epoch: [96][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4601 (2.3169)	Acc@1 52.344 (57.615)	Acc@5 83.594 (85.680)
Epoch: [96][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4294 (2.3206)	Acc@1 58.594 (57.704)	Acc@5 85.938 (85.628)
Epoch: [96][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4924 (2.3320)	Acc@1 51.562 (57.355)	Acc@5 84.375 (85.501)
Epoch: [96][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3547 (2.3372)	Acc@1 56.250 (57.173)	Acc@5 86.719 (85.408)
Epoch: [96][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4581 (2.3381)	Acc@1 58.594 (57.258)	Acc@5 81.250 (85.472)
Epoch: [96][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3439 (2.3441)	Acc@1 60.156 (57.181)	Acc@5 82.812 (85.411)
Epoch: [96][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5678 (2.3483)	Acc@1 51.562 (56.959)	Acc@5 83.594 (85.405)
Epoch: [96][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3363 (2.3479)	Acc@1 60.156 (56.949)	Acc@5 81.250 (85.360)
Epoch: [96][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4967 (2.3482)	Acc@1 53.906 (56.944)	Acc@5 84.375 (85.408)
Epoch: [96][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3668 (2.3518)	Acc@1 59.375 (56.954)	Acc@5 86.719 (85.376)
Epoch: [96][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2291 (2.3536)	Acc@1 64.062 (56.974)	Acc@5 85.938 (85.295)
Epoch: [96][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3371 (2.3534)	Acc@1 52.344 (56.891)	Acc@5 83.594 (85.242)
Epoch: [96][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3356 (2.3588)	Acc@1 59.375 (56.783)	Acc@5 88.281 (85.186)
Epoch: [96][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5861 (2.3663)	Acc@1 53.906 (56.604)	Acc@5 82.812 (85.043)
Epoch: [96][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4361 (2.3652)	Acc@1 53.125 (56.578)	Acc@5 84.375 (85.072)
Epoch: [96][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4941 (2.3688)	Acc@1 50.000 (56.454)	Acc@5 85.938 (85.023)
Epoch: [96][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2498 (2.3724)	Acc@1 61.719 (56.375)	Acc@5 89.062 (84.948)
Epoch: [96][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3367 (2.3739)	Acc@1 60.938 (56.382)	Acc@5 83.594 (84.923)
Epoch: [96][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4520 (2.3741)	Acc@1 55.469 (56.362)	Acc@5 84.375 (84.937)
Epoch: [96][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2319 (2.3752)	Acc@1 64.062 (56.386)	Acc@5 88.281 (84.920)
Epoch: [96][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2369 (2.3747)	Acc@1 58.594 (56.406)	Acc@5 88.281 (84.952)
Epoch: [96][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6508 (2.3777)	Acc@1 53.125 (56.377)	Acc@5 82.812 (84.917)
Epoch: [96][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4217 (2.3796)	Acc@1 53.125 (56.278)	Acc@5 85.156 (84.895)
Epoch: [96][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2371 (2.3812)	Acc@1 62.500 (56.260)	Acc@5 84.375 (84.837)
Epoch: [96][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2254 (2.3832)	Acc@1 64.062 (56.224)	Acc@5 91.406 (84.786)
Epoch: [96][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4694 (2.3840)	Acc@1 50.000 (56.193)	Acc@5 83.594 (84.771)
Epoch: [96][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3504 (2.3850)	Acc@1 59.375 (56.159)	Acc@5 82.812 (84.753)
Epoch: [96][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7288 (2.3877)	Acc@1 51.562 (56.092)	Acc@5 75.781 (84.710)
Epoch: [96][370/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3634 (2.3903)	Acc@1 60.156 (56.052)	Acc@5 83.594 (84.670)
Epoch: [96][380/391]	Time 0.014 (0.011)	Data 0.002 (0.002)	Loss 2.2559 (2.3919)	Acc@1 58.594 (56.004)	Acc@5 90.625 (84.650)
Epoch: [96][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.8957 (2.3930)	Acc@1 75.000 (55.976)	Acc@5 90.000 (84.638)
num momentum params: 26
[0.1, 2.392994507522583, 2.1601618826389313, 55.976, 46.55, tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>), 4.333090782165527, 0.3501241207122803]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [97 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [97][0/391]	Time 0.052 (0.052)	Data 0.173 (0.173)	Loss 2.1114 (2.1114)	Acc@1 64.844 (64.844)	Acc@5 90.625 (90.625)
Epoch: [97][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.5818 (2.3759)	Acc@1 46.875 (55.114)	Acc@5 86.719 (86.080)
Epoch: [97][20/391]	Time 0.010 (0.013)	Data 0.001 (0.009)	Loss 2.2606 (2.3429)	Acc@1 60.156 (56.176)	Acc@5 86.719 (86.458)
Epoch: [97][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.2202 (2.3153)	Acc@1 56.250 (56.779)	Acc@5 89.062 (86.492)
Epoch: [97][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.4103 (2.3235)	Acc@1 56.250 (57.031)	Acc@5 85.156 (86.300)
Epoch: [97][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.3715 (2.3259)	Acc@1 54.688 (56.878)	Acc@5 89.062 (86.244)
Epoch: [97][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3765 (2.3332)	Acc@1 55.469 (57.006)	Acc@5 82.812 (85.899)
Epoch: [97][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.1049 (2.3444)	Acc@1 64.062 (56.932)	Acc@5 89.844 (85.574)
Epoch: [97][80/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 2.1494 (2.3528)	Acc@1 61.719 (56.780)	Acc@5 85.938 (85.465)
Epoch: [97][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2236 (2.3505)	Acc@1 62.500 (56.851)	Acc@5 86.719 (85.517)
Epoch: [97][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3858 (2.3530)	Acc@1 57.812 (56.838)	Acc@5 81.250 (85.419)
Epoch: [97][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3089 (2.3585)	Acc@1 57.812 (56.672)	Acc@5 85.156 (85.325)
Epoch: [97][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3509 (2.3616)	Acc@1 60.156 (56.579)	Acc@5 87.500 (85.311)
Epoch: [97][130/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6251 (2.3676)	Acc@1 48.438 (56.411)	Acc@5 79.688 (85.109)
Epoch: [97][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3582 (2.3684)	Acc@1 55.469 (56.377)	Acc@5 83.594 (85.095)
Epoch: [97][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.6141 (2.3751)	Acc@1 49.219 (56.255)	Acc@5 85.156 (84.965)
Epoch: [97][160/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5886 (2.3808)	Acc@1 55.469 (56.231)	Acc@5 82.031 (84.880)
Epoch: [97][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5292 (2.3835)	Acc@1 49.219 (56.223)	Acc@5 80.469 (84.727)
Epoch: [97][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3281 (2.3845)	Acc@1 57.031 (56.142)	Acc@5 89.844 (84.759)
Epoch: [97][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3015 (2.3852)	Acc@1 59.375 (56.140)	Acc@5 86.719 (84.706)
Epoch: [97][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2107 (2.3840)	Acc@1 63.281 (56.203)	Acc@5 85.156 (84.717)
Epoch: [97][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3746 (2.3862)	Acc@1 55.469 (56.180)	Acc@5 85.938 (84.623)
Epoch: [97][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4393 (2.3890)	Acc@1 55.469 (56.094)	Acc@5 84.375 (84.605)
Epoch: [97][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1837 (2.3886)	Acc@1 63.281 (56.074)	Acc@5 86.719 (84.622)
Epoch: [97][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5050 (2.3905)	Acc@1 53.125 (55.997)	Acc@5 81.250 (84.592)
Epoch: [97][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1704 (2.3914)	Acc@1 59.375 (55.933)	Acc@5 85.156 (84.587)
Epoch: [97][260/391]	Time 0.013 (0.011)	Data 0.003 (0.002)	Loss 2.3710 (2.3893)	Acc@1 56.250 (55.963)	Acc@5 83.594 (84.591)
Epoch: [97][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3608 (2.3897)	Acc@1 58.594 (55.953)	Acc@5 80.469 (84.588)
Epoch: [97][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4325 (2.3901)	Acc@1 51.562 (55.897)	Acc@5 83.594 (84.564)
Epoch: [97][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1772 (2.3885)	Acc@1 60.156 (55.928)	Acc@5 89.844 (84.592)
Epoch: [97][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2488 (2.3851)	Acc@1 62.500 (56.009)	Acc@5 85.156 (84.640)
Epoch: [97][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3538 (2.3864)	Acc@1 58.594 (55.981)	Acc@5 85.156 (84.631)
Epoch: [97][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7008 (2.3871)	Acc@1 46.094 (55.977)	Acc@5 82.031 (84.614)
Epoch: [97][330/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2347 (2.3894)	Acc@1 56.250 (55.884)	Acc@5 87.500 (84.559)
Epoch: [97][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4926 (2.3916)	Acc@1 56.250 (55.840)	Acc@5 82.812 (84.499)
Epoch: [97][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4462 (2.3956)	Acc@1 53.125 (55.729)	Acc@5 84.375 (84.480)
Epoch: [97][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6339 (2.3961)	Acc@1 51.562 (55.728)	Acc@5 78.906 (84.438)
Epoch: [97][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4953 (2.3966)	Acc@1 54.688 (55.757)	Acc@5 81.250 (84.407)
Epoch: [97][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2059 (2.3982)	Acc@1 64.062 (55.717)	Acc@5 85.938 (84.410)
Epoch: [97][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.3753 (2.3990)	Acc@1 55.000 (55.706)	Acc@5 85.000 (84.374)
num momentum params: 26
[0.1, 2.3989601418304445, 1.9968307256698608, 55.706, 48.84, tensor(0.3309, device='cuda:0', grad_fn=<DivBackward0>), 4.34240984916687, 0.35652709007263184]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [98 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [98][0/391]	Time 0.057 (0.057)	Data 0.168 (0.168)	Loss 2.0493 (2.0493)	Acc@1 64.062 (64.062)	Acc@5 89.844 (89.844)
Epoch: [98][10/391]	Time 0.011 (0.017)	Data 0.001 (0.016)	Loss 2.2053 (2.2381)	Acc@1 59.375 (60.227)	Acc@5 86.719 (87.642)
Epoch: [98][20/391]	Time 0.010 (0.014)	Data 0.001 (0.009)	Loss 2.6217 (2.2944)	Acc@1 55.469 (58.705)	Acc@5 83.594 (86.756)
Epoch: [98][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.5311 (2.3248)	Acc@1 53.125 (58.392)	Acc@5 82.812 (86.013)
Epoch: [98][40/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.2637 (2.3407)	Acc@1 59.375 (57.622)	Acc@5 86.719 (85.842)
Epoch: [98][50/391]	Time 0.010 (0.013)	Data 0.001 (0.005)	Loss 2.3255 (2.3371)	Acc@1 56.250 (57.506)	Acc@5 82.031 (85.846)
Epoch: [98][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2918 (2.3522)	Acc@1 60.156 (57.287)	Acc@5 85.938 (85.400)
Epoch: [98][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5139 (2.3632)	Acc@1 52.344 (56.800)	Acc@5 83.594 (85.453)
Epoch: [98][80/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.1587 (2.3555)	Acc@1 61.719 (56.993)	Acc@5 87.500 (85.494)
Epoch: [98][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3125 (2.3564)	Acc@1 56.250 (56.988)	Acc@5 85.938 (85.311)
Epoch: [98][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.6500 (2.3596)	Acc@1 57.812 (57.085)	Acc@5 76.562 (85.218)
Epoch: [98][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3061 (2.3584)	Acc@1 51.562 (56.954)	Acc@5 89.062 (85.325)
Epoch: [98][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3759 (2.3615)	Acc@1 54.688 (56.818)	Acc@5 87.500 (85.279)
Epoch: [98][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4596 (2.3617)	Acc@1 51.562 (56.590)	Acc@5 82.812 (85.341)
Epoch: [98][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.3628 (2.3646)	Acc@1 54.688 (56.400)	Acc@5 87.500 (85.411)
Epoch: [98][150/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5129 (2.3697)	Acc@1 53.906 (56.286)	Acc@5 82.812 (85.270)
Epoch: [98][160/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4527 (2.3728)	Acc@1 50.000 (56.114)	Acc@5 82.812 (85.263)
Epoch: [98][170/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3039 (2.3764)	Acc@1 60.156 (56.058)	Acc@5 85.938 (85.220)
Epoch: [98][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5140 (2.3801)	Acc@1 52.344 (55.879)	Acc@5 84.375 (85.143)
Epoch: [98][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1440 (2.3758)	Acc@1 59.375 (55.939)	Acc@5 88.281 (85.140)
Epoch: [98][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6707 (2.3766)	Acc@1 49.219 (55.958)	Acc@5 76.562 (85.059)
Epoch: [98][210/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2754 (2.3767)	Acc@1 53.906 (55.998)	Acc@5 89.062 (85.101)
Epoch: [98][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3724 (2.3789)	Acc@1 56.250 (56.027)	Acc@5 85.156 (85.025)
Epoch: [98][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2748 (2.3845)	Acc@1 57.812 (55.892)	Acc@5 86.719 (84.984)
Epoch: [98][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6281 (2.3880)	Acc@1 52.344 (55.822)	Acc@5 79.688 (84.916)
Epoch: [98][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6090 (2.3916)	Acc@1 45.312 (55.727)	Acc@5 80.469 (84.895)
Epoch: [98][260/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.5247 (2.3925)	Acc@1 55.469 (55.687)	Acc@5 78.125 (84.887)
Epoch: [98][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4895 (2.3961)	Acc@1 55.469 (55.645)	Acc@5 82.031 (84.805)
Epoch: [98][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5921 (2.3971)	Acc@1 56.250 (55.658)	Acc@5 78.906 (84.761)
Epoch: [98][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2411 (2.3994)	Acc@1 59.375 (55.681)	Acc@5 87.500 (84.748)
Epoch: [98][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5288 (2.4019)	Acc@1 55.469 (55.650)	Acc@5 81.250 (84.686)
Epoch: [98][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3344 (2.4042)	Acc@1 53.906 (55.562)	Acc@5 89.062 (84.664)
Epoch: [98][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4372 (2.4073)	Acc@1 50.000 (55.517)	Acc@5 85.938 (84.599)
Epoch: [98][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3114 (2.4077)	Acc@1 57.812 (55.549)	Acc@5 84.375 (84.602)
Epoch: [98][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1813 (2.4078)	Acc@1 64.062 (55.558)	Acc@5 85.156 (84.577)
Epoch: [98][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3730 (2.4110)	Acc@1 54.688 (55.471)	Acc@5 85.156 (84.524)
Epoch: [98][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5995 (2.4112)	Acc@1 48.438 (55.423)	Acc@5 81.250 (84.531)
Epoch: [98][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3865 (2.4115)	Acc@1 60.938 (55.422)	Acc@5 85.938 (84.535)
Epoch: [98][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3642 (2.4113)	Acc@1 57.812 (55.401)	Acc@5 84.375 (84.529)
Epoch: [98][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.4206 (2.4114)	Acc@1 57.500 (55.406)	Acc@5 85.000 (84.530)
num momentum params: 26
[0.1, 2.4113639807128906, 2.00309668302536, 55.406, 47.31, tensor(0.3301, device='cuda:0', grad_fn=<DivBackward0>), 4.339747905731201, 0.3471205234527588]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [99 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [99][0/391]	Time 0.055 (0.055)	Data 0.169 (0.169)	Loss 2.2899 (2.2899)	Acc@1 60.938 (60.938)	Acc@5 85.156 (85.156)
Epoch: [99][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.3695 (2.3500)	Acc@1 58.594 (56.818)	Acc@5 85.156 (85.298)
Epoch: [99][20/391]	Time 0.011 (0.014)	Data 0.001 (0.009)	Loss 2.0670 (2.2933)	Acc@1 65.625 (58.036)	Acc@5 88.281 (86.570)
Epoch: [99][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.5636 (2.2983)	Acc@1 53.906 (57.712)	Acc@5 80.469 (86.820)
Epoch: [99][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.2423 (2.3154)	Acc@1 56.250 (57.298)	Acc@5 86.719 (86.242)
Epoch: [99][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.6719 (2.3312)	Acc@1 43.750 (56.863)	Acc@5 81.250 (86.091)
Epoch: [99][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3699 (2.3390)	Acc@1 55.469 (56.609)	Acc@5 84.375 (85.848)
Epoch: [99][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4234 (2.3388)	Acc@1 54.688 (56.591)	Acc@5 86.719 (85.882)
Epoch: [99][80/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2911 (2.3393)	Acc@1 53.125 (56.539)	Acc@5 86.719 (85.802)
Epoch: [99][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5897 (2.3404)	Acc@1 53.906 (56.628)	Acc@5 82.031 (85.817)
Epoch: [99][100/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 2.4789 (2.3501)	Acc@1 52.344 (56.443)	Acc@5 84.375 (85.644)
Epoch: [99][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2734 (2.3527)	Acc@1 60.156 (56.454)	Acc@5 82.812 (85.515)
Epoch: [99][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5534 (2.3591)	Acc@1 52.344 (56.269)	Acc@5 82.812 (85.447)
Epoch: [99][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4429 (2.3613)	Acc@1 55.469 (56.167)	Acc@5 83.594 (85.448)
Epoch: [99][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4961 (2.3614)	Acc@1 52.344 (56.172)	Acc@5 85.156 (85.444)
Epoch: [99][150/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4150 (2.3638)	Acc@1 52.344 (56.079)	Acc@5 85.938 (85.430)
Epoch: [99][160/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.3664 (2.3688)	Acc@1 54.688 (55.983)	Acc@5 87.500 (85.399)
Epoch: [99][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6273 (2.3755)	Acc@1 50.781 (55.821)	Acc@5 79.688 (85.284)
Epoch: [99][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3480 (2.3779)	Acc@1 58.594 (55.840)	Acc@5 83.594 (85.268)
Epoch: [99][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6557 (2.3809)	Acc@1 49.219 (55.747)	Acc@5 82.031 (85.205)
Epoch: [99][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2766 (2.3825)	Acc@1 60.156 (55.718)	Acc@5 84.375 (85.168)
Epoch: [99][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5189 (2.3852)	Acc@1 51.562 (55.669)	Acc@5 84.375 (85.086)
Epoch: [99][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4975 (2.3839)	Acc@1 58.594 (55.755)	Acc@5 82.031 (85.082)
Epoch: [99][230/391]	Time 0.020 (0.011)	Data 0.001 (0.002)	Loss 2.4641 (2.3864)	Acc@1 50.000 (55.695)	Acc@5 85.938 (85.058)
Epoch: [99][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5951 (2.3888)	Acc@1 52.344 (55.644)	Acc@5 81.250 (85.023)
Epoch: [99][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3800 (2.3910)	Acc@1 53.906 (55.593)	Acc@5 85.938 (84.998)
Epoch: [99][260/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3793 (2.3884)	Acc@1 54.688 (55.678)	Acc@5 83.594 (85.048)
Epoch: [99][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6557 (2.3891)	Acc@1 46.875 (55.659)	Acc@5 79.688 (85.027)
Epoch: [99][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4111 (2.3917)	Acc@1 59.375 (55.602)	Acc@5 85.156 (85.028)
Epoch: [99][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6142 (2.3932)	Acc@1 48.438 (55.552)	Acc@5 79.688 (85.009)
Epoch: [99][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3576 (2.3977)	Acc@1 58.594 (55.482)	Acc@5 82.031 (84.899)
Epoch: [99][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4791 (2.3978)	Acc@1 47.656 (55.484)	Acc@5 88.281 (84.895)
Epoch: [99][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4052 (2.3999)	Acc@1 55.469 (55.408)	Acc@5 85.938 (84.842)
Epoch: [99][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3338 (2.4013)	Acc@1 54.688 (55.372)	Acc@5 84.375 (84.821)
Epoch: [99][340/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 2.4593 (2.4025)	Acc@1 53.125 (55.363)	Acc@5 82.812 (84.767)
Epoch: [99][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2243 (2.4036)	Acc@1 61.719 (55.333)	Acc@5 90.625 (84.753)
Epoch: [99][360/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.7992 (2.4041)	Acc@1 49.219 (55.343)	Acc@5 75.000 (84.743)
Epoch: [99][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3258 (2.4029)	Acc@1 58.594 (55.387)	Acc@5 84.375 (84.762)
Epoch: [99][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5196 (2.4042)	Acc@1 56.250 (55.407)	Acc@5 83.594 (84.742)
Epoch: [99][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5362 (2.4047)	Acc@1 52.500 (55.418)	Acc@5 81.250 (84.712)
num momentum params: 26
[0.1, 2.404729994659424, 2.068120919466019, 55.418, 47.52, tensor(0.3311, device='cuda:0', grad_fn=<DivBackward0>), 4.338575124740601, 0.35195231437683105]
Non Pruning Epoch - module.conv1.weight: [34, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [34]
Non Pruning Epoch - module.bn1.bias: [34]
Non Pruning Epoch - module.conv2.weight: [110, 34, 3, 3]
Non Pruning Epoch - module.bn2.weight: [110]
Non Pruning Epoch - module.bn2.bias: [110]
Non Pruning Epoch - module.conv3.weight: [216, 110, 3, 3]
Non Pruning Epoch - module.bn3.weight: [216]
Non Pruning Epoch - module.bn3.bias: [216]
Non Pruning Epoch - module.conv4.weight: [247, 216, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [366, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [366]
Non Pruning Epoch - module.bn5.bias: [366]
Non Pruning Epoch - module.conv6.weight: [322, 366, 3, 3]
Non Pruning Epoch - module.bn6.weight: [322]
Non Pruning Epoch - module.bn6.bias: [322]
Non Pruning Epoch - module.conv7.weight: [208, 322, 3, 3]
Non Pruning Epoch - module.bn7.weight: [208]
Non Pruning Epoch - module.bn7.bias: [208]
Non Pruning Epoch - module.conv8.weight: [200, 208, 3, 3]
Non Pruning Epoch - module.bn8.weight: [200]
Non Pruning Epoch - module.bn8.bias: [200]
Non Pruning Epoch - module.fc.weight: [100, 200]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [100 | 180] LR: 0.100000
module.conv1.weight [34, 3, 3, 3]
module.conv2.weight [110, 34, 3, 3]
module.conv3.weight [216, 110, 3, 3]
module.conv4.weight [247, 216, 3, 3]
module.conv5.weight [366, 247, 3, 3]
module.conv6.weight [322, 366, 3, 3]
module.conv7.weight [208, 322, 3, 3]
module.conv8.weight [200, 208, 3, 3]
Epoch: [100][0/391]	Time 0.054 (0.054)	Data 0.167 (0.167)	Loss 2.4761 (2.4761)	Acc@1 51.562 (51.562)	Acc@5 83.594 (83.594)
Epoch: [100][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.2852 (2.2916)	Acc@1 56.250 (57.031)	Acc@5 88.281 (87.003)
Epoch: [100][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.4224 (2.3279)	Acc@1 56.250 (56.808)	Acc@5 85.156 (86.719)
Epoch: [100][30/391]	Time 0.013 (0.013)	Data 0.001 (0.007)	Loss 2.1522 (2.3033)	Acc@1 58.594 (57.182)	Acc@5 91.406 (86.769)
Epoch: [100][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.5918 (2.3074)	Acc@1 49.219 (56.936)	Acc@5 80.469 (86.928)
Epoch: [100][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.0703 (2.3087)	Acc@1 65.625 (56.710)	Acc@5 88.281 (86.918)
Epoch: [100][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.6494 (2.3164)	Acc@1 47.656 (56.429)	Acc@5 77.344 (86.693)
Epoch: [100][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2519 (2.3221)	Acc@1 66.406 (56.536)	Acc@5 85.156 (86.389)
Epoch: [100][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3645 (2.3382)	Acc@1 52.344 (56.231)	Acc@5 86.719 (86.130)
Epoch: [100][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4686 (2.3493)	Acc@1 51.562 (56.190)	Acc@5 80.469 (85.792)
Epoch: [100][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4729 (2.3620)	Acc@1 55.469 (55.979)	Acc@5 84.375 (85.566)
Epoch: [100][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4015 (2.3699)	Acc@1 58.594 (55.912)	Acc@5 85.156 (85.396)
Epoch: [100][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3115 (2.3681)	Acc@1 61.719 (56.108)	Acc@5 85.156 (85.440)
Epoch: [100][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2169 (2.3635)	Acc@1 60.938 (56.220)	Acc@5 88.281 (85.431)
Epoch: [100][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5766 (2.3652)	Acc@1 52.344 (56.161)	Acc@5 81.250 (85.400)
Epoch: [100][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3499 (2.3716)	Acc@1 55.469 (56.074)	Acc@5 83.594 (85.260)
Epoch: [100][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3995 (2.3727)	Acc@1 49.219 (56.061)	Acc@5 87.500 (85.214)
Epoch: [100][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3138 (2.3750)	Acc@1 59.375 (56.035)	Acc@5 83.594 (85.170)
Epoch: [100][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4068 (2.3794)	Acc@1 55.469 (55.956)	Acc@5 81.250 (85.074)
Epoch: [100][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3807 (2.3842)	Acc@1 57.031 (55.919)	Acc@5 87.500 (84.976)
Epoch: [100][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5434 (2.3887)	Acc@1 51.562 (55.846)	Acc@5 87.500 (84.907)
Epoch: [100][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1742 (2.3891)	Acc@1 58.594 (55.754)	Acc@5 87.500 (84.945)
Epoch: [100][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6243 (2.3916)	Acc@1 46.875 (55.716)	Acc@5 79.688 (84.863)
Epoch: [100][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1487 (2.3962)	Acc@1 64.062 (55.668)	Acc@5 90.625 (84.757)
Epoch: [100][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7052 (2.3986)	Acc@1 50.000 (55.631)	Acc@5 81.250 (84.745)
Epoch: [100][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4327 (2.4033)	Acc@1 53.906 (55.494)	Acc@5 83.594 (84.686)
Epoch: [100][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2297 (2.4035)	Acc@1 62.500 (55.484)	Acc@5 85.938 (84.695)
Epoch: [100][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5506 (2.4050)	Acc@1 53.125 (55.451)	Acc@5 79.688 (84.666)
Epoch: [100][280/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.5239 (2.4044)	Acc@1 53.906 (55.435)	Acc@5 82.812 (84.664)
Epoch: [100][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4195 (2.4057)	Acc@1 57.031 (55.418)	Acc@5 84.375 (84.627)
Epoch: [100][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3202 (2.4062)	Acc@1 60.938 (55.409)	Acc@5 85.156 (84.663)
Epoch: [100][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4787 (2.4108)	Acc@1 54.688 (55.245)	Acc@5 82.812 (84.581)
Epoch: [100][320/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3781 (2.4144)	Acc@1 53.125 (55.160)	Acc@5 84.375 (84.531)
Epoch: [100][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3399 (2.4145)	Acc@1 54.688 (55.183)	Acc@5 88.281 (84.502)
Epoch: [100][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5766 (2.4152)	Acc@1 53.906 (55.203)	Acc@5 82.031 (84.496)
Epoch: [100][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3429 (2.4173)	Acc@1 57.812 (55.119)	Acc@5 85.156 (84.455)
Epoch: [100][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8322 (2.4173)	Acc@1 48.438 (55.155)	Acc@5 76.562 (84.444)
Epoch: [100][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5724 (2.4180)	Acc@1 49.219 (55.144)	Acc@5 81.250 (84.417)
Epoch: [100][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4647 (2.4184)	Acc@1 53.125 (55.143)	Acc@5 81.250 (84.400)
Epoch: [100][390/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 2.3175 (2.4201)	Acc@1 57.500 (55.136)	Acc@5 85.000 (84.390)
num momentum params: 26
[0.1, 2.4200823819732666, 2.1041768765449524, 55.136, 46.07, tensor(0.3293, device='cuda:0', grad_fn=<DivBackward0>), 4.3358283042907715, 0.3425750732421875]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [34, 3, 3, 3]
Before - module.bn1.weight: [34]
Before - module.bn1.bias: [34]
Before - module.conv2.weight: [110, 34, 3, 3]
Before - module.bn2.weight: [110]
Before - module.bn2.bias: [110]
Before - module.conv3.weight: [216, 110, 3, 3]
Before - module.bn3.weight: [216]
Before - module.bn3.bias: [216]
Before - module.conv4.weight: [247, 216, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [366, 247, 3, 3]
Before - module.bn5.weight: [366]
Before - module.bn5.bias: [366]
Before - module.conv6.weight: [322, 366, 3, 3]
Before - module.bn6.weight: [322]
Before - module.bn6.bias: [322]
Before - module.conv7.weight: [208, 322, 3, 3]
Before - module.bn7.weight: [208]
Before - module.bn7.bias: [208]
Before - module.conv8.weight: [200, 208, 3, 3]
Before - module.bn8.weight: [200]
Before - module.bn8.bias: [200]
Before - module.fc.weight: [100, 200]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [34, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 34 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [110, 34, 3, 3] >> [109, 32, 3, 3]
[module.bn2.weight]: 110 >> 109
running_mean [109]
running_var [109]
num_batches_tracked []
[module.conv3.weight]: [216, 110, 3, 3] >> [211, 109, 3, 3]
[module.bn3.weight]: 216 >> 211
running_mean [211]
running_var [211]
num_batches_tracked []
[module.conv4.weight]: [247, 216, 3, 3] >> [247, 211, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [366, 247, 3, 3] >> [361, 247, 3, 3]
[module.bn5.weight]: 366 >> 361
running_mean [361]
running_var [361]
num_batches_tracked []
[module.conv6.weight]: [322, 366, 3, 3] >> [321, 361, 3, 3]
[module.bn6.weight]: 322 >> 321
running_mean [321]
running_var [321]
num_batches_tracked []
[module.conv7.weight]: [208, 322, 3, 3] >> [207, 321, 3, 3]
[module.bn7.weight]: 208 >> 207
running_mean [207]
running_var [207]
num_batches_tracked []
[module.conv8.weight]: [200, 208, 3, 3] >> [198, 207, 3, 3]
[module.bn8.weight]: 200 >> 198
running_mean [198]
running_var [198]
num_batches_tracked []
[module.fc.weight]: [100, 200] >> [100, 198]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [109, 32, 3, 3]
After - module.bn2.weight: [109]
After - module.bn2.bias: [109]
After - module.conv3.weight: [211, 109, 3, 3]
After - module.bn3.weight: [211]
After - module.bn3.bias: [211]
After - module.conv4.weight: [247, 211, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [361, 247, 3, 3]
After - module.bn5.weight: [361]
After - module.bn5.bias: [361]
After - module.conv6.weight: [321, 361, 3, 3]
After - module.bn6.weight: [321]
After - module.bn6.bias: [321]
After - module.conv7.weight: [207, 321, 3, 3]
After - module.bn7.weight: [207]
After - module.bn7.bias: [207]
After - module.conv8.weight: [198, 207, 3, 3]
After - module.bn8.weight: [198]
After - module.bn8.bias: [198]
After - module.fc.weight: [100, 198]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [109, 32, 3, 3]
conv3 --> [211, 109, 3, 3]
conv4 --> [247, 211, 3, 3]
conv5 --> [361, 247, 3, 3]
conv6 --> [321, 361, 3, 3]
conv7 --> [207, 321, 3, 3]
conv8 --> [198, 207, 3, 3]
fc --> [198, 100]
1, 354336768, 884736, 32
2, 3359195136, 8036352, 109
3, 6040825344, 13247424, 211
4, 13688842752, 30019392, 247
5, 6984986112, 12840048, 361
6, 9077654016, 16686864, 321
7, 1837126656, 2392092, 207
8, 1133180928, 1475496, 198
fc, 7603200, 19800, 0
===================
FLOP REPORT: 16595215200000.0 41006400000.0 85602204 102516 1686 6.756425857543945
[INFO] Storing checkpoint...

Epoch: [101 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [101][0/391]	Time 0.251 (0.251)	Data 0.184 (0.184)	Loss 2.1814 (2.1814)	Acc@1 66.406 (66.406)	Acc@5 85.156 (85.156)
Epoch: [101][10/391]	Time 0.011 (0.033)	Data 0.001 (0.018)	Loss 2.1703 (2.2672)	Acc@1 57.812 (58.452)	Acc@5 87.500 (85.795)
Epoch: [101][20/391]	Time 0.010 (0.023)	Data 0.001 (0.010)	Loss 2.3892 (2.2918)	Acc@1 56.250 (57.738)	Acc@5 83.594 (85.789)
Epoch: [101][30/391]	Time 0.011 (0.019)	Data 0.001 (0.007)	Loss 2.2471 (2.3135)	Acc@1 57.812 (56.905)	Acc@5 87.500 (85.837)
Epoch: [101][40/391]	Time 0.012 (0.017)	Data 0.001 (0.006)	Loss 2.4905 (2.3264)	Acc@1 51.562 (56.764)	Acc@5 82.031 (85.595)
Epoch: [101][50/391]	Time 0.012 (0.016)	Data 0.001 (0.005)	Loss 2.2492 (2.3241)	Acc@1 59.375 (57.215)	Acc@5 87.500 (85.478)
Epoch: [101][60/391]	Time 0.010 (0.015)	Data 0.001 (0.004)	Loss 2.4918 (2.3273)	Acc@1 53.906 (56.916)	Acc@5 80.469 (85.617)
Epoch: [101][70/391]	Time 0.011 (0.014)	Data 0.001 (0.004)	Loss 2.2280 (2.3226)	Acc@1 61.719 (57.163)	Acc@5 90.625 (85.750)
Epoch: [101][80/391]	Time 0.011 (0.014)	Data 0.001 (0.004)	Loss 2.3550 (2.3311)	Acc@1 57.812 (56.993)	Acc@5 84.375 (85.658)
Epoch: [101][90/391]	Time 0.010 (0.014)	Data 0.001 (0.003)	Loss 2.6163 (2.3360)	Acc@1 50.000 (56.971)	Acc@5 80.469 (85.620)
Epoch: [101][100/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.2875 (2.3365)	Acc@1 59.375 (57.024)	Acc@5 87.500 (85.644)
Epoch: [101][110/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.2898 (2.3367)	Acc@1 57.031 (56.975)	Acc@5 91.406 (85.593)
Epoch: [101][120/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.5616 (2.3443)	Acc@1 49.219 (56.896)	Acc@5 82.812 (85.479)
Epoch: [101][130/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.5062 (2.3527)	Acc@1 50.781 (56.620)	Acc@5 78.906 (85.323)
Epoch: [101][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3675 (2.3517)	Acc@1 54.688 (56.577)	Acc@5 85.156 (85.411)
Epoch: [101][150/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.0937 (2.3491)	Acc@1 64.062 (56.638)	Acc@5 89.844 (85.472)
Epoch: [101][160/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2760 (2.3486)	Acc@1 59.375 (56.687)	Acc@5 90.625 (85.472)
Epoch: [101][170/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.6402 (2.3564)	Acc@1 48.438 (56.506)	Acc@5 84.375 (85.353)
Epoch: [101][180/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3527 (2.3643)	Acc@1 55.469 (56.379)	Acc@5 89.062 (85.307)
Epoch: [101][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4620 (2.3674)	Acc@1 54.688 (56.324)	Acc@5 86.719 (85.304)
Epoch: [101][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3046 (2.3715)	Acc@1 57.031 (56.203)	Acc@5 85.156 (85.242)
Epoch: [101][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4692 (2.3734)	Acc@1 48.438 (56.061)	Acc@5 82.812 (85.227)
Epoch: [101][220/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3850 (2.3731)	Acc@1 58.594 (56.147)	Acc@5 83.594 (85.241)
Epoch: [101][230/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4271 (2.3764)	Acc@1 53.125 (56.044)	Acc@5 86.719 (85.231)
Epoch: [101][240/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5532 (2.3801)	Acc@1 50.781 (55.978)	Acc@5 82.812 (85.124)
Epoch: [101][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.2272 (2.3816)	Acc@1 62.500 (55.982)	Acc@5 85.156 (85.100)
Epoch: [101][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4164 (2.3848)	Acc@1 55.469 (55.894)	Acc@5 83.594 (85.087)
Epoch: [101][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2193 (2.3907)	Acc@1 60.938 (55.734)	Acc@5 90.625 (84.963)
Epoch: [101][280/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5358 (2.3924)	Acc@1 49.219 (55.697)	Acc@5 81.250 (84.895)
Epoch: [101][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1411 (2.3929)	Acc@1 62.500 (55.700)	Acc@5 86.719 (84.861)
Epoch: [101][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4212 (2.3945)	Acc@1 59.375 (55.671)	Acc@5 79.688 (84.790)
Epoch: [101][310/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5447 (2.3968)	Acc@1 53.906 (55.650)	Acc@5 82.812 (84.749)
Epoch: [101][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3647 (2.4002)	Acc@1 56.250 (55.571)	Acc@5 85.938 (84.708)
Epoch: [101][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3314 (2.4041)	Acc@1 63.281 (55.511)	Acc@5 80.469 (84.644)
Epoch: [101][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2890 (2.4073)	Acc@1 58.594 (55.448)	Acc@5 85.938 (84.563)
Epoch: [101][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4159 (2.4085)	Acc@1 56.250 (55.400)	Acc@5 85.156 (84.529)
Epoch: [101][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5981 (2.4082)	Acc@1 53.125 (55.482)	Acc@5 80.469 (84.524)
Epoch: [101][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3883 (2.4073)	Acc@1 57.031 (55.532)	Acc@5 85.156 (84.535)
Epoch: [101][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3833 (2.4087)	Acc@1 53.906 (55.483)	Acc@5 84.375 (84.541)
Epoch: [101][390/391]	Time 0.147 (0.012)	Data 0.001 (0.002)	Loss 2.5351 (2.4093)	Acc@1 51.250 (55.442)	Acc@5 80.000 (84.512)
num momentum params: 26
[0.1, 2.409307073059082, 1.936516009569168, 55.442, 48.91, tensor(0.3301, device='cuda:0', grad_fn=<DivBackward0>), 4.571487665176392, 0.409071683883667]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [102 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [102][0/391]	Time 0.047 (0.047)	Data 0.199 (0.199)	Loss 2.3455 (2.3455)	Acc@1 54.688 (54.688)	Acc@5 87.500 (87.500)
Epoch: [102][10/391]	Time 0.011 (0.016)	Data 0.001 (0.019)	Loss 2.0662 (2.2881)	Acc@1 63.281 (58.168)	Acc@5 91.406 (87.216)
Epoch: [102][20/391]	Time 0.012 (0.014)	Data 0.001 (0.011)	Loss 2.2097 (2.2748)	Acc@1 57.812 (58.408)	Acc@5 89.844 (86.942)
Epoch: [102][30/391]	Time 0.012 (0.013)	Data 0.001 (0.008)	Loss 2.4920 (2.3152)	Acc@1 47.656 (56.905)	Acc@5 85.938 (86.442)
Epoch: [102][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.0081 (2.3060)	Acc@1 68.750 (57.184)	Acc@5 89.062 (86.643)
Epoch: [102][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4973 (2.3307)	Acc@1 54.688 (56.572)	Acc@5 84.375 (86.382)
Epoch: [102][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4552 (2.3490)	Acc@1 54.688 (56.365)	Acc@5 86.719 (86.091)
Epoch: [102][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3592 (2.3565)	Acc@1 53.125 (56.349)	Acc@5 84.375 (85.662)
Epoch: [102][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2726 (2.3584)	Acc@1 57.031 (56.308)	Acc@5 89.062 (85.629)
Epoch: [102][90/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 2.2796 (2.3559)	Acc@1 53.906 (56.233)	Acc@5 88.281 (85.723)
Epoch: [102][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3927 (2.3572)	Acc@1 53.906 (56.258)	Acc@5 81.250 (85.605)
Epoch: [102][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4447 (2.3562)	Acc@1 55.469 (56.370)	Acc@5 81.250 (85.508)
Epoch: [102][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1587 (2.3540)	Acc@1 60.938 (56.399)	Acc@5 88.281 (85.563)
Epoch: [102][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2236 (2.3499)	Acc@1 58.594 (56.512)	Acc@5 84.375 (85.604)
Epoch: [102][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4130 (2.3517)	Acc@1 53.125 (56.555)	Acc@5 85.156 (85.527)
Epoch: [102][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6565 (2.3583)	Acc@1 50.781 (56.400)	Acc@5 81.250 (85.456)
Epoch: [102][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3898 (2.3597)	Acc@1 56.250 (56.357)	Acc@5 83.594 (85.423)
Epoch: [102][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4470 (2.3601)	Acc@1 54.688 (56.355)	Acc@5 84.375 (85.389)
Epoch: [102][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4305 (2.3579)	Acc@1 54.688 (56.440)	Acc@5 87.500 (85.432)
Epoch: [102][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3741 (2.3621)	Acc@1 55.469 (56.311)	Acc@5 84.375 (85.455)
Epoch: [102][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4238 (2.3682)	Acc@1 55.469 (56.180)	Acc@5 82.031 (85.308)
Epoch: [102][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4467 (2.3703)	Acc@1 54.688 (56.194)	Acc@5 82.812 (85.249)
Epoch: [102][220/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.2536 (2.3693)	Acc@1 61.719 (56.211)	Acc@5 84.375 (85.262)
Epoch: [102][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3545 (2.3691)	Acc@1 56.250 (56.199)	Acc@5 88.281 (85.258)
Epoch: [102][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3705 (2.3692)	Acc@1 55.469 (56.221)	Acc@5 85.938 (85.244)
Epoch: [102][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5778 (2.3745)	Acc@1 50.000 (56.163)	Acc@5 81.250 (85.166)
Epoch: [102][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3944 (2.3747)	Acc@1 54.688 (56.178)	Acc@5 84.375 (85.159)
Epoch: [102][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4080 (2.3789)	Acc@1 55.469 (56.109)	Acc@5 85.156 (85.096)
Epoch: [102][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4325 (2.3817)	Acc@1 52.344 (56.005)	Acc@5 87.500 (85.078)
Epoch: [102][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6173 (2.3839)	Acc@1 50.000 (55.931)	Acc@5 83.594 (85.054)
Epoch: [102][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3718 (2.3834)	Acc@1 60.156 (55.975)	Acc@5 83.594 (85.071)
Epoch: [102][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6071 (2.3875)	Acc@1 53.125 (55.876)	Acc@5 81.250 (85.011)
Epoch: [102][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4082 (2.3875)	Acc@1 53.906 (55.868)	Acc@5 88.281 (85.037)
Epoch: [102][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5770 (2.3908)	Acc@1 53.906 (55.764)	Acc@5 82.812 (84.982)
Epoch: [102][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6070 (2.3933)	Acc@1 54.688 (55.707)	Acc@5 76.562 (84.913)
Epoch: [102][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5121 (2.3929)	Acc@1 52.344 (55.698)	Acc@5 84.375 (84.958)
Epoch: [102][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8376 (2.3945)	Acc@1 49.219 (55.655)	Acc@5 81.250 (84.942)
Epoch: [102][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4705 (2.3954)	Acc@1 47.656 (55.601)	Acc@5 85.156 (84.944)
Epoch: [102][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3617 (2.3964)	Acc@1 55.469 (55.590)	Acc@5 84.375 (84.906)
Epoch: [102][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.3562 (2.3988)	Acc@1 57.500 (55.542)	Acc@5 82.500 (84.880)
num momentum params: 26
[0.1, 2.398805088043213, 2.197072772979736, 55.542, 44.74, tensor(0.3314, device='cuda:0', grad_fn=<DivBackward0>), 4.189028024673462, 0.3177018165588379]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [103 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [103][0/391]	Time 0.054 (0.054)	Data 0.175 (0.175)	Loss 1.9319 (1.9319)	Acc@1 66.406 (66.406)	Acc@5 91.406 (91.406)
Epoch: [103][10/391]	Time 0.014 (0.016)	Data 0.000 (0.018)	Loss 2.5026 (2.3514)	Acc@1 50.781 (56.108)	Acc@5 80.469 (84.375)
Epoch: [103][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.1824 (2.3334)	Acc@1 57.812 (56.510)	Acc@5 89.062 (84.821)
Epoch: [103][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.2085 (2.3217)	Acc@1 63.281 (57.157)	Acc@5 85.156 (85.433)
Epoch: [103][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.6579 (2.3327)	Acc@1 49.219 (56.574)	Acc@5 78.906 (85.347)
Epoch: [103][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3334 (2.3530)	Acc@1 56.250 (56.158)	Acc@5 88.281 (85.187)
Epoch: [103][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3183 (2.3413)	Acc@1 64.844 (56.737)	Acc@5 85.938 (85.118)
Epoch: [103][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4200 (2.3444)	Acc@1 56.250 (56.789)	Acc@5 81.250 (84.914)
Epoch: [103][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3558 (2.3353)	Acc@1 57.812 (56.848)	Acc@5 82.812 (85.185)
Epoch: [103][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2321 (2.3403)	Acc@1 58.594 (56.817)	Acc@5 90.625 (85.208)
Epoch: [103][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2908 (2.3489)	Acc@1 60.156 (56.498)	Acc@5 85.156 (85.118)
Epoch: [103][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3681 (2.3468)	Acc@1 54.688 (56.679)	Acc@5 85.156 (85.086)
Epoch: [103][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4261 (2.3555)	Acc@1 60.156 (56.508)	Acc@5 82.812 (85.034)
Epoch: [103][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.7471 (2.3659)	Acc@1 51.562 (56.363)	Acc@5 77.344 (84.876)
Epoch: [103][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2959 (2.3671)	Acc@1 54.688 (56.372)	Acc@5 85.938 (84.946)
Epoch: [103][150/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 2.3292 (2.3677)	Acc@1 56.250 (56.317)	Acc@5 87.500 (84.975)
Epoch: [103][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4454 (2.3742)	Acc@1 50.781 (56.187)	Acc@5 85.938 (84.933)
Epoch: [103][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0757 (2.3784)	Acc@1 66.406 (56.181)	Acc@5 88.281 (84.836)
Epoch: [103][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5191 (2.3826)	Acc@1 54.688 (56.112)	Acc@5 81.250 (84.807)
Epoch: [103][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3792 (2.3793)	Acc@1 51.562 (56.197)	Acc@5 85.938 (84.833)
Epoch: [103][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2085 (2.3796)	Acc@1 64.062 (56.219)	Acc@5 87.500 (84.764)
Epoch: [103][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1986 (2.3850)	Acc@1 59.375 (56.061)	Acc@5 89.844 (84.738)
Epoch: [103][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3395 (2.3886)	Acc@1 54.688 (55.932)	Acc@5 82.812 (84.665)
Epoch: [103][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4000 (2.3874)	Acc@1 57.031 (55.939)	Acc@5 82.812 (84.673)
Epoch: [103][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2490 (2.3884)	Acc@1 60.156 (55.955)	Acc@5 89.844 (84.660)
Epoch: [103][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2747 (2.3894)	Acc@1 60.938 (55.917)	Acc@5 85.156 (84.587)
Epoch: [103][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2007 (2.3899)	Acc@1 59.375 (55.888)	Acc@5 89.062 (84.635)
Epoch: [103][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5260 (2.3911)	Acc@1 54.688 (55.936)	Acc@5 82.812 (84.617)
Epoch: [103][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3666 (2.3902)	Acc@1 54.688 (55.986)	Acc@5 82.031 (84.634)
Epoch: [103][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2561 (2.3890)	Acc@1 58.594 (55.957)	Acc@5 87.500 (84.681)
Epoch: [103][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4217 (2.3881)	Acc@1 57.812 (55.985)	Acc@5 82.812 (84.673)
Epoch: [103][310/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2626 (2.3863)	Acc@1 60.938 (56.006)	Acc@5 86.719 (84.717)
Epoch: [103][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5072 (2.3881)	Acc@1 55.469 (55.994)	Acc@5 85.156 (84.684)
Epoch: [103][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3904 (2.3910)	Acc@1 58.594 (55.964)	Acc@5 82.812 (84.628)
Epoch: [103][340/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.6045 (2.3924)	Acc@1 50.781 (55.897)	Acc@5 78.906 (84.583)
Epoch: [103][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1787 (2.3947)	Acc@1 57.812 (55.807)	Acc@5 89.062 (84.575)
Epoch: [103][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7604 (2.3953)	Acc@1 46.875 (55.780)	Acc@5 79.688 (84.572)
Epoch: [103][370/391]	Time 0.015 (0.011)	Data 0.001 (0.002)	Loss 2.4033 (2.3955)	Acc@1 56.250 (55.799)	Acc@5 82.812 (84.560)
Epoch: [103][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2777 (2.3964)	Acc@1 60.938 (55.795)	Acc@5 88.281 (84.564)
Epoch: [103][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6258 (2.3971)	Acc@1 47.500 (55.798)	Acc@5 76.250 (84.550)
num momentum params: 26
[0.1, 2.3970786475372314, 2.210583255290985, 55.798, 44.7, tensor(0.3314, device='cuda:0', grad_fn=<DivBackward0>), 4.228506088256836, 0.33677148818969727]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [104 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [104][0/391]	Time 0.052 (0.052)	Data 0.169 (0.169)	Loss 2.3557 (2.3557)	Acc@1 57.031 (57.031)	Acc@5 88.281 (88.281)
Epoch: [104][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.5005 (2.3771)	Acc@1 52.344 (56.676)	Acc@5 85.938 (86.151)
Epoch: [104][20/391]	Time 0.010 (0.013)	Data 0.001 (0.009)	Loss 2.4546 (2.3328)	Acc@1 52.344 (56.957)	Acc@5 82.812 (86.644)
Epoch: [104][30/391]	Time 0.012 (0.012)	Data 0.001 (0.007)	Loss 2.2824 (2.3392)	Acc@1 61.719 (56.804)	Acc@5 89.062 (86.467)
Epoch: [104][40/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2601 (2.3408)	Acc@1 57.812 (56.612)	Acc@5 85.938 (86.528)
Epoch: [104][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.6322 (2.3536)	Acc@1 51.562 (56.296)	Acc@5 79.688 (86.305)
Epoch: [104][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3855 (2.3484)	Acc@1 57.812 (56.391)	Acc@5 83.594 (86.270)
Epoch: [104][70/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2880 (2.3470)	Acc@1 57.031 (56.283)	Acc@5 83.594 (86.147)
Epoch: [104][80/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3075 (2.3508)	Acc@1 55.469 (56.404)	Acc@5 84.375 (85.986)
Epoch: [104][90/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2403 (2.3601)	Acc@1 60.938 (56.224)	Acc@5 86.719 (85.731)
Epoch: [104][100/391]	Time 0.012 (0.011)	Data 0.002 (0.003)	Loss 2.6060 (2.3662)	Acc@1 52.344 (56.173)	Acc@5 82.812 (85.558)
Epoch: [104][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2019 (2.3533)	Acc@1 59.375 (56.560)	Acc@5 89.062 (85.719)
Epoch: [104][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4620 (2.3566)	Acc@1 55.469 (56.573)	Acc@5 79.688 (85.557)
Epoch: [104][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2900 (2.3539)	Acc@1 56.250 (56.566)	Acc@5 88.281 (85.538)
Epoch: [104][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3243 (2.3580)	Acc@1 57.031 (56.505)	Acc@5 85.938 (85.489)
Epoch: [104][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3375 (2.3604)	Acc@1 55.469 (56.498)	Acc@5 86.719 (85.374)
Epoch: [104][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2774 (2.3627)	Acc@1 53.125 (56.449)	Acc@5 84.375 (85.278)
Epoch: [104][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3322 (2.3656)	Acc@1 57.031 (56.296)	Acc@5 82.812 (85.220)
Epoch: [104][180/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3112 (2.3677)	Acc@1 56.250 (56.319)	Acc@5 87.500 (85.221)
Epoch: [104][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6210 (2.3740)	Acc@1 50.781 (56.197)	Acc@5 82.812 (85.115)
Epoch: [104][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4718 (2.3772)	Acc@1 53.125 (56.013)	Acc@5 82.031 (85.079)
Epoch: [104][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4095 (2.3769)	Acc@1 57.031 (56.043)	Acc@5 85.156 (85.038)
Epoch: [104][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1781 (2.3790)	Acc@1 62.500 (55.914)	Acc@5 82.812 (85.011)
Epoch: [104][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5181 (2.3853)	Acc@1 52.344 (55.763)	Acc@5 85.156 (84.936)
Epoch: [104][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3488 (2.3866)	Acc@1 57.031 (55.786)	Acc@5 83.594 (84.916)
Epoch: [104][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4650 (2.3887)	Acc@1 53.906 (55.730)	Acc@5 84.375 (84.876)
Epoch: [104][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4696 (2.3904)	Acc@1 50.781 (55.630)	Acc@5 85.938 (84.875)
Epoch: [104][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5840 (2.3902)	Acc@1 49.219 (55.587)	Acc@5 77.344 (84.819)
Epoch: [104][280/391]	Time 0.015 (0.011)	Data 0.001 (0.002)	Loss 2.4277 (2.3882)	Acc@1 57.031 (55.700)	Acc@5 82.812 (84.864)
Epoch: [104][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8103 (2.3941)	Acc@1 49.219 (55.582)	Acc@5 78.906 (84.810)
Epoch: [104][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4534 (2.3967)	Acc@1 53.125 (55.492)	Acc@5 86.719 (84.790)
Epoch: [104][310/391]	Time 0.010 (0.011)	Data 0.004 (0.002)	Loss 2.4903 (2.3982)	Acc@1 56.250 (55.444)	Acc@5 80.469 (84.762)
Epoch: [104][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4314 (2.3979)	Acc@1 57.031 (55.464)	Acc@5 82.812 (84.711)
Epoch: [104][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3301 (2.3985)	Acc@1 57.812 (55.471)	Acc@5 87.500 (84.672)
Epoch: [104][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3059 (2.3985)	Acc@1 53.125 (55.482)	Acc@5 85.938 (84.677)
Epoch: [104][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2247 (2.3983)	Acc@1 59.375 (55.511)	Acc@5 89.062 (84.687)
Epoch: [104][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4211 (2.3981)	Acc@1 53.125 (55.523)	Acc@5 89.062 (84.708)
Epoch: [104][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5051 (2.4015)	Acc@1 56.250 (55.429)	Acc@5 81.250 (84.638)
Epoch: [104][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5723 (2.4043)	Acc@1 53.125 (55.354)	Acc@5 82.812 (84.609)
Epoch: [104][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.4275 (2.4051)	Acc@1 57.500 (55.312)	Acc@5 82.500 (84.582)
num momentum params: 26
[0.1, 2.405127795410156, 2.237424520254135, 55.312, 44.29, tensor(0.3305, device='cuda:0', grad_fn=<DivBackward0>), 4.2291717529296875, 0.340939998626709]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [105 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [105][0/391]	Time 0.054 (0.054)	Data 0.177 (0.177)	Loss 2.1587 (2.1587)	Acc@1 63.281 (63.281)	Acc@5 85.938 (85.938)
Epoch: [105][10/391]	Time 0.010 (0.015)	Data 0.001 (0.017)	Loss 2.3261 (2.3007)	Acc@1 57.812 (59.659)	Acc@5 86.719 (86.506)
Epoch: [105][20/391]	Time 0.010 (0.013)	Data 0.001 (0.010)	Loss 2.1590 (2.2697)	Acc@1 61.719 (58.817)	Acc@5 89.844 (87.463)
Epoch: [105][30/391]	Time 0.011 (0.012)	Data 0.001 (0.007)	Loss 2.4353 (2.2896)	Acc@1 54.688 (58.266)	Acc@5 85.156 (86.820)
Epoch: [105][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.0911 (2.3006)	Acc@1 63.281 (58.194)	Acc@5 87.500 (86.471)
Epoch: [105][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2537 (2.3154)	Acc@1 59.375 (57.828)	Acc@5 84.375 (86.167)
Epoch: [105][60/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.1467 (2.3167)	Acc@1 66.406 (57.953)	Acc@5 89.062 (86.014)
Epoch: [105][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.1482 (2.3119)	Acc@1 58.594 (57.857)	Acc@5 90.625 (86.136)
Epoch: [105][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.3427 (2.3249)	Acc@1 55.469 (57.542)	Acc@5 85.156 (85.889)
Epoch: [105][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4759 (2.3323)	Acc@1 50.000 (57.357)	Acc@5 82.812 (85.671)
Epoch: [105][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5426 (2.3386)	Acc@1 50.000 (57.271)	Acc@5 81.250 (85.551)
Epoch: [105][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3936 (2.3453)	Acc@1 58.594 (57.172)	Acc@5 84.375 (85.396)
Epoch: [105][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4820 (2.3387)	Acc@1 52.344 (57.160)	Acc@5 83.594 (85.544)
Epoch: [105][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3109 (2.3377)	Acc@1 54.688 (57.174)	Acc@5 85.156 (85.580)
Epoch: [105][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3993 (2.3457)	Acc@1 57.031 (57.020)	Acc@5 82.031 (85.461)
Epoch: [105][150/391]	Time 0.009 (0.011)	Data 0.002 (0.003)	Loss 2.5760 (2.3528)	Acc@1 50.781 (56.845)	Acc@5 81.250 (85.374)
Epoch: [105][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5701 (2.3615)	Acc@1 50.000 (56.561)	Acc@5 85.938 (85.278)
Epoch: [105][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3161 (2.3646)	Acc@1 60.156 (56.433)	Acc@5 84.375 (85.257)
Epoch: [105][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6004 (2.3732)	Acc@1 50.781 (56.155)	Acc@5 78.125 (85.096)
Epoch: [105][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4750 (2.3733)	Acc@1 51.562 (56.152)	Acc@5 82.812 (85.103)
Epoch: [105][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4714 (2.3718)	Acc@1 56.250 (56.277)	Acc@5 83.594 (85.148)
Epoch: [105][210/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3760 (2.3740)	Acc@1 55.469 (56.157)	Acc@5 85.938 (85.101)
Epoch: [105][220/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3729 (2.3727)	Acc@1 57.812 (56.222)	Acc@5 82.031 (85.110)
Epoch: [105][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4469 (2.3727)	Acc@1 56.250 (56.220)	Acc@5 79.688 (85.116)
Epoch: [105][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4593 (2.3780)	Acc@1 53.125 (56.111)	Acc@5 82.812 (85.040)
Epoch: [105][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1439 (2.3790)	Acc@1 64.844 (56.038)	Acc@5 85.156 (85.016)
Epoch: [105][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2589 (2.3794)	Acc@1 57.812 (56.043)	Acc@5 85.938 (85.007)
Epoch: [105][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3552 (2.3831)	Acc@1 55.469 (55.904)	Acc@5 85.156 (84.952)
Epoch: [105][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8931 (2.3860)	Acc@1 42.969 (55.839)	Acc@5 79.688 (84.912)
Epoch: [105][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6759 (2.3919)	Acc@1 46.875 (55.713)	Acc@5 74.219 (84.802)
Epoch: [105][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4900 (2.3953)	Acc@1 52.344 (55.614)	Acc@5 85.156 (84.767)
Epoch: [105][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7707 (2.3948)	Acc@1 48.438 (55.652)	Acc@5 81.250 (84.754)
Epoch: [105][320/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.3570 (2.3949)	Acc@1 53.125 (55.663)	Acc@5 83.594 (84.747)
Epoch: [105][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2663 (2.3956)	Acc@1 57.812 (55.655)	Acc@5 86.719 (84.750)
Epoch: [105][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2389 (2.3956)	Acc@1 62.500 (55.666)	Acc@5 89.844 (84.774)
Epoch: [105][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2706 (2.3959)	Acc@1 57.031 (55.645)	Acc@5 88.281 (84.778)
Epoch: [105][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4981 (2.3964)	Acc@1 49.219 (55.631)	Acc@5 84.375 (84.773)
Epoch: [105][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4559 (2.3973)	Acc@1 52.344 (55.599)	Acc@5 83.594 (84.744)
Epoch: [105][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5952 (2.4006)	Acc@1 52.344 (55.500)	Acc@5 82.812 (84.711)
Epoch: [105][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.4865 (2.4023)	Acc@1 53.750 (55.464)	Acc@5 87.500 (84.674)
num momentum params: 26
[0.1, 2.4023021298217775, 2.0996762704849243, 55.464, 45.28, tensor(0.3308, device='cuda:0', grad_fn=<DivBackward0>), 4.225361108779907, 0.3368864059448242]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [106 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [106][0/391]	Time 0.054 (0.054)	Data 0.169 (0.169)	Loss 2.1270 (2.1270)	Acc@1 60.938 (60.938)	Acc@5 89.062 (89.062)
Epoch: [106][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.2758 (2.2506)	Acc@1 52.344 (57.741)	Acc@5 89.062 (87.358)
Epoch: [106][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.4122 (2.3433)	Acc@1 50.000 (55.915)	Acc@5 89.062 (85.677)
Epoch: [106][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.5474 (2.3396)	Acc@1 55.469 (55.872)	Acc@5 82.031 (86.038)
Epoch: [106][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.3645 (2.3442)	Acc@1 58.594 (55.716)	Acc@5 83.594 (86.109)
Epoch: [106][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.5464 (2.3431)	Acc@1 54.688 (56.051)	Acc@5 81.250 (85.999)
Epoch: [106][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2148 (2.3393)	Acc@1 64.062 (56.173)	Acc@5 83.594 (86.142)
Epoch: [106][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.3059 (2.3557)	Acc@1 55.469 (55.810)	Acc@5 85.938 (85.750)
Epoch: [106][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.2089 (2.3620)	Acc@1 62.500 (55.874)	Acc@5 88.281 (85.581)
Epoch: [106][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2456 (2.3568)	Acc@1 58.594 (56.087)	Acc@5 87.500 (85.689)
Epoch: [106][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1902 (2.3549)	Acc@1 61.719 (56.366)	Acc@5 87.500 (85.613)
Epoch: [106][110/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3711 (2.3548)	Acc@1 57.031 (56.461)	Acc@5 82.812 (85.536)
Epoch: [106][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.4704 (2.3527)	Acc@1 49.219 (56.534)	Acc@5 84.375 (85.602)
Epoch: [106][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3694 (2.3478)	Acc@1 50.000 (56.554)	Acc@5 86.719 (85.717)
Epoch: [106][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0961 (2.3505)	Acc@1 62.500 (56.571)	Acc@5 89.844 (85.605)
Epoch: [106][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4309 (2.3553)	Acc@1 52.344 (56.514)	Acc@5 88.281 (85.575)
Epoch: [106][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2759 (2.3560)	Acc@1 57.812 (56.430)	Acc@5 88.281 (85.603)
Epoch: [106][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.0974 (2.3534)	Acc@1 62.500 (56.469)	Acc@5 89.062 (85.641)
Epoch: [106][180/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.4454 (2.3577)	Acc@1 56.250 (56.388)	Acc@5 84.375 (85.506)
Epoch: [106][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7434 (2.3635)	Acc@1 51.562 (56.303)	Acc@5 80.469 (85.377)
Epoch: [106][200/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.3682 (2.3676)	Acc@1 49.219 (56.161)	Acc@5 88.281 (85.339)
Epoch: [106][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.8532 (2.3737)	Acc@1 42.969 (56.043)	Acc@5 79.688 (85.264)
Epoch: [106][220/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3316 (2.3758)	Acc@1 52.344 (56.013)	Acc@5 86.719 (85.185)
Epoch: [106][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3461 (2.3806)	Acc@1 57.812 (55.888)	Acc@5 86.719 (85.062)
Epoch: [106][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6098 (2.3836)	Acc@1 47.656 (55.867)	Acc@5 84.375 (85.020)
Epoch: [106][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6104 (2.3833)	Acc@1 50.000 (55.889)	Acc@5 81.250 (85.013)
Epoch: [106][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4828 (2.3836)	Acc@1 55.469 (55.891)	Acc@5 81.250 (84.956)
Epoch: [106][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3839 (2.3799)	Acc@1 55.469 (55.973)	Acc@5 85.156 (85.012)
Epoch: [106][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3792 (2.3786)	Acc@1 57.031 (56.047)	Acc@5 83.594 (85.051)
Epoch: [106][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3650 (2.3822)	Acc@1 61.719 (56.014)	Acc@5 85.938 (84.976)
Epoch: [106][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1843 (2.3857)	Acc@1 59.375 (55.944)	Acc@5 89.062 (84.946)
Epoch: [106][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2314 (2.3870)	Acc@1 59.375 (55.883)	Acc@5 85.938 (84.890)
Epoch: [106][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5077 (2.3879)	Acc@1 49.219 (55.807)	Acc@5 80.469 (84.828)
Epoch: [106][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2426 (2.3902)	Acc@1 59.375 (55.702)	Acc@5 85.938 (84.776)
Epoch: [106][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3294 (2.3931)	Acc@1 57.031 (55.645)	Acc@5 88.281 (84.737)
Epoch: [106][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4073 (2.3944)	Acc@1 55.469 (55.605)	Acc@5 82.812 (84.707)
Epoch: [106][360/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.7050 (2.3970)	Acc@1 51.562 (55.560)	Acc@5 75.000 (84.643)
Epoch: [106][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3595 (2.3972)	Acc@1 57.031 (55.568)	Acc@5 87.500 (84.659)
Epoch: [106][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6161 (2.3991)	Acc@1 52.344 (55.512)	Acc@5 78.906 (84.650)
Epoch: [106][390/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2157 (2.3978)	Acc@1 61.250 (55.556)	Acc@5 87.500 (84.658)
num momentum params: 26
[0.1, 2.397757240524292, 2.119217040538788, 55.556, 46.12, tensor(0.3314, device='cuda:0', grad_fn=<DivBackward0>), 4.227750778198242, 0.36737966537475586]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [107 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [107][0/391]	Time 0.053 (0.053)	Data 0.178 (0.178)	Loss 2.3338 (2.3338)	Acc@1 53.906 (53.906)	Acc@5 86.719 (86.719)
Epoch: [107][10/391]	Time 0.012 (0.016)	Data 0.001 (0.018)	Loss 2.2790 (2.3605)	Acc@1 59.375 (54.972)	Acc@5 87.500 (86.364)
Epoch: [107][20/391]	Time 0.010 (0.013)	Data 0.001 (0.010)	Loss 2.2250 (2.3583)	Acc@1 57.031 (55.692)	Acc@5 83.594 (85.714)
Epoch: [107][30/391]	Time 0.011 (0.012)	Data 0.001 (0.007)	Loss 2.3906 (2.3430)	Acc@1 58.594 (56.804)	Acc@5 84.375 (85.786)
Epoch: [107][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.3145 (2.3400)	Acc@1 54.688 (56.669)	Acc@5 88.281 (85.861)
Epoch: [107][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2223 (2.3350)	Acc@1 64.844 (56.955)	Acc@5 85.156 (85.815)
Epoch: [107][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4939 (2.3345)	Acc@1 50.000 (56.980)	Acc@5 84.375 (85.758)
Epoch: [107][70/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2292 (2.3251)	Acc@1 59.375 (57.042)	Acc@5 89.844 (86.037)
Epoch: [107][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.3516 (2.3257)	Acc@1 57.812 (57.224)	Acc@5 82.031 (85.957)
Epoch: [107][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2991 (2.3223)	Acc@1 57.031 (57.254)	Acc@5 83.594 (85.998)
Epoch: [107][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2694 (2.3156)	Acc@1 60.156 (57.403)	Acc@5 88.281 (86.092)
Epoch: [107][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.7133 (2.3258)	Acc@1 51.562 (57.179)	Acc@5 78.125 (85.839)
Epoch: [107][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5598 (2.3330)	Acc@1 47.656 (57.018)	Acc@5 81.250 (85.679)
Epoch: [107][130/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.5441 (2.3403)	Acc@1 56.250 (56.954)	Acc@5 78.906 (85.490)
Epoch: [107][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0888 (2.3426)	Acc@1 65.625 (56.992)	Acc@5 88.281 (85.483)
Epoch: [107][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6551 (2.3462)	Acc@1 52.344 (56.829)	Acc@5 78.125 (85.430)
Epoch: [107][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4317 (2.3516)	Acc@1 46.875 (56.628)	Acc@5 83.594 (85.350)
Epoch: [107][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2262 (2.3580)	Acc@1 59.375 (56.501)	Acc@5 87.500 (85.243)
Epoch: [107][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4149 (2.3634)	Acc@1 50.000 (56.310)	Acc@5 85.938 (85.174)
Epoch: [107][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3951 (2.3703)	Acc@1 55.469 (56.213)	Acc@5 85.156 (85.074)
Epoch: [107][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4394 (2.3735)	Acc@1 56.250 (56.153)	Acc@5 85.938 (85.036)
Epoch: [107][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3573 (2.3745)	Acc@1 50.781 (56.050)	Acc@5 87.500 (84.993)
Epoch: [107][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4255 (2.3769)	Acc@1 53.125 (55.988)	Acc@5 89.844 (85.029)
Epoch: [107][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2291 (2.3770)	Acc@1 60.156 (55.979)	Acc@5 89.062 (85.021)
Epoch: [107][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4106 (2.3792)	Acc@1 52.344 (55.893)	Acc@5 82.812 (84.981)
Epoch: [107][250/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.3262 (2.3818)	Acc@1 55.469 (55.892)	Acc@5 92.969 (84.960)
Epoch: [107][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3139 (2.3827)	Acc@1 56.250 (55.858)	Acc@5 85.156 (84.932)
Epoch: [107][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4833 (2.3845)	Acc@1 50.781 (55.795)	Acc@5 81.250 (84.877)
Epoch: [107][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4944 (2.3848)	Acc@1 54.688 (55.783)	Acc@5 81.250 (84.875)
Epoch: [107][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5372 (2.3883)	Acc@1 50.781 (55.729)	Acc@5 82.812 (84.799)
Epoch: [107][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4323 (2.3871)	Acc@1 50.781 (55.759)	Acc@5 85.938 (84.834)
Epoch: [107][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3941 (2.3885)	Acc@1 55.469 (55.710)	Acc@5 82.031 (84.857)
Epoch: [107][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2532 (2.3914)	Acc@1 59.375 (55.620)	Acc@5 84.375 (84.794)
Epoch: [107][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2007 (2.3918)	Acc@1 61.719 (55.606)	Acc@5 87.500 (84.764)
Epoch: [107][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3271 (2.3942)	Acc@1 58.594 (55.531)	Acc@5 85.938 (84.721)
Epoch: [107][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3114 (2.3960)	Acc@1 57.812 (55.502)	Acc@5 83.594 (84.671)
Epoch: [107][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5914 (2.3973)	Acc@1 51.562 (55.519)	Acc@5 81.250 (84.594)
Epoch: [107][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3500 (2.3962)	Acc@1 63.281 (55.580)	Acc@5 82.031 (84.605)
Epoch: [107][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4386 (2.3953)	Acc@1 55.469 (55.608)	Acc@5 89.062 (84.611)
Epoch: [107][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.0809 (2.3953)	Acc@1 62.500 (55.584)	Acc@5 91.250 (84.614)
num momentum params: 26
[0.1, 2.3953121337890626, 1.901644961833954, 55.584, 50.12, tensor(0.3315, device='cuda:0', grad_fn=<DivBackward0>), 4.250417709350586, 0.34690141677856445]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [108 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [108][0/391]	Time 0.055 (0.055)	Data 0.178 (0.178)	Loss 2.1140 (2.1140)	Acc@1 59.375 (59.375)	Acc@5 92.188 (92.188)
Epoch: [108][10/391]	Time 0.010 (0.017)	Data 0.001 (0.017)	Loss 2.1884 (2.2932)	Acc@1 62.500 (58.594)	Acc@5 87.500 (86.506)
Epoch: [108][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.3166 (2.3137)	Acc@1 61.719 (58.296)	Acc@5 82.812 (85.826)
Epoch: [108][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 1.9522 (2.2830)	Acc@1 64.062 (58.997)	Acc@5 92.969 (86.265)
Epoch: [108][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.2949 (2.3095)	Acc@1 57.812 (58.098)	Acc@5 83.594 (85.690)
Epoch: [108][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2227 (2.3137)	Acc@1 63.281 (58.272)	Acc@5 82.812 (85.616)
Epoch: [108][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2365 (2.3216)	Acc@1 60.938 (58.107)	Acc@5 86.719 (85.489)
Epoch: [108][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.7358 (2.3420)	Acc@1 49.219 (57.526)	Acc@5 79.688 (85.200)
Epoch: [108][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.6460 (2.3600)	Acc@1 52.344 (57.157)	Acc@5 80.469 (84.954)
Epoch: [108][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3835 (2.3594)	Acc@1 58.594 (56.997)	Acc@5 85.156 (85.139)
Epoch: [108][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4166 (2.3610)	Acc@1 55.469 (56.822)	Acc@5 85.938 (85.210)
Epoch: [108][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4023 (2.3618)	Acc@1 51.562 (56.715)	Acc@5 86.719 (85.304)
Epoch: [108][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3771 (2.3636)	Acc@1 56.250 (56.624)	Acc@5 82.812 (85.163)
Epoch: [108][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4033 (2.3635)	Acc@1 57.031 (56.578)	Acc@5 85.938 (85.228)
Epoch: [108][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4543 (2.3662)	Acc@1 55.469 (56.483)	Acc@5 82.812 (85.151)
Epoch: [108][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.7625 (2.3702)	Acc@1 46.875 (56.364)	Acc@5 80.469 (85.141)
Epoch: [108][160/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4691 (2.3721)	Acc@1 54.688 (56.313)	Acc@5 82.812 (85.083)
Epoch: [108][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4217 (2.3740)	Acc@1 52.344 (56.332)	Acc@5 83.594 (85.015)
Epoch: [108][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7513 (2.3767)	Acc@1 45.312 (56.203)	Acc@5 79.688 (84.958)
Epoch: [108][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6836 (2.3816)	Acc@1 49.219 (56.074)	Acc@5 77.344 (84.825)
Epoch: [108][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3346 (2.3887)	Acc@1 60.156 (56.013)	Acc@5 85.156 (84.670)
Epoch: [108][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6217 (2.3947)	Acc@1 56.250 (55.946)	Acc@5 85.156 (84.590)
Epoch: [108][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6253 (2.3958)	Acc@1 49.219 (55.889)	Acc@5 82.031 (84.591)
Epoch: [108][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5465 (2.3954)	Acc@1 46.875 (55.864)	Acc@5 87.500 (84.646)
Epoch: [108][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5318 (2.3981)	Acc@1 51.562 (55.832)	Acc@5 85.156 (84.602)
Epoch: [108][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3533 (2.3951)	Acc@1 53.125 (55.876)	Acc@5 85.156 (84.640)
Epoch: [108][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2232 (2.3939)	Acc@1 60.938 (55.822)	Acc@5 86.719 (84.662)
Epoch: [108][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6017 (2.3939)	Acc@1 51.562 (55.789)	Acc@5 80.469 (84.669)
Epoch: [108][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3552 (2.3947)	Acc@1 60.938 (55.752)	Acc@5 82.812 (84.664)
Epoch: [108][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5286 (2.3975)	Acc@1 52.344 (55.718)	Acc@5 84.375 (84.649)
Epoch: [108][300/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.3495 (2.3972)	Acc@1 52.344 (55.679)	Acc@5 82.812 (84.645)
Epoch: [108][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2724 (2.3966)	Acc@1 60.156 (55.682)	Acc@5 89.062 (84.659)
Epoch: [108][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3309 (2.3983)	Acc@1 54.688 (55.615)	Acc@5 85.938 (84.660)
Epoch: [108][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2360 (2.3960)	Acc@1 57.031 (55.667)	Acc@5 87.500 (84.689)
Epoch: [108][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5468 (2.3971)	Acc@1 55.469 (55.641)	Acc@5 85.938 (84.687)
Epoch: [108][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4205 (2.4012)	Acc@1 57.812 (55.567)	Acc@5 84.375 (84.613)
Epoch: [108][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3617 (2.4040)	Acc@1 54.688 (55.519)	Acc@5 83.594 (84.576)
Epoch: [108][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5660 (2.4036)	Acc@1 49.219 (55.498)	Acc@5 82.812 (84.598)
Epoch: [108][380/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3417 (2.4029)	Acc@1 56.250 (55.481)	Acc@5 82.031 (84.594)
Epoch: [108][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4024 (2.4036)	Acc@1 60.000 (55.486)	Acc@5 81.250 (84.580)
num momentum params: 26
[0.1, 2.4035601694488524, 2.0713657760620117, 55.486, 46.92, tensor(0.3303, device='cuda:0', grad_fn=<DivBackward0>), 4.22748064994812, 0.340040922164917]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [109 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [109][0/391]	Time 0.057 (0.057)	Data 0.173 (0.173)	Loss 2.3175 (2.3175)	Acc@1 57.812 (57.812)	Acc@5 87.500 (87.500)
Epoch: [109][10/391]	Time 0.012 (0.016)	Data 0.001 (0.017)	Loss 2.2690 (2.2806)	Acc@1 61.719 (59.020)	Acc@5 88.281 (86.790)
Epoch: [109][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.3018 (2.2744)	Acc@1 53.125 (58.631)	Acc@5 89.844 (86.942)
Epoch: [109][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.4351 (2.2814)	Acc@1 55.469 (58.795)	Acc@5 84.375 (86.794)
Epoch: [109][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.1186 (2.2900)	Acc@1 61.719 (58.441)	Acc@5 88.281 (86.414)
Epoch: [109][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.5089 (2.3203)	Acc@1 53.906 (57.843)	Acc@5 82.812 (85.922)
Epoch: [109][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.5615 (2.3278)	Acc@1 50.781 (57.697)	Acc@5 82.812 (85.886)
Epoch: [109][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3048 (2.3378)	Acc@1 57.812 (57.185)	Acc@5 86.719 (85.651)
Epoch: [109][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2204 (2.3281)	Acc@1 57.812 (57.359)	Acc@5 89.844 (85.783)
Epoch: [109][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4762 (2.3348)	Acc@1 54.688 (57.237)	Acc@5 82.812 (85.586)
Epoch: [109][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.6403 (2.3439)	Acc@1 50.000 (56.923)	Acc@5 85.156 (85.551)
Epoch: [109][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6265 (2.3500)	Acc@1 51.562 (56.764)	Acc@5 78.906 (85.473)
Epoch: [109][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2270 (2.3494)	Acc@1 57.812 (56.773)	Acc@5 85.938 (85.434)
Epoch: [109][130/391]	Time 0.012 (0.011)	Data 0.002 (0.003)	Loss 2.3173 (2.3588)	Acc@1 57.812 (56.620)	Acc@5 85.156 (85.222)
Epoch: [109][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.4029 (2.3606)	Acc@1 57.031 (56.654)	Acc@5 83.594 (85.173)
Epoch: [109][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3831 (2.3668)	Acc@1 56.250 (56.529)	Acc@5 85.156 (85.063)
Epoch: [109][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2420 (2.3649)	Acc@1 56.250 (56.614)	Acc@5 86.719 (85.093)
Epoch: [109][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5285 (2.3678)	Acc@1 53.125 (56.588)	Acc@5 82.812 (85.042)
Epoch: [109][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5195 (2.3735)	Acc@1 48.438 (56.310)	Acc@5 85.938 (85.014)
Epoch: [109][190/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4030 (2.3751)	Acc@1 54.688 (56.193)	Acc@5 85.156 (84.923)
Epoch: [109][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5680 (2.3831)	Acc@1 55.469 (56.009)	Acc@5 83.594 (84.787)
Epoch: [109][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2793 (2.3859)	Acc@1 57.031 (55.969)	Acc@5 85.938 (84.727)
Epoch: [109][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6176 (2.3894)	Acc@1 51.562 (55.911)	Acc@5 74.219 (84.633)
Epoch: [109][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4482 (2.3943)	Acc@1 58.594 (55.834)	Acc@5 80.469 (84.541)
Epoch: [109][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2254 (2.3934)	Acc@1 60.156 (55.845)	Acc@5 89.062 (84.602)
Epoch: [109][250/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2244 (2.3958)	Acc@1 58.594 (55.761)	Acc@5 89.062 (84.531)
Epoch: [109][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2380 (2.3934)	Acc@1 66.406 (55.909)	Acc@5 84.375 (84.543)
Epoch: [109][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4898 (2.3928)	Acc@1 54.688 (55.950)	Acc@5 82.031 (84.577)
Epoch: [109][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4033 (2.3932)	Acc@1 54.688 (55.897)	Acc@5 85.156 (84.586)
Epoch: [109][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1946 (2.3940)	Acc@1 60.938 (55.871)	Acc@5 88.281 (84.595)
Epoch: [109][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1246 (2.3910)	Acc@1 60.156 (55.949)	Acc@5 89.844 (84.640)
Epoch: [109][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5983 (2.3924)	Acc@1 55.469 (55.939)	Acc@5 79.688 (84.644)
Epoch: [109][320/391]	Time 0.012 (0.011)	Data 0.003 (0.002)	Loss 2.4285 (2.3946)	Acc@1 52.344 (55.880)	Acc@5 85.938 (84.640)
Epoch: [109][330/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4965 (2.3964)	Acc@1 50.000 (55.783)	Acc@5 85.156 (84.630)
Epoch: [109][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6664 (2.3990)	Acc@1 51.562 (55.737)	Acc@5 81.250 (84.595)
Epoch: [109][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2593 (2.4003)	Acc@1 57.031 (55.691)	Acc@5 85.938 (84.578)
Epoch: [109][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1998 (2.4018)	Acc@1 60.156 (55.640)	Acc@5 87.500 (84.546)
Epoch: [109][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4975 (2.4038)	Acc@1 50.000 (55.580)	Acc@5 80.469 (84.522)
Epoch: [109][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4199 (2.4061)	Acc@1 51.562 (55.485)	Acc@5 86.719 (84.496)
Epoch: [109][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.3867 (2.4075)	Acc@1 57.500 (55.464)	Acc@5 83.750 (84.490)
num momentum params: 26
[0.1, 2.4075017181396485, 1.9978056728839875, 55.464, 48.57, tensor(0.3307, device='cuda:0', grad_fn=<DivBackward0>), 4.259861469268799, 0.3354179859161377]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [109, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [109]
Non Pruning Epoch - module.bn2.bias: [109]
Non Pruning Epoch - module.conv3.weight: [211, 109, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [361, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [361]
Non Pruning Epoch - module.bn5.bias: [361]
Non Pruning Epoch - module.conv6.weight: [321, 361, 3, 3]
Non Pruning Epoch - module.bn6.weight: [321]
Non Pruning Epoch - module.bn6.bias: [321]
Non Pruning Epoch - module.conv7.weight: [207, 321, 3, 3]
Non Pruning Epoch - module.bn7.weight: [207]
Non Pruning Epoch - module.bn7.bias: [207]
Non Pruning Epoch - module.conv8.weight: [198, 207, 3, 3]
Non Pruning Epoch - module.bn8.weight: [198]
Non Pruning Epoch - module.bn8.bias: [198]
Non Pruning Epoch - module.fc.weight: [100, 198]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [110 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [109, 32, 3, 3]
module.conv3.weight [211, 109, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [361, 247, 3, 3]
module.conv6.weight [321, 361, 3, 3]
module.conv7.weight [207, 321, 3, 3]
module.conv8.weight [198, 207, 3, 3]
Epoch: [110][0/391]	Time 0.057 (0.057)	Data 0.168 (0.168)	Loss 2.2767 (2.2767)	Acc@1 57.031 (57.031)	Acc@5 85.156 (85.156)
Epoch: [110][10/391]	Time 0.012 (0.016)	Data 0.001 (0.017)	Loss 2.4835 (2.3471)	Acc@1 53.125 (56.534)	Acc@5 83.594 (85.085)
Epoch: [110][20/391]	Time 0.010 (0.014)	Data 0.001 (0.009)	Loss 2.2799 (2.3359)	Acc@1 58.594 (56.473)	Acc@5 90.625 (85.751)
Epoch: [110][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.4695 (2.3607)	Acc@1 51.562 (55.771)	Acc@5 82.031 (84.929)
Epoch: [110][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.4335 (2.3497)	Acc@1 56.250 (56.498)	Acc@5 87.500 (85.042)
Epoch: [110][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2055 (2.3383)	Acc@1 63.281 (57.062)	Acc@5 87.500 (85.432)
Epoch: [110][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.6341 (2.3554)	Acc@1 46.875 (56.455)	Acc@5 85.156 (85.464)
Epoch: [110][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.4666 (2.3658)	Acc@1 52.344 (56.228)	Acc@5 85.938 (85.332)
Epoch: [110][80/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1999 (2.3675)	Acc@1 59.375 (56.269)	Acc@5 89.062 (85.243)
Epoch: [110][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.7390 (2.3783)	Acc@1 45.312 (55.932)	Acc@5 78.906 (85.165)
Epoch: [110][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4656 (2.3777)	Acc@1 54.688 (55.910)	Acc@5 82.031 (85.156)
Epoch: [110][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2731 (2.3801)	Acc@1 58.594 (55.891)	Acc@5 88.281 (85.170)
Epoch: [110][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3660 (2.3745)	Acc@1 55.469 (56.140)	Acc@5 87.500 (85.208)
Epoch: [110][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4267 (2.3727)	Acc@1 52.344 (56.232)	Acc@5 86.719 (85.228)
Epoch: [110][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4187 (2.3754)	Acc@1 55.469 (56.145)	Acc@5 82.031 (85.195)
Epoch: [110][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5276 (2.3826)	Acc@1 47.656 (55.971)	Acc@5 87.500 (85.032)
Epoch: [110][160/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4178 (2.3837)	Acc@1 56.250 (55.949)	Acc@5 82.031 (84.986)
Epoch: [110][170/391]	Time 0.016 (0.011)	Data 0.001 (0.002)	Loss 2.3552 (2.3858)	Acc@1 56.250 (55.916)	Acc@5 87.500 (84.928)
Epoch: [110][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2360 (2.3816)	Acc@1 59.375 (56.021)	Acc@5 85.156 (84.919)
Epoch: [110][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3134 (2.3817)	Acc@1 57.031 (56.021)	Acc@5 85.156 (84.886)
Epoch: [110][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3641 (2.3863)	Acc@1 53.906 (55.939)	Acc@5 85.938 (84.799)
Epoch: [110][210/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.6509 (2.3908)	Acc@1 52.344 (55.791)	Acc@5 81.250 (84.705)
Epoch: [110][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5431 (2.3900)	Acc@1 52.344 (55.783)	Acc@5 82.812 (84.707)
Epoch: [110][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6911 (2.3915)	Acc@1 47.656 (55.705)	Acc@5 83.594 (84.774)
Epoch: [110][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1324 (2.3910)	Acc@1 59.375 (55.731)	Acc@5 89.844 (84.790)
Epoch: [110][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6437 (2.3926)	Acc@1 50.781 (55.715)	Acc@5 81.250 (84.789)
Epoch: [110][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2709 (2.3935)	Acc@1 62.500 (55.744)	Acc@5 87.500 (84.752)
Epoch: [110][270/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3204 (2.3916)	Acc@1 61.719 (55.829)	Acc@5 87.500 (84.764)
Epoch: [110][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6995 (2.3938)	Acc@1 46.094 (55.758)	Acc@5 80.469 (84.717)
Epoch: [110][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5999 (2.3936)	Acc@1 52.344 (55.777)	Acc@5 82.812 (84.740)
Epoch: [110][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3417 (2.3957)	Acc@1 57.812 (55.739)	Acc@5 84.375 (84.725)
Epoch: [110][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3367 (2.3965)	Acc@1 57.031 (55.733)	Acc@5 85.156 (84.742)
Epoch: [110][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3568 (2.3981)	Acc@1 57.812 (55.688)	Acc@5 83.594 (84.679)
Epoch: [110][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4274 (2.4002)	Acc@1 58.594 (55.648)	Acc@5 85.938 (84.658)
Epoch: [110][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.7474 (2.4058)	Acc@1 52.344 (55.524)	Acc@5 82.031 (84.558)
Epoch: [110][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3780 (2.4056)	Acc@1 57.031 (55.540)	Acc@5 86.719 (84.540)
Epoch: [110][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2635 (2.4074)	Acc@1 61.719 (55.529)	Acc@5 86.719 (84.537)
Epoch: [110][370/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4603 (2.4093)	Acc@1 54.688 (55.494)	Acc@5 84.375 (84.453)
Epoch: [110][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5124 (2.4113)	Acc@1 53.125 (55.424)	Acc@5 80.469 (84.396)
Epoch: [110][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.4961 (2.4101)	Acc@1 56.250 (55.460)	Acc@5 78.750 (84.422)
num momentum params: 26
[0.1, 2.4100849778747557, 1.8963426375389099, 55.46, 50.05, tensor(0.3307, device='cuda:0', grad_fn=<DivBackward0>), 4.23724889755249, 0.34840965270996094]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [109, 32, 3, 3]
Before - module.bn2.weight: [109]
Before - module.bn2.bias: [109]
Before - module.conv3.weight: [211, 109, 3, 3]
Before - module.bn3.weight: [211]
Before - module.bn3.bias: [211]
Before - module.conv4.weight: [247, 211, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [361, 247, 3, 3]
Before - module.bn5.weight: [361]
Before - module.bn5.bias: [361]
Before - module.conv6.weight: [321, 361, 3, 3]
Before - module.bn6.weight: [321]
Before - module.bn6.bias: [321]
Before - module.conv7.weight: [207, 321, 3, 3]
Before - module.bn7.weight: [207]
Before - module.bn7.bias: [207]
Before - module.conv8.weight: [198, 207, 3, 3]
Before - module.bn8.weight: [198]
Before - module.bn8.bias: [198]
Before - module.fc.weight: [100, 198]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [109, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 109 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [211, 109, 3, 3] >> [211, 108, 3, 3]
[module.bn3.weight]: 211 >> 211
running_mean [211]
running_var [211]
num_batches_tracked []
[module.conv4.weight]: [247, 211, 3, 3] >> [247, 211, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [361, 247, 3, 3] >> [360, 247, 3, 3]
[module.bn5.weight]: 361 >> 360
running_mean [360]
running_var [360]
num_batches_tracked []
[module.conv6.weight]: [321, 361, 3, 3] >> [316, 360, 3, 3]
[module.bn6.weight]: 321 >> 316
running_mean [316]
running_var [316]
num_batches_tracked []
[module.conv7.weight]: [207, 321, 3, 3] >> [199, 316, 3, 3]
[module.bn7.weight]: 207 >> 199
running_mean [199]
running_var [199]
num_batches_tracked []
[module.conv8.weight]: [198, 207, 3, 3] >> [196, 199, 3, 3]
[module.bn8.weight]: 198 >> 196
running_mean [196]
running_var [196]
num_batches_tracked []
[module.fc.weight]: [100, 198] >> [100, 196]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [211, 108, 3, 3]
After - module.bn3.weight: [211]
After - module.bn3.bias: [211]
After - module.conv4.weight: [247, 211, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [360, 247, 3, 3]
After - module.bn5.weight: [360]
After - module.bn5.bias: [360]
After - module.conv6.weight: [316, 360, 3, 3]
After - module.bn6.weight: [316]
After - module.bn6.bias: [316]
After - module.conv7.weight: [199, 316, 3, 3]
After - module.bn7.weight: [199]
After - module.bn7.bias: [199]
After - module.conv8.weight: [196, 199, 3, 3]
After - module.bn8.weight: [196]
After - module.bn8.bias: [196]
After - module.fc.weight: [100, 196]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [211, 108, 3, 3]
conv4 --> [247, 211, 3, 3]
conv5 --> [360, 247, 3, 3]
conv6 --> [316, 360, 3, 3]
conv7 --> [199, 316, 3, 3]
conv8 --> [196, 199, 3, 3]
fc --> [196, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5985404928, 13125888, 211
4, 13688842752, 30019392, 247
5, 6965637120, 12804480, 360
6, 8911503360, 16381440, 316
7, 1738616832, 2263824, 199
8, 1078382592, 1404144, 196
fc, 7526400, 19600, 0
===================
FLOP REPORT: 16429151400000.0 40849600000.0 84866128 102124 1669 6.616001129150391
[INFO] Storing checkpoint...

Epoch: [111 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [111][0/391]	Time 0.234 (0.234)	Data 0.188 (0.188)	Loss 2.2520 (2.2520)	Acc@1 63.281 (63.281)	Acc@5 85.938 (85.938)
Epoch: [111][10/391]	Time 0.012 (0.032)	Data 0.001 (0.018)	Loss 2.5140 (2.2371)	Acc@1 55.469 (61.151)	Acc@5 82.031 (87.287)
Epoch: [111][20/391]	Time 0.011 (0.022)	Data 0.001 (0.010)	Loss 2.3665 (2.2587)	Acc@1 58.594 (59.896)	Acc@5 85.156 (86.868)
Epoch: [111][30/391]	Time 0.011 (0.018)	Data 0.001 (0.007)	Loss 2.7126 (2.2840)	Acc@1 53.906 (59.425)	Acc@5 74.219 (86.089)
Epoch: [111][40/391]	Time 0.010 (0.017)	Data 0.001 (0.006)	Loss 2.2575 (2.3097)	Acc@1 60.938 (58.232)	Acc@5 87.500 (85.823)
Epoch: [111][50/391]	Time 0.010 (0.015)	Data 0.001 (0.005)	Loss 2.3692 (2.3143)	Acc@1 57.812 (58.104)	Acc@5 88.281 (85.846)
Epoch: [111][60/391]	Time 0.010 (0.015)	Data 0.001 (0.005)	Loss 2.6637 (2.3131)	Acc@1 46.875 (57.889)	Acc@5 77.344 (85.861)
Epoch: [111][70/391]	Time 0.011 (0.014)	Data 0.001 (0.004)	Loss 2.5208 (2.3241)	Acc@1 46.875 (57.427)	Acc@5 85.156 (85.772)
Epoch: [111][80/391]	Time 0.010 (0.014)	Data 0.002 (0.004)	Loss 2.7392 (2.3359)	Acc@1 45.312 (57.350)	Acc@5 79.688 (85.542)
Epoch: [111][90/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 2.1969 (2.3498)	Acc@1 59.375 (56.877)	Acc@5 90.625 (85.276)
Epoch: [111][100/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.3268 (2.3553)	Acc@1 55.469 (56.830)	Acc@5 88.281 (85.311)
Epoch: [111][110/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.5324 (2.3596)	Acc@1 50.000 (56.722)	Acc@5 85.156 (85.213)
Epoch: [111][120/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.4743 (2.3585)	Acc@1 56.250 (56.799)	Acc@5 82.812 (85.214)
Epoch: [111][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4066 (2.3646)	Acc@1 57.812 (56.578)	Acc@5 84.375 (85.222)
Epoch: [111][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3033 (2.3667)	Acc@1 57.031 (56.488)	Acc@5 86.719 (85.123)
Epoch: [111][150/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4698 (2.3690)	Acc@1 60.156 (56.540)	Acc@5 78.906 (85.011)
Epoch: [111][160/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3277 (2.3758)	Acc@1 57.031 (56.371)	Acc@5 86.719 (84.904)
Epoch: [111][170/391]	Time 0.019 (0.012)	Data 0.001 (0.003)	Loss 2.3852 (2.3810)	Acc@1 53.906 (56.309)	Acc@5 85.938 (84.800)
Epoch: [111][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3213 (2.3838)	Acc@1 62.500 (56.177)	Acc@5 85.156 (84.785)
Epoch: [111][190/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3603 (2.3875)	Acc@1 57.031 (56.037)	Acc@5 87.500 (84.747)
Epoch: [111][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.1348 (2.3876)	Acc@1 59.375 (56.009)	Acc@5 88.281 (84.775)
Epoch: [111][210/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.7486 (2.3871)	Acc@1 51.562 (56.087)	Acc@5 79.688 (84.816)
Epoch: [111][220/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5210 (2.3882)	Acc@1 54.688 (56.126)	Acc@5 85.156 (84.827)
Epoch: [111][230/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3867 (2.3892)	Acc@1 59.375 (56.034)	Acc@5 81.250 (84.842)
Epoch: [111][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2482 (2.3906)	Acc@1 61.719 (56.020)	Acc@5 88.281 (84.819)
Epoch: [111][250/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.6320 (2.3934)	Acc@1 50.000 (55.945)	Acc@5 76.562 (84.730)
Epoch: [111][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.5691 (2.3940)	Acc@1 49.219 (55.933)	Acc@5 83.594 (84.740)
Epoch: [111][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7869 (2.3969)	Acc@1 48.438 (55.907)	Acc@5 71.875 (84.678)
Epoch: [111][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4487 (2.3989)	Acc@1 52.344 (55.875)	Acc@5 82.812 (84.620)
Epoch: [111][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7365 (2.3984)	Acc@1 43.750 (55.837)	Acc@5 79.688 (84.635)
Epoch: [111][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4545 (2.3998)	Acc@1 54.688 (55.817)	Acc@5 82.031 (84.598)
Epoch: [111][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3374 (2.3987)	Acc@1 57.031 (55.793)	Acc@5 82.031 (84.626)
Epoch: [111][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3564 (2.4014)	Acc@1 60.938 (55.746)	Acc@5 82.031 (84.558)
Epoch: [111][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4252 (2.4046)	Acc@1 54.688 (55.643)	Acc@5 82.812 (84.524)
Epoch: [111][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5771 (2.4060)	Acc@1 53.906 (55.590)	Acc@5 78.906 (84.517)
Epoch: [111][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4738 (2.4062)	Acc@1 53.906 (55.587)	Acc@5 83.594 (84.515)
Epoch: [111][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4993 (2.4072)	Acc@1 51.562 (55.551)	Acc@5 79.688 (84.501)
Epoch: [111][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3064 (2.4069)	Acc@1 54.688 (55.536)	Acc@5 90.625 (84.516)
Epoch: [111][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3293 (2.4056)	Acc@1 58.594 (55.561)	Acc@5 85.938 (84.535)
Epoch: [111][390/391]	Time 0.106 (0.011)	Data 0.001 (0.002)	Loss 2.1915 (2.4052)	Acc@1 57.500 (55.572)	Acc@5 88.750 (84.508)
num momentum params: 26
[0.1, 2.4052412197875976, 1.9509723830223082, 55.572, 49.14, tensor(0.3315, device='cuda:0', grad_fn=<DivBackward0>), 4.494916200637817, 0.3930468559265137]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [112 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [112][0/391]	Time 0.056 (0.056)	Data 0.181 (0.181)	Loss 2.2282 (2.2282)	Acc@1 55.469 (55.469)	Acc@5 86.719 (86.719)
Epoch: [112][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.3348 (2.2878)	Acc@1 59.375 (58.807)	Acc@5 84.375 (86.364)
Epoch: [112][20/391]	Time 0.011 (0.013)	Data 0.001 (0.010)	Loss 2.1303 (2.2900)	Acc@1 60.938 (58.705)	Acc@5 87.500 (85.826)
Epoch: [112][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.0995 (2.2822)	Acc@1 58.594 (58.090)	Acc@5 92.188 (86.316)
Epoch: [112][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.4434 (2.2976)	Acc@1 50.000 (57.698)	Acc@5 84.375 (86.071)
Epoch: [112][50/391]	Time 0.010 (0.012)	Data 0.002 (0.005)	Loss 2.2721 (2.2847)	Acc@1 53.906 (57.751)	Acc@5 87.500 (86.259)
Epoch: [112][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4920 (2.3046)	Acc@1 56.250 (57.646)	Acc@5 82.031 (86.168)
Epoch: [112][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.5080 (2.3069)	Acc@1 57.031 (57.757)	Acc@5 82.812 (86.103)
Epoch: [112][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2992 (2.3177)	Acc@1 54.688 (57.465)	Acc@5 85.156 (85.889)
Epoch: [112][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6021 (2.3404)	Acc@1 50.781 (56.971)	Acc@5 79.688 (85.560)
Epoch: [112][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2803 (2.3455)	Acc@1 62.500 (56.683)	Acc@5 88.281 (85.543)
Epoch: [112][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2047 (2.3518)	Acc@1 60.156 (56.539)	Acc@5 87.500 (85.367)
Epoch: [112][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2841 (2.3536)	Acc@1 57.812 (56.515)	Acc@5 88.281 (85.421)
Epoch: [112][130/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.1525 (2.3549)	Acc@1 62.500 (56.512)	Acc@5 86.719 (85.377)
Epoch: [112][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2550 (2.3580)	Acc@1 59.375 (56.444)	Acc@5 85.938 (85.322)
Epoch: [112][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5807 (2.3645)	Acc@1 52.344 (56.343)	Acc@5 84.375 (85.244)
Epoch: [112][160/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3138 (2.3649)	Acc@1 60.156 (56.415)	Acc@5 89.062 (85.205)
Epoch: [112][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3813 (2.3651)	Acc@1 55.469 (56.337)	Acc@5 83.594 (85.280)
Epoch: [112][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6311 (2.3690)	Acc@1 46.094 (56.272)	Acc@5 81.250 (85.217)
Epoch: [112][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5374 (2.3745)	Acc@1 53.125 (56.197)	Acc@5 81.250 (85.144)
Epoch: [112][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4455 (2.3754)	Acc@1 58.594 (56.161)	Acc@5 82.812 (85.168)
Epoch: [112][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5976 (2.3793)	Acc@1 52.344 (56.117)	Acc@5 78.906 (85.053)
Epoch: [112][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5408 (2.3819)	Acc@1 47.656 (55.995)	Acc@5 85.938 (85.029)
Epoch: [112][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1455 (2.3846)	Acc@1 60.156 (55.949)	Acc@5 92.969 (84.953)
Epoch: [112][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3943 (2.3896)	Acc@1 54.688 (55.838)	Acc@5 82.812 (84.920)
Epoch: [112][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3529 (2.3935)	Acc@1 58.594 (55.802)	Acc@5 83.594 (84.901)
Epoch: [112][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1088 (2.3930)	Acc@1 60.156 (55.825)	Acc@5 90.625 (84.899)
Epoch: [112][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2672 (2.3909)	Acc@1 56.250 (55.927)	Acc@5 88.281 (84.882)
Epoch: [112][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5924 (2.3909)	Acc@1 46.875 (55.891)	Acc@5 84.375 (84.912)
Epoch: [112][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2655 (2.3932)	Acc@1 60.938 (55.858)	Acc@5 88.281 (84.853)
Epoch: [112][300/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3920 (2.3934)	Acc@1 55.469 (55.876)	Acc@5 82.031 (84.811)
Epoch: [112][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4385 (2.3931)	Acc@1 53.125 (55.856)	Acc@5 82.812 (84.800)
Epoch: [112][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3681 (2.3928)	Acc@1 55.469 (55.887)	Acc@5 82.812 (84.786)
Epoch: [112][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2287 (2.3941)	Acc@1 62.500 (55.884)	Acc@5 84.375 (84.772)
Epoch: [112][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6070 (2.3968)	Acc@1 50.781 (55.785)	Acc@5 81.250 (84.753)
Epoch: [112][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3136 (2.3968)	Acc@1 57.031 (55.789)	Acc@5 88.281 (84.767)
Epoch: [112][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7465 (2.3987)	Acc@1 47.656 (55.789)	Acc@5 75.781 (84.721)
Epoch: [112][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 3.0213 (2.4020)	Acc@1 42.969 (55.764)	Acc@5 73.438 (84.657)
Epoch: [112][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5868 (2.4032)	Acc@1 48.438 (55.727)	Acc@5 83.594 (84.650)
Epoch: [112][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.5970 (2.4032)	Acc@1 51.250 (55.748)	Acc@5 80.000 (84.630)
num momentum params: 26
[0.1, 2.4031758627319335, 1.886701490879059, 55.748, 49.98, tensor(0.3319, device='cuda:0', grad_fn=<DivBackward0>), 4.23536229133606, 0.3416297435760498]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [113 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [113][0/391]	Time 0.054 (0.054)	Data 0.195 (0.195)	Loss 2.1427 (2.1427)	Acc@1 65.625 (65.625)	Acc@5 86.719 (86.719)
Epoch: [113][10/391]	Time 0.013 (0.016)	Data 0.002 (0.019)	Loss 2.3799 (2.3646)	Acc@1 54.688 (55.895)	Acc@5 85.938 (84.588)
Epoch: [113][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.4199 (2.3417)	Acc@1 50.000 (56.622)	Acc@5 88.281 (85.268)
Epoch: [113][30/391]	Time 0.010 (0.013)	Data 0.002 (0.008)	Loss 2.2188 (2.3285)	Acc@1 60.938 (57.132)	Acc@5 85.938 (85.786)
Epoch: [113][40/391]	Time 0.011 (0.012)	Data 0.005 (0.006)	Loss 2.2353 (2.3104)	Acc@1 65.625 (57.717)	Acc@5 85.156 (85.766)
Epoch: [113][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3319 (2.3202)	Acc@1 56.250 (57.338)	Acc@5 82.812 (85.723)
Epoch: [113][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4451 (2.3347)	Acc@1 53.906 (56.890)	Acc@5 86.719 (85.489)
Epoch: [113][70/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 2.4415 (2.3475)	Acc@1 53.125 (56.778)	Acc@5 84.375 (85.310)
Epoch: [113][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3959 (2.3420)	Acc@1 54.688 (56.954)	Acc@5 79.688 (85.233)
Epoch: [113][90/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4896 (2.3449)	Acc@1 57.031 (56.885)	Acc@5 79.688 (85.088)
Epoch: [113][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2546 (2.3391)	Acc@1 57.812 (57.008)	Acc@5 86.719 (85.218)
Epoch: [113][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.7448 (2.3467)	Acc@1 49.219 (56.806)	Acc@5 82.031 (85.156)
Epoch: [113][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3548 (2.3489)	Acc@1 59.375 (56.850)	Acc@5 85.938 (85.214)
Epoch: [113][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4032 (2.3494)	Acc@1 55.469 (56.870)	Acc@5 83.594 (85.234)
Epoch: [113][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6266 (2.3551)	Acc@1 50.000 (56.754)	Acc@5 77.344 (85.228)
Epoch: [113][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3550 (2.3541)	Acc@1 56.250 (56.705)	Acc@5 85.156 (85.301)
Epoch: [113][160/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.6702 (2.3627)	Acc@1 50.000 (56.439)	Acc@5 80.469 (85.166)
Epoch: [113][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5777 (2.3654)	Acc@1 52.344 (56.351)	Acc@5 80.469 (85.106)
Epoch: [113][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3338 (2.3693)	Acc@1 56.250 (56.233)	Acc@5 89.844 (85.096)
Epoch: [113][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2336 (2.3741)	Acc@1 59.375 (56.197)	Acc@5 88.281 (85.009)
Epoch: [113][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7061 (2.3843)	Acc@1 47.656 (55.970)	Acc@5 82.812 (84.810)
Epoch: [113][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3307 (2.3838)	Acc@1 54.688 (55.932)	Acc@5 85.156 (84.842)
Epoch: [113][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2914 (2.3807)	Acc@1 55.469 (55.978)	Acc@5 87.500 (84.912)
Epoch: [113][230/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.0282 (2.3784)	Acc@1 68.750 (56.064)	Acc@5 89.844 (84.933)
Epoch: [113][240/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.0780 (2.3815)	Acc@1 64.844 (56.085)	Acc@5 88.281 (84.910)
Epoch: [113][250/391]	Time 0.009 (0.011)	Data 0.011 (0.002)	Loss 2.6018 (2.3819)	Acc@1 50.781 (56.094)	Acc@5 81.250 (84.901)
Epoch: [113][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3349 (2.3814)	Acc@1 56.250 (56.073)	Acc@5 82.812 (84.890)
Epoch: [113][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4192 (2.3820)	Acc@1 52.344 (56.037)	Acc@5 82.812 (84.868)
Epoch: [113][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5098 (2.3835)	Acc@1 59.375 (56.069)	Acc@5 82.031 (84.850)
Epoch: [113][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6091 (2.3859)	Acc@1 54.688 (56.008)	Acc@5 77.344 (84.805)
Epoch: [113][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5030 (2.3871)	Acc@1 53.906 (55.996)	Acc@5 84.375 (84.808)
Epoch: [113][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3167 (2.3875)	Acc@1 57.031 (56.014)	Acc@5 85.938 (84.787)
Epoch: [113][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4379 (2.3899)	Acc@1 53.125 (55.958)	Acc@5 82.812 (84.747)
Epoch: [113][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1757 (2.3902)	Acc@1 58.594 (55.917)	Acc@5 87.500 (84.729)
Epoch: [113][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6428 (2.3904)	Acc@1 55.469 (55.938)	Acc@5 73.438 (84.735)
Epoch: [113][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4145 (2.3902)	Acc@1 55.469 (55.947)	Acc@5 82.812 (84.740)
Epoch: [113][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3411 (2.3901)	Acc@1 57.031 (55.923)	Acc@5 85.938 (84.743)
Epoch: [113][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5141 (2.3907)	Acc@1 50.781 (55.854)	Acc@5 83.594 (84.756)
Epoch: [113][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4731 (2.3921)	Acc@1 57.031 (55.807)	Acc@5 82.812 (84.779)
Epoch: [113][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.7743 (2.3920)	Acc@1 43.750 (55.810)	Acc@5 76.250 (84.778)
num momentum params: 26
[0.1, 2.39200844291687, 1.9338250303268432, 55.81, 49.58, tensor(0.3328, device='cuda:0', grad_fn=<DivBackward0>), 4.214877128601074, 0.33838343620300293]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [114 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [114][0/391]	Time 0.055 (0.055)	Data 0.173 (0.173)	Loss 2.3943 (2.3943)	Acc@1 54.688 (54.688)	Acc@5 86.719 (86.719)
Epoch: [114][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.0601 (2.3130)	Acc@1 64.062 (58.168)	Acc@5 92.188 (86.577)
Epoch: [114][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.1248 (2.2996)	Acc@1 59.375 (58.408)	Acc@5 88.281 (86.793)
Epoch: [114][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.3823 (2.3122)	Acc@1 54.688 (58.745)	Acc@5 85.938 (86.215)
Epoch: [114][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.1458 (2.3195)	Acc@1 61.719 (58.289)	Acc@5 88.281 (85.976)
Epoch: [114][50/391]	Time 0.015 (0.012)	Data 0.001 (0.005)	Loss 2.3714 (2.3224)	Acc@1 55.469 (57.874)	Acc@5 82.031 (85.830)
Epoch: [114][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3707 (2.3189)	Acc@1 53.906 (57.659)	Acc@5 83.594 (85.899)
Epoch: [114][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.7015 (2.3292)	Acc@1 50.781 (57.427)	Acc@5 76.562 (85.772)
Epoch: [114][80/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.4038 (2.3317)	Acc@1 57.812 (57.263)	Acc@5 85.156 (85.716)
Epoch: [114][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5687 (2.3433)	Acc@1 55.469 (57.126)	Acc@5 85.938 (85.603)
Epoch: [114][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4765 (2.3522)	Acc@1 54.688 (57.008)	Acc@5 85.938 (85.473)
Epoch: [114][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4278 (2.3525)	Acc@1 55.469 (57.045)	Acc@5 85.938 (85.501)
Epoch: [114][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3728 (2.3508)	Acc@1 55.469 (56.960)	Acc@5 88.281 (85.544)
Epoch: [114][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.6214 (2.3579)	Acc@1 50.781 (56.739)	Acc@5 82.812 (85.478)
Epoch: [114][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5735 (2.3563)	Acc@1 46.094 (56.776)	Acc@5 84.375 (85.478)
Epoch: [114][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0399 (2.3589)	Acc@1 65.625 (56.679)	Acc@5 89.844 (85.477)
Epoch: [114][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1236 (2.3604)	Acc@1 56.250 (56.580)	Acc@5 90.625 (85.525)
Epoch: [114][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3784 (2.3604)	Acc@1 58.594 (56.615)	Acc@5 82.812 (85.503)
Epoch: [114][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2238 (2.3644)	Acc@1 60.938 (56.457)	Acc@5 89.844 (85.489)
Epoch: [114][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1581 (2.3667)	Acc@1 65.625 (56.446)	Acc@5 89.062 (85.414)
Epoch: [114][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4689 (2.3709)	Acc@1 53.125 (56.367)	Acc@5 84.375 (85.300)
Epoch: [114][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2290 (2.3726)	Acc@1 57.812 (56.309)	Acc@5 89.062 (85.293)
Epoch: [114][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0840 (2.3710)	Acc@1 65.625 (56.303)	Acc@5 87.500 (85.361)
Epoch: [114][230/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.4446 (2.3706)	Acc@1 56.250 (56.304)	Acc@5 87.500 (85.363)
Epoch: [114][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3754 (2.3741)	Acc@1 54.688 (56.266)	Acc@5 87.500 (85.283)
Epoch: [114][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3642 (2.3758)	Acc@1 57.031 (56.259)	Acc@5 85.938 (85.247)
Epoch: [114][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3025 (2.3760)	Acc@1 60.156 (56.232)	Acc@5 89.062 (85.264)
Epoch: [114][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3207 (2.3776)	Acc@1 58.594 (56.218)	Acc@5 85.938 (85.191)
Epoch: [114][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3693 (2.3793)	Acc@1 55.469 (56.142)	Acc@5 86.719 (85.156)
Epoch: [114][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7668 (2.3835)	Acc@1 45.312 (56.065)	Acc@5 78.906 (85.022)
Epoch: [114][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5948 (2.3877)	Acc@1 53.906 (56.014)	Acc@5 76.562 (84.969)
Epoch: [114][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4437 (2.3888)	Acc@1 50.781 (55.949)	Acc@5 82.812 (84.943)
Epoch: [114][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3782 (2.3899)	Acc@1 57.812 (55.960)	Acc@5 81.250 (84.893)
Epoch: [114][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4872 (2.3878)	Acc@1 55.469 (55.976)	Acc@5 84.375 (84.944)
Epoch: [114][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3831 (2.3920)	Acc@1 54.688 (55.861)	Acc@5 85.938 (84.881)
Epoch: [114][350/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.2934 (2.3945)	Acc@1 60.156 (55.789)	Acc@5 85.938 (84.805)
Epoch: [114][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3331 (2.3971)	Acc@1 61.719 (55.722)	Acc@5 83.594 (84.745)
Epoch: [114][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2403 (2.3975)	Acc@1 62.500 (55.730)	Acc@5 86.719 (84.737)
Epoch: [114][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4093 (2.4007)	Acc@1 54.688 (55.651)	Acc@5 85.156 (84.701)
Epoch: [114][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.1540 (2.4033)	Acc@1 62.500 (55.558)	Acc@5 86.250 (84.634)
num momentum params: 26
[0.1, 2.4032951097106934, 1.9958436393737793, 55.558, 47.36, tensor(0.3312, device='cuda:0', grad_fn=<DivBackward0>), 4.233414649963379, 0.3295435905456543]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [115 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [115][0/391]	Time 0.062 (0.062)	Data 0.179 (0.179)	Loss 2.2226 (2.2226)	Acc@1 54.688 (54.688)	Acc@5 88.281 (88.281)
Epoch: [115][10/391]	Time 0.012 (0.016)	Data 0.001 (0.018)	Loss 2.2174 (2.3286)	Acc@1 57.031 (56.250)	Acc@5 89.844 (85.440)
Epoch: [115][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.0219 (2.3433)	Acc@1 67.188 (56.585)	Acc@5 89.844 (85.454)
Epoch: [115][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.0938 (2.3292)	Acc@1 62.500 (57.182)	Acc@5 87.500 (85.811)
Epoch: [115][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.4228 (2.3307)	Acc@1 52.344 (57.031)	Acc@5 83.594 (85.671)
Epoch: [115][50/391]	Time 0.012 (0.012)	Data 0.002 (0.005)	Loss 2.4295 (2.3474)	Acc@1 53.125 (56.817)	Acc@5 83.594 (85.371)
Epoch: [115][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4169 (2.3593)	Acc@1 58.594 (56.698)	Acc@5 83.594 (85.195)
Epoch: [115][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3654 (2.3528)	Acc@1 53.906 (56.734)	Acc@5 81.250 (85.255)
Epoch: [115][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4538 (2.3483)	Acc@1 50.000 (56.761)	Acc@5 85.938 (85.368)
Epoch: [115][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4489 (2.3475)	Acc@1 53.906 (56.842)	Acc@5 82.031 (85.337)
Epoch: [115][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3655 (2.3494)	Acc@1 53.125 (56.853)	Acc@5 85.156 (85.303)
Epoch: [115][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3994 (2.3456)	Acc@1 57.031 (56.947)	Acc@5 84.375 (85.431)
Epoch: [115][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3798 (2.3614)	Acc@1 57.812 (56.612)	Acc@5 82.812 (85.182)
Epoch: [115][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3710 (2.3664)	Acc@1 54.688 (56.459)	Acc@5 89.062 (85.043)
Epoch: [115][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3404 (2.3669)	Acc@1 53.906 (56.483)	Acc@5 89.062 (85.145)
Epoch: [115][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2885 (2.3717)	Acc@1 59.375 (56.431)	Acc@5 83.594 (85.058)
Epoch: [115][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2122 (2.3724)	Acc@1 62.500 (56.434)	Acc@5 87.500 (84.977)
Epoch: [115][170/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.8792 (2.3796)	Acc@1 45.312 (56.191)	Acc@5 74.219 (84.864)
Epoch: [115][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3539 (2.3821)	Acc@1 58.594 (56.133)	Acc@5 85.156 (84.850)
Epoch: [115][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4369 (2.3862)	Acc@1 57.812 (56.062)	Acc@5 80.469 (84.755)
Epoch: [115][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7149 (2.3895)	Acc@1 51.562 (55.966)	Acc@5 79.688 (84.740)
Epoch: [115][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5797 (2.3931)	Acc@1 53.906 (55.909)	Acc@5 83.594 (84.712)
Epoch: [115][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3716 (2.3953)	Acc@1 52.344 (55.829)	Acc@5 85.938 (84.665)
Epoch: [115][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5595 (2.3948)	Acc@1 47.656 (55.783)	Acc@5 82.812 (84.646)
Epoch: [115][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2295 (2.3949)	Acc@1 61.719 (55.799)	Acc@5 85.938 (84.644)
Epoch: [115][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1353 (2.3935)	Acc@1 63.281 (55.911)	Acc@5 89.062 (84.661)
Epoch: [115][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8292 (2.3974)	Acc@1 46.875 (55.828)	Acc@5 82.812 (84.576)
Epoch: [115][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4990 (2.3971)	Acc@1 56.250 (55.829)	Acc@5 82.031 (84.591)
Epoch: [115][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5032 (2.3978)	Acc@1 52.344 (55.839)	Acc@5 84.375 (84.611)
Epoch: [115][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5353 (2.4007)	Acc@1 53.125 (55.721)	Acc@5 81.250 (84.566)
Epoch: [115][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5590 (2.4052)	Acc@1 51.562 (55.619)	Acc@5 83.594 (84.497)
Epoch: [115][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5316 (2.4086)	Acc@1 56.250 (55.554)	Acc@5 79.688 (84.428)
Epoch: [115][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5689 (2.4093)	Acc@1 54.688 (55.549)	Acc@5 77.344 (84.399)
Epoch: [115][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2934 (2.4082)	Acc@1 54.688 (55.591)	Acc@5 86.719 (84.453)
Epoch: [115][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3634 (2.4106)	Acc@1 58.594 (55.521)	Acc@5 84.375 (84.402)
Epoch: [115][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5507 (2.4121)	Acc@1 50.000 (55.500)	Acc@5 82.031 (84.375)
Epoch: [115][360/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.0498 (2.4128)	Acc@1 66.406 (55.508)	Acc@5 89.844 (84.351)
Epoch: [115][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1403 (2.4122)	Acc@1 64.062 (55.540)	Acc@5 90.625 (84.364)
Epoch: [115][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7806 (2.4132)	Acc@1 52.344 (55.534)	Acc@5 74.219 (84.313)
Epoch: [115][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.5019 (2.4140)	Acc@1 52.500 (55.540)	Acc@5 78.750 (84.278)
num momentum params: 26
[0.1, 2.414047420349121, 1.8758624970912934, 55.54, 50.22, tensor(0.3301, device='cuda:0', grad_fn=<DivBackward0>), 4.238724231719971, 0.3284482955932617]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [116 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [116][0/391]	Time 0.052 (0.052)	Data 0.182 (0.182)	Loss 2.2878 (2.2878)	Acc@1 57.031 (57.031)	Acc@5 84.375 (84.375)
Epoch: [116][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 2.3059 (2.3205)	Acc@1 53.906 (56.534)	Acc@5 84.375 (86.151)
Epoch: [116][20/391]	Time 0.011 (0.013)	Data 0.002 (0.010)	Loss 2.3447 (2.3099)	Acc@1 54.688 (57.292)	Acc@5 84.375 (86.421)
Epoch: [116][30/391]	Time 0.010 (0.012)	Data 0.001 (0.007)	Loss 2.4703 (2.3180)	Acc@1 53.906 (57.208)	Acc@5 77.344 (85.534)
Epoch: [116][40/391]	Time 0.013 (0.012)	Data 0.003 (0.006)	Loss 2.3197 (2.2937)	Acc@1 58.594 (58.022)	Acc@5 89.844 (85.938)
Epoch: [116][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4552 (2.3025)	Acc@1 54.688 (57.690)	Acc@5 84.375 (86.045)
Epoch: [116][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.0207 (2.3018)	Acc@1 63.281 (57.697)	Acc@5 92.188 (86.130)
Epoch: [116][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3647 (2.3150)	Acc@1 59.375 (57.526)	Acc@5 82.031 (85.860)
Epoch: [116][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4835 (2.3265)	Acc@1 54.688 (57.378)	Acc@5 85.156 (85.735)
Epoch: [116][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5133 (2.3342)	Acc@1 51.562 (57.186)	Acc@5 81.250 (85.491)
Epoch: [116][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1356 (2.3423)	Acc@1 64.062 (57.062)	Acc@5 84.375 (85.203)
Epoch: [116][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1902 (2.3455)	Acc@1 63.281 (57.130)	Acc@5 85.938 (85.234)
Epoch: [116][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1987 (2.3447)	Acc@1 61.719 (57.135)	Acc@5 86.719 (85.240)
Epoch: [116][130/391]	Time 0.012 (0.011)	Data 0.002 (0.003)	Loss 2.3375 (2.3490)	Acc@1 57.031 (56.942)	Acc@5 83.594 (85.192)
Epoch: [116][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5110 (2.3532)	Acc@1 53.906 (56.932)	Acc@5 78.906 (85.129)
Epoch: [116][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1433 (2.3543)	Acc@1 59.375 (56.850)	Acc@5 87.500 (85.141)
Epoch: [116][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.9677 (2.3514)	Acc@1 64.062 (57.012)	Acc@5 92.188 (85.103)
Epoch: [116][170/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.4524 (2.3523)	Acc@1 53.125 (57.018)	Acc@5 82.031 (85.056)
Epoch: [116][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3891 (2.3585)	Acc@1 55.469 (56.837)	Acc@5 85.156 (84.979)
Epoch: [116][190/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.3785 (2.3584)	Acc@1 57.812 (56.806)	Acc@5 83.594 (84.927)
Epoch: [116][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2941 (2.3584)	Acc@1 56.250 (56.821)	Acc@5 85.938 (84.950)
Epoch: [116][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3418 (2.3623)	Acc@1 55.469 (56.691)	Acc@5 82.812 (84.816)
Epoch: [116][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4151 (2.3659)	Acc@1 56.250 (56.554)	Acc@5 81.250 (84.771)
Epoch: [116][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3482 (2.3688)	Acc@1 55.469 (56.477)	Acc@5 89.062 (84.761)
Epoch: [116][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3561 (2.3706)	Acc@1 55.469 (56.419)	Acc@5 87.500 (84.751)
Epoch: [116][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7457 (2.3734)	Acc@1 49.219 (56.368)	Acc@5 75.000 (84.702)
Epoch: [116][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5629 (2.3757)	Acc@1 53.125 (56.292)	Acc@5 84.375 (84.683)
Epoch: [116][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3925 (2.3777)	Acc@1 60.938 (56.259)	Acc@5 81.250 (84.637)
Epoch: [116][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5373 (2.3796)	Acc@1 51.562 (56.200)	Acc@5 84.375 (84.634)
Epoch: [116][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.7057 (2.3800)	Acc@1 51.562 (56.194)	Acc@5 75.781 (84.601)
Epoch: [116][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2678 (2.3813)	Acc@1 59.375 (56.193)	Acc@5 87.500 (84.609)
Epoch: [116][310/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.5833 (2.3841)	Acc@1 55.469 (56.137)	Acc@5 85.156 (84.601)
Epoch: [116][320/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4166 (2.3863)	Acc@1 51.562 (56.055)	Acc@5 85.156 (84.558)
Epoch: [116][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3528 (2.3874)	Acc@1 57.031 (56.002)	Acc@5 88.281 (84.540)
Epoch: [116][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5445 (2.3900)	Acc@1 56.250 (55.957)	Acc@5 78.906 (84.510)
Epoch: [116][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5236 (2.3922)	Acc@1 53.125 (55.905)	Acc@5 83.594 (84.475)
Epoch: [116][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6590 (2.3951)	Acc@1 44.531 (55.837)	Acc@5 82.031 (84.429)
Epoch: [116][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4585 (2.3975)	Acc@1 49.219 (55.755)	Acc@5 82.031 (84.360)
Epoch: [116][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2148 (2.3971)	Acc@1 54.688 (55.774)	Acc@5 89.844 (84.375)
Epoch: [116][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.8926 (2.3985)	Acc@1 50.000 (55.774)	Acc@5 68.750 (84.324)
num momentum params: 26
[0.1, 2.3985128286743165, 2.0339794635772703, 55.774, 47.54, tensor(0.3319, device='cuda:0', grad_fn=<DivBackward0>), 4.259047985076904, 0.336193323135376]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [117 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [117][0/391]	Time 0.057 (0.057)	Data 0.182 (0.182)	Loss 2.5882 (2.5882)	Acc@1 53.125 (53.125)	Acc@5 80.469 (80.469)
Epoch: [117][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 2.3932 (2.3765)	Acc@1 59.375 (56.250)	Acc@5 86.719 (85.866)
Epoch: [117][20/391]	Time 0.012 (0.013)	Data 0.001 (0.010)	Loss 2.4570 (2.3558)	Acc@1 50.781 (56.845)	Acc@5 83.594 (85.305)
Epoch: [117][30/391]	Time 0.011 (0.012)	Data 0.001 (0.007)	Loss 2.0982 (2.3227)	Acc@1 60.938 (57.737)	Acc@5 91.406 (85.862)
Epoch: [117][40/391]	Time 0.013 (0.012)	Data 0.001 (0.006)	Loss 2.5323 (2.3239)	Acc@1 51.562 (57.412)	Acc@5 82.031 (85.976)
Epoch: [117][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2890 (2.3139)	Acc@1 58.594 (57.904)	Acc@5 86.719 (86.075)
Epoch: [117][60/391]	Time 0.009 (0.012)	Data 0.001 (0.004)	Loss 2.3252 (2.3229)	Acc@1 53.906 (57.761)	Acc@5 87.500 (86.027)
Epoch: [117][70/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2410 (2.3316)	Acc@1 55.469 (57.548)	Acc@5 89.062 (85.926)
Epoch: [117][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5164 (2.3401)	Acc@1 53.125 (57.253)	Acc@5 81.250 (85.696)
Epoch: [117][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.1460 (2.3449)	Acc@1 64.062 (57.151)	Acc@5 87.500 (85.440)
Epoch: [117][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2492 (2.3491)	Acc@1 62.500 (57.093)	Acc@5 86.719 (85.334)
Epoch: [117][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4611 (2.3532)	Acc@1 57.812 (57.066)	Acc@5 80.469 (85.297)
Epoch: [117][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3051 (2.3544)	Acc@1 58.594 (57.051)	Acc@5 83.594 (85.124)
Epoch: [117][130/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.3247 (2.3522)	Acc@1 59.375 (57.139)	Acc@5 83.594 (85.198)
Epoch: [117][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.0825 (2.3501)	Acc@1 57.812 (57.159)	Acc@5 90.625 (85.239)
Epoch: [117][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3593 (2.3565)	Acc@1 58.594 (56.897)	Acc@5 82.812 (85.187)
Epoch: [117][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3543 (2.3594)	Acc@1 57.031 (56.818)	Acc@5 82.812 (85.171)
Epoch: [117][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5705 (2.3651)	Acc@1 54.688 (56.721)	Acc@5 78.125 (85.111)
Epoch: [117][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2425 (2.3668)	Acc@1 60.938 (56.604)	Acc@5 86.719 (85.117)
Epoch: [117][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3530 (2.3658)	Acc@1 60.156 (56.585)	Acc@5 84.375 (85.136)
Epoch: [117][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4457 (2.3644)	Acc@1 56.250 (56.643)	Acc@5 82.031 (85.137)
Epoch: [117][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3827 (2.3647)	Acc@1 54.688 (56.631)	Acc@5 86.719 (85.119)
Epoch: [117][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3640 (2.3661)	Acc@1 54.688 (56.596)	Acc@5 83.594 (85.089)
Epoch: [117][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5133 (2.3664)	Acc@1 51.562 (56.571)	Acc@5 80.469 (85.106)
Epoch: [117][240/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2618 (2.3684)	Acc@1 57.031 (56.532)	Acc@5 85.156 (85.088)
Epoch: [117][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4234 (2.3731)	Acc@1 54.688 (56.403)	Acc@5 86.719 (85.050)
Epoch: [117][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6626 (2.3753)	Acc@1 49.219 (56.346)	Acc@5 78.906 (85.054)
Epoch: [117][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7096 (2.3831)	Acc@1 52.344 (56.192)	Acc@5 81.250 (84.905)
Epoch: [117][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6461 (2.3848)	Acc@1 49.219 (56.108)	Acc@5 79.688 (84.903)
Epoch: [117][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4273 (2.3860)	Acc@1 55.469 (56.065)	Acc@5 85.938 (84.890)
Epoch: [117][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2504 (2.3865)	Acc@1 60.938 (56.068)	Acc@5 84.375 (84.904)
Epoch: [117][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4549 (2.3890)	Acc@1 53.906 (56.029)	Acc@5 82.031 (84.847)
Epoch: [117][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5630 (2.3866)	Acc@1 44.531 (56.104)	Acc@5 85.938 (84.901)
Epoch: [117][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5337 (2.3883)	Acc@1 49.219 (56.023)	Acc@5 80.469 (84.894)
Epoch: [117][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4296 (2.3870)	Acc@1 53.906 (56.046)	Acc@5 85.156 (84.927)
Epoch: [117][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4441 (2.3888)	Acc@1 47.656 (56.005)	Acc@5 85.156 (84.896)
Epoch: [117][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7576 (2.3912)	Acc@1 46.094 (55.923)	Acc@5 82.031 (84.847)
Epoch: [117][370/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.8541 (2.3964)	Acc@1 47.656 (55.818)	Acc@5 76.562 (84.725)
Epoch: [117][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6204 (2.3990)	Acc@1 55.469 (55.756)	Acc@5 83.594 (84.711)
Epoch: [117][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.4907 (2.3988)	Acc@1 52.500 (55.756)	Acc@5 85.000 (84.688)
num momentum params: 26
[0.1, 2.3988254919433594, 2.139011105298996, 55.756, 44.99, tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>), 4.249541997909546, 0.3366057872772217]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [118 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [118][0/391]	Time 0.056 (0.056)	Data 0.177 (0.177)	Loss 2.4457 (2.4457)	Acc@1 57.031 (57.031)	Acc@5 82.031 (82.031)
Epoch: [118][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.3002 (2.3715)	Acc@1 58.594 (56.108)	Acc@5 85.938 (85.440)
Epoch: [118][20/391]	Time 0.011 (0.013)	Data 0.001 (0.010)	Loss 2.4327 (2.3551)	Acc@1 53.125 (56.734)	Acc@5 84.375 (85.751)
Epoch: [118][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.2157 (2.3342)	Acc@1 54.688 (56.729)	Acc@5 89.062 (86.089)
Epoch: [118][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.3211 (2.3327)	Acc@1 57.031 (56.974)	Acc@5 87.500 (86.033)
Epoch: [118][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.5052 (2.3505)	Acc@1 48.438 (56.526)	Acc@5 84.375 (85.815)
Epoch: [118][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3806 (2.3439)	Acc@1 56.250 (56.711)	Acc@5 83.594 (85.745)
Epoch: [118][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.2359 (2.3443)	Acc@1 64.062 (56.800)	Acc@5 92.188 (85.904)
Epoch: [118][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.4139 (2.3516)	Acc@1 53.125 (56.607)	Acc@5 83.594 (85.802)
Epoch: [118][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2780 (2.3479)	Acc@1 57.812 (56.619)	Acc@5 83.594 (85.834)
Epoch: [118][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1212 (2.3496)	Acc@1 62.500 (56.598)	Acc@5 86.719 (85.690)
Epoch: [118][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0915 (2.3513)	Acc@1 68.750 (56.651)	Acc@5 89.062 (85.536)
Epoch: [118][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4792 (2.3551)	Acc@1 53.125 (56.566)	Acc@5 89.844 (85.453)
Epoch: [118][130/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3249 (2.3525)	Acc@1 60.156 (56.691)	Acc@5 82.812 (85.389)
Epoch: [118][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1877 (2.3586)	Acc@1 61.719 (56.510)	Acc@5 87.500 (85.300)
Epoch: [118][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2320 (2.3603)	Acc@1 59.375 (56.576)	Acc@5 92.969 (85.249)
Epoch: [118][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2617 (2.3597)	Acc@1 57.812 (56.570)	Acc@5 84.375 (85.282)
Epoch: [118][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4285 (2.3611)	Acc@1 50.781 (56.460)	Acc@5 88.281 (85.371)
Epoch: [118][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5944 (2.3694)	Acc@1 52.344 (56.280)	Acc@5 83.594 (85.290)
Epoch: [118][190/391]	Time 0.009 (0.011)	Data 0.001 (0.002)	Loss 2.5136 (2.3731)	Acc@1 52.344 (56.148)	Acc@5 82.031 (85.291)
Epoch: [118][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3537 (2.3724)	Acc@1 60.156 (56.172)	Acc@5 85.938 (85.300)
Epoch: [118][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4255 (2.3755)	Acc@1 52.344 (56.113)	Acc@5 83.594 (85.245)
Epoch: [118][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6042 (2.3761)	Acc@1 54.688 (56.105)	Acc@5 83.594 (85.262)
Epoch: [118][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3823 (2.3778)	Acc@1 54.688 (56.128)	Acc@5 82.031 (85.190)
Epoch: [118][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3103 (2.3813)	Acc@1 61.719 (56.059)	Acc@5 83.594 (85.114)
Epoch: [118][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4061 (2.3802)	Acc@1 54.688 (56.066)	Acc@5 81.250 (85.088)
Epoch: [118][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3751 (2.3838)	Acc@1 57.812 (56.008)	Acc@5 87.500 (85.007)
Epoch: [118][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4511 (2.3871)	Acc@1 57.812 (55.927)	Acc@5 86.719 (84.954)
Epoch: [118][280/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3159 (2.3891)	Acc@1 58.594 (55.911)	Acc@5 85.156 (84.889)
Epoch: [118][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3524 (2.3879)	Acc@1 55.469 (55.863)	Acc@5 85.156 (84.915)
Epoch: [118][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3961 (2.3897)	Acc@1 58.594 (55.837)	Acc@5 84.375 (84.855)
Epoch: [118][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6654 (2.3909)	Acc@1 50.781 (55.818)	Acc@5 78.906 (84.820)
Epoch: [118][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1972 (2.3913)	Acc@1 55.469 (55.827)	Acc@5 89.062 (84.857)
Epoch: [118][330/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.1756 (2.3917)	Acc@1 60.156 (55.832)	Acc@5 85.938 (84.833)
Epoch: [118][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5745 (2.3938)	Acc@1 53.125 (55.808)	Acc@5 79.688 (84.813)
Epoch: [118][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4081 (2.3955)	Acc@1 56.250 (55.767)	Acc@5 85.938 (84.769)
Epoch: [118][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3325 (2.3950)	Acc@1 56.250 (55.791)	Acc@5 85.938 (84.765)
Epoch: [118][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2169 (2.3950)	Acc@1 61.719 (55.806)	Acc@5 85.156 (84.741)
Epoch: [118][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3567 (2.3952)	Acc@1 59.375 (55.832)	Acc@5 86.719 (84.715)
Epoch: [118][390/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.4829 (2.3962)	Acc@1 56.250 (55.784)	Acc@5 81.250 (84.712)
num momentum params: 26
[0.1, 2.396233968887329, 2.161643925905228, 55.784, 46.15, tensor(0.3321, device='cuda:0', grad_fn=<DivBackward0>), 4.255626916885376, 0.33617711067199707]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [119 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [119][0/391]	Time 0.051 (0.051)	Data 0.194 (0.194)	Loss 2.2927 (2.2927)	Acc@1 57.031 (57.031)	Acc@5 84.375 (84.375)
Epoch: [119][10/391]	Time 0.012 (0.016)	Data 0.002 (0.019)	Loss 2.3452 (2.3512)	Acc@1 56.250 (57.244)	Acc@5 85.156 (84.659)
Epoch: [119][20/391]	Time 0.010 (0.014)	Data 0.002 (0.011)	Loss 2.2895 (2.3468)	Acc@1 60.156 (57.254)	Acc@5 84.375 (85.082)
Epoch: [119][30/391]	Time 0.012 (0.013)	Data 0.001 (0.008)	Loss 2.2777 (2.3347)	Acc@1 61.719 (57.334)	Acc@5 84.375 (85.333)
Epoch: [119][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.2386 (2.3377)	Acc@1 62.500 (57.165)	Acc@5 89.062 (85.423)
Epoch: [119][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4672 (2.3475)	Acc@1 53.125 (56.801)	Acc@5 82.031 (85.294)
Epoch: [119][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.3155 (2.3417)	Acc@1 60.156 (57.172)	Acc@5 85.156 (85.464)
Epoch: [119][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3329 (2.3475)	Acc@1 57.031 (57.020)	Acc@5 85.938 (85.365)
Epoch: [119][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.6860 (2.3483)	Acc@1 50.781 (57.070)	Acc@5 78.125 (85.243)
Epoch: [119][90/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3329 (2.3453)	Acc@1 52.344 (57.023)	Acc@5 86.719 (85.251)
Epoch: [119][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3801 (2.3468)	Acc@1 57.812 (56.892)	Acc@5 86.719 (85.288)
Epoch: [119][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5024 (2.3589)	Acc@1 52.344 (56.743)	Acc@5 84.375 (85.114)
Epoch: [119][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5616 (2.3675)	Acc@1 49.219 (56.463)	Acc@5 82.031 (85.001)
Epoch: [119][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5386 (2.3704)	Acc@1 55.469 (56.292)	Acc@5 82.031 (85.043)
Epoch: [119][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4306 (2.3676)	Acc@1 53.125 (56.361)	Acc@5 85.938 (85.045)
Epoch: [119][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5963 (2.3698)	Acc@1 52.344 (56.266)	Acc@5 85.938 (85.073)
Epoch: [119][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4611 (2.3780)	Acc@1 55.469 (56.080)	Acc@5 82.812 (84.894)
Epoch: [119][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4169 (2.3753)	Acc@1 57.812 (56.181)	Acc@5 82.031 (84.942)
Epoch: [119][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4871 (2.3815)	Acc@1 53.906 (56.090)	Acc@5 82.812 (84.850)
Epoch: [119][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6983 (2.3847)	Acc@1 51.562 (56.095)	Acc@5 80.469 (84.825)
Epoch: [119][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3873 (2.3877)	Acc@1 57.031 (56.028)	Acc@5 80.469 (84.779)
Epoch: [119][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3794 (2.3934)	Acc@1 54.688 (55.909)	Acc@5 82.031 (84.679)
Epoch: [119][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4553 (2.3974)	Acc@1 57.031 (55.843)	Acc@5 81.250 (84.587)
Epoch: [119][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6092 (2.3972)	Acc@1 52.344 (55.875)	Acc@5 82.031 (84.625)
Epoch: [119][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5129 (2.3993)	Acc@1 50.781 (55.851)	Acc@5 84.375 (84.621)
Epoch: [119][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5408 (2.4005)	Acc@1 55.469 (55.789)	Acc@5 80.469 (84.646)
Epoch: [119][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4224 (2.3996)	Acc@1 55.469 (55.804)	Acc@5 82.812 (84.653)
Epoch: [119][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4644 (2.3999)	Acc@1 55.469 (55.771)	Acc@5 82.812 (84.643)
Epoch: [119][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3557 (2.4041)	Acc@1 60.156 (55.700)	Acc@5 84.375 (84.572)
Epoch: [119][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5879 (2.4057)	Acc@1 53.906 (55.665)	Acc@5 83.594 (84.528)
Epoch: [119][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2773 (2.4037)	Acc@1 62.500 (55.708)	Acc@5 84.375 (84.559)
Epoch: [119][310/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.5996 (2.4054)	Acc@1 51.562 (55.642)	Acc@5 80.469 (84.518)
Epoch: [119][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4798 (2.4052)	Acc@1 55.469 (55.680)	Acc@5 82.031 (84.523)
Epoch: [119][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6805 (2.4071)	Acc@1 53.125 (55.669)	Acc@5 79.688 (84.491)
Epoch: [119][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4566 (2.4065)	Acc@1 55.469 (55.666)	Acc@5 82.812 (84.499)
Epoch: [119][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3149 (2.4064)	Acc@1 61.719 (55.678)	Acc@5 88.281 (84.509)
Epoch: [119][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3495 (2.4050)	Acc@1 55.469 (55.739)	Acc@5 82.031 (84.542)
Epoch: [119][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3385 (2.4050)	Acc@1 57.812 (55.759)	Acc@5 85.156 (84.533)
Epoch: [119][380/391]	Time 0.011 (0.011)	Data 0.000 (0.002)	Loss 2.5142 (2.4061)	Acc@1 57.812 (55.766)	Acc@5 80.469 (84.529)
Epoch: [119][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5306 (2.4099)	Acc@1 55.000 (55.688)	Acc@5 81.250 (84.488)
num momentum params: 26
[0.1, 2.4099370195007324, 2.1637807750701903, 55.688, 45.04, tensor(0.3303, device='cuda:0', grad_fn=<DivBackward0>), 4.247631072998047, 0.3388059139251709]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [211, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [211]
Non Pruning Epoch - module.bn3.bias: [211]
Non Pruning Epoch - module.conv4.weight: [247, 211, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [360, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [360]
Non Pruning Epoch - module.bn5.bias: [360]
Non Pruning Epoch - module.conv6.weight: [316, 360, 3, 3]
Non Pruning Epoch - module.bn6.weight: [316]
Non Pruning Epoch - module.bn6.bias: [316]
Non Pruning Epoch - module.conv7.weight: [199, 316, 3, 3]
Non Pruning Epoch - module.bn7.weight: [199]
Non Pruning Epoch - module.bn7.bias: [199]
Non Pruning Epoch - module.conv8.weight: [196, 199, 3, 3]
Non Pruning Epoch - module.bn8.weight: [196]
Non Pruning Epoch - module.bn8.bias: [196]
Non Pruning Epoch - module.fc.weight: [100, 196]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [120 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [211, 108, 3, 3]
module.conv4.weight [247, 211, 3, 3]
module.conv5.weight [360, 247, 3, 3]
module.conv6.weight [316, 360, 3, 3]
module.conv7.weight [199, 316, 3, 3]
module.conv8.weight [196, 199, 3, 3]
Epoch: [120][0/391]	Time 0.055 (0.055)	Data 0.173 (0.173)	Loss 2.3909 (2.3909)	Acc@1 57.031 (57.031)	Acc@5 83.594 (83.594)
Epoch: [120][10/391]	Time 0.010 (0.016)	Data 0.001 (0.017)	Loss 2.5416 (2.3454)	Acc@1 50.781 (55.966)	Acc@5 82.031 (84.375)
Epoch: [120][20/391]	Time 0.010 (0.013)	Data 0.001 (0.010)	Loss 2.3304 (2.3356)	Acc@1 53.125 (56.176)	Acc@5 86.719 (85.491)
Epoch: [120][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.3119 (2.3320)	Acc@1 57.031 (56.628)	Acc@5 85.156 (85.459)
Epoch: [120][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.4271 (2.3373)	Acc@1 54.688 (56.269)	Acc@5 80.469 (85.252)
Epoch: [120][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2694 (2.3325)	Acc@1 57.812 (56.633)	Acc@5 92.969 (85.539)
Epoch: [120][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2605 (2.3350)	Acc@1 62.500 (56.660)	Acc@5 87.500 (85.630)
Epoch: [120][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.5630 (2.3363)	Acc@1 50.781 (56.734)	Acc@5 82.031 (85.640)
Epoch: [120][80/391]	Time 0.010 (0.011)	Data 0.002 (0.004)	Loss 2.2191 (2.3486)	Acc@1 61.719 (56.520)	Acc@5 90.625 (85.368)
Epoch: [120][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3242 (2.3461)	Acc@1 55.469 (56.559)	Acc@5 87.500 (85.448)
Epoch: [120][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3139 (2.3476)	Acc@1 57.031 (56.513)	Acc@5 86.719 (85.396)
Epoch: [120][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3933 (2.3535)	Acc@1 55.469 (56.665)	Acc@5 83.594 (85.262)
Epoch: [120][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3105 (2.3533)	Acc@1 56.250 (56.508)	Acc@5 83.594 (85.292)
Epoch: [120][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5692 (2.3562)	Acc@1 53.125 (56.512)	Acc@5 82.812 (85.162)
Epoch: [120][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2315 (2.3591)	Acc@1 59.375 (56.383)	Acc@5 86.719 (85.145)
Epoch: [120][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.7424 (2.3690)	Acc@1 46.875 (56.219)	Acc@5 81.250 (85.032)
Epoch: [120][160/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3950 (2.3724)	Acc@1 53.906 (56.177)	Acc@5 79.688 (84.962)
Epoch: [120][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3688 (2.3739)	Acc@1 49.219 (56.149)	Acc@5 85.938 (84.964)
Epoch: [120][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7373 (2.3722)	Acc@1 50.000 (56.207)	Acc@5 82.031 (84.984)
Epoch: [120][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2866 (2.3725)	Acc@1 52.344 (56.258)	Acc@5 86.719 (84.956)
Epoch: [120][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5925 (2.3743)	Acc@1 53.125 (56.215)	Acc@5 84.375 (84.950)
Epoch: [120][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5049 (2.3759)	Acc@1 50.781 (56.143)	Acc@5 89.844 (84.930)
Epoch: [120][220/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4160 (2.3797)	Acc@1 55.469 (56.123)	Acc@5 82.031 (84.905)
Epoch: [120][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2603 (2.3802)	Acc@1 56.250 (56.169)	Acc@5 85.156 (84.845)
Epoch: [120][240/391]	Time 0.009 (0.011)	Data 0.001 (0.002)	Loss 2.5351 (2.3833)	Acc@1 54.688 (56.192)	Acc@5 82.031 (84.709)
Epoch: [120][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5242 (2.3855)	Acc@1 49.219 (56.116)	Acc@5 83.594 (84.646)
Epoch: [120][260/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3550 (2.3856)	Acc@1 57.812 (56.112)	Acc@5 83.594 (84.671)
Epoch: [120][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5785 (2.3892)	Acc@1 50.781 (56.014)	Acc@5 82.031 (84.594)
Epoch: [120][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6178 (2.3922)	Acc@1 48.438 (55.911)	Acc@5 82.812 (84.595)
Epoch: [120][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6669 (2.3945)	Acc@1 54.688 (55.882)	Acc@5 79.688 (84.555)
Epoch: [120][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3924 (2.3985)	Acc@1 58.594 (55.762)	Acc@5 83.594 (84.489)
Epoch: [120][310/391]	Time 0.018 (0.011)	Data 0.001 (0.002)	Loss 2.4678 (2.4026)	Acc@1 53.906 (55.707)	Acc@5 82.031 (84.418)
Epoch: [120][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1174 (2.4010)	Acc@1 61.719 (55.756)	Acc@5 91.406 (84.475)
Epoch: [120][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3984 (2.4004)	Acc@1 53.906 (55.750)	Acc@5 84.375 (84.486)
Epoch: [120][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3482 (2.4003)	Acc@1 50.781 (55.755)	Acc@5 85.156 (84.485)
Epoch: [120][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5246 (2.4020)	Acc@1 55.469 (55.676)	Acc@5 82.031 (84.484)
Epoch: [120][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3649 (2.4001)	Acc@1 57.812 (55.746)	Acc@5 84.375 (84.520)
Epoch: [120][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4259 (2.3976)	Acc@1 57.031 (55.850)	Acc@5 82.031 (84.546)
Epoch: [120][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4324 (2.3983)	Acc@1 56.250 (55.828)	Acc@5 82.031 (84.494)
Epoch: [120][390/391]	Time 0.013 (0.011)	Data 0.000 (0.002)	Loss 2.4475 (2.3990)	Acc@1 56.250 (55.822)	Acc@5 83.750 (84.488)
num momentum params: 26
[0.1, 2.3990449645996095, 2.295528690814972, 55.822, 42.88, tensor(0.3316, device='cuda:0', grad_fn=<DivBackward0>), 4.176568508148193, 0.3297128677368164]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [108, 32, 3, 3]
Before - module.bn2.weight: [108]
Before - module.bn2.bias: [108]
Before - module.conv3.weight: [211, 108, 3, 3]
Before - module.bn3.weight: [211]
Before - module.bn3.bias: [211]
Before - module.conv4.weight: [247, 211, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [360, 247, 3, 3]
Before - module.bn5.weight: [360]
Before - module.bn5.bias: [360]
Before - module.conv6.weight: [316, 360, 3, 3]
Before - module.bn6.weight: [316]
Before - module.bn6.bias: [316]
Before - module.conv7.weight: [199, 316, 3, 3]
Before - module.bn7.weight: [199]
Before - module.bn7.bias: [199]
Before - module.conv8.weight: [196, 199, 3, 3]
Before - module.bn8.weight: [196]
Before - module.bn8.bias: [196]
Before - module.fc.weight: [100, 196]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [108, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 108 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [211, 108, 3, 3] >> [209, 108, 3, 3]
[module.bn3.weight]: 211 >> 209
running_mean [209]
running_var [209]
num_batches_tracked []
[module.conv4.weight]: [247, 211, 3, 3] >> [247, 209, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [360, 247, 3, 3] >> [359, 247, 3, 3]
[module.bn5.weight]: 360 >> 359
running_mean [359]
running_var [359]
num_batches_tracked []
[module.conv6.weight]: [316, 360, 3, 3] >> [313, 359, 3, 3]
[module.bn6.weight]: 316 >> 313
running_mean [313]
running_var [313]
num_batches_tracked []
[module.conv7.weight]: [199, 316, 3, 3] >> [194, 313, 3, 3]
[module.bn7.weight]: 199 >> 194
running_mean [194]
running_var [194]
num_batches_tracked []
[module.conv8.weight]: [196, 199, 3, 3] >> [192, 194, 3, 3]
[module.bn8.weight]: 196 >> 192
running_mean [192]
running_var [192]
num_batches_tracked []
[module.fc.weight]: [100, 196] >> [100, 192]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [209, 108, 3, 3]
After - module.bn3.weight: [209]
After - module.bn3.bias: [209]
After - module.conv4.weight: [247, 209, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [359, 247, 3, 3]
After - module.bn5.weight: [359]
After - module.bn5.bias: [359]
After - module.conv6.weight: [313, 359, 3, 3]
After - module.bn6.weight: [313]
After - module.bn6.bias: [313]
After - module.conv7.weight: [194, 313, 3, 3]
After - module.bn7.weight: [194]
After - module.bn7.bias: [194]
After - module.conv8.weight: [192, 194, 3, 3]
After - module.bn8.weight: [192]
After - module.bn8.bias: [192]
After - module.fc.weight: [100, 192]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [209, 108, 3, 3]
conv4 --> [247, 209, 3, 3]
conv5 --> [359, 247, 3, 3]
conv6 --> [313, 359, 3, 3]
conv7 --> [194, 313, 3, 3]
conv8 --> [192, 194, 3, 3]
fc --> [192, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5928671232, 13001472, 209
4, 13559090688, 29734848, 247
5, 6946288128, 12768912, 359
6, 8802381312, 16180848, 313
7, 1678841856, 2185992, 194
8, 1029832704, 1340928, 192
fc, 7372800, 19200, 0
===================
FLOP REPORT: 16263747000000.0 40758400000.0 84079560 101896 1654 6.507604598999023
[INFO] Storing checkpoint...

Epoch: [121 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [121][0/391]	Time 0.232 (0.232)	Data 0.191 (0.191)	Loss 2.2095 (2.2095)	Acc@1 62.500 (62.500)	Acc@5 89.844 (89.844)
Epoch: [121][10/391]	Time 0.011 (0.032)	Data 0.001 (0.019)	Loss 2.2143 (2.2840)	Acc@1 57.812 (58.665)	Acc@5 90.625 (87.074)
Epoch: [121][20/391]	Time 0.012 (0.022)	Data 0.001 (0.011)	Loss 2.4327 (2.2624)	Acc@1 50.781 (58.929)	Acc@5 89.844 (87.277)
Epoch: [121][30/391]	Time 0.010 (0.019)	Data 0.001 (0.008)	Loss 2.7051 (2.2795)	Acc@1 52.344 (58.543)	Acc@5 82.031 (87.198)
Epoch: [121][40/391]	Time 0.010 (0.017)	Data 0.001 (0.006)	Loss 2.3390 (2.3035)	Acc@1 59.375 (58.136)	Acc@5 82.812 (86.509)
Epoch: [121][50/391]	Time 0.010 (0.015)	Data 0.001 (0.005)	Loss 2.1408 (2.3191)	Acc@1 65.625 (57.828)	Acc@5 87.500 (86.045)
Epoch: [121][60/391]	Time 0.012 (0.015)	Data 0.003 (0.005)	Loss 2.2517 (2.3322)	Acc@1 55.469 (57.582)	Acc@5 84.375 (85.835)
Epoch: [121][70/391]	Time 0.011 (0.014)	Data 0.001 (0.004)	Loss 2.2928 (2.3276)	Acc@1 60.938 (57.636)	Acc@5 82.031 (85.827)
Epoch: [121][80/391]	Time 0.010 (0.014)	Data 0.002 (0.004)	Loss 2.2930 (2.3319)	Acc@1 57.031 (57.494)	Acc@5 84.375 (85.764)
Epoch: [121][90/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 2.2590 (2.3344)	Acc@1 59.375 (57.572)	Acc@5 88.281 (85.697)
Epoch: [121][100/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.4381 (2.3430)	Acc@1 60.938 (57.472)	Acc@5 81.250 (85.396)
Epoch: [121][110/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.2493 (2.3476)	Acc@1 57.031 (57.439)	Acc@5 86.719 (85.241)
Epoch: [121][120/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.7295 (2.3568)	Acc@1 44.531 (57.160)	Acc@5 80.469 (85.130)
Epoch: [121][130/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.4460 (2.3578)	Acc@1 57.031 (57.007)	Acc@5 82.812 (85.168)
Epoch: [121][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2795 (2.3597)	Acc@1 60.156 (56.887)	Acc@5 85.938 (85.084)
Epoch: [121][150/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4444 (2.3630)	Acc@1 50.000 (56.793)	Acc@5 86.719 (85.053)
Epoch: [121][160/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.5145 (2.3617)	Acc@1 50.000 (56.818)	Acc@5 81.250 (85.069)
Epoch: [121][170/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2668 (2.3603)	Acc@1 60.938 (56.954)	Acc@5 84.375 (85.074)
Epoch: [121][180/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2708 (2.3602)	Acc@1 53.906 (56.936)	Acc@5 88.281 (85.113)
Epoch: [121][190/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5000 (2.3633)	Acc@1 57.812 (56.806)	Acc@5 78.125 (85.074)
Epoch: [121][200/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5922 (2.3704)	Acc@1 50.000 (56.658)	Acc@5 83.594 (84.966)
Epoch: [121][210/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3019 (2.3716)	Acc@1 57.812 (56.568)	Acc@5 87.500 (84.967)
Epoch: [121][220/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4561 (2.3748)	Acc@1 52.344 (56.356)	Acc@5 86.719 (84.930)
Epoch: [121][230/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.7657 (2.3763)	Acc@1 46.875 (56.341)	Acc@5 82.031 (84.913)
Epoch: [121][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.6182 (2.3809)	Acc@1 52.344 (56.244)	Acc@5 82.812 (84.877)
Epoch: [121][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.3181 (2.3840)	Acc@1 53.906 (56.163)	Acc@5 85.938 (84.758)
Epoch: [121][260/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5554 (2.3862)	Acc@1 55.469 (56.169)	Acc@5 83.594 (84.731)
Epoch: [121][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3548 (2.3893)	Acc@1 60.156 (56.094)	Acc@5 82.031 (84.672)
Epoch: [121][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3386 (2.3904)	Acc@1 59.375 (56.061)	Acc@5 86.719 (84.698)
Epoch: [121][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3992 (2.3921)	Acc@1 53.906 (56.006)	Acc@5 83.594 (84.711)
Epoch: [121][300/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2696 (2.3933)	Acc@1 55.469 (55.972)	Acc@5 90.625 (84.715)
Epoch: [121][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4378 (2.3963)	Acc@1 58.594 (55.903)	Acc@5 78.125 (84.621)
Epoch: [121][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3527 (2.3961)	Acc@1 57.031 (55.973)	Acc@5 83.594 (84.599)
Epoch: [121][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3539 (2.3962)	Acc@1 56.250 (55.990)	Acc@5 85.156 (84.564)
Epoch: [121][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8170 (2.3966)	Acc@1 48.438 (55.991)	Acc@5 76.562 (84.535)
Epoch: [121][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3993 (2.3979)	Acc@1 50.000 (55.954)	Acc@5 90.625 (84.544)
Epoch: [121][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3160 (2.3975)	Acc@1 59.375 (55.923)	Acc@5 85.156 (84.565)
Epoch: [121][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3894 (2.3976)	Acc@1 57.812 (55.905)	Acc@5 88.281 (84.594)
Epoch: [121][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5565 (2.3995)	Acc@1 46.094 (55.819)	Acc@5 80.469 (84.568)
Epoch: [121][390/391]	Time 0.127 (0.012)	Data 0.000 (0.002)	Loss 2.4211 (2.3989)	Acc@1 56.250 (55.824)	Acc@5 82.500 (84.574)
num momentum params: 26
[0.1, 2.3989450926208495, 2.053418694734573, 55.824, 46.96, tensor(0.3316, device='cuda:0', grad_fn=<DivBackward0>), 4.52250862121582, 0.3946285247802734]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [122 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [122][0/391]	Time 0.059 (0.059)	Data 0.192 (0.192)	Loss 2.4666 (2.4666)	Acc@1 54.688 (54.688)	Acc@5 83.594 (83.594)
Epoch: [122][10/391]	Time 0.012 (0.018)	Data 0.001 (0.019)	Loss 2.1924 (2.3167)	Acc@1 64.844 (57.812)	Acc@5 88.281 (85.653)
Epoch: [122][20/391]	Time 0.010 (0.015)	Data 0.001 (0.010)	Loss 2.0860 (2.3138)	Acc@1 62.500 (58.333)	Acc@5 86.719 (85.714)
Epoch: [122][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.3033 (2.3420)	Acc@1 55.469 (57.056)	Acc@5 88.281 (85.181)
Epoch: [122][40/391]	Time 0.013 (0.013)	Data 0.001 (0.006)	Loss 2.4330 (2.3442)	Acc@1 55.469 (56.745)	Acc@5 85.156 (85.271)
Epoch: [122][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.5208 (2.3544)	Acc@1 51.562 (56.112)	Acc@5 84.375 (85.355)
Epoch: [122][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2779 (2.3624)	Acc@1 57.812 (55.981)	Acc@5 84.375 (85.079)
Epoch: [122][70/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 2.4318 (2.3644)	Acc@1 57.031 (56.041)	Acc@5 83.594 (85.112)
Epoch: [122][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2616 (2.3605)	Acc@1 60.156 (56.289)	Acc@5 85.156 (85.118)
Epoch: [122][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.1445 (2.3649)	Acc@1 61.719 (56.061)	Acc@5 86.719 (85.182)
Epoch: [122][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.5049 (2.3732)	Acc@1 54.688 (55.863)	Acc@5 84.375 (84.955)
Epoch: [122][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3273 (2.3781)	Acc@1 54.688 (55.694)	Acc@5 89.844 (84.931)
Epoch: [122][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4088 (2.3827)	Acc@1 53.906 (55.643)	Acc@5 88.281 (84.801)
Epoch: [122][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6370 (2.3896)	Acc@1 54.688 (55.588)	Acc@5 78.906 (84.721)
Epoch: [122][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.4163 (2.3902)	Acc@1 53.906 (55.657)	Acc@5 85.938 (84.774)
Epoch: [122][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4442 (2.3887)	Acc@1 54.688 (55.717)	Acc@5 88.281 (84.851)
Epoch: [122][160/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.7821 (2.3921)	Acc@1 50.781 (55.609)	Acc@5 77.344 (84.787)
Epoch: [122][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6881 (2.3983)	Acc@1 49.219 (55.464)	Acc@5 79.688 (84.617)
Epoch: [122][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2738 (2.3978)	Acc@1 60.156 (55.512)	Acc@5 84.375 (84.604)
Epoch: [122][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8223 (2.4021)	Acc@1 47.656 (55.477)	Acc@5 81.250 (84.563)
Epoch: [122][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6238 (2.4022)	Acc@1 55.469 (55.496)	Acc@5 76.562 (84.542)
Epoch: [122][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7606 (2.4026)	Acc@1 51.562 (55.532)	Acc@5 77.344 (84.531)
Epoch: [122][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4880 (2.4058)	Acc@1 53.125 (55.458)	Acc@5 85.156 (84.481)
Epoch: [122][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4927 (2.4068)	Acc@1 50.781 (55.418)	Acc@5 84.375 (84.470)
Epoch: [122][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5033 (2.4076)	Acc@1 55.469 (55.430)	Acc@5 81.250 (84.453)
Epoch: [122][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3698 (2.4098)	Acc@1 51.562 (55.332)	Acc@5 86.719 (84.440)
Epoch: [122][260/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.3935 (2.4091)	Acc@1 53.906 (55.352)	Acc@5 84.375 (84.477)
Epoch: [122][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4502 (2.4072)	Acc@1 56.250 (55.411)	Acc@5 81.250 (84.485)
Epoch: [122][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3234 (2.4061)	Acc@1 56.250 (55.466)	Acc@5 83.594 (84.514)
Epoch: [122][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5607 (2.4085)	Acc@1 53.125 (55.410)	Acc@5 82.812 (84.477)
Epoch: [122][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3527 (2.4104)	Acc@1 57.812 (55.378)	Acc@5 84.375 (84.440)
Epoch: [122][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5938 (2.4153)	Acc@1 56.250 (55.336)	Acc@5 80.469 (84.322)
Epoch: [122][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5606 (2.4144)	Acc@1 48.438 (55.354)	Acc@5 83.594 (84.329)
Epoch: [122][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4143 (2.4129)	Acc@1 59.375 (55.433)	Acc@5 84.375 (84.316)
Epoch: [122][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4146 (2.4107)	Acc@1 56.250 (55.469)	Acc@5 81.250 (84.364)
Epoch: [122][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2666 (2.4079)	Acc@1 58.594 (55.522)	Acc@5 85.938 (84.384)
Epoch: [122][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6551 (2.4095)	Acc@1 49.219 (55.477)	Acc@5 78.125 (84.356)
Epoch: [122][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3759 (2.4124)	Acc@1 57.031 (55.412)	Acc@5 85.156 (84.346)
Epoch: [122][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7086 (2.4134)	Acc@1 48.438 (55.420)	Acc@5 78.125 (84.322)
Epoch: [122][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.7452 (2.4176)	Acc@1 51.250 (55.334)	Acc@5 81.250 (84.282)
num momentum params: 26
[0.1, 2.417627009124756, 2.1931073558330536, 55.334, 44.03, tensor(0.3292, device='cuda:0', grad_fn=<DivBackward0>), 4.215161085128784, 0.3311045169830322]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [123 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [123][0/391]	Time 0.051 (0.051)	Data 0.192 (0.192)	Loss 2.5374 (2.5374)	Acc@1 53.125 (53.125)	Acc@5 82.031 (82.031)
Epoch: [123][10/391]	Time 0.013 (0.016)	Data 0.001 (0.018)	Loss 2.2607 (2.3997)	Acc@1 60.938 (56.676)	Acc@5 85.156 (84.020)
Epoch: [123][20/391]	Time 0.012 (0.013)	Data 0.001 (0.010)	Loss 2.2483 (2.3765)	Acc@1 60.938 (56.622)	Acc@5 84.375 (84.710)
Epoch: [123][30/391]	Time 0.012 (0.012)	Data 0.001 (0.007)	Loss 2.1517 (2.3498)	Acc@1 60.156 (57.107)	Acc@5 91.406 (85.005)
Epoch: [123][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.3424 (2.3521)	Acc@1 60.156 (56.936)	Acc@5 89.062 (85.099)
Epoch: [123][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.1980 (2.3373)	Acc@1 60.156 (57.384)	Acc@5 87.500 (85.325)
Epoch: [123][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2544 (2.3416)	Acc@1 64.844 (57.351)	Acc@5 81.250 (85.246)
Epoch: [123][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.3031 (2.3388)	Acc@1 53.906 (57.328)	Acc@5 82.812 (85.233)
Epoch: [123][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.6742 (2.3568)	Acc@1 50.000 (56.800)	Acc@5 76.562 (84.983)
Epoch: [123][90/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2439 (2.3466)	Acc@1 58.594 (56.980)	Acc@5 88.281 (85.199)
Epoch: [123][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4320 (2.3525)	Acc@1 54.688 (56.846)	Acc@5 85.938 (85.110)
Epoch: [123][110/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.3510 (2.3480)	Acc@1 53.125 (56.926)	Acc@5 88.281 (85.142)
Epoch: [123][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5283 (2.3512)	Acc@1 54.688 (56.980)	Acc@5 80.469 (85.092)
Epoch: [123][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4050 (2.3595)	Acc@1 57.812 (56.751)	Acc@5 82.812 (84.936)
Epoch: [123][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3847 (2.3638)	Acc@1 53.125 (56.588)	Acc@5 85.938 (84.968)
Epoch: [123][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2440 (2.3681)	Acc@1 60.938 (56.400)	Acc@5 87.500 (84.908)
Epoch: [123][160/391]	Time 0.011 (0.011)	Data 0.000 (0.003)	Loss 2.3008 (2.3707)	Acc@1 57.031 (56.366)	Acc@5 87.500 (84.802)
Epoch: [123][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5548 (2.3740)	Acc@1 51.562 (56.204)	Acc@5 78.906 (84.818)
Epoch: [123][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5318 (2.3751)	Acc@1 53.125 (56.203)	Acc@5 80.469 (84.794)
Epoch: [123][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4467 (2.3765)	Acc@1 57.812 (56.205)	Acc@5 79.688 (84.804)
Epoch: [123][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3952 (2.3826)	Acc@1 53.906 (56.028)	Acc@5 85.938 (84.725)
Epoch: [123][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4343 (2.3847)	Acc@1 61.719 (56.024)	Acc@5 84.375 (84.701)
Epoch: [123][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3881 (2.3906)	Acc@1 53.125 (55.918)	Acc@5 85.156 (84.615)
Epoch: [123][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1892 (2.3945)	Acc@1 60.938 (55.871)	Acc@5 90.625 (84.575)
Epoch: [123][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3957 (2.3948)	Acc@1 57.031 (55.887)	Acc@5 85.156 (84.618)
Epoch: [123][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4016 (2.3956)	Acc@1 54.688 (55.905)	Acc@5 85.156 (84.627)
Epoch: [123][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5541 (2.3937)	Acc@1 50.781 (56.002)	Acc@5 82.812 (84.653)
Epoch: [123][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5990 (2.3905)	Acc@1 52.344 (56.054)	Acc@5 82.812 (84.701)
Epoch: [123][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6112 (2.3908)	Acc@1 52.344 (56.030)	Acc@5 80.469 (84.703)
Epoch: [123][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3179 (2.3924)	Acc@1 57.812 (55.957)	Acc@5 85.938 (84.692)
Epoch: [123][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5151 (2.3948)	Acc@1 51.562 (55.887)	Acc@5 79.688 (84.609)
Epoch: [123][310/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3102 (2.3955)	Acc@1 58.594 (55.898)	Acc@5 82.031 (84.578)
Epoch: [123][320/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4519 (2.3978)	Acc@1 57.812 (55.887)	Acc@5 82.031 (84.531)
Epoch: [123][330/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2630 (2.3984)	Acc@1 57.031 (55.842)	Acc@5 84.375 (84.488)
Epoch: [123][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8145 (2.4013)	Acc@1 50.000 (55.778)	Acc@5 79.688 (84.485)
Epoch: [123][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2884 (2.4022)	Acc@1 64.062 (55.783)	Acc@5 85.156 (84.460)
Epoch: [123][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1203 (2.4002)	Acc@1 61.719 (55.845)	Acc@5 87.500 (84.451)
Epoch: [123][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4745 (2.3999)	Acc@1 55.469 (55.844)	Acc@5 81.250 (84.472)
Epoch: [123][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3611 (2.3990)	Acc@1 58.594 (55.865)	Acc@5 85.938 (84.475)
Epoch: [123][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.5764 (2.4014)	Acc@1 47.500 (55.800)	Acc@5 83.750 (84.446)
num momentum params: 26
[0.1, 2.401412763290405, 1.8843357694149017, 55.8, 49.84, tensor(0.3307, device='cuda:0', grad_fn=<DivBackward0>), 4.253912687301636, 0.35478711128234863]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [124 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [124][0/391]	Time 0.057 (0.057)	Data 0.184 (0.184)	Loss 2.1331 (2.1331)	Acc@1 57.812 (57.812)	Acc@5 90.625 (90.625)
Epoch: [124][10/391]	Time 0.012 (0.018)	Data 0.001 (0.018)	Loss 2.2432 (2.2853)	Acc@1 64.062 (58.381)	Acc@5 91.406 (87.500)
Epoch: [124][20/391]	Time 0.011 (0.015)	Data 0.002 (0.010)	Loss 2.3858 (2.3017)	Acc@1 53.125 (57.887)	Acc@5 85.938 (87.091)
Epoch: [124][30/391]	Time 0.010 (0.013)	Data 0.002 (0.007)	Loss 2.2869 (2.2918)	Acc@1 57.031 (57.586)	Acc@5 86.719 (87.349)
Epoch: [124][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.3106 (2.2988)	Acc@1 54.688 (57.584)	Acc@5 89.062 (86.738)
Epoch: [124][50/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 2.3940 (2.3041)	Acc@1 57.031 (57.261)	Acc@5 88.281 (86.657)
Epoch: [124][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4133 (2.3088)	Acc@1 58.594 (57.364)	Acc@5 81.250 (86.488)
Epoch: [124][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4896 (2.3125)	Acc@1 57.031 (57.240)	Acc@5 82.812 (86.455)
Epoch: [124][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.1953 (2.3120)	Acc@1 60.156 (57.301)	Acc@5 86.719 (86.246)
Epoch: [124][90/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4485 (2.3146)	Acc@1 54.688 (57.229)	Acc@5 84.375 (86.186)
Epoch: [124][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4970 (2.3188)	Acc@1 49.219 (57.062)	Acc@5 84.375 (86.154)
Epoch: [124][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2716 (2.3241)	Acc@1 57.031 (56.954)	Acc@5 89.844 (86.106)
Epoch: [124][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3802 (2.3219)	Acc@1 54.688 (57.173)	Acc@5 89.062 (86.138)
Epoch: [124][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.6194 (2.3292)	Acc@1 50.000 (57.019)	Acc@5 78.125 (85.985)
Epoch: [124][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3766 (2.3415)	Acc@1 58.594 (56.704)	Acc@5 82.812 (85.755)
Epoch: [124][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4460 (2.3469)	Acc@1 57.812 (56.654)	Acc@5 80.469 (85.591)
Epoch: [124][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5995 (2.3523)	Acc@1 48.438 (56.556)	Acc@5 81.250 (85.477)
Epoch: [124][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5750 (2.3569)	Acc@1 51.562 (56.469)	Acc@5 78.906 (85.426)
Epoch: [124][180/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4733 (2.3638)	Acc@1 53.125 (56.302)	Acc@5 82.031 (85.290)
Epoch: [124][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5753 (2.3683)	Acc@1 50.000 (56.225)	Acc@5 82.031 (85.230)
Epoch: [124][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7986 (2.3720)	Acc@1 44.531 (56.079)	Acc@5 80.469 (85.203)
Epoch: [124][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2340 (2.3723)	Acc@1 57.031 (56.080)	Acc@5 89.844 (85.219)
Epoch: [124][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4978 (2.3741)	Acc@1 52.344 (56.077)	Acc@5 84.375 (85.199)
Epoch: [124][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3970 (2.3768)	Acc@1 57.031 (56.061)	Acc@5 85.156 (85.170)
Epoch: [124][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3574 (2.3835)	Acc@1 58.594 (55.897)	Acc@5 85.156 (85.062)
Epoch: [124][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2712 (2.3857)	Acc@1 55.469 (55.886)	Acc@5 90.625 (85.044)
Epoch: [124][260/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.1369 (2.3852)	Acc@1 64.844 (55.921)	Acc@5 88.281 (85.013)
Epoch: [124][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6796 (2.3856)	Acc@1 47.656 (55.901)	Acc@5 79.688 (85.021)
Epoch: [124][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4750 (2.3860)	Acc@1 53.906 (55.969)	Acc@5 80.469 (84.981)
Epoch: [124][290/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.4213 (2.3859)	Acc@1 52.344 (55.992)	Acc@5 91.406 (85.014)
Epoch: [124][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4631 (2.3849)	Acc@1 57.031 (56.042)	Acc@5 81.250 (85.011)
Epoch: [124][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7047 (2.3887)	Acc@1 50.781 (55.964)	Acc@5 80.469 (84.950)
Epoch: [124][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5022 (2.3882)	Acc@1 52.344 (55.941)	Acc@5 78.906 (84.918)
Epoch: [124][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6243 (2.3902)	Acc@1 50.781 (55.924)	Acc@5 77.344 (84.840)
Epoch: [124][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3372 (2.3919)	Acc@1 57.031 (55.865)	Acc@5 82.812 (84.790)
Epoch: [124][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1921 (2.3908)	Acc@1 60.156 (55.843)	Acc@5 89.062 (84.822)
Epoch: [124][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6177 (2.3908)	Acc@1 51.562 (55.834)	Acc@5 78.906 (84.814)
Epoch: [124][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8241 (2.3923)	Acc@1 52.344 (55.825)	Acc@5 73.438 (84.767)
Epoch: [124][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6108 (2.3953)	Acc@1 51.562 (55.774)	Acc@5 84.375 (84.756)
Epoch: [124][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.7716 (2.3992)	Acc@1 50.000 (55.694)	Acc@5 80.000 (84.676)
num momentum params: 26
[0.1, 2.3991565534210206, 1.9934904325008391, 55.694, 47.94, tensor(0.3308, device='cuda:0', grad_fn=<DivBackward0>), 4.324166536331177, 0.3350343704223633]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [125 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [125][0/391]	Time 0.061 (0.061)	Data 0.174 (0.174)	Loss 2.4177 (2.4177)	Acc@1 58.594 (58.594)	Acc@5 83.594 (83.594)
Epoch: [125][10/391]	Time 0.011 (0.016)	Data 0.001 (0.017)	Loss 2.3749 (2.3376)	Acc@1 55.469 (55.611)	Acc@5 81.250 (86.506)
Epoch: [125][20/391]	Time 0.010 (0.014)	Data 0.002 (0.010)	Loss 2.6410 (2.3204)	Acc@1 54.688 (56.734)	Acc@5 78.125 (86.347)
Epoch: [125][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.3245 (2.3078)	Acc@1 53.125 (56.905)	Acc@5 84.375 (86.593)
Epoch: [125][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.3640 (2.3337)	Acc@1 53.125 (56.479)	Acc@5 83.594 (85.747)
Epoch: [125][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.4434 (2.3401)	Acc@1 52.344 (56.572)	Acc@5 82.031 (85.401)
Epoch: [125][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.1998 (2.3520)	Acc@1 62.500 (56.506)	Acc@5 85.156 (85.207)
Epoch: [125][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3557 (2.3545)	Acc@1 63.281 (56.525)	Acc@5 82.031 (85.178)
Epoch: [125][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.3844 (2.3645)	Acc@1 53.125 (56.105)	Acc@5 83.594 (85.089)
Epoch: [125][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4250 (2.3665)	Acc@1 57.812 (55.984)	Acc@5 86.719 (85.156)
Epoch: [125][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2486 (2.3657)	Acc@1 60.156 (56.119)	Acc@5 85.938 (85.187)
Epoch: [125][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4331 (2.3706)	Acc@1 54.688 (55.926)	Acc@5 85.938 (85.142)
Epoch: [125][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4815 (2.3771)	Acc@1 55.469 (55.721)	Acc@5 78.906 (85.040)
Epoch: [125][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4567 (2.3819)	Acc@1 54.688 (55.612)	Acc@5 82.812 (84.942)
Epoch: [125][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3604 (2.3828)	Acc@1 55.469 (55.613)	Acc@5 86.719 (84.874)
Epoch: [125][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3557 (2.3806)	Acc@1 53.125 (55.702)	Acc@5 86.719 (84.929)
Epoch: [125][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6233 (2.3854)	Acc@1 54.688 (55.639)	Acc@5 77.344 (84.841)
Epoch: [125][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5750 (2.3858)	Acc@1 53.906 (55.688)	Acc@5 82.812 (84.836)
Epoch: [125][180/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.6941 (2.3807)	Acc@1 46.094 (55.754)	Acc@5 78.906 (84.949)
Epoch: [125][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7006 (2.3787)	Acc@1 50.781 (55.816)	Acc@5 79.688 (84.993)
Epoch: [125][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6414 (2.3802)	Acc@1 50.000 (55.854)	Acc@5 80.469 (84.970)
Epoch: [125][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3987 (2.3795)	Acc@1 60.938 (55.913)	Acc@5 82.812 (84.949)
Epoch: [125][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4563 (2.3825)	Acc@1 55.469 (55.907)	Acc@5 84.375 (84.881)
Epoch: [125][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2029 (2.3845)	Acc@1 57.812 (55.844)	Acc@5 84.375 (84.852)
Epoch: [125][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5403 (2.3832)	Acc@1 54.688 (55.848)	Acc@5 85.156 (84.933)
Epoch: [125][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4025 (2.3829)	Acc@1 59.375 (55.867)	Acc@5 82.031 (84.957)
Epoch: [125][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1203 (2.3874)	Acc@1 62.500 (55.789)	Acc@5 86.719 (84.866)
Epoch: [125][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4028 (2.3921)	Acc@1 53.906 (55.671)	Acc@5 86.719 (84.799)
Epoch: [125][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3327 (2.3918)	Acc@1 58.594 (55.663)	Acc@5 85.156 (84.825)
Epoch: [125][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5800 (2.3928)	Acc@1 54.688 (55.684)	Acc@5 79.688 (84.799)
Epoch: [125][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6568 (2.3938)	Acc@1 50.000 (55.645)	Acc@5 80.469 (84.819)
Epoch: [125][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4907 (2.3967)	Acc@1 51.562 (55.564)	Acc@5 83.594 (84.747)
Epoch: [125][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6594 (2.4004)	Acc@1 50.781 (55.474)	Acc@5 78.906 (84.655)
Epoch: [125][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3702 (2.4022)	Acc@1 55.469 (55.518)	Acc@5 88.281 (84.639)
Epoch: [125][340/391]	Time 0.014 (0.011)	Data 0.002 (0.002)	Loss 2.3819 (2.4016)	Acc@1 53.125 (55.581)	Acc@5 82.812 (84.641)
Epoch: [125][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5085 (2.4039)	Acc@1 46.875 (55.542)	Acc@5 85.156 (84.622)
Epoch: [125][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4191 (2.4039)	Acc@1 50.781 (55.538)	Acc@5 84.375 (84.620)
Epoch: [125][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5183 (2.4053)	Acc@1 53.906 (55.519)	Acc@5 82.031 (84.611)
Epoch: [125][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1859 (2.4070)	Acc@1 57.812 (55.471)	Acc@5 89.062 (84.582)
Epoch: [125][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.6279 (2.4099)	Acc@1 48.750 (55.412)	Acc@5 80.000 (84.524)
num momentum params: 26
[0.1, 2.409940269317627, 2.1126065063476562, 55.412, 45.95, tensor(0.3302, device='cuda:0', grad_fn=<DivBackward0>), 4.250610589981079, 0.34033966064453125]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [126 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [126][0/391]	Time 0.055 (0.055)	Data 0.188 (0.188)	Loss 2.3240 (2.3240)	Acc@1 55.469 (55.469)	Acc@5 85.938 (85.938)
Epoch: [126][10/391]	Time 0.014 (0.016)	Data 0.001 (0.018)	Loss 2.5413 (2.2723)	Acc@1 54.688 (58.878)	Acc@5 85.938 (87.287)
Epoch: [126][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.2406 (2.2573)	Acc@1 67.188 (59.933)	Acc@5 85.156 (87.500)
Epoch: [126][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.3954 (2.3029)	Acc@1 53.125 (58.291)	Acc@5 86.719 (86.845)
Epoch: [126][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.1339 (2.3159)	Acc@1 64.844 (57.736)	Acc@5 86.719 (86.490)
Epoch: [126][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1429 (2.2996)	Acc@1 61.719 (57.996)	Acc@5 88.281 (86.719)
Epoch: [126][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3555 (2.3042)	Acc@1 54.688 (57.684)	Acc@5 84.375 (86.603)
Epoch: [126][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.1997 (2.3173)	Acc@1 64.062 (57.251)	Acc@5 85.156 (86.411)
Epoch: [126][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3520 (2.3299)	Acc@1 56.250 (57.031)	Acc@5 84.375 (86.227)
Epoch: [126][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4328 (2.3349)	Acc@1 52.344 (56.808)	Acc@5 88.281 (86.118)
Epoch: [126][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5468 (2.3472)	Acc@1 52.344 (56.482)	Acc@5 84.375 (85.814)
Epoch: [126][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5824 (2.3573)	Acc@1 49.219 (56.363)	Acc@5 80.469 (85.515)
Epoch: [126][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6971 (2.3605)	Acc@1 49.219 (56.373)	Acc@5 80.469 (85.460)
Epoch: [126][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3904 (2.3693)	Acc@1 60.156 (56.214)	Acc@5 87.500 (85.299)
Epoch: [126][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4759 (2.3697)	Acc@1 54.688 (56.250)	Acc@5 81.250 (85.245)
Epoch: [126][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6761 (2.3723)	Acc@1 47.656 (56.209)	Acc@5 79.688 (85.229)
Epoch: [126][160/391]	Time 0.013 (0.011)	Data 0.001 (0.003)	Loss 2.1976 (2.3707)	Acc@1 64.062 (56.197)	Acc@5 85.938 (85.268)
Epoch: [126][170/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2964 (2.3735)	Acc@1 60.156 (56.186)	Acc@5 87.500 (85.238)
Epoch: [126][180/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3679 (2.3738)	Acc@1 54.688 (56.125)	Acc@5 85.156 (85.294)
Epoch: [126][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3671 (2.3723)	Acc@1 53.906 (56.160)	Acc@5 87.500 (85.340)
Epoch: [126][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1760 (2.3725)	Acc@1 62.500 (56.207)	Acc@5 88.281 (85.316)
Epoch: [126][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3441 (2.3696)	Acc@1 58.594 (56.324)	Acc@5 84.375 (85.338)
Epoch: [126][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2761 (2.3699)	Acc@1 54.688 (56.250)	Acc@5 85.938 (85.333)
Epoch: [126][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5209 (2.3705)	Acc@1 47.656 (56.179)	Acc@5 78.906 (85.302)
Epoch: [126][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6173 (2.3740)	Acc@1 50.781 (56.091)	Acc@5 87.500 (85.270)
Epoch: [126][250/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.5490 (2.3763)	Acc@1 49.219 (56.063)	Acc@5 84.375 (85.259)
Epoch: [126][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6258 (2.3803)	Acc@1 53.906 (56.002)	Acc@5 81.250 (85.180)
Epoch: [126][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4524 (2.3828)	Acc@1 55.469 (55.988)	Acc@5 87.500 (85.078)
Epoch: [126][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2805 (2.3833)	Acc@1 58.594 (56.019)	Acc@5 84.375 (85.031)
Epoch: [126][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4060 (2.3858)	Acc@1 56.250 (55.925)	Acc@5 84.375 (85.017)
Epoch: [126][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2301 (2.3863)	Acc@1 57.812 (55.902)	Acc@5 88.281 (85.047)
Epoch: [126][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2514 (2.3875)	Acc@1 57.031 (55.893)	Acc@5 87.500 (85.021)
Epoch: [126][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3150 (2.3887)	Acc@1 59.375 (55.846)	Acc@5 83.594 (84.991)
Epoch: [126][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6018 (2.3892)	Acc@1 49.219 (55.828)	Acc@5 78.125 (84.967)
Epoch: [126][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4402 (2.3922)	Acc@1 52.344 (55.751)	Acc@5 84.375 (84.918)
Epoch: [126][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4885 (2.3965)	Acc@1 50.781 (55.642)	Acc@5 86.719 (84.849)
Epoch: [126][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5651 (2.4001)	Acc@1 53.906 (55.557)	Acc@5 84.375 (84.788)
Epoch: [126][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2893 (2.3996)	Acc@1 60.156 (55.559)	Acc@5 86.719 (84.830)
Epoch: [126][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3764 (2.4021)	Acc@1 60.156 (55.502)	Acc@5 85.156 (84.758)
Epoch: [126][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.3359 (2.4016)	Acc@1 57.500 (55.538)	Acc@5 86.250 (84.766)
num momentum params: 26
[0.1, 2.401648896865845, 2.282573047876358, 55.538, 43.31, tensor(0.3317, device='cuda:0', grad_fn=<DivBackward0>), 4.254573583602905, 0.3369269371032715]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [127 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [127][0/391]	Time 0.056 (0.056)	Data 0.184 (0.184)	Loss 2.4504 (2.4504)	Acc@1 50.781 (50.781)	Acc@5 86.719 (86.719)
Epoch: [127][10/391]	Time 0.010 (0.016)	Data 0.001 (0.018)	Loss 2.2655 (2.3617)	Acc@1 62.500 (56.747)	Acc@5 85.156 (85.085)
Epoch: [127][20/391]	Time 0.013 (0.014)	Data 0.001 (0.010)	Loss 2.2875 (2.3514)	Acc@1 56.250 (56.845)	Acc@5 86.719 (85.379)
Epoch: [127][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.2198 (2.3474)	Acc@1 58.594 (57.132)	Acc@5 88.281 (85.585)
Epoch: [127][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.4030 (2.3590)	Acc@1 54.688 (56.860)	Acc@5 84.375 (85.480)
Epoch: [127][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4878 (2.3602)	Acc@1 53.906 (56.939)	Acc@5 82.031 (85.340)
Epoch: [127][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.1948 (2.3445)	Acc@1 57.031 (57.249)	Acc@5 87.500 (85.643)
Epoch: [127][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4436 (2.3471)	Acc@1 55.469 (57.273)	Acc@5 84.375 (85.486)
Epoch: [127][80/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.3330 (2.3377)	Acc@1 57.031 (57.562)	Acc@5 85.938 (85.581)
Epoch: [127][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1341 (2.3309)	Acc@1 62.500 (57.641)	Acc@5 89.844 (85.723)
Epoch: [127][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4018 (2.3342)	Acc@1 53.125 (57.426)	Acc@5 85.938 (85.721)
Epoch: [127][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2144 (2.3395)	Acc@1 56.250 (57.221)	Acc@5 85.156 (85.628)
Epoch: [127][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2735 (2.3403)	Acc@1 62.500 (57.296)	Acc@5 83.594 (85.563)
Epoch: [127][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6302 (2.3457)	Acc@1 57.031 (57.204)	Acc@5 79.688 (85.389)
Epoch: [127][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4003 (2.3465)	Acc@1 55.469 (57.059)	Acc@5 82.812 (85.422)
Epoch: [127][150/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.4838 (2.3482)	Acc@1 53.906 (57.021)	Acc@5 82.031 (85.374)
Epoch: [127][160/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.5260 (2.3531)	Acc@1 49.219 (56.895)	Acc@5 85.938 (85.365)
Epoch: [127][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6218 (2.3553)	Acc@1 54.688 (56.844)	Acc@5 79.688 (85.362)
Epoch: [127][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6983 (2.3592)	Acc@1 46.875 (56.733)	Acc@5 78.125 (85.346)
Epoch: [127][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3068 (2.3644)	Acc@1 56.250 (56.659)	Acc@5 86.719 (85.246)
Epoch: [127][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3667 (2.3684)	Acc@1 52.344 (56.553)	Acc@5 82.812 (85.183)
Epoch: [127][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6304 (2.3714)	Acc@1 48.438 (56.446)	Acc@5 78.125 (85.104)
Epoch: [127][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4461 (2.3708)	Acc@1 48.438 (56.487)	Acc@5 84.375 (85.110)
Epoch: [127][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1431 (2.3697)	Acc@1 61.719 (56.497)	Acc@5 92.969 (85.112)
Epoch: [127][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4106 (2.3741)	Acc@1 57.812 (56.380)	Acc@5 85.938 (85.030)
Epoch: [127][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5182 (2.3783)	Acc@1 50.000 (56.269)	Acc@5 85.938 (85.001)
Epoch: [127][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6677 (2.3832)	Acc@1 44.531 (56.097)	Acc@5 83.594 (84.938)
Epoch: [127][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3561 (2.3840)	Acc@1 59.375 (56.045)	Acc@5 82.812 (84.931)
Epoch: [127][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4432 (2.3847)	Acc@1 53.125 (56.003)	Acc@5 85.156 (84.906)
Epoch: [127][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.8921 (2.3850)	Acc@1 46.094 (56.016)	Acc@5 76.562 (84.917)
Epoch: [127][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3222 (2.3844)	Acc@1 54.688 (56.032)	Acc@5 88.281 (84.964)
Epoch: [127][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4465 (2.3873)	Acc@1 56.250 (56.019)	Acc@5 82.031 (84.898)
Epoch: [127][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4747 (2.3900)	Acc@1 50.781 (55.963)	Acc@5 84.375 (84.842)
Epoch: [127][330/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.6264 (2.3922)	Acc@1 50.781 (55.870)	Acc@5 81.250 (84.781)
Epoch: [127][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3462 (2.3924)	Acc@1 57.812 (55.918)	Acc@5 87.500 (84.787)
Epoch: [127][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1964 (2.3950)	Acc@1 61.719 (55.858)	Acc@5 89.062 (84.753)
Epoch: [127][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0776 (2.3952)	Acc@1 60.156 (55.824)	Acc@5 88.281 (84.793)
Epoch: [127][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5080 (2.3954)	Acc@1 51.562 (55.791)	Acc@5 82.031 (84.769)
Epoch: [127][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5759 (2.3953)	Acc@1 53.906 (55.834)	Acc@5 79.688 (84.758)
Epoch: [127][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.5397 (2.3964)	Acc@1 52.500 (55.766)	Acc@5 85.000 (84.730)
num momentum params: 26
[0.1, 2.3964410945129395, 2.0410444474220277, 55.766, 47.67, tensor(0.3318, device='cuda:0', grad_fn=<DivBackward0>), 4.27762508392334, 0.3353264331817627]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [128 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [128][0/391]	Time 0.060 (0.060)	Data 0.181 (0.181)	Loss 2.3968 (2.3968)	Acc@1 59.375 (59.375)	Acc@5 83.594 (83.594)
Epoch: [128][10/391]	Time 0.011 (0.017)	Data 0.001 (0.018)	Loss 2.4080 (2.3522)	Acc@1 57.812 (56.889)	Acc@5 84.375 (86.009)
Epoch: [128][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.2974 (2.3099)	Acc@1 57.031 (57.701)	Acc@5 85.156 (86.644)
Epoch: [128][30/391]	Time 0.010 (0.013)	Data 0.002 (0.007)	Loss 2.1742 (2.3111)	Acc@1 61.719 (57.611)	Acc@5 90.625 (86.794)
Epoch: [128][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.2962 (2.2968)	Acc@1 64.062 (58.384)	Acc@5 86.719 (86.757)
Epoch: [128][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3683 (2.3033)	Acc@1 63.281 (58.241)	Acc@5 85.156 (86.719)
Epoch: [128][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4471 (2.3075)	Acc@1 55.469 (58.056)	Acc@5 84.375 (86.539)
Epoch: [128][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4334 (2.3198)	Acc@1 55.469 (57.702)	Acc@5 81.250 (86.312)
Epoch: [128][80/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 2.2676 (2.3304)	Acc@1 57.031 (57.475)	Acc@5 88.281 (86.169)
Epoch: [128][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4311 (2.3274)	Acc@1 56.250 (57.469)	Acc@5 85.938 (86.289)
Epoch: [128][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2929 (2.3269)	Acc@1 60.938 (57.673)	Acc@5 85.938 (86.146)
Epoch: [128][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5653 (2.3296)	Acc@1 52.344 (57.517)	Acc@5 83.594 (86.092)
Epoch: [128][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5835 (2.3356)	Acc@1 53.906 (57.503)	Acc@5 81.250 (85.950)
Epoch: [128][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5501 (2.3428)	Acc@1 50.781 (57.240)	Acc@5 80.469 (85.836)
Epoch: [128][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2319 (2.3405)	Acc@1 57.812 (57.353)	Acc@5 89.062 (85.816)
Epoch: [128][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6461 (2.3485)	Acc@1 54.688 (57.181)	Acc@5 82.031 (85.622)
Epoch: [128][160/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.5616 (2.3564)	Acc@1 51.562 (56.934)	Acc@5 81.250 (85.520)
Epoch: [128][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2595 (2.3642)	Acc@1 61.719 (56.766)	Acc@5 85.156 (85.403)
Epoch: [128][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5180 (2.3635)	Acc@1 51.562 (56.794)	Acc@5 82.812 (85.402)
Epoch: [128][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.1998 (2.3630)	Acc@1 59.375 (56.786)	Acc@5 84.375 (85.369)
Epoch: [128][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4621 (2.3658)	Acc@1 59.375 (56.709)	Acc@5 83.594 (85.323)
Epoch: [128][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.0386 (2.3664)	Acc@1 65.625 (56.620)	Acc@5 89.062 (85.334)
Epoch: [128][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3935 (2.3678)	Acc@1 53.906 (56.586)	Acc@5 82.812 (85.230)
Epoch: [128][230/391]	Time 0.011 (0.011)	Data 0.005 (0.002)	Loss 2.6546 (2.3705)	Acc@1 50.781 (56.456)	Acc@5 82.031 (85.200)
Epoch: [128][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3518 (2.3732)	Acc@1 58.594 (56.412)	Acc@5 82.031 (85.111)
Epoch: [128][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4084 (2.3755)	Acc@1 56.250 (56.393)	Acc@5 88.281 (85.060)
Epoch: [128][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3141 (2.3763)	Acc@1 57.031 (56.427)	Acc@5 87.500 (85.051)
Epoch: [128][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6386 (2.3806)	Acc@1 50.000 (56.316)	Acc@5 82.031 (84.975)
Epoch: [128][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4451 (2.3776)	Acc@1 59.375 (56.433)	Acc@5 84.375 (85.009)
Epoch: [128][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4932 (2.3808)	Acc@1 53.906 (56.314)	Acc@5 86.719 (84.947)
Epoch: [128][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3299 (2.3829)	Acc@1 58.594 (56.237)	Acc@5 83.594 (84.902)
Epoch: [128][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5160 (2.3866)	Acc@1 50.781 (56.119)	Acc@5 85.156 (84.825)
Epoch: [128][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2381 (2.3851)	Acc@1 62.500 (56.170)	Acc@5 86.719 (84.845)
Epoch: [128][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3679 (2.3864)	Acc@1 53.125 (56.118)	Acc@5 86.719 (84.809)
Epoch: [128][340/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.3077 (2.3876)	Acc@1 55.469 (56.108)	Acc@5 85.156 (84.785)
Epoch: [128][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3478 (2.3883)	Acc@1 59.375 (56.103)	Acc@5 85.156 (84.809)
Epoch: [128][360/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3253 (2.3887)	Acc@1 57.031 (56.094)	Acc@5 86.719 (84.782)
Epoch: [128][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3666 (2.3897)	Acc@1 57.812 (56.048)	Acc@5 87.500 (84.748)
Epoch: [128][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7566 (2.3911)	Acc@1 49.219 (56.014)	Acc@5 78.906 (84.709)
Epoch: [128][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.5003 (2.3923)	Acc@1 57.500 (56.024)	Acc@5 80.000 (84.664)
num momentum params: 26
[0.1, 2.3923053424835206, 2.0586376714706422, 56.024, 46.4, tensor(0.3329, device='cuda:0', grad_fn=<DivBackward0>), 4.316897630691528, 0.3415076732635498]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [129 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [129][0/391]	Time 0.058 (0.058)	Data 0.179 (0.179)	Loss 2.5766 (2.5766)	Acc@1 54.688 (54.688)	Acc@5 81.250 (81.250)
Epoch: [129][10/391]	Time 0.010 (0.017)	Data 0.001 (0.019)	Loss 2.5680 (2.3738)	Acc@1 53.125 (57.244)	Acc@5 81.250 (86.080)
Epoch: [129][20/391]	Time 0.012 (0.014)	Data 0.001 (0.011)	Loss 2.1939 (2.3194)	Acc@1 64.062 (58.929)	Acc@5 86.719 (86.310)
Epoch: [129][30/391]	Time 0.012 (0.013)	Data 0.001 (0.008)	Loss 2.2516 (2.3059)	Acc@1 62.500 (58.644)	Acc@5 83.594 (86.190)
Epoch: [129][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.3734 (2.3301)	Acc@1 57.031 (57.984)	Acc@5 85.938 (86.014)
Epoch: [129][50/391]	Time 0.010 (0.013)	Data 0.001 (0.005)	Loss 2.3597 (2.3316)	Acc@1 57.812 (58.042)	Acc@5 81.250 (85.876)
Epoch: [129][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4108 (2.3314)	Acc@1 54.688 (57.902)	Acc@5 86.719 (86.027)
Epoch: [129][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5861 (2.3339)	Acc@1 48.438 (57.934)	Acc@5 82.812 (86.081)
Epoch: [129][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4971 (2.3380)	Acc@1 51.562 (57.745)	Acc@5 84.375 (85.889)
Epoch: [129][90/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4297 (2.3454)	Acc@1 52.344 (57.512)	Acc@5 86.719 (85.740)
Epoch: [129][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3469 (2.3508)	Acc@1 56.250 (57.310)	Acc@5 84.375 (85.659)
Epoch: [129][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2711 (2.3506)	Acc@1 58.594 (57.158)	Acc@5 89.062 (85.656)
Epoch: [129][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5085 (2.3604)	Acc@1 55.469 (56.909)	Acc@5 82.812 (85.492)
Epoch: [129][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4203 (2.3613)	Acc@1 56.250 (56.799)	Acc@5 82.812 (85.472)
Epoch: [129][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2329 (2.3665)	Acc@1 60.938 (56.605)	Acc@5 89.062 (85.411)
Epoch: [129][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1170 (2.3655)	Acc@1 61.719 (56.529)	Acc@5 86.719 (85.425)
Epoch: [129][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1356 (2.3690)	Acc@1 63.281 (56.459)	Acc@5 89.062 (85.404)
Epoch: [129][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3740 (2.3761)	Acc@1 54.688 (56.200)	Acc@5 82.812 (85.207)
Epoch: [129][180/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4045 (2.3779)	Acc@1 60.156 (56.116)	Acc@5 84.375 (85.156)
Epoch: [129][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2610 (2.3780)	Acc@1 59.375 (56.123)	Acc@5 86.719 (85.124)
Epoch: [129][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5680 (2.3796)	Acc@1 51.562 (56.106)	Acc@5 82.031 (85.121)
Epoch: [129][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.0832 (2.3779)	Acc@1 64.844 (56.206)	Acc@5 85.938 (85.101)
Epoch: [129][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3486 (2.3818)	Acc@1 56.250 (56.112)	Acc@5 86.719 (85.117)
Epoch: [129][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4426 (2.3850)	Acc@1 54.688 (56.034)	Acc@5 85.156 (85.041)
Epoch: [129][240/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.1434 (2.3804)	Acc@1 63.281 (56.137)	Acc@5 87.500 (85.140)
Epoch: [129][250/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.5531 (2.3833)	Acc@1 52.344 (56.076)	Acc@5 82.812 (85.057)
Epoch: [129][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3855 (2.3830)	Acc@1 53.125 (56.103)	Acc@5 86.719 (85.084)
Epoch: [129][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2096 (2.3841)	Acc@1 62.500 (56.025)	Acc@5 87.500 (85.073)
Epoch: [129][280/391]	Time 0.016 (0.011)	Data 0.001 (0.002)	Loss 2.2176 (2.3832)	Acc@1 58.594 (56.047)	Acc@5 85.156 (85.062)
Epoch: [129][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5311 (2.3850)	Acc@1 55.469 (56.057)	Acc@5 82.031 (85.017)
Epoch: [129][300/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.5781 (2.3860)	Acc@1 50.781 (56.011)	Acc@5 82.812 (84.962)
Epoch: [129][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1603 (2.3845)	Acc@1 63.281 (56.064)	Acc@5 89.844 (84.993)
Epoch: [129][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5410 (2.3862)	Acc@1 51.562 (56.031)	Acc@5 82.812 (84.947)
Epoch: [129][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1971 (2.3865)	Acc@1 57.812 (56.023)	Acc@5 87.500 (84.960)
Epoch: [129][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3459 (2.3881)	Acc@1 57.031 (55.961)	Acc@5 86.719 (84.932)
Epoch: [129][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2211 (2.3874)	Acc@1 66.406 (55.985)	Acc@5 87.500 (84.936)
Epoch: [129][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2208 (2.3873)	Acc@1 58.594 (56.001)	Acc@5 85.938 (84.946)
Epoch: [129][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5463 (2.3889)	Acc@1 50.781 (55.989)	Acc@5 78.906 (84.906)
Epoch: [129][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3320 (2.3927)	Acc@1 55.469 (55.873)	Acc@5 85.938 (84.853)
Epoch: [129][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.2029 (2.3940)	Acc@1 58.750 (55.838)	Acc@5 86.250 (84.820)
num momentum params: 26
[0.1, 2.393965068511963, 1.8634366405010223, 55.838, 50.63, tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>), 4.292653799057007, 0.3234219551086426]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [209, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [209]
Non Pruning Epoch - module.bn3.bias: [209]
Non Pruning Epoch - module.conv4.weight: [247, 209, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [359, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [359]
Non Pruning Epoch - module.bn5.bias: [359]
Non Pruning Epoch - module.conv6.weight: [313, 359, 3, 3]
Non Pruning Epoch - module.bn6.weight: [313]
Non Pruning Epoch - module.bn6.bias: [313]
Non Pruning Epoch - module.conv7.weight: [194, 313, 3, 3]
Non Pruning Epoch - module.bn7.weight: [194]
Non Pruning Epoch - module.bn7.bias: [194]
Non Pruning Epoch - module.conv8.weight: [192, 194, 3, 3]
Non Pruning Epoch - module.bn8.weight: [192]
Non Pruning Epoch - module.bn8.bias: [192]
Non Pruning Epoch - module.fc.weight: [100, 192]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [130 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [209, 108, 3, 3]
module.conv4.weight [247, 209, 3, 3]
module.conv5.weight [359, 247, 3, 3]
module.conv6.weight [313, 359, 3, 3]
module.conv7.weight [194, 313, 3, 3]
module.conv8.weight [192, 194, 3, 3]
Epoch: [130][0/391]	Time 0.056 (0.056)	Data 0.193 (0.193)	Loss 2.2408 (2.2408)	Acc@1 58.594 (58.594)	Acc@5 87.500 (87.500)
Epoch: [130][10/391]	Time 0.012 (0.016)	Data 0.001 (0.019)	Loss 2.5428 (2.3001)	Acc@1 48.438 (57.244)	Acc@5 86.719 (86.009)
Epoch: [130][20/391]	Time 0.011 (0.014)	Data 0.001 (0.011)	Loss 2.4797 (2.3207)	Acc@1 53.906 (57.329)	Acc@5 82.812 (85.751)
Epoch: [130][30/391]	Time 0.010 (0.013)	Data 0.001 (0.008)	Loss 2.1131 (2.3234)	Acc@1 60.156 (56.779)	Acc@5 89.844 (86.215)
Epoch: [130][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.2734 (2.3104)	Acc@1 58.594 (57.241)	Acc@5 84.375 (86.490)
Epoch: [130][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3246 (2.3283)	Acc@1 62.500 (56.893)	Acc@5 83.594 (86.167)
Epoch: [130][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1614 (2.3319)	Acc@1 62.500 (56.839)	Acc@5 84.375 (86.194)
Epoch: [130][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2832 (2.3335)	Acc@1 60.156 (56.855)	Acc@5 85.156 (86.103)
Epoch: [130][80/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.5753 (2.3428)	Acc@1 53.906 (56.790)	Acc@5 84.375 (85.725)
Epoch: [130][90/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.3747 (2.3471)	Acc@1 53.125 (56.628)	Acc@5 84.375 (85.731)
Epoch: [130][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3787 (2.3477)	Acc@1 57.812 (56.675)	Acc@5 82.812 (85.628)
Epoch: [130][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2863 (2.3465)	Acc@1 53.125 (56.679)	Acc@5 89.844 (85.593)
Epoch: [130][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4560 (2.3503)	Acc@1 53.125 (56.547)	Acc@5 78.906 (85.486)
Epoch: [130][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3893 (2.3558)	Acc@1 49.219 (56.417)	Acc@5 84.375 (85.448)
Epoch: [130][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4323 (2.3531)	Acc@1 52.344 (56.494)	Acc@5 85.938 (85.455)
Epoch: [130][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2862 (2.3537)	Acc@1 58.594 (56.467)	Acc@5 86.719 (85.544)
Epoch: [130][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.7524 (2.3564)	Acc@1 50.000 (56.391)	Acc@5 78.906 (85.447)
Epoch: [130][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1689 (2.3556)	Acc@1 62.500 (56.469)	Acc@5 87.500 (85.462)
Epoch: [130][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4164 (2.3595)	Acc@1 50.781 (56.297)	Acc@5 85.938 (85.441)
Epoch: [130][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4485 (2.3629)	Acc@1 54.688 (56.242)	Acc@5 82.812 (85.348)
Epoch: [130][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3028 (2.3668)	Acc@1 56.250 (56.266)	Acc@5 85.156 (85.296)
Epoch: [130][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3857 (2.3710)	Acc@1 57.812 (56.243)	Acc@5 82.031 (85.275)
Epoch: [130][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3670 (2.3697)	Acc@1 55.469 (56.299)	Acc@5 85.938 (85.337)
Epoch: [130][230/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.2520 (2.3699)	Acc@1 57.031 (56.284)	Acc@5 87.500 (85.268)
Epoch: [130][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1926 (2.3697)	Acc@1 58.594 (56.240)	Acc@5 88.281 (85.270)
Epoch: [130][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3830 (2.3717)	Acc@1 55.469 (56.191)	Acc@5 85.156 (85.222)
Epoch: [130][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4636 (2.3758)	Acc@1 49.219 (56.088)	Acc@5 85.938 (85.138)
Epoch: [130][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3998 (2.3763)	Acc@1 57.812 (56.140)	Acc@5 78.906 (85.104)
Epoch: [130][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6413 (2.3793)	Acc@1 53.906 (56.069)	Acc@5 80.469 (85.020)
Epoch: [130][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.8222 (2.3833)	Acc@1 48.438 (56.000)	Acc@5 78.906 (84.936)
Epoch: [130][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6338 (2.3855)	Acc@1 49.219 (55.977)	Acc@5 82.812 (84.923)
Epoch: [130][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4444 (2.3890)	Acc@1 50.781 (55.866)	Acc@5 83.594 (84.857)
Epoch: [130][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4105 (2.3889)	Acc@1 57.812 (55.902)	Acc@5 84.375 (84.859)
Epoch: [130][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4865 (2.3874)	Acc@1 57.031 (55.946)	Acc@5 84.375 (84.892)
Epoch: [130][340/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5543 (2.3892)	Acc@1 51.562 (55.938)	Acc@5 82.812 (84.831)
Epoch: [130][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1467 (2.3897)	Acc@1 59.375 (55.912)	Acc@5 89.844 (84.838)
Epoch: [130][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4114 (2.3911)	Acc@1 59.375 (55.871)	Acc@5 85.156 (84.819)
Epoch: [130][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3133 (2.3899)	Acc@1 55.469 (55.913)	Acc@5 84.375 (84.861)
Epoch: [130][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2813 (2.3928)	Acc@1 57.812 (55.856)	Acc@5 86.719 (84.816)
Epoch: [130][390/391]	Time 0.015 (0.011)	Data 0.000 (0.002)	Loss 2.6725 (2.3929)	Acc@1 50.000 (55.858)	Acc@5 78.750 (84.820)
num momentum params: 26
[0.1, 2.3929495993041994, 2.0555524861812593, 55.858, 46.9, tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>), 4.266908884048462, 0.33560824394226074]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [108, 32, 3, 3]
Before - module.bn2.weight: [108]
Before - module.bn2.bias: [108]
Before - module.conv3.weight: [209, 108, 3, 3]
Before - module.bn3.weight: [209]
Before - module.bn3.bias: [209]
Before - module.conv4.weight: [247, 209, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [359, 247, 3, 3]
Before - module.bn5.weight: [359]
Before - module.bn5.bias: [359]
Before - module.conv6.weight: [313, 359, 3, 3]
Before - module.bn6.weight: [313]
Before - module.bn6.bias: [313]
Before - module.conv7.weight: [194, 313, 3, 3]
Before - module.bn7.weight: [194]
Before - module.bn7.bias: [194]
Before - module.conv8.weight: [192, 194, 3, 3]
Before - module.bn8.weight: [192]
Before - module.bn8.bias: [192]
Before - module.fc.weight: [100, 192]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [108, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 108 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [209, 108, 3, 3] >> [206, 108, 3, 3]
[module.bn3.weight]: 209 >> 206
running_mean [206]
running_var [206]
num_batches_tracked []
[module.conv4.weight]: [247, 209, 3, 3] >> [247, 206, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [359, 247, 3, 3] >> [357, 247, 3, 3]
[module.bn5.weight]: 359 >> 357
running_mean [357]
running_var [357]
num_batches_tracked []
[module.conv6.weight]: [313, 359, 3, 3] >> [311, 357, 3, 3]
[module.bn6.weight]: 313 >> 311
running_mean [311]
running_var [311]
num_batches_tracked []
[module.conv7.weight]: [194, 313, 3, 3] >> [192, 311, 3, 3]
[module.bn7.weight]: 194 >> 192
running_mean [192]
running_var [192]
num_batches_tracked []
[module.conv8.weight]: [192, 194, 3, 3] >> [191, 192, 3, 3]
[module.bn8.weight]: 192 >> 191
running_mean [191]
running_var [191]
num_batches_tracked []
[module.fc.weight]: [100, 192] >> [100, 191]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [206, 108, 3, 3]
After - module.bn3.weight: [206]
After - module.bn3.bias: [206]
After - module.conv4.weight: [247, 206, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [357, 247, 3, 3]
After - module.bn5.weight: [357]
After - module.bn5.bias: [357]
After - module.conv6.weight: [311, 357, 3, 3]
After - module.bn6.weight: [311]
After - module.bn6.bias: [311]
After - module.conv7.weight: [192, 311, 3, 3]
After - module.bn7.weight: [192]
After - module.bn7.bias: [192]
After - module.conv8.weight: [191, 192, 3, 3]
After - module.bn8.weight: [191]
After - module.bn8.bias: [191]
After - module.fc.weight: [100, 191]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [206, 108, 3, 3]
conv4 --> [247, 206, 3, 3]
conv5 --> [357, 247, 3, 3]
conv6 --> [311, 357, 3, 3]
conv7 --> [192, 311, 3, 3]
conv8 --> [191, 192, 3, 3]
fc --> [191, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5843570688, 12814848, 206
4, 13364462592, 29308032, 247
5, 6907590144, 12697776, 357
6, 8697411072, 15987888, 311
7, 1650917376, 2149632, 192
8, 1013907456, 1320192, 191
fc, 7334400, 19100, 0
===================
FLOP REPORT: 16081213800000.0 40651200000.0 83144828 101628 1644 6.430402755737305
[INFO] Storing checkpoint...

Epoch: [131 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [131][0/391]	Time 0.241 (0.241)	Data 0.182 (0.182)	Loss 2.2778 (2.2778)	Acc@1 57.031 (57.031)	Acc@5 87.500 (87.500)
Epoch: [131][10/391]	Time 0.010 (0.033)	Data 0.001 (0.018)	Loss 2.6117 (2.3523)	Acc@1 50.781 (56.889)	Acc@5 82.031 (85.653)
Epoch: [131][20/391]	Time 0.012 (0.023)	Data 0.001 (0.010)	Loss 2.3536 (2.2920)	Acc@1 53.906 (58.073)	Acc@5 85.156 (86.421)
Epoch: [131][30/391]	Time 0.013 (0.019)	Data 0.001 (0.007)	Loss 2.1326 (2.2621)	Acc@1 59.375 (58.317)	Acc@5 91.406 (86.794)
Epoch: [131][40/391]	Time 0.010 (0.017)	Data 0.001 (0.006)	Loss 1.8754 (2.2719)	Acc@1 68.750 (57.774)	Acc@5 92.188 (86.604)
Epoch: [131][50/391]	Time 0.013 (0.016)	Data 0.001 (0.005)	Loss 2.1825 (2.2725)	Acc@1 60.938 (57.705)	Acc@5 87.500 (86.566)
Epoch: [131][60/391]	Time 0.011 (0.015)	Data 0.001 (0.004)	Loss 2.2744 (2.2764)	Acc@1 55.469 (57.684)	Acc@5 87.500 (86.514)
Epoch: [131][70/391]	Time 0.011 (0.014)	Data 0.001 (0.004)	Loss 2.3805 (2.2950)	Acc@1 54.688 (57.295)	Acc@5 85.938 (86.334)
Epoch: [131][80/391]	Time 0.011 (0.014)	Data 0.001 (0.004)	Loss 2.1705 (2.3069)	Acc@1 62.500 (57.157)	Acc@5 92.188 (86.179)
Epoch: [131][90/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.3093 (2.3091)	Acc@1 60.938 (57.169)	Acc@5 88.281 (86.264)
Epoch: [131][100/391]	Time 0.011 (0.013)	Data 0.002 (0.003)	Loss 2.3045 (2.3118)	Acc@1 57.812 (57.186)	Acc@5 87.500 (86.177)
Epoch: [131][110/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.4151 (2.3227)	Acc@1 54.688 (56.848)	Acc@5 85.938 (86.078)
Epoch: [131][120/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.7498 (2.3344)	Acc@1 47.656 (56.618)	Acc@5 77.344 (85.925)
Epoch: [131][130/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.1878 (2.3488)	Acc@1 52.344 (56.333)	Acc@5 90.625 (85.633)
Epoch: [131][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3439 (2.3516)	Acc@1 53.125 (56.333)	Acc@5 82.812 (85.588)
Epoch: [131][150/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2880 (2.3608)	Acc@1 55.469 (56.012)	Acc@5 85.156 (85.446)
Epoch: [131][160/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3136 (2.3653)	Acc@1 56.250 (56.041)	Acc@5 85.156 (85.365)
Epoch: [131][170/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.6547 (2.3744)	Acc@1 55.469 (55.866)	Acc@5 82.812 (85.321)
Epoch: [131][180/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 2.2730 (2.3799)	Acc@1 57.031 (55.814)	Acc@5 86.719 (85.169)
Epoch: [131][190/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 2.3149 (2.3802)	Acc@1 57.031 (55.788)	Acc@5 86.719 (85.169)
Epoch: [131][200/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5398 (2.3800)	Acc@1 54.688 (55.854)	Acc@5 79.688 (85.148)
Epoch: [131][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2252 (2.3788)	Acc@1 62.500 (55.924)	Acc@5 85.938 (85.153)
Epoch: [131][220/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 2.4127 (2.3810)	Acc@1 59.375 (55.889)	Acc@5 84.375 (85.124)
Epoch: [131][230/391]	Time 0.010 (0.012)	Data 0.003 (0.002)	Loss 2.3752 (2.3850)	Acc@1 55.469 (55.851)	Acc@5 78.125 (84.936)
Epoch: [131][240/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5027 (2.3824)	Acc@1 53.906 (55.936)	Acc@5 83.594 (84.952)
Epoch: [131][250/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 2.6268 (2.3820)	Acc@1 53.125 (55.985)	Acc@5 83.594 (84.979)
Epoch: [131][260/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4831 (2.3819)	Acc@1 54.688 (56.028)	Acc@5 82.031 (84.974)
Epoch: [131][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.5443 (2.3811)	Acc@1 50.781 (56.014)	Acc@5 80.469 (85.001)
Epoch: [131][280/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.4567 (2.3838)	Acc@1 53.906 (55.955)	Acc@5 85.938 (84.973)
Epoch: [131][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.6773 (2.3883)	Acc@1 49.219 (55.901)	Acc@5 74.219 (84.877)
Epoch: [131][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3155 (2.3888)	Acc@1 55.469 (55.874)	Acc@5 85.938 (84.884)
Epoch: [131][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3173 (2.3923)	Acc@1 53.125 (55.810)	Acc@5 84.375 (84.832)
Epoch: [131][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3484 (2.3923)	Acc@1 60.938 (55.812)	Acc@5 85.156 (84.796)
Epoch: [131][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3925 (2.3911)	Acc@1 54.688 (55.853)	Acc@5 87.500 (84.842)
Epoch: [131][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3441 (2.3931)	Acc@1 57.031 (55.810)	Acc@5 85.938 (84.787)
Epoch: [131][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.0956 (2.3932)	Acc@1 62.500 (55.818)	Acc@5 89.844 (84.789)
Epoch: [131][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5957 (2.3937)	Acc@1 49.219 (55.811)	Acc@5 78.125 (84.819)
Epoch: [131][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4932 (2.3976)	Acc@1 53.125 (55.721)	Acc@5 85.938 (84.765)
Epoch: [131][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4382 (2.3991)	Acc@1 54.688 (55.674)	Acc@5 84.375 (84.742)
Epoch: [131][390/391]	Time 0.121 (0.012)	Data 0.001 (0.002)	Loss 2.6828 (2.3997)	Acc@1 47.500 (55.650)	Acc@5 80.000 (84.752)
num momentum params: 26
[0.1, 2.3996815188598632, 2.4001720297336577, 55.65, 41.83, tensor(0.3316, device='cuda:0', grad_fn=<DivBackward0>), 4.545868396759033, 0.39785337448120117]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [132 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [132][0/391]	Time 0.049 (0.049)	Data 0.184 (0.184)	Loss 2.1661 (2.1661)	Acc@1 58.594 (58.594)	Acc@5 91.406 (91.406)
Epoch: [132][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 2.2480 (2.2755)	Acc@1 56.250 (57.670)	Acc@5 89.062 (87.216)
Epoch: [132][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.3160 (2.2857)	Acc@1 57.031 (57.440)	Acc@5 87.500 (87.091)
Epoch: [132][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.4613 (2.2911)	Acc@1 57.812 (57.560)	Acc@5 84.375 (86.744)
Epoch: [132][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.2023 (2.3032)	Acc@1 59.375 (57.241)	Acc@5 89.062 (86.433)
Epoch: [132][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1051 (2.3207)	Acc@1 58.594 (56.618)	Acc@5 86.719 (86.382)
Epoch: [132][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.1883 (2.3263)	Acc@1 65.625 (56.647)	Acc@5 85.938 (86.232)
Epoch: [132][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3450 (2.3275)	Acc@1 60.156 (56.514)	Acc@5 85.938 (86.268)
Epoch: [132][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2618 (2.3267)	Acc@1 53.906 (56.510)	Acc@5 85.156 (86.169)
Epoch: [132][90/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3383 (2.3191)	Acc@1 57.812 (56.894)	Acc@5 85.156 (86.212)
Epoch: [132][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3708 (2.3223)	Acc@1 62.500 (57.016)	Acc@5 82.812 (86.084)
Epoch: [132][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4006 (2.3279)	Acc@1 54.688 (57.045)	Acc@5 85.938 (85.923)
Epoch: [132][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3893 (2.3316)	Acc@1 49.219 (56.921)	Acc@5 89.844 (85.905)
Epoch: [132][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4172 (2.3362)	Acc@1 60.938 (56.924)	Acc@5 80.469 (85.854)
Epoch: [132][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3357 (2.3401)	Acc@1 57.812 (56.943)	Acc@5 85.156 (85.760)
Epoch: [132][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6122 (2.3449)	Acc@1 53.906 (56.824)	Acc@5 82.812 (85.694)
Epoch: [132][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2658 (2.3439)	Acc@1 58.594 (56.832)	Acc@5 86.719 (85.700)
Epoch: [132][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7656 (2.3489)	Acc@1 47.656 (56.675)	Acc@5 79.688 (85.673)
Epoch: [132][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6135 (2.3535)	Acc@1 50.000 (56.591)	Acc@5 84.375 (85.579)
Epoch: [132][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3637 (2.3576)	Acc@1 53.906 (56.467)	Acc@5 85.156 (85.475)
Epoch: [132][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4748 (2.3556)	Acc@1 53.906 (56.549)	Acc@5 83.594 (85.487)
Epoch: [132][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3228 (2.3590)	Acc@1 60.156 (56.531)	Acc@5 84.375 (85.430)
Epoch: [132][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4279 (2.3617)	Acc@1 53.906 (56.480)	Acc@5 82.812 (85.375)
Epoch: [132][230/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.6969 (2.3639)	Acc@1 50.781 (56.406)	Acc@5 78.906 (85.308)
Epoch: [132][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5587 (2.3664)	Acc@1 44.531 (56.318)	Acc@5 85.156 (85.302)
Epoch: [132][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4565 (2.3673)	Acc@1 52.344 (56.300)	Acc@5 85.156 (85.278)
Epoch: [132][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7902 (2.3730)	Acc@1 51.562 (56.238)	Acc@5 78.125 (85.165)
Epoch: [132][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2836 (2.3748)	Acc@1 57.031 (56.166)	Acc@5 83.594 (85.110)
Epoch: [132][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2655 (2.3782)	Acc@1 60.938 (56.122)	Acc@5 85.938 (85.023)
Epoch: [132][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4525 (2.3785)	Acc@1 54.688 (56.073)	Acc@5 82.031 (85.009)
Epoch: [132][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6183 (2.3796)	Acc@1 50.781 (56.040)	Acc@5 78.125 (84.975)
Epoch: [132][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7105 (2.3787)	Acc@1 47.656 (56.072)	Acc@5 80.469 (85.013)
Epoch: [132][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2970 (2.3803)	Acc@1 56.250 (56.038)	Acc@5 84.375 (84.969)
Epoch: [132][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3634 (2.3810)	Acc@1 59.375 (56.042)	Acc@5 85.156 (84.972)
Epoch: [132][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3798 (2.3814)	Acc@1 57.812 (56.060)	Acc@5 84.375 (84.936)
Epoch: [132][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4716 (2.3834)	Acc@1 53.906 (55.994)	Acc@5 84.375 (84.898)
Epoch: [132][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2274 (2.3849)	Acc@1 61.719 (55.992)	Acc@5 89.062 (84.855)
Epoch: [132][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4300 (2.3869)	Acc@1 50.781 (55.909)	Acc@5 85.156 (84.857)
Epoch: [132][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4421 (2.3892)	Acc@1 47.656 (55.850)	Acc@5 83.594 (84.832)
Epoch: [132][390/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.4748 (2.3905)	Acc@1 50.000 (55.806)	Acc@5 85.000 (84.786)
num momentum params: 26
[0.1, 2.3904962947082518, 1.9222849988937378, 55.806, 49.45, tensor(0.3322, device='cuda:0', grad_fn=<DivBackward0>), 4.244110822677612, 0.3347585201263428]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [133 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [133][0/391]	Time 0.049 (0.049)	Data 0.179 (0.179)	Loss 2.3415 (2.3415)	Acc@1 57.812 (57.812)	Acc@5 84.375 (84.375)
Epoch: [133][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 2.3666 (2.3560)	Acc@1 57.031 (56.037)	Acc@5 86.719 (85.440)
Epoch: [133][20/391]	Time 0.011 (0.013)	Data 0.002 (0.010)	Loss 2.4424 (2.3522)	Acc@1 53.906 (56.176)	Acc@5 81.250 (85.714)
Epoch: [133][30/391]	Time 0.010 (0.013)	Data 0.002 (0.007)	Loss 2.1166 (2.3116)	Acc@1 60.156 (57.132)	Acc@5 90.625 (86.240)
Epoch: [133][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.4299 (2.3202)	Acc@1 55.469 (57.069)	Acc@5 85.156 (85.842)
Epoch: [133][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2675 (2.3267)	Acc@1 61.719 (57.138)	Acc@5 83.594 (85.692)
Epoch: [133][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2819 (2.3349)	Acc@1 60.938 (57.198)	Acc@5 83.594 (85.451)
Epoch: [133][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.3147 (2.3450)	Acc@1 57.812 (56.976)	Acc@5 89.062 (85.222)
Epoch: [133][80/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2244 (2.3543)	Acc@1 56.250 (56.916)	Acc@5 84.375 (84.992)
Epoch: [133][90/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.0932 (2.3583)	Acc@1 60.938 (56.757)	Acc@5 89.062 (85.036)
Epoch: [133][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0989 (2.3527)	Acc@1 64.062 (56.869)	Acc@5 88.281 (85.102)
Epoch: [133][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5239 (2.3535)	Acc@1 56.250 (56.813)	Acc@5 82.812 (85.121)
Epoch: [133][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2111 (2.3534)	Acc@1 64.062 (56.754)	Acc@5 83.594 (85.092)
Epoch: [133][130/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.4294 (2.3539)	Acc@1 55.469 (56.661)	Acc@5 82.031 (85.168)
Epoch: [133][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6252 (2.3558)	Acc@1 48.438 (56.521)	Acc@5 82.812 (85.145)
Epoch: [133][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4986 (2.3626)	Acc@1 54.688 (56.421)	Acc@5 83.594 (85.073)
Epoch: [133][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2612 (2.3667)	Acc@1 58.594 (56.299)	Acc@5 89.844 (85.054)
Epoch: [133][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2983 (2.3673)	Acc@1 57.812 (56.309)	Acc@5 89.062 (85.060)
Epoch: [133][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3074 (2.3702)	Acc@1 58.594 (56.310)	Acc@5 84.375 (84.953)
Epoch: [133][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1385 (2.3720)	Acc@1 64.844 (56.283)	Acc@5 89.062 (84.923)
Epoch: [133][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7257 (2.3748)	Acc@1 50.000 (56.215)	Acc@5 82.812 (84.923)
Epoch: [133][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4543 (2.3778)	Acc@1 53.125 (56.157)	Acc@5 84.375 (84.879)
Epoch: [133][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3124 (2.3817)	Acc@1 55.469 (56.126)	Acc@5 86.719 (84.810)
Epoch: [133][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4317 (2.3837)	Acc@1 58.594 (56.064)	Acc@5 83.594 (84.737)
Epoch: [133][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2077 (2.3827)	Acc@1 60.938 (56.013)	Acc@5 88.281 (84.806)
Epoch: [133][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2473 (2.3840)	Acc@1 59.375 (55.967)	Acc@5 86.719 (84.752)
Epoch: [133][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6415 (2.3848)	Acc@1 52.344 (55.954)	Acc@5 82.812 (84.797)
Epoch: [133][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3306 (2.3878)	Acc@1 52.344 (55.901)	Acc@5 85.938 (84.779)
Epoch: [133][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6030 (2.3892)	Acc@1 55.469 (55.864)	Acc@5 78.125 (84.770)
Epoch: [133][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4148 (2.3919)	Acc@1 57.031 (55.823)	Acc@5 85.938 (84.711)
Epoch: [133][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6073 (2.3921)	Acc@1 49.219 (55.806)	Acc@5 80.469 (84.668)
Epoch: [133][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4767 (2.3923)	Acc@1 54.688 (55.785)	Acc@5 83.594 (84.641)
Epoch: [133][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4210 (2.3900)	Acc@1 59.375 (55.873)	Acc@5 82.031 (84.650)
Epoch: [133][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6628 (2.3917)	Acc@1 46.094 (55.802)	Acc@5 82.812 (84.632)
Epoch: [133][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7633 (2.3937)	Acc@1 43.750 (55.741)	Acc@5 81.250 (84.595)
Epoch: [133][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5613 (2.3945)	Acc@1 50.781 (55.680)	Acc@5 81.250 (84.575)
Epoch: [133][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1883 (2.3958)	Acc@1 64.062 (55.731)	Acc@5 86.719 (84.570)
Epoch: [133][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4481 (2.3973)	Acc@1 56.250 (55.700)	Acc@5 82.031 (84.548)
Epoch: [133][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2513 (2.3974)	Acc@1 60.938 (55.659)	Acc@5 87.500 (84.580)
Epoch: [133][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.8618 (2.3981)	Acc@1 47.500 (55.676)	Acc@5 77.500 (84.554)
num momentum params: 26
[0.1, 2.3980832971954347, 2.011512762308121, 55.676, 47.45, tensor(0.3304, device='cuda:0', grad_fn=<DivBackward0>), 4.2541399002075195, 0.3325047492980957]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [134 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [134][0/391]	Time 0.057 (0.057)	Data 0.185 (0.185)	Loss 2.2461 (2.2461)	Acc@1 60.938 (60.938)	Acc@5 84.375 (84.375)
Epoch: [134][10/391]	Time 0.012 (0.016)	Data 0.001 (0.018)	Loss 2.1700 (2.2932)	Acc@1 57.031 (57.173)	Acc@5 90.625 (87.216)
Epoch: [134][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.3303 (2.2584)	Acc@1 52.344 (58.519)	Acc@5 83.594 (87.463)
Epoch: [134][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 2.6523 (2.2581)	Acc@1 50.781 (58.619)	Acc@5 78.906 (87.072)
Epoch: [134][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.1045 (2.2596)	Acc@1 61.719 (58.479)	Acc@5 89.844 (87.176)
Epoch: [134][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2625 (2.2585)	Acc@1 59.375 (58.670)	Acc@5 85.938 (86.979)
Epoch: [134][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4247 (2.2663)	Acc@1 53.125 (58.453)	Acc@5 83.594 (86.821)
Epoch: [134][70/391]	Time 0.012 (0.011)	Data 0.001 (0.004)	Loss 2.4851 (2.2766)	Acc@1 59.375 (58.286)	Acc@5 85.938 (86.752)
Epoch: [134][80/391]	Time 0.011 (0.011)	Data 0.002 (0.004)	Loss 2.7484 (2.2947)	Acc@1 46.875 (58.034)	Acc@5 79.688 (86.400)
Epoch: [134][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6025 (2.3082)	Acc@1 53.906 (57.684)	Acc@5 85.156 (86.281)
Epoch: [134][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5355 (2.3194)	Acc@1 53.125 (57.619)	Acc@5 81.250 (86.038)
Epoch: [134][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5770 (2.3245)	Acc@1 52.344 (57.439)	Acc@5 81.250 (85.945)
Epoch: [134][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4313 (2.3337)	Acc@1 55.469 (57.257)	Acc@5 81.250 (85.808)
Epoch: [134][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5068 (2.3307)	Acc@1 50.000 (57.288)	Acc@5 85.938 (85.908)
Epoch: [134][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4010 (2.3371)	Acc@1 57.812 (57.153)	Acc@5 82.031 (85.805)
Epoch: [134][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3060 (2.3392)	Acc@1 60.156 (57.119)	Acc@5 87.500 (85.777)
Epoch: [134][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3612 (2.3447)	Acc@1 54.688 (56.924)	Acc@5 85.156 (85.671)
Epoch: [134][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4278 (2.3389)	Acc@1 60.938 (57.123)	Acc@5 83.594 (85.800)
Epoch: [134][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3073 (2.3433)	Acc@1 52.344 (57.031)	Acc@5 86.719 (85.713)
Epoch: [134][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5685 (2.3456)	Acc@1 47.656 (56.954)	Acc@5 81.250 (85.668)
Epoch: [134][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2574 (2.3513)	Acc@1 60.156 (56.849)	Acc@5 82.812 (85.545)
Epoch: [134][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4433 (2.3592)	Acc@1 52.344 (56.676)	Acc@5 85.938 (85.441)
Epoch: [134][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5597 (2.3635)	Acc@1 44.531 (56.529)	Acc@5 81.250 (85.372)
Epoch: [134][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3029 (2.3657)	Acc@1 57.031 (56.422)	Acc@5 86.719 (85.352)
Epoch: [134][240/391]	Time 0.017 (0.011)	Data 0.001 (0.002)	Loss 2.4164 (2.3698)	Acc@1 57.812 (56.325)	Acc@5 84.375 (85.296)
Epoch: [134][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4462 (2.3724)	Acc@1 56.250 (56.210)	Acc@5 80.469 (85.243)
Epoch: [134][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3833 (2.3735)	Acc@1 53.125 (56.097)	Acc@5 88.281 (85.306)
Epoch: [134][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4277 (2.3750)	Acc@1 53.125 (56.109)	Acc@5 84.375 (85.240)
Epoch: [134][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4641 (2.3774)	Acc@1 52.344 (56.000)	Acc@5 82.812 (85.198)
Epoch: [134][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2998 (2.3816)	Acc@1 60.156 (55.914)	Acc@5 86.719 (85.127)
Epoch: [134][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4155 (2.3817)	Acc@1 55.469 (55.889)	Acc@5 85.938 (85.110)
Epoch: [134][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6211 (2.3824)	Acc@1 51.562 (55.901)	Acc@5 84.375 (85.109)
Epoch: [134][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4976 (2.3868)	Acc@1 54.688 (55.817)	Acc@5 82.812 (85.003)
Epoch: [134][330/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4450 (2.3902)	Acc@1 57.031 (55.735)	Acc@5 82.812 (84.911)
Epoch: [134][340/391]	Time 0.012 (0.011)	Data 0.007 (0.002)	Loss 2.3382 (2.3940)	Acc@1 54.688 (55.638)	Acc@5 83.594 (84.799)
Epoch: [134][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5123 (2.3957)	Acc@1 57.812 (55.647)	Acc@5 81.250 (84.765)
Epoch: [134][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3417 (2.3950)	Acc@1 59.375 (55.705)	Acc@5 82.812 (84.769)
Epoch: [134][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2745 (2.3940)	Acc@1 60.156 (55.726)	Acc@5 82.812 (84.769)
Epoch: [134][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6005 (2.3958)	Acc@1 53.906 (55.692)	Acc@5 82.812 (84.742)
Epoch: [134][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.5141 (2.3984)	Acc@1 58.750 (55.632)	Acc@5 81.250 (84.684)
num momentum params: 26
[0.1, 2.3984031103515626, 2.106735653877258, 55.632, 46.11, tensor(0.3308, device='cuda:0', grad_fn=<DivBackward0>), 4.159243106842041, 0.3336007595062256]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [135 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [135][0/391]	Time 0.053 (0.053)	Data 0.185 (0.185)	Loss 2.3160 (2.3160)	Acc@1 53.906 (53.906)	Acc@5 87.500 (87.500)
Epoch: [135][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 2.3968 (2.3474)	Acc@1 60.938 (56.250)	Acc@5 84.375 (85.795)
Epoch: [135][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.3044 (2.2834)	Acc@1 54.688 (57.217)	Acc@5 88.281 (86.868)
Epoch: [135][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.3564 (2.2747)	Acc@1 56.250 (57.384)	Acc@5 87.500 (86.794)
Epoch: [135][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.5189 (2.2754)	Acc@1 49.219 (57.679)	Acc@5 84.375 (86.871)
Epoch: [135][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.1446 (2.2918)	Acc@1 57.031 (57.016)	Acc@5 87.500 (86.780)
Epoch: [135][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.0872 (2.2960)	Acc@1 60.156 (56.980)	Acc@5 88.281 (86.514)
Epoch: [135][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2990 (2.3013)	Acc@1 53.906 (56.811)	Acc@5 85.938 (86.444)
Epoch: [135][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3688 (2.3112)	Acc@1 60.938 (56.645)	Acc@5 83.594 (86.294)
Epoch: [135][90/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2762 (2.3108)	Acc@1 59.375 (56.937)	Acc@5 89.062 (86.281)
Epoch: [135][100/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 2.2910 (2.3116)	Acc@1 57.812 (56.784)	Acc@5 83.594 (86.278)
Epoch: [135][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4028 (2.3177)	Acc@1 52.344 (56.553)	Acc@5 85.156 (86.184)
Epoch: [135][120/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 2.2514 (2.3206)	Acc@1 59.375 (56.612)	Acc@5 85.938 (85.983)
Epoch: [135][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3453 (2.3251)	Acc@1 62.500 (56.697)	Acc@5 85.938 (85.878)
Epoch: [135][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4539 (2.3308)	Acc@1 56.250 (56.654)	Acc@5 81.250 (85.732)
Epoch: [135][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3462 (2.3339)	Acc@1 60.156 (56.638)	Acc@5 82.031 (85.674)
Epoch: [135][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5051 (2.3313)	Acc@1 52.344 (56.658)	Acc@5 84.375 (85.724)
Epoch: [135][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4115 (2.3380)	Acc@1 53.125 (56.561)	Acc@5 85.938 (85.595)
Epoch: [135][180/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2812 (2.3440)	Acc@1 59.375 (56.384)	Acc@5 87.500 (85.553)
Epoch: [135][190/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4391 (2.3475)	Acc@1 50.000 (56.307)	Acc@5 84.375 (85.500)
Epoch: [135][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2445 (2.3533)	Acc@1 65.625 (56.227)	Acc@5 89.062 (85.432)
Epoch: [135][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3144 (2.3615)	Acc@1 58.594 (56.046)	Acc@5 85.156 (85.290)
Epoch: [135][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3822 (2.3659)	Acc@1 57.031 (55.925)	Acc@5 85.156 (85.230)
Epoch: [135][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2109 (2.3674)	Acc@1 62.500 (55.892)	Acc@5 88.281 (85.214)
Epoch: [135][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4713 (2.3727)	Acc@1 54.688 (55.803)	Acc@5 84.375 (85.143)
Epoch: [135][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4259 (2.3744)	Acc@1 56.250 (55.824)	Acc@5 85.156 (85.094)
Epoch: [135][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2439 (2.3776)	Acc@1 57.031 (55.723)	Acc@5 85.938 (85.040)
Epoch: [135][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4748 (2.3791)	Acc@1 56.250 (55.671)	Acc@5 82.812 (85.029)
Epoch: [135][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4334 (2.3806)	Acc@1 53.906 (55.630)	Acc@5 82.812 (85.020)
Epoch: [135][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5661 (2.3837)	Acc@1 48.438 (55.571)	Acc@5 83.594 (84.984)
Epoch: [135][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4167 (2.3856)	Acc@1 53.125 (55.528)	Acc@5 83.594 (84.928)
Epoch: [135][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3602 (2.3878)	Acc@1 58.594 (55.469)	Acc@5 85.156 (84.865)
Epoch: [135][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3397 (2.3892)	Acc@1 59.375 (55.498)	Acc@5 84.375 (84.820)
Epoch: [135][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5400 (2.3909)	Acc@1 49.219 (55.485)	Acc@5 81.250 (84.797)
Epoch: [135][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5802 (2.3927)	Acc@1 48.438 (55.418)	Acc@5 80.469 (84.730)
Epoch: [135][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5731 (2.3941)	Acc@1 52.344 (55.411)	Acc@5 81.250 (84.680)
Epoch: [135][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3852 (2.3956)	Acc@1 53.906 (55.391)	Acc@5 85.938 (84.678)
Epoch: [135][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5088 (2.3959)	Acc@1 49.219 (55.416)	Acc@5 85.938 (84.678)
Epoch: [135][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1661 (2.3955)	Acc@1 67.188 (55.471)	Acc@5 89.062 (84.691)
Epoch: [135][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.0684 (2.3942)	Acc@1 61.250 (55.496)	Acc@5 91.250 (84.700)
num momentum params: 26
[0.1, 2.394243674621582, 2.341612082719803, 55.496, 42.91, tensor(0.3315, device='cuda:0', grad_fn=<DivBackward0>), 4.290550231933594, 0.3353464603424072]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [136 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [136][0/391]	Time 0.057 (0.057)	Data 0.177 (0.177)	Loss 2.0287 (2.0287)	Acc@1 65.625 (65.625)	Acc@5 88.281 (88.281)
Epoch: [136][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 1.9918 (2.3139)	Acc@1 67.969 (56.818)	Acc@5 88.281 (85.653)
Epoch: [136][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.1092 (2.2611)	Acc@1 64.062 (59.263)	Acc@5 88.281 (86.793)
Epoch: [136][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.1705 (2.2626)	Acc@1 60.156 (59.073)	Acc@5 92.188 (86.820)
Epoch: [136][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.5002 (2.2866)	Acc@1 57.812 (58.556)	Acc@5 82.031 (86.376)
Epoch: [136][50/391]	Time 0.014 (0.012)	Data 0.001 (0.005)	Loss 2.6066 (2.2956)	Acc@1 51.562 (58.211)	Acc@5 80.469 (86.106)
Epoch: [136][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2139 (2.3027)	Acc@1 60.156 (57.992)	Acc@5 87.500 (86.091)
Epoch: [136][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2474 (2.3117)	Acc@1 54.688 (57.724)	Acc@5 88.281 (86.048)
Epoch: [136][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.4082 (2.3206)	Acc@1 53.906 (57.542)	Acc@5 85.156 (85.889)
Epoch: [136][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3891 (2.3260)	Acc@1 53.906 (57.383)	Acc@5 85.938 (85.826)
Epoch: [136][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2875 (2.3268)	Acc@1 57.812 (57.426)	Acc@5 85.938 (85.791)
Epoch: [136][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.7565 (2.3400)	Acc@1 49.219 (57.095)	Acc@5 76.562 (85.515)
Epoch: [136][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6395 (2.3416)	Acc@1 46.094 (56.947)	Acc@5 85.938 (85.589)
Epoch: [136][130/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2703 (2.3468)	Acc@1 58.594 (56.817)	Acc@5 86.719 (85.460)
Epoch: [136][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3263 (2.3447)	Acc@1 57.031 (56.810)	Acc@5 85.938 (85.522)
Epoch: [136][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3318 (2.3488)	Acc@1 57.031 (56.669)	Acc@5 89.062 (85.472)
Epoch: [136][160/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2922 (2.3483)	Acc@1 62.500 (56.692)	Acc@5 82.812 (85.418)
Epoch: [136][170/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3135 (2.3501)	Acc@1 53.906 (56.661)	Acc@5 90.625 (85.408)
Epoch: [136][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4098 (2.3550)	Acc@1 52.344 (56.470)	Acc@5 86.719 (85.355)
Epoch: [136][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4929 (2.3594)	Acc@1 45.312 (56.307)	Acc@5 83.594 (85.287)
Epoch: [136][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3477 (2.3601)	Acc@1 57.812 (56.382)	Acc@5 82.031 (85.230)
Epoch: [136][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5182 (2.3641)	Acc@1 51.562 (56.254)	Acc@5 88.281 (85.175)
Epoch: [136][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3395 (2.3631)	Acc@1 57.812 (56.328)	Acc@5 85.156 (85.199)
Epoch: [136][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3251 (2.3638)	Acc@1 57.031 (56.311)	Acc@5 83.594 (85.136)
Epoch: [136][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2835 (2.3652)	Acc@1 53.906 (56.240)	Acc@5 86.719 (85.143)
Epoch: [136][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4182 (2.3663)	Acc@1 54.688 (56.191)	Acc@5 84.375 (85.134)
Epoch: [136][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3944 (2.3711)	Acc@1 57.031 (56.142)	Acc@5 85.156 (85.040)
Epoch: [136][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2712 (2.3701)	Acc@1 53.906 (56.155)	Acc@5 87.500 (85.021)
Epoch: [136][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5166 (2.3734)	Acc@1 52.344 (56.058)	Acc@5 83.594 (84.967)
Epoch: [136][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5000 (2.3777)	Acc@1 48.438 (55.984)	Acc@5 85.156 (84.869)
Epoch: [136][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4184 (2.3791)	Acc@1 58.594 (55.990)	Acc@5 82.031 (84.853)
Epoch: [136][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2852 (2.3823)	Acc@1 60.156 (55.936)	Acc@5 85.938 (84.789)
Epoch: [136][320/391]	Time 0.015 (0.011)	Data 0.001 (0.002)	Loss 2.4459 (2.3845)	Acc@1 53.125 (55.904)	Acc@5 83.594 (84.764)
Epoch: [136][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6550 (2.3865)	Acc@1 49.219 (55.887)	Acc@5 84.375 (84.736)
Epoch: [136][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3669 (2.3889)	Acc@1 53.906 (55.812)	Acc@5 84.375 (84.705)
Epoch: [136][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5547 (2.3900)	Acc@1 51.562 (55.771)	Acc@5 84.375 (84.684)
Epoch: [136][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3395 (2.3929)	Acc@1 55.469 (55.694)	Acc@5 85.156 (84.646)
Epoch: [136][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3194 (2.3921)	Acc@1 60.938 (55.730)	Acc@5 86.719 (84.647)
Epoch: [136][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4769 (2.3941)	Acc@1 49.219 (55.694)	Acc@5 83.594 (84.605)
Epoch: [136][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.2753 (2.3956)	Acc@1 57.500 (55.654)	Acc@5 91.250 (84.568)
num momentum params: 26
[0.1, 2.3955587226867676, 2.065002735853195, 55.654, 47.79, tensor(0.3310, device='cuda:0', grad_fn=<DivBackward0>), 4.245887994766235, 0.34647035598754883]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [137 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [137][0/391]	Time 0.055 (0.055)	Data 0.189 (0.189)	Loss 2.3602 (2.3602)	Acc@1 50.781 (50.781)	Acc@5 85.156 (85.156)
Epoch: [137][10/391]	Time 0.013 (0.016)	Data 0.001 (0.018)	Loss 2.6208 (2.3952)	Acc@1 50.000 (53.906)	Acc@5 80.469 (84.730)
Epoch: [137][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.2615 (2.3530)	Acc@1 57.031 (55.208)	Acc@5 84.375 (85.565)
Epoch: [137][30/391]	Time 0.012 (0.014)	Data 0.001 (0.007)	Loss 2.2529 (2.3436)	Acc@1 57.812 (56.149)	Acc@5 85.156 (85.963)
Epoch: [137][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.4288 (2.3271)	Acc@1 56.250 (56.745)	Acc@5 82.031 (86.014)
Epoch: [137][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4977 (2.3508)	Acc@1 59.375 (56.419)	Acc@5 84.375 (85.646)
Epoch: [137][60/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 2.4596 (2.3473)	Acc@1 57.031 (56.609)	Acc@5 82.031 (85.515)
Epoch: [137][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4023 (2.3639)	Acc@1 59.375 (56.217)	Acc@5 83.594 (85.189)
Epoch: [137][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.5186 (2.3668)	Acc@1 50.000 (56.019)	Acc@5 82.812 (85.156)
Epoch: [137][90/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3513 (2.3713)	Acc@1 58.594 (55.898)	Acc@5 87.500 (85.165)
Epoch: [137][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4945 (2.3659)	Acc@1 53.125 (56.049)	Acc@5 83.594 (85.265)
Epoch: [137][110/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.3709 (2.3601)	Acc@1 58.594 (56.306)	Acc@5 82.031 (85.325)
Epoch: [137][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2012 (2.3623)	Acc@1 58.594 (56.256)	Acc@5 88.281 (85.266)
Epoch: [137][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4863 (2.3695)	Acc@1 57.031 (56.071)	Acc@5 82.812 (85.174)
Epoch: [137][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5460 (2.3693)	Acc@1 53.125 (56.084)	Acc@5 84.375 (85.262)
Epoch: [137][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3926 (2.3743)	Acc@1 56.250 (56.053)	Acc@5 82.812 (85.167)
Epoch: [137][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1779 (2.3694)	Acc@1 60.938 (56.279)	Acc@5 87.500 (85.278)
Epoch: [137][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1367 (2.3662)	Acc@1 60.156 (56.360)	Acc@5 92.188 (85.325)
Epoch: [137][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4270 (2.3665)	Acc@1 57.031 (56.397)	Acc@5 84.375 (85.294)
Epoch: [137][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2847 (2.3649)	Acc@1 59.375 (56.455)	Acc@5 87.500 (85.295)
Epoch: [137][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5673 (2.3686)	Acc@1 53.125 (56.324)	Acc@5 78.125 (85.261)
Epoch: [137][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5525 (2.3705)	Acc@1 49.219 (56.239)	Acc@5 80.469 (85.230)
Epoch: [137][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4722 (2.3748)	Acc@1 51.562 (56.098)	Acc@5 82.812 (85.142)
Epoch: [137][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3749 (2.3764)	Acc@1 54.688 (56.057)	Acc@5 82.812 (85.092)
Epoch: [137][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4037 (2.3780)	Acc@1 53.906 (55.981)	Acc@5 82.031 (85.062)
Epoch: [137][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4786 (2.3802)	Acc@1 56.250 (55.936)	Acc@5 81.250 (85.032)
Epoch: [137][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4884 (2.3795)	Acc@1 50.781 (55.987)	Acc@5 82.031 (85.019)
Epoch: [137][270/391]	Time 0.015 (0.011)	Data 0.001 (0.002)	Loss 2.1956 (2.3780)	Acc@1 60.156 (56.028)	Acc@5 85.156 (84.992)
Epoch: [137][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4058 (2.3807)	Acc@1 54.688 (56.008)	Acc@5 88.281 (84.900)
Epoch: [137][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.4463 (2.3848)	Acc@1 55.469 (55.866)	Acc@5 82.031 (84.823)
Epoch: [137][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3551 (2.3862)	Acc@1 59.375 (55.822)	Acc@5 87.500 (84.832)
Epoch: [137][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4227 (2.3870)	Acc@1 53.906 (55.810)	Acc@5 85.156 (84.835)
Epoch: [137][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5374 (2.3872)	Acc@1 51.562 (55.790)	Acc@5 85.156 (84.837)
Epoch: [137][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2591 (2.3882)	Acc@1 60.938 (55.752)	Acc@5 88.281 (84.812)
Epoch: [137][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4858 (2.3900)	Acc@1 59.375 (55.712)	Acc@5 81.250 (84.797)
Epoch: [137][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3721 (2.3920)	Acc@1 57.031 (55.662)	Acc@5 85.156 (84.747)
Epoch: [137][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4392 (2.3935)	Acc@1 55.469 (55.657)	Acc@5 83.594 (84.708)
Epoch: [137][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6688 (2.3954)	Acc@1 47.656 (55.614)	Acc@5 81.250 (84.685)
Epoch: [137][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4060 (2.3965)	Acc@1 61.719 (55.643)	Acc@5 85.156 (84.654)
Epoch: [137][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.1148 (2.3969)	Acc@1 65.000 (55.638)	Acc@5 88.750 (84.648)
num momentum params: 26
[0.1, 2.3968798725128173, 1.9725600492954254, 55.638, 48.45, tensor(0.3306, device='cuda:0', grad_fn=<DivBackward0>), 4.267576456069946, 0.33257532119750977]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [138 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [138][0/391]	Time 0.062 (0.062)	Data 0.186 (0.186)	Loss 2.2445 (2.2445)	Acc@1 57.031 (57.031)	Acc@5 89.062 (89.062)
Epoch: [138][10/391]	Time 0.011 (0.017)	Data 0.001 (0.018)	Loss 2.3968 (2.3011)	Acc@1 57.031 (57.386)	Acc@5 84.375 (86.719)
Epoch: [138][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.6100 (2.3329)	Acc@1 52.344 (57.366)	Acc@5 79.688 (85.714)
Epoch: [138][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.3270 (2.3070)	Acc@1 60.156 (58.468)	Acc@5 85.156 (86.164)
Epoch: [138][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 2.2476 (2.2901)	Acc@1 61.719 (58.632)	Acc@5 85.156 (86.223)
Epoch: [138][50/391]	Time 0.017 (0.013)	Data 0.002 (0.005)	Loss 1.9728 (2.2713)	Acc@1 65.625 (59.053)	Acc@5 90.625 (86.566)
Epoch: [138][60/391]	Time 0.010 (0.013)	Data 0.001 (0.004)	Loss 2.2483 (2.2832)	Acc@1 58.594 (58.786)	Acc@5 89.062 (86.386)
Epoch: [138][70/391]	Time 0.018 (0.012)	Data 0.001 (0.004)	Loss 2.3394 (2.2867)	Acc@1 56.250 (58.836)	Acc@5 85.156 (86.213)
Epoch: [138][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.5509 (2.3037)	Acc@1 49.219 (58.275)	Acc@5 85.156 (85.899)
Epoch: [138][90/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2334 (2.3124)	Acc@1 55.469 (58.122)	Acc@5 88.281 (85.869)
Epoch: [138][100/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.4738 (2.3197)	Acc@1 53.125 (58.068)	Acc@5 84.375 (85.705)
Epoch: [138][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.1848 (2.3269)	Acc@1 62.500 (57.967)	Acc@5 89.844 (85.480)
Epoch: [138][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.3255 (2.3345)	Acc@1 50.000 (57.612)	Acc@5 89.062 (85.369)
Epoch: [138][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.4408 (2.3386)	Acc@1 53.125 (57.425)	Acc@5 84.375 (85.258)
Epoch: [138][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.6058 (2.3448)	Acc@1 47.656 (57.209)	Acc@5 78.906 (85.145)
Epoch: [138][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.1822 (2.3477)	Acc@1 59.375 (57.109)	Acc@5 89.062 (85.151)
Epoch: [138][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4726 (2.3516)	Acc@1 53.125 (56.915)	Acc@5 85.156 (85.098)
Epoch: [138][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0512 (2.3529)	Acc@1 61.719 (56.894)	Acc@5 89.062 (85.111)
Epoch: [138][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4164 (2.3560)	Acc@1 57.031 (56.785)	Acc@5 83.594 (85.117)
Epoch: [138][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3481 (2.3562)	Acc@1 54.688 (56.765)	Acc@5 89.062 (85.148)
Epoch: [138][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1721 (2.3611)	Acc@1 61.719 (56.693)	Acc@5 83.594 (84.989)
Epoch: [138][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4387 (2.3646)	Acc@1 53.125 (56.546)	Acc@5 85.938 (84.908)
Epoch: [138][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3583 (2.3664)	Acc@1 57.031 (56.487)	Acc@5 88.281 (84.891)
Epoch: [138][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2311 (2.3687)	Acc@1 64.062 (56.497)	Acc@5 87.500 (84.825)
Epoch: [138][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3948 (2.3699)	Acc@1 54.688 (56.448)	Acc@5 81.250 (84.790)
Epoch: [138][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5447 (2.3712)	Acc@1 55.469 (56.396)	Acc@5 84.375 (84.823)
Epoch: [138][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4613 (2.3711)	Acc@1 50.781 (56.379)	Acc@5 83.594 (84.803)
Epoch: [138][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3373 (2.3704)	Acc@1 57.031 (56.420)	Acc@5 88.281 (84.856)
Epoch: [138][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2608 (2.3716)	Acc@1 53.906 (56.397)	Acc@5 87.500 (84.853)
Epoch: [138][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5367 (2.3743)	Acc@1 54.688 (56.387)	Acc@5 82.031 (84.842)
Epoch: [138][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2891 (2.3793)	Acc@1 56.250 (56.247)	Acc@5 85.938 (84.772)
Epoch: [138][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6278 (2.3837)	Acc@1 51.562 (56.122)	Acc@5 82.031 (84.729)
Epoch: [138][320/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3916 (2.3846)	Acc@1 54.688 (56.106)	Acc@5 83.594 (84.743)
Epoch: [138][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3689 (2.3856)	Acc@1 50.000 (56.087)	Acc@5 85.156 (84.736)
Epoch: [138][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2937 (2.3845)	Acc@1 56.250 (56.094)	Acc@5 85.938 (84.730)
Epoch: [138][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5766 (2.3872)	Acc@1 50.781 (55.965)	Acc@5 84.375 (84.716)
Epoch: [138][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3144 (2.3884)	Acc@1 54.688 (55.895)	Acc@5 89.062 (84.736)
Epoch: [138][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4651 (2.3894)	Acc@1 50.000 (55.858)	Acc@5 84.375 (84.720)
Epoch: [138][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6581 (2.3949)	Acc@1 46.875 (55.772)	Acc@5 81.250 (84.627)
Epoch: [138][390/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.2805 (2.3972)	Acc@1 56.250 (55.708)	Acc@5 87.500 (84.592)
num momentum params: 26
[0.1, 2.3971722555541994, 2.114948319196701, 55.708, 45.67, tensor(0.3313, device='cuda:0', grad_fn=<DivBackward0>), 4.276941776275635, 0.3328745365142822]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [139 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [139][0/391]	Time 0.057 (0.057)	Data 0.192 (0.192)	Loss 2.4724 (2.4724)	Acc@1 56.250 (56.250)	Acc@5 85.156 (85.156)
Epoch: [139][10/391]	Time 0.012 (0.017)	Data 0.001 (0.019)	Loss 2.4345 (2.3276)	Acc@1 58.594 (56.960)	Acc@5 82.812 (85.511)
Epoch: [139][20/391]	Time 0.013 (0.015)	Data 0.001 (0.010)	Loss 2.5996 (2.3109)	Acc@1 55.469 (57.292)	Acc@5 83.594 (85.900)
Epoch: [139][30/391]	Time 0.011 (0.014)	Data 0.001 (0.007)	Loss 2.1635 (2.3367)	Acc@1 63.281 (57.409)	Acc@5 88.281 (85.282)
Epoch: [139][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 2.4986 (2.3233)	Acc@1 54.688 (57.736)	Acc@5 85.938 (85.575)
Epoch: [139][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2233 (2.3281)	Acc@1 57.812 (57.414)	Acc@5 85.156 (85.524)
Epoch: [139][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.2872 (2.3159)	Acc@1 54.688 (57.556)	Acc@5 87.500 (85.873)
Epoch: [139][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4600 (2.3318)	Acc@1 57.031 (57.240)	Acc@5 83.594 (85.574)
Epoch: [139][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.6344 (2.3420)	Acc@1 52.344 (56.944)	Acc@5 81.250 (85.484)
Epoch: [139][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1827 (2.3460)	Acc@1 60.156 (56.739)	Acc@5 90.625 (85.577)
Epoch: [139][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3512 (2.3495)	Acc@1 54.688 (56.652)	Acc@5 84.375 (85.388)
Epoch: [139][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4172 (2.3507)	Acc@1 54.688 (56.630)	Acc@5 85.156 (85.445)
Epoch: [139][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.8240 (2.3513)	Acc@1 46.094 (56.553)	Acc@5 78.125 (85.402)
Epoch: [139][130/391]	Time 0.012 (0.011)	Data 0.002 (0.003)	Loss 2.3309 (2.3603)	Acc@1 57.812 (56.333)	Acc@5 82.031 (85.281)
Epoch: [139][140/391]	Time 0.012 (0.011)	Data 0.002 (0.003)	Loss 2.1266 (2.3635)	Acc@1 61.719 (56.222)	Acc@5 89.844 (85.262)
Epoch: [139][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3545 (2.3630)	Acc@1 53.906 (56.214)	Acc@5 87.500 (85.343)
Epoch: [139][160/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.4040 (2.3638)	Acc@1 50.781 (56.255)	Acc@5 88.281 (85.258)
Epoch: [139][170/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.0955 (2.3648)	Acc@1 67.188 (56.213)	Acc@5 87.500 (85.243)
Epoch: [139][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1785 (2.3661)	Acc@1 65.625 (56.159)	Acc@5 86.719 (85.277)
Epoch: [139][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2543 (2.3685)	Acc@1 64.062 (56.103)	Acc@5 85.156 (85.226)
Epoch: [139][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2223 (2.3673)	Acc@1 58.594 (56.098)	Acc@5 85.938 (85.250)
Epoch: [139][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3502 (2.3683)	Acc@1 59.375 (56.054)	Acc@5 85.156 (85.278)
Epoch: [139][220/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4277 (2.3689)	Acc@1 53.125 (56.052)	Acc@5 85.156 (85.220)
Epoch: [139][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3304 (2.3698)	Acc@1 55.469 (56.013)	Acc@5 86.719 (85.207)
Epoch: [139][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5047 (2.3711)	Acc@1 60.156 (56.065)	Acc@5 82.812 (85.159)
Epoch: [139][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4687 (2.3742)	Acc@1 56.250 (56.023)	Acc@5 82.812 (85.103)
Epoch: [139][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5497 (2.3799)	Acc@1 53.906 (55.852)	Acc@5 86.719 (85.051)
Epoch: [139][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1764 (2.3814)	Acc@1 60.938 (55.820)	Acc@5 86.719 (85.015)
Epoch: [139][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4098 (2.3841)	Acc@1 56.250 (55.777)	Acc@5 85.938 (84.967)
Epoch: [139][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4538 (2.3866)	Acc@1 60.156 (55.745)	Acc@5 82.812 (84.933)
Epoch: [139][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4965 (2.3863)	Acc@1 50.000 (55.728)	Acc@5 79.688 (84.941)
Epoch: [139][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2634 (2.3867)	Acc@1 60.938 (55.790)	Acc@5 85.938 (84.925)
Epoch: [139][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4940 (2.3900)	Acc@1 50.781 (55.705)	Acc@5 85.938 (84.898)
Epoch: [139][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3437 (2.3922)	Acc@1 55.469 (55.650)	Acc@5 86.719 (84.868)
Epoch: [139][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3944 (2.3914)	Acc@1 57.812 (55.714)	Acc@5 84.375 (84.888)
Epoch: [139][350/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.0382 (2.3910)	Acc@1 64.844 (55.758)	Acc@5 90.625 (84.900)
Epoch: [139][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3327 (2.3911)	Acc@1 60.156 (55.754)	Acc@5 86.719 (84.899)
Epoch: [139][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2847 (2.3917)	Acc@1 52.344 (55.751)	Acc@5 88.281 (84.908)
Epoch: [139][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4301 (2.3920)	Acc@1 50.781 (55.752)	Acc@5 87.500 (84.918)
Epoch: [139][390/391]	Time 0.015 (0.011)	Data 0.000 (0.002)	Loss 2.4829 (2.3931)	Acc@1 53.750 (55.738)	Acc@5 81.250 (84.892)
num momentum params: 26
[0.1, 2.393130592727661, 1.9113232064247132, 55.738, 49.54, tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>), 4.277778148651123, 0.3376443386077881]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [206, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [206]
Non Pruning Epoch - module.bn3.bias: [206]
Non Pruning Epoch - module.conv4.weight: [247, 206, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [311, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [311]
Non Pruning Epoch - module.bn6.bias: [311]
Non Pruning Epoch - module.conv7.weight: [192, 311, 3, 3]
Non Pruning Epoch - module.bn7.weight: [192]
Non Pruning Epoch - module.bn7.bias: [192]
Non Pruning Epoch - module.conv8.weight: [191, 192, 3, 3]
Non Pruning Epoch - module.bn8.weight: [191]
Non Pruning Epoch - module.bn8.bias: [191]
Non Pruning Epoch - module.fc.weight: [100, 191]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [140 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [206, 108, 3, 3]
module.conv4.weight [247, 206, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [311, 357, 3, 3]
module.conv7.weight [192, 311, 3, 3]
module.conv8.weight [191, 192, 3, 3]
Epoch: [140][0/391]	Time 0.054 (0.054)	Data 0.186 (0.186)	Loss 2.2901 (2.2901)	Acc@1 61.719 (61.719)	Acc@5 86.719 (86.719)
Epoch: [140][10/391]	Time 0.010 (0.016)	Data 0.001 (0.019)	Loss 2.1541 (2.2125)	Acc@1 60.938 (59.091)	Acc@5 88.281 (87.216)
Epoch: [140][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.2012 (2.2531)	Acc@1 61.719 (58.408)	Acc@5 86.719 (86.272)
Epoch: [140][30/391]	Time 0.010 (0.013)	Data 0.001 (0.008)	Loss 2.3942 (2.2655)	Acc@1 57.031 (58.619)	Acc@5 79.688 (86.064)
Epoch: [140][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.3155 (2.2501)	Acc@1 57.812 (59.013)	Acc@5 86.719 (86.376)
Epoch: [140][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4375 (2.2856)	Acc@1 52.344 (58.364)	Acc@5 85.938 (85.876)
Epoch: [140][60/391]	Time 0.013 (0.012)	Data 0.001 (0.005)	Loss 2.4225 (2.3061)	Acc@1 50.781 (57.889)	Acc@5 84.375 (85.630)
Epoch: [140][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.0695 (2.3141)	Acc@1 64.844 (57.868)	Acc@5 91.406 (85.629)
Epoch: [140][80/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.5023 (2.3289)	Acc@1 50.781 (57.514)	Acc@5 82.812 (85.446)
Epoch: [140][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4140 (2.3343)	Acc@1 58.594 (57.263)	Acc@5 85.938 (85.362)
Epoch: [140][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3993 (2.3340)	Acc@1 60.156 (57.379)	Acc@5 82.812 (85.412)
Epoch: [140][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2726 (2.3372)	Acc@1 60.938 (57.376)	Acc@5 85.938 (85.206)
Epoch: [140][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4374 (2.3475)	Acc@1 54.688 (57.109)	Acc@5 86.719 (85.163)
Epoch: [140][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3276 (2.3492)	Acc@1 50.781 (56.888)	Acc@5 90.625 (85.126)
Epoch: [140][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5461 (2.3553)	Acc@1 57.031 (56.799)	Acc@5 82.812 (85.051)
Epoch: [140][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2797 (2.3573)	Acc@1 59.375 (56.716)	Acc@5 87.500 (85.011)
Epoch: [140][160/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.5510 (2.3561)	Acc@1 48.438 (56.726)	Acc@5 79.688 (85.059)
Epoch: [140][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5044 (2.3581)	Acc@1 57.031 (56.716)	Acc@5 80.469 (85.056)
Epoch: [140][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2861 (2.3606)	Acc@1 57.031 (56.569)	Acc@5 87.500 (84.997)
Epoch: [140][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7210 (2.3667)	Acc@1 40.625 (56.438)	Acc@5 79.688 (84.870)
Epoch: [140][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5572 (2.3749)	Acc@1 53.125 (56.308)	Acc@5 81.250 (84.721)
Epoch: [140][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6743 (2.3766)	Acc@1 54.688 (56.269)	Acc@5 80.469 (84.671)
Epoch: [140][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5658 (2.3806)	Acc@1 50.781 (56.172)	Acc@5 82.812 (84.668)
Epoch: [140][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2562 (2.3851)	Acc@1 62.500 (56.091)	Acc@5 89.062 (84.639)
Epoch: [140][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2357 (2.3842)	Acc@1 63.281 (56.130)	Acc@5 88.281 (84.667)
Epoch: [140][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2606 (2.3874)	Acc@1 58.594 (56.066)	Acc@5 86.719 (84.618)
Epoch: [140][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6818 (2.3882)	Acc@1 52.344 (56.008)	Acc@5 82.031 (84.638)
Epoch: [140][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4285 (2.3883)	Acc@1 57.031 (56.005)	Acc@5 82.031 (84.640)
Epoch: [140][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3162 (2.3878)	Acc@1 59.375 (56.055)	Acc@5 85.938 (84.678)
Epoch: [140][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5062 (2.3902)	Acc@1 52.344 (56.022)	Acc@5 82.812 (84.670)
Epoch: [140][300/391]	Time 0.009 (0.011)	Data 0.001 (0.002)	Loss 2.4943 (2.3953)	Acc@1 56.250 (55.952)	Acc@5 81.250 (84.583)
Epoch: [140][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1706 (2.3959)	Acc@1 60.938 (55.939)	Acc@5 86.719 (84.571)
Epoch: [140][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1944 (2.3986)	Acc@1 58.594 (55.880)	Acc@5 87.500 (84.497)
Epoch: [140][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3463 (2.3976)	Acc@1 60.938 (55.915)	Acc@5 86.719 (84.517)
Epoch: [140][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3776 (2.3985)	Acc@1 53.906 (55.879)	Acc@5 83.594 (84.499)
Epoch: [140][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5092 (2.3996)	Acc@1 54.688 (55.867)	Acc@5 86.719 (84.526)
Epoch: [140][360/391]	Time 0.017 (0.011)	Data 0.001 (0.002)	Loss 2.2942 (2.4012)	Acc@1 61.719 (55.843)	Acc@5 85.938 (84.505)
Epoch: [140][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6775 (2.4045)	Acc@1 47.656 (55.711)	Acc@5 84.375 (84.482)
Epoch: [140][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.0961 (2.4048)	Acc@1 63.281 (55.727)	Acc@5 85.938 (84.453)
Epoch: [140][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4465 (2.4053)	Acc@1 58.750 (55.724)	Acc@5 80.000 (84.440)
num momentum params: 26
[0.1, 2.4052593130493163, 2.215931112766266, 55.724, 44.88, tensor(0.3312, device='cuda:0', grad_fn=<DivBackward0>), 4.208291292190552, 0.34766507148742676]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [108, 32, 3, 3]
Before - module.bn2.weight: [108]
Before - module.bn2.bias: [108]
Before - module.conv3.weight: [206, 108, 3, 3]
Before - module.bn3.weight: [206]
Before - module.bn3.bias: [206]
Before - module.conv4.weight: [247, 206, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [357, 247, 3, 3]
Before - module.bn5.weight: [357]
Before - module.bn5.bias: [357]
Before - module.conv6.weight: [311, 357, 3, 3]
Before - module.bn6.weight: [311]
Before - module.bn6.bias: [311]
Before - module.conv7.weight: [192, 311, 3, 3]
Before - module.bn7.weight: [192]
Before - module.bn7.bias: [192]
Before - module.conv8.weight: [191, 192, 3, 3]
Before - module.bn8.weight: [191]
Before - module.bn8.bias: [191]
Before - module.fc.weight: [100, 191]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [108, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 108 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [206, 108, 3, 3] >> [202, 108, 3, 3]
[module.bn3.weight]: 206 >> 202
running_mean [202]
running_var [202]
num_batches_tracked []
[module.conv4.weight]: [247, 206, 3, 3] >> [247, 202, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [357, 247, 3, 3] >> [357, 247, 3, 3]
[module.bn5.weight]: 357 >> 357
running_mean [357]
running_var [357]
num_batches_tracked []
[module.conv6.weight]: [311, 357, 3, 3] >> [307, 357, 3, 3]
[module.bn6.weight]: 311 >> 307
running_mean [307]
running_var [307]
num_batches_tracked []
[module.conv7.weight]: [192, 311, 3, 3] >> [191, 307, 3, 3]
[module.bn7.weight]: 192 >> 191
running_mean [191]
running_var [191]
num_batches_tracked []
[module.conv8.weight]: [191, 192, 3, 3] >> [187, 191, 3, 3]
[module.bn8.weight]: 191 >> 187
running_mean [187]
running_var [187]
num_batches_tracked []
[module.fc.weight]: [100, 191] >> [100, 187]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [202, 108, 3, 3]
After - module.bn3.weight: [202]
After - module.bn3.bias: [202]
After - module.conv4.weight: [247, 202, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [357, 247, 3, 3]
After - module.bn5.weight: [357]
After - module.bn5.bias: [357]
After - module.conv6.weight: [307, 357, 3, 3]
After - module.bn6.weight: [307]
After - module.bn6.bias: [307]
After - module.conv7.weight: [191, 307, 3, 3]
After - module.bn7.weight: [191]
After - module.bn7.bias: [191]
After - module.conv8.weight: [187, 191, 3, 3]
After - module.bn8.weight: [187]
After - module.bn8.bias: [187]
After - module.fc.weight: [100, 187]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [202, 108, 3, 3]
conv4 --> [247, 202, 3, 3]
conv5 --> [357, 247, 3, 3]
conv6 --> [307, 357, 3, 3]
conv7 --> [191, 307, 3, 3]
conv8 --> [187, 191, 3, 3]
fc --> [187, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5730103296, 12566016, 202
4, 13104958464, 28738944, 247
5, 6907590144, 12697776, 357
6, 8585547264, 15782256, 307
7, 1621195776, 2110932, 191
8, 987503616, 1285812, 187
fc, 7180800, 18700, 0
===================
FLOP REPORT: 15869841000000.0 40515200000.0 82047796 101288 1631 6.3458709716796875
[INFO] Storing checkpoint...

Epoch: [141 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [141][0/391]	Time 0.186 (0.186)	Data 0.192 (0.192)	Loss 2.2153 (2.2153)	Acc@1 57.812 (57.812)	Acc@5 87.500 (87.500)
Epoch: [141][10/391]	Time 0.013 (0.029)	Data 0.001 (0.019)	Loss 2.2955 (2.2960)	Acc@1 57.812 (57.173)	Acc@5 82.812 (86.364)
Epoch: [141][20/391]	Time 0.011 (0.021)	Data 0.001 (0.011)	Loss 2.3129 (2.3307)	Acc@1 58.594 (56.920)	Acc@5 84.375 (85.119)
Epoch: [141][30/391]	Time 0.011 (0.018)	Data 0.001 (0.008)	Loss 2.4570 (2.3504)	Acc@1 56.250 (56.552)	Acc@5 78.125 (84.652)
Epoch: [141][40/391]	Time 0.012 (0.016)	Data 0.001 (0.006)	Loss 2.2584 (2.3499)	Acc@1 55.469 (56.421)	Acc@5 88.281 (84.947)
Epoch: [141][50/391]	Time 0.010 (0.015)	Data 0.001 (0.005)	Loss 2.3165 (2.3542)	Acc@1 59.375 (56.296)	Acc@5 83.594 (84.911)
Epoch: [141][60/391]	Time 0.010 (0.014)	Data 0.001 (0.005)	Loss 2.4194 (2.3498)	Acc@1 50.000 (56.237)	Acc@5 85.938 (85.118)
Epoch: [141][70/391]	Time 0.012 (0.014)	Data 0.001 (0.004)	Loss 2.3567 (2.3467)	Acc@1 57.812 (56.382)	Acc@5 84.375 (85.244)
Epoch: [141][80/391]	Time 0.010 (0.013)	Data 0.001 (0.004)	Loss 2.6056 (2.3458)	Acc@1 49.219 (56.491)	Acc@5 79.688 (85.311)
Epoch: [141][90/391]	Time 0.010 (0.013)	Data 0.001 (0.004)	Loss 2.3776 (2.3460)	Acc@1 53.906 (56.542)	Acc@5 88.281 (85.388)
Epoch: [141][100/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 2.2162 (2.3517)	Acc@1 60.156 (56.575)	Acc@5 86.719 (85.288)
Epoch: [141][110/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 2.5382 (2.3571)	Acc@1 55.469 (56.320)	Acc@5 78.906 (85.135)
Epoch: [141][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6295 (2.3528)	Acc@1 51.562 (56.463)	Acc@5 81.250 (85.201)
Epoch: [141][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3532 (2.3475)	Acc@1 56.250 (56.602)	Acc@5 87.500 (85.281)
Epoch: [141][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.2924 (2.3463)	Acc@1 59.375 (56.605)	Acc@5 85.156 (85.300)
Epoch: [141][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4536 (2.3533)	Acc@1 52.344 (56.488)	Acc@5 82.031 (85.182)
Epoch: [141][160/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3401 (2.3522)	Acc@1 60.156 (56.570)	Acc@5 83.594 (85.190)
Epoch: [141][170/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.2967 (2.3516)	Acc@1 57.031 (56.561)	Acc@5 86.719 (85.147)
Epoch: [141][180/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4823 (2.3544)	Acc@1 55.469 (56.565)	Acc@5 82.812 (85.079)
Epoch: [141][190/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.4550 (2.3610)	Acc@1 57.812 (56.418)	Acc@5 84.375 (84.952)
Epoch: [141][200/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.7260 (2.3659)	Acc@1 46.875 (56.277)	Acc@5 81.250 (84.942)
Epoch: [141][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.7401 (2.3723)	Acc@1 49.219 (56.183)	Acc@5 78.906 (84.867)
Epoch: [141][220/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.2132 (2.3751)	Acc@1 59.375 (56.133)	Acc@5 87.500 (84.866)
Epoch: [141][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 2.2269 (2.3740)	Acc@1 61.719 (56.159)	Acc@5 89.062 (84.899)
Epoch: [141][240/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3534 (2.3746)	Acc@1 57.812 (56.143)	Acc@5 82.031 (84.890)
Epoch: [141][250/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3670 (2.3742)	Acc@1 56.250 (56.135)	Acc@5 89.062 (84.913)
Epoch: [141][260/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3057 (2.3742)	Acc@1 57.031 (56.127)	Acc@5 88.281 (84.932)
Epoch: [141][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 2.3036 (2.3782)	Acc@1 60.156 (56.048)	Acc@5 83.594 (84.874)
Epoch: [141][280/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.3582 (2.3805)	Acc@1 55.469 (55.989)	Acc@5 83.594 (84.795)
Epoch: [141][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3154 (2.3811)	Acc@1 54.688 (55.995)	Acc@5 83.594 (84.780)
Epoch: [141][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5509 (2.3822)	Acc@1 52.344 (55.967)	Acc@5 85.938 (84.790)
Epoch: [141][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4778 (2.3856)	Acc@1 55.469 (55.921)	Acc@5 84.375 (84.784)
Epoch: [141][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3180 (2.3879)	Acc@1 59.375 (55.829)	Acc@5 85.156 (84.757)
Epoch: [141][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3284 (2.3912)	Acc@1 59.375 (55.780)	Acc@5 81.250 (84.665)
Epoch: [141][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4006 (2.3918)	Acc@1 54.688 (55.803)	Acc@5 86.719 (84.696)
Epoch: [141][350/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.2955 (2.3910)	Acc@1 57.031 (55.800)	Acc@5 86.719 (84.707)
Epoch: [141][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1661 (2.3902)	Acc@1 58.594 (55.824)	Acc@5 89.062 (84.728)
Epoch: [141][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2844 (2.3934)	Acc@1 60.938 (55.806)	Acc@5 85.156 (84.659)
Epoch: [141][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3774 (2.3914)	Acc@1 55.469 (55.834)	Acc@5 85.938 (84.691)
Epoch: [141][390/391]	Time 0.099 (0.011)	Data 0.001 (0.002)	Loss 2.3640 (2.3933)	Acc@1 58.750 (55.786)	Acc@5 88.750 (84.678)
num momentum params: 26
[0.1, 2.39328871963501, 2.1532480084896086, 55.786, 45.29, tensor(0.3322, device='cuda:0', grad_fn=<DivBackward0>), 4.469098329544067, 0.39705729484558105]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [142 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [142][0/391]	Time 0.060 (0.060)	Data 0.185 (0.185)	Loss 2.3199 (2.3199)	Acc@1 53.906 (53.906)	Acc@5 85.938 (85.938)
Epoch: [142][10/391]	Time 0.012 (0.016)	Data 0.001 (0.018)	Loss 2.2541 (2.2513)	Acc@1 57.812 (59.375)	Acc@5 85.156 (86.364)
Epoch: [142][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.2680 (2.2713)	Acc@1 60.156 (58.296)	Acc@5 87.500 (86.458)
Epoch: [142][30/391]	Time 0.011 (0.013)	Data 0.001 (0.007)	Loss 2.4269 (2.2880)	Acc@1 54.688 (57.737)	Acc@5 85.938 (86.391)
Epoch: [142][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.2866 (2.3040)	Acc@1 60.156 (57.584)	Acc@5 85.938 (86.300)
Epoch: [142][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.0878 (2.3057)	Acc@1 63.281 (57.292)	Acc@5 92.969 (86.428)
Epoch: [142][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3538 (2.3110)	Acc@1 57.812 (57.211)	Acc@5 82.031 (86.245)
Epoch: [142][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.0481 (2.3034)	Acc@1 67.188 (57.812)	Acc@5 88.281 (86.235)
Epoch: [142][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.2025 (2.3043)	Acc@1 63.281 (57.948)	Acc@5 85.156 (86.101)
Epoch: [142][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4765 (2.3199)	Acc@1 56.250 (57.538)	Acc@5 82.031 (85.766)
Epoch: [142][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2981 (2.3213)	Acc@1 58.594 (57.611)	Acc@5 85.156 (85.636)
Epoch: [142][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5989 (2.3215)	Acc@1 47.656 (57.531)	Acc@5 82.031 (85.684)
Epoch: [142][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1619 (2.3227)	Acc@1 59.375 (57.457)	Acc@5 90.625 (85.628)
Epoch: [142][130/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2401 (2.3278)	Acc@1 53.906 (57.395)	Acc@5 91.406 (85.544)
Epoch: [142][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2818 (2.3312)	Acc@1 60.938 (57.264)	Acc@5 86.719 (85.600)
Epoch: [142][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4812 (2.3388)	Acc@1 53.125 (57.067)	Acc@5 82.812 (85.487)
Epoch: [142][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4423 (2.3414)	Acc@1 53.125 (56.910)	Acc@5 85.938 (85.574)
Epoch: [142][170/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 2.4371 (2.3502)	Acc@1 56.250 (56.698)	Acc@5 83.594 (85.476)
Epoch: [142][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6022 (2.3600)	Acc@1 51.562 (56.461)	Acc@5 83.594 (85.320)
Epoch: [142][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5099 (2.3633)	Acc@1 54.688 (56.405)	Acc@5 78.906 (85.226)
Epoch: [142][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7260 (2.3656)	Acc@1 47.656 (56.339)	Acc@5 80.469 (85.191)
Epoch: [142][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1206 (2.3629)	Acc@1 62.500 (56.428)	Acc@5 89.062 (85.253)
Epoch: [142][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6959 (2.3707)	Acc@1 46.875 (56.229)	Acc@5 85.156 (85.177)
Epoch: [142][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5304 (2.3709)	Acc@1 51.562 (56.240)	Acc@5 82.031 (85.122)
Epoch: [142][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2707 (2.3714)	Acc@1 61.719 (56.302)	Acc@5 89.062 (85.114)
Epoch: [142][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1075 (2.3681)	Acc@1 64.062 (56.378)	Acc@5 87.500 (85.184)
Epoch: [142][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.7744 (2.3724)	Acc@1 50.000 (56.268)	Acc@5 77.344 (85.087)
Epoch: [142][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1589 (2.3759)	Acc@1 61.719 (56.247)	Acc@5 89.062 (85.047)
Epoch: [142][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5758 (2.3784)	Acc@1 50.781 (56.197)	Acc@5 82.031 (84.970)
Epoch: [142][290/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.4925 (2.3805)	Acc@1 55.469 (56.164)	Acc@5 81.250 (84.928)
Epoch: [142][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4284 (2.3839)	Acc@1 57.031 (56.102)	Acc@5 83.594 (84.858)
Epoch: [142][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2891 (2.3846)	Acc@1 60.938 (56.122)	Acc@5 86.719 (84.807)
Epoch: [142][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3002 (2.3845)	Acc@1 56.250 (56.158)	Acc@5 88.281 (84.816)
Epoch: [142][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4407 (2.3863)	Acc@1 51.562 (56.101)	Acc@5 85.938 (84.807)
Epoch: [142][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3669 (2.3876)	Acc@1 53.906 (56.037)	Acc@5 85.156 (84.792)
Epoch: [142][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4539 (2.3894)	Acc@1 56.250 (55.983)	Acc@5 82.812 (84.789)
Epoch: [142][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2791 (2.3909)	Acc@1 60.156 (55.930)	Acc@5 87.500 (84.778)
Epoch: [142][370/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 2.4423 (2.3925)	Acc@1 57.812 (55.917)	Acc@5 83.594 (84.746)
Epoch: [142][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.6184 (2.3945)	Acc@1 46.094 (55.836)	Acc@5 82.031 (84.730)
Epoch: [142][390/391]	Time 0.015 (0.011)	Data 0.000 (0.002)	Loss 2.6423 (2.3945)	Acc@1 50.000 (55.848)	Acc@5 77.500 (84.700)
num momentum params: 26
[0.1, 2.394520379486084, 1.936179300546646, 55.848, 48.99, tensor(0.3323, device='cuda:0', grad_fn=<DivBackward0>), 4.221124649047852, 0.3388075828552246]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [143 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [143][0/391]	Time 0.052 (0.052)	Data 0.186 (0.186)	Loss 1.9999 (1.9999)	Acc@1 63.281 (63.281)	Acc@5 92.969 (92.969)
Epoch: [143][10/391]	Time 0.011 (0.016)	Data 0.001 (0.019)	Loss 2.3786 (2.3259)	Acc@1 55.469 (56.889)	Acc@5 85.156 (85.938)
Epoch: [143][20/391]	Time 0.012 (0.014)	Data 0.001 (0.010)	Loss 2.1524 (2.3051)	Acc@1 57.031 (57.143)	Acc@5 88.281 (86.161)
Epoch: [143][30/391]	Time 0.011 (0.013)	Data 0.001 (0.008)	Loss 2.1506 (2.2986)	Acc@1 57.812 (57.560)	Acc@5 88.281 (86.240)
Epoch: [143][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.5512 (2.3184)	Acc@1 53.906 (57.260)	Acc@5 82.812 (85.899)
Epoch: [143][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 2.3335 (2.3051)	Acc@1 59.375 (57.613)	Acc@5 82.812 (86.275)
Epoch: [143][60/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.2905 (2.3034)	Acc@1 56.250 (57.825)	Acc@5 89.062 (86.309)
Epoch: [143][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.1066 (2.2959)	Acc@1 64.062 (58.143)	Acc@5 87.500 (86.312)
Epoch: [143][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3945 (2.2982)	Acc@1 58.594 (58.111)	Acc@5 85.156 (86.188)
Epoch: [143][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4513 (2.3019)	Acc@1 51.562 (57.890)	Acc@5 85.156 (86.212)
Epoch: [143][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4035 (2.3096)	Acc@1 56.250 (57.704)	Acc@5 86.719 (86.224)
Epoch: [143][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5584 (2.3191)	Acc@1 47.656 (57.355)	Acc@5 82.031 (86.015)
Epoch: [143][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3612 (2.3279)	Acc@1 57.031 (57.296)	Acc@5 84.375 (85.892)
Epoch: [143][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.6544 (2.3388)	Acc@1 54.688 (57.091)	Acc@5 76.562 (85.687)
Epoch: [143][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4229 (2.3479)	Acc@1 51.562 (56.893)	Acc@5 88.281 (85.550)
Epoch: [143][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3257 (2.3498)	Acc@1 62.500 (56.954)	Acc@5 82.031 (85.498)
Epoch: [143][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3258 (2.3525)	Acc@1 59.375 (56.852)	Acc@5 87.500 (85.515)
Epoch: [143][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.4827 (2.3597)	Acc@1 52.344 (56.684)	Acc@5 83.594 (85.376)
Epoch: [143][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2914 (2.3632)	Acc@1 63.281 (56.643)	Acc@5 86.719 (85.299)
Epoch: [143][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5971 (2.3699)	Acc@1 45.312 (56.418)	Acc@5 84.375 (85.177)
Epoch: [143][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2269 (2.3706)	Acc@1 64.062 (56.440)	Acc@5 85.938 (85.141)
Epoch: [143][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6294 (2.3730)	Acc@1 50.000 (56.376)	Acc@5 82.031 (85.138)
Epoch: [143][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5521 (2.3779)	Acc@1 53.125 (56.222)	Acc@5 84.375 (85.075)
Epoch: [143][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4855 (2.3838)	Acc@1 53.906 (56.030)	Acc@5 82.031 (85.007)
Epoch: [143][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3918 (2.3883)	Acc@1 56.250 (56.004)	Acc@5 86.719 (84.933)
Epoch: [143][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2359 (2.3872)	Acc@1 58.594 (56.029)	Acc@5 87.500 (84.935)
Epoch: [143][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2128 (2.3859)	Acc@1 57.031 (56.026)	Acc@5 89.844 (84.983)
Epoch: [143][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5366 (2.3904)	Acc@1 53.125 (55.901)	Acc@5 83.594 (84.888)
Epoch: [143][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3654 (2.3916)	Acc@1 60.156 (55.916)	Acc@5 87.500 (84.823)
Epoch: [143][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3125 (2.3885)	Acc@1 60.156 (55.982)	Acc@5 82.812 (84.874)
Epoch: [143][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7706 (2.3920)	Acc@1 47.656 (55.902)	Acc@5 79.688 (84.811)
Epoch: [143][310/391]	Time 0.014 (0.011)	Data 0.002 (0.002)	Loss 2.9632 (2.3946)	Acc@1 46.875 (55.856)	Acc@5 75.781 (84.769)
Epoch: [143][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5124 (2.3956)	Acc@1 57.031 (55.844)	Acc@5 80.469 (84.747)
Epoch: [143][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5083 (2.3974)	Acc@1 51.562 (55.828)	Acc@5 82.812 (84.703)
Epoch: [143][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5684 (2.3982)	Acc@1 53.906 (55.842)	Acc@5 82.812 (84.664)
Epoch: [143][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6546 (2.4002)	Acc@1 52.344 (55.774)	Acc@5 80.469 (84.624)
Epoch: [143][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3364 (2.4007)	Acc@1 55.469 (55.726)	Acc@5 85.156 (84.635)
Epoch: [143][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4346 (2.4020)	Acc@1 60.156 (55.713)	Acc@5 82.812 (84.602)
Epoch: [143][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5721 (2.4042)	Acc@1 49.219 (55.666)	Acc@5 81.250 (84.574)
Epoch: [143][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.1732 (2.4034)	Acc@1 56.250 (55.656)	Acc@5 88.750 (84.588)
num momentum params: 26
[0.1, 2.403370648574829, 1.9329600608348847, 55.656, 49.21, tensor(0.3308, device='cuda:0', grad_fn=<DivBackward0>), 4.2041778564453125, 0.3274261951446533]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [144 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [144][0/391]	Time 0.061 (0.061)	Data 0.181 (0.181)	Loss 2.5726 (2.5726)	Acc@1 51.562 (51.562)	Acc@5 81.250 (81.250)
Epoch: [144][10/391]	Time 0.012 (0.017)	Data 0.001 (0.018)	Loss 2.6085 (2.3356)	Acc@1 50.781 (56.463)	Acc@5 80.469 (85.866)
Epoch: [144][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 2.3726 (2.2928)	Acc@1 53.125 (57.292)	Acc@5 89.844 (87.091)
Epoch: [144][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 2.2453 (2.3089)	Acc@1 61.719 (57.510)	Acc@5 85.156 (86.694)
Epoch: [144][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.4220 (2.3167)	Acc@1 56.250 (57.641)	Acc@5 82.031 (86.681)
Epoch: [144][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2897 (2.3370)	Acc@1 60.938 (57.138)	Acc@5 87.500 (86.397)
Epoch: [144][60/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 2.3554 (2.3460)	Acc@1 57.031 (56.775)	Acc@5 83.594 (86.206)
Epoch: [144][70/391]	Time 0.014 (0.012)	Data 0.001 (0.004)	Loss 2.5115 (2.3426)	Acc@1 54.688 (57.009)	Acc@5 79.688 (86.103)
Epoch: [144][80/391]	Time 0.011 (0.012)	Data 0.003 (0.004)	Loss 2.2356 (2.3370)	Acc@1 60.938 (57.079)	Acc@5 86.719 (86.169)
Epoch: [144][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6086 (2.3493)	Acc@1 53.125 (56.860)	Acc@5 85.156 (86.075)
Epoch: [144][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3700 (2.3545)	Acc@1 56.250 (56.737)	Acc@5 85.156 (85.876)
Epoch: [144][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1197 (2.3551)	Acc@1 63.281 (56.841)	Acc@5 92.188 (85.783)
Epoch: [144][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1092 (2.3606)	Acc@1 61.719 (56.786)	Acc@5 89.062 (85.582)
Epoch: [144][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2738 (2.3549)	Acc@1 60.938 (57.007)	Acc@5 85.938 (85.562)
Epoch: [144][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.5511 (2.3564)	Acc@1 53.125 (56.898)	Acc@5 77.344 (85.489)
Epoch: [144][150/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 2.5458 (2.3605)	Acc@1 51.562 (56.845)	Acc@5 83.594 (85.363)
Epoch: [144][160/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.3013 (2.3678)	Acc@1 60.938 (56.764)	Acc@5 85.938 (85.258)
Epoch: [144][170/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.5730 (2.3709)	Acc@1 52.344 (56.652)	Acc@5 80.469 (85.165)
Epoch: [144][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6184 (2.3757)	Acc@1 53.125 (56.487)	Acc@5 78.906 (85.057)
Epoch: [144][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3746 (2.3766)	Acc@1 51.562 (56.459)	Acc@5 85.938 (85.111)
Epoch: [144][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4427 (2.3763)	Acc@1 54.688 (56.464)	Acc@5 83.594 (85.102)
Epoch: [144][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2232 (2.3731)	Acc@1 57.031 (56.554)	Acc@5 89.062 (85.141)
Epoch: [144][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3711 (2.3764)	Acc@1 57.812 (56.448)	Acc@5 85.156 (85.135)
Epoch: [144][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3834 (2.3805)	Acc@1 52.344 (56.287)	Acc@5 84.375 (85.065)
Epoch: [144][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5126 (2.3833)	Acc@1 48.438 (56.214)	Acc@5 83.594 (85.049)
Epoch: [144][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5108 (2.3839)	Acc@1 50.781 (56.116)	Acc@5 84.375 (85.091)
Epoch: [144][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4044 (2.3843)	Acc@1 60.938 (56.109)	Acc@5 85.156 (85.069)
Epoch: [144][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2006 (2.3845)	Acc@1 57.812 (56.129)	Acc@5 87.500 (85.018)
Epoch: [144][280/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 2.2812 (2.3885)	Acc@1 61.719 (56.030)	Acc@5 85.938 (84.987)
Epoch: [144][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2663 (2.3878)	Acc@1 59.375 (56.075)	Acc@5 84.375 (84.960)
Epoch: [144][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3125 (2.3872)	Acc@1 56.250 (56.118)	Acc@5 85.938 (84.980)
Epoch: [144][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3600 (2.3885)	Acc@1 57.031 (56.089)	Acc@5 84.375 (84.938)
Epoch: [144][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3909 (2.3901)	Acc@1 53.125 (56.050)	Acc@5 84.375 (84.923)
Epoch: [144][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2555 (2.3888)	Acc@1 61.719 (56.068)	Acc@5 85.938 (84.946)
Epoch: [144][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7022 (2.3889)	Acc@1 46.875 (56.087)	Acc@5 81.250 (84.950)
Epoch: [144][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6138 (2.3913)	Acc@1 51.562 (56.041)	Acc@5 82.812 (84.900)
Epoch: [144][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3025 (2.3912)	Acc@1 58.594 (56.027)	Acc@5 87.500 (84.884)
Epoch: [144][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2851 (2.3911)	Acc@1 54.688 (56.033)	Acc@5 88.281 (84.842)
Epoch: [144][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3625 (2.3929)	Acc@1 59.375 (55.975)	Acc@5 84.375 (84.822)
Epoch: [144][390/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 2.3781 (2.3948)	Acc@1 53.750 (55.934)	Acc@5 91.250 (84.804)
num momentum params: 26
[0.1, 2.394807908554077, 1.975703364610672, 55.934, 48.9, tensor(0.3319, device='cuda:0', grad_fn=<DivBackward0>), 4.230482816696167, 0.3418128490447998]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [145 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [145][0/391]	Time 0.058 (0.058)	Data 0.188 (0.188)	Loss 2.3249 (2.3249)	Acc@1 58.594 (58.594)	Acc@5 85.938 (85.938)
Epoch: [145][10/391]	Time 0.010 (0.017)	Data 0.001 (0.018)	Loss 2.4126 (2.3226)	Acc@1 58.594 (58.736)	Acc@5 84.375 (86.293)
Epoch: [145][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.2533 (2.2981)	Acc@1 60.156 (58.557)	Acc@5 89.062 (86.384)
Epoch: [145][30/391]	Time 0.015 (0.013)	Data 0.001 (0.007)	Loss 2.3000 (2.2974)	Acc@1 53.906 (57.939)	Acc@5 86.719 (86.316)
Epoch: [145][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.3320 (2.3045)	Acc@1 59.375 (57.889)	Acc@5 85.938 (86.185)
Epoch: [145][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.3203 (2.3286)	Acc@1 60.938 (57.552)	Acc@5 85.938 (85.754)
Epoch: [145][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.1672 (2.3382)	Acc@1 60.156 (57.428)	Acc@5 88.281 (85.425)
Epoch: [145][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4191 (2.3437)	Acc@1 54.688 (57.207)	Acc@5 79.688 (85.288)
Epoch: [145][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.5646 (2.3492)	Acc@1 52.344 (56.944)	Acc@5 79.688 (85.204)
Epoch: [145][90/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 2.3114 (2.3408)	Acc@1 54.688 (57.057)	Acc@5 89.062 (85.337)
Epoch: [145][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 2.4676 (2.3373)	Acc@1 53.125 (57.085)	Acc@5 82.031 (85.342)
Epoch: [145][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.3109 (2.3398)	Acc@1 55.469 (57.010)	Acc@5 86.719 (85.304)
Epoch: [145][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 2.6492 (2.3452)	Acc@1 50.000 (56.850)	Acc@5 81.250 (85.201)
Epoch: [145][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5324 (2.3482)	Acc@1 46.875 (56.626)	Acc@5 84.375 (85.192)
Epoch: [145][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2133 (2.3457)	Acc@1 58.594 (56.754)	Acc@5 89.062 (85.239)
Epoch: [145][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.6492 (2.3447)	Acc@1 50.781 (56.798)	Acc@5 80.469 (85.306)
Epoch: [145][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2236 (2.3466)	Acc@1 56.250 (56.823)	Acc@5 88.281 (85.297)
Epoch: [145][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6011 (2.3498)	Acc@1 48.438 (56.753)	Acc@5 79.688 (85.266)
Epoch: [145][180/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.1476 (2.3514)	Acc@1 58.594 (56.733)	Acc@5 89.844 (85.212)
Epoch: [145][190/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2720 (2.3522)	Acc@1 57.031 (56.716)	Acc@5 85.156 (85.189)
Epoch: [145][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5962 (2.3607)	Acc@1 51.562 (56.522)	Acc@5 78.906 (85.059)
Epoch: [145][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4209 (2.3629)	Acc@1 56.250 (56.454)	Acc@5 85.938 (85.049)
Epoch: [145][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4774 (2.3649)	Acc@1 45.312 (56.342)	Acc@5 82.812 (85.036)
Epoch: [145][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.2369 (2.3677)	Acc@1 57.031 (56.260)	Acc@5 87.500 (84.991)
Epoch: [145][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6775 (2.3689)	Acc@1 50.000 (56.266)	Acc@5 81.250 (84.962)
Epoch: [145][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.5844 (2.3712)	Acc@1 52.344 (56.281)	Acc@5 79.688 (84.885)
Epoch: [145][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2605 (2.3729)	Acc@1 57.812 (56.250)	Acc@5 89.062 (84.890)
Epoch: [145][270/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.4743 (2.3762)	Acc@1 54.688 (56.117)	Acc@5 83.594 (84.868)
Epoch: [145][280/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.5194 (2.3817)	Acc@1 49.219 (55.986)	Acc@5 85.938 (84.764)
Epoch: [145][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2683 (2.3825)	Acc@1 57.031 (55.973)	Acc@5 89.062 (84.729)
Epoch: [145][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5073 (2.3855)	Acc@1 47.656 (55.879)	Acc@5 82.812 (84.689)
Epoch: [145][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2798 (2.3866)	Acc@1 53.906 (55.858)	Acc@5 89.844 (84.669)
Epoch: [145][320/391]	Time 0.022 (0.011)	Data 0.001 (0.002)	Loss 2.2428 (2.3893)	Acc@1 59.375 (55.824)	Acc@5 89.062 (84.665)
Epoch: [145][330/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3452 (2.3882)	Acc@1 59.375 (55.851)	Acc@5 82.812 (84.682)
Epoch: [145][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5871 (2.3899)	Acc@1 57.031 (55.817)	Acc@5 77.344 (84.643)
Epoch: [145][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4471 (2.3900)	Acc@1 51.562 (55.814)	Acc@5 81.250 (84.622)
Epoch: [145][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4705 (2.3920)	Acc@1 50.781 (55.728)	Acc@5 84.375 (84.581)
Epoch: [145][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7928 (2.3951)	Acc@1 45.312 (55.616)	Acc@5 75.781 (84.527)
Epoch: [145][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4776 (2.3966)	Acc@1 53.125 (55.577)	Acc@5 79.688 (84.486)
Epoch: [145][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.6685 (2.3975)	Acc@1 47.500 (55.552)	Acc@5 75.000 (84.510)
num momentum params: 26
[0.1, 2.3974903836822508, 2.040903433561325, 55.552, 46.69, tensor(0.3315, device='cuda:0', grad_fn=<DivBackward0>), 4.304468393325806, 0.33208632469177246]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [146 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [146][0/391]	Time 0.059 (0.059)	Data 0.187 (0.187)	Loss 2.3594 (2.3594)	Acc@1 55.469 (55.469)	Acc@5 83.594 (83.594)
Epoch: [146][10/391]	Time 0.011 (0.017)	Data 0.001 (0.018)	Loss 2.2059 (2.3369)	Acc@1 60.938 (56.676)	Acc@5 90.625 (85.227)
Epoch: [146][20/391]	Time 0.010 (0.015)	Data 0.001 (0.010)	Loss 2.3766 (2.3571)	Acc@1 59.375 (56.548)	Acc@5 84.375 (85.156)
Epoch: [146][30/391]	Time 0.013 (0.014)	Data 0.001 (0.007)	Loss 2.3116 (2.3360)	Acc@1 56.250 (57.132)	Acc@5 87.500 (85.106)
Epoch: [146][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 2.2454 (2.3224)	Acc@1 56.250 (57.355)	Acc@5 87.500 (85.442)
Epoch: [146][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.4565 (2.3102)	Acc@1 53.125 (57.445)	Acc@5 82.031 (85.539)
Epoch: [146][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2269 (2.3119)	Acc@1 57.031 (57.275)	Acc@5 85.938 (85.592)
Epoch: [146][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2463 (2.3126)	Acc@1 60.938 (57.328)	Acc@5 85.938 (85.684)
Epoch: [146][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.4141 (2.3180)	Acc@1 55.469 (57.272)	Acc@5 85.156 (85.658)
Epoch: [146][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2963 (2.3237)	Acc@1 60.938 (57.246)	Acc@5 84.375 (85.663)
Epoch: [146][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4786 (2.3257)	Acc@1 57.812 (57.287)	Acc@5 82.031 (85.605)
Epoch: [146][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3079 (2.3271)	Acc@1 56.250 (57.081)	Acc@5 86.719 (85.649)
Epoch: [146][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4456 (2.3323)	Acc@1 57.031 (56.967)	Acc@5 83.594 (85.505)
Epoch: [146][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.1842 (2.3345)	Acc@1 57.812 (56.870)	Acc@5 85.938 (85.592)
Epoch: [146][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4128 (2.3344)	Acc@1 56.250 (56.992)	Acc@5 82.031 (85.566)
Epoch: [146][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4166 (2.3352)	Acc@1 56.250 (56.964)	Acc@5 84.375 (85.581)
Epoch: [146][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4082 (2.3338)	Acc@1 53.125 (56.968)	Acc@5 82.812 (85.632)
Epoch: [146][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5027 (2.3386)	Acc@1 57.031 (56.963)	Acc@5 82.812 (85.604)
Epoch: [146][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5603 (2.3432)	Acc@1 53.125 (56.872)	Acc@5 77.344 (85.428)
Epoch: [146][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3926 (2.3498)	Acc@1 54.688 (56.733)	Acc@5 85.938 (85.304)
Epoch: [146][200/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 2.4920 (2.3534)	Acc@1 50.781 (56.576)	Acc@5 81.250 (85.281)
Epoch: [146][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5892 (2.3580)	Acc@1 53.125 (56.476)	Acc@5 85.938 (85.219)
Epoch: [146][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4859 (2.3609)	Acc@1 58.594 (56.374)	Acc@5 80.469 (85.202)
Epoch: [146][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5932 (2.3641)	Acc@1 49.219 (56.243)	Acc@5 79.688 (85.109)
Epoch: [146][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3892 (2.3652)	Acc@1 57.031 (56.260)	Acc@5 81.250 (85.121)
Epoch: [146][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3164 (2.3671)	Acc@1 55.469 (56.188)	Acc@5 86.719 (85.072)
Epoch: [146][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3766 (2.3691)	Acc@1 54.688 (56.070)	Acc@5 85.938 (85.045)
Epoch: [146][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6594 (2.3733)	Acc@1 52.344 (55.996)	Acc@5 78.906 (84.986)
Epoch: [146][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3182 (2.3745)	Acc@1 57.031 (55.964)	Acc@5 87.500 (84.981)
Epoch: [146][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.5694 (2.3791)	Acc@1 53.906 (55.882)	Acc@5 84.375 (84.944)
Epoch: [146][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1188 (2.3808)	Acc@1 63.281 (55.866)	Acc@5 89.062 (84.923)
Epoch: [146][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3511 (2.3809)	Acc@1 57.812 (55.893)	Acc@5 88.281 (84.928)
Epoch: [146][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4540 (2.3816)	Acc@1 54.688 (55.885)	Acc@5 87.500 (84.927)
Epoch: [146][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1768 (2.3836)	Acc@1 57.812 (55.823)	Acc@5 89.062 (84.875)
Epoch: [146][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3675 (2.3871)	Acc@1 53.125 (55.753)	Acc@5 85.156 (84.854)
Epoch: [146][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6328 (2.3892)	Acc@1 49.219 (55.709)	Acc@5 80.469 (84.856)
Epoch: [146][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4597 (2.3912)	Acc@1 53.906 (55.657)	Acc@5 85.938 (84.816)
Epoch: [146][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3353 (2.3933)	Acc@1 57.031 (55.589)	Acc@5 82.812 (84.752)
Epoch: [146][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5053 (2.3945)	Acc@1 53.906 (55.567)	Acc@5 80.469 (84.705)
Epoch: [146][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.4056 (2.3950)	Acc@1 51.250 (55.550)	Acc@5 91.250 (84.682)
num momentum params: 26
[0.1, 2.3949926446533203, 2.035617022514343, 55.55, 48.02, tensor(0.3315, device='cuda:0', grad_fn=<DivBackward0>), 4.208435773849487, 0.34890007972717285]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [147 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [147][0/391]	Time 0.057 (0.057)	Data 0.184 (0.184)	Loss 2.4684 (2.4684)	Acc@1 52.344 (52.344)	Acc@5 84.375 (84.375)
Epoch: [147][10/391]	Time 0.012 (0.017)	Data 0.001 (0.018)	Loss 2.2786 (2.2999)	Acc@1 57.812 (58.381)	Acc@5 85.938 (84.730)
Epoch: [147][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.3958 (2.2762)	Acc@1 54.688 (59.003)	Acc@5 84.375 (85.528)
Epoch: [147][30/391]	Time 0.019 (0.013)	Data 0.002 (0.007)	Loss 2.4635 (2.3004)	Acc@1 47.656 (58.569)	Acc@5 86.719 (85.509)
Epoch: [147][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 2.2371 (2.2822)	Acc@1 57.812 (59.165)	Acc@5 85.156 (85.728)
Epoch: [147][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.1579 (2.2829)	Acc@1 59.375 (59.053)	Acc@5 89.844 (85.769)
Epoch: [147][60/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.2490 (2.3024)	Acc@1 63.281 (58.427)	Acc@5 89.062 (85.592)
Epoch: [147][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.8974 (2.3114)	Acc@1 47.656 (58.044)	Acc@5 75.000 (85.398)
Epoch: [147][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.2273 (2.3203)	Acc@1 59.375 (57.870)	Acc@5 85.938 (85.340)
Epoch: [147][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4558 (2.3258)	Acc@1 51.562 (57.667)	Acc@5 82.812 (85.225)
Epoch: [147][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.2323 (2.3229)	Acc@1 57.812 (57.557)	Acc@5 87.500 (85.365)
Epoch: [147][110/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.2881 (2.3240)	Acc@1 60.156 (57.517)	Acc@5 81.250 (85.389)
Epoch: [147][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.3120 (2.3340)	Acc@1 55.469 (57.212)	Acc@5 85.156 (85.260)
Epoch: [147][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5035 (2.3416)	Acc@1 51.562 (56.942)	Acc@5 89.062 (85.252)
Epoch: [147][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3188 (2.3475)	Acc@1 55.469 (56.693)	Acc@5 88.281 (85.245)
Epoch: [147][150/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.5028 (2.3521)	Acc@1 48.438 (56.638)	Acc@5 84.375 (85.203)
Epoch: [147][160/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.5098 (2.3582)	Acc@1 53.906 (56.478)	Acc@5 81.250 (85.088)
Epoch: [147][170/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.4992 (2.3567)	Acc@1 53.906 (56.501)	Acc@5 82.812 (85.165)
Epoch: [147][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6008 (2.3599)	Acc@1 55.469 (56.440)	Acc@5 79.688 (85.100)
Epoch: [147][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2977 (2.3608)	Acc@1 56.250 (56.422)	Acc@5 83.594 (85.132)
Epoch: [147][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4915 (2.3631)	Acc@1 54.688 (56.429)	Acc@5 82.812 (85.129)
Epoch: [147][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1346 (2.3650)	Acc@1 64.844 (56.413)	Acc@5 92.188 (85.141)
Epoch: [147][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1654 (2.3662)	Acc@1 60.156 (56.413)	Acc@5 85.938 (85.103)
Epoch: [147][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5788 (2.3690)	Acc@1 51.562 (56.274)	Acc@5 79.688 (85.034)
Epoch: [147][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7955 (2.3747)	Acc@1 47.656 (56.117)	Acc@5 78.125 (84.926)
Epoch: [147][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2838 (2.3754)	Acc@1 56.250 (56.054)	Acc@5 90.625 (84.991)
Epoch: [147][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6751 (2.3772)	Acc@1 54.688 (56.049)	Acc@5 80.469 (85.004)
Epoch: [147][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2168 (2.3782)	Acc@1 59.375 (56.016)	Acc@5 90.625 (84.986)
Epoch: [147][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4245 (2.3814)	Acc@1 53.906 (55.947)	Acc@5 83.594 (84.923)
Epoch: [147][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5150 (2.3851)	Acc@1 54.688 (55.871)	Acc@5 85.156 (84.882)
Epoch: [147][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3016 (2.3849)	Acc@1 53.125 (55.900)	Acc@5 87.500 (84.868)
Epoch: [147][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4847 (2.3874)	Acc@1 53.125 (55.891)	Acc@5 81.250 (84.805)
Epoch: [147][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.7190 (2.3878)	Acc@1 49.219 (55.844)	Acc@5 81.250 (84.840)
Epoch: [147][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3378 (2.3913)	Acc@1 58.594 (55.828)	Acc@5 85.156 (84.743)
Epoch: [147][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4947 (2.3931)	Acc@1 52.344 (55.806)	Acc@5 82.812 (84.728)
Epoch: [147][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6209 (2.3935)	Acc@1 50.781 (55.760)	Acc@5 77.344 (84.738)
Epoch: [147][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4613 (2.3945)	Acc@1 55.469 (55.750)	Acc@5 83.594 (84.736)
Epoch: [147][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4986 (2.3953)	Acc@1 53.125 (55.766)	Acc@5 82.812 (84.752)
Epoch: [147][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.1649 (2.3943)	Acc@1 64.062 (55.819)	Acc@5 85.938 (84.744)
Epoch: [147][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.2820 (2.3957)	Acc@1 60.000 (55.828)	Acc@5 86.250 (84.688)
num momentum params: 26
[0.1, 2.3956750923919676, 2.138294142484665, 55.828, 45.66, tensor(0.3320, device='cuda:0', grad_fn=<DivBackward0>), 4.1581339836120605, 0.34142136573791504]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [148 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [148][0/391]	Time 0.057 (0.057)	Data 0.191 (0.191)	Loss 2.1417 (2.1417)	Acc@1 60.938 (60.938)	Acc@5 88.281 (88.281)
Epoch: [148][10/391]	Time 0.012 (0.016)	Data 0.001 (0.019)	Loss 2.3043 (2.2223)	Acc@1 57.031 (58.878)	Acc@5 86.719 (87.216)
Epoch: [148][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 2.3583 (2.2568)	Acc@1 57.031 (58.631)	Acc@5 82.812 (86.496)
Epoch: [148][30/391]	Time 0.010 (0.013)	Data 0.001 (0.008)	Loss 2.0619 (2.2973)	Acc@1 64.062 (57.964)	Acc@5 88.281 (85.837)
Epoch: [148][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.3376 (2.3055)	Acc@1 51.562 (57.622)	Acc@5 85.156 (85.995)
Epoch: [148][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.3201 (2.3101)	Acc@1 53.125 (57.460)	Acc@5 91.406 (86.152)
Epoch: [148][60/391]	Time 0.010 (0.012)	Data 0.002 (0.005)	Loss 2.2948 (2.3154)	Acc@1 59.375 (57.377)	Acc@5 84.375 (86.104)
Epoch: [148][70/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 2.1227 (2.3224)	Acc@1 63.281 (57.086)	Acc@5 88.281 (85.993)
Epoch: [148][80/391]	Time 0.010 (0.011)	Data 0.002 (0.004)	Loss 2.2736 (2.3229)	Acc@1 60.156 (57.002)	Acc@5 84.375 (85.995)
Epoch: [148][90/391]	Time 0.012 (0.011)	Data 0.001 (0.004)	Loss 2.3513 (2.3208)	Acc@1 57.812 (57.083)	Acc@5 85.156 (85.963)
Epoch: [148][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.3692 (2.3254)	Acc@1 56.250 (57.000)	Acc@5 82.812 (85.767)
Epoch: [148][110/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2403 (2.3284)	Acc@1 60.938 (56.954)	Acc@5 85.156 (85.600)
Epoch: [148][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2685 (2.3304)	Acc@1 60.156 (56.883)	Acc@5 85.156 (85.660)
Epoch: [148][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.4398 (2.3345)	Acc@1 56.250 (56.715)	Acc@5 82.812 (85.538)
Epoch: [148][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6140 (2.3380)	Acc@1 47.656 (56.654)	Acc@5 82.812 (85.494)
Epoch: [148][150/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.7265 (2.3417)	Acc@1 50.781 (56.612)	Acc@5 79.688 (85.467)
Epoch: [148][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.6711 (2.3448)	Acc@1 44.531 (56.468)	Acc@5 82.031 (85.486)
Epoch: [148][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.0542 (2.3482)	Acc@1 64.844 (56.460)	Acc@5 89.062 (85.398)
Epoch: [148][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5407 (2.3547)	Acc@1 53.125 (56.414)	Acc@5 79.688 (85.303)
Epoch: [148][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.3585 (2.3578)	Acc@1 57.812 (56.397)	Acc@5 85.938 (85.193)
Epoch: [148][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3998 (2.3588)	Acc@1 53.906 (56.351)	Acc@5 82.031 (85.160)
Epoch: [148][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2102 (2.3580)	Acc@1 60.156 (56.424)	Acc@5 87.500 (85.160)
Epoch: [148][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5547 (2.3645)	Acc@1 49.219 (56.324)	Acc@5 80.469 (85.061)
Epoch: [148][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.6765 (2.3670)	Acc@1 47.656 (56.243)	Acc@5 78.906 (85.028)
Epoch: [148][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 2.0360 (2.3694)	Acc@1 64.062 (56.192)	Acc@5 92.188 (85.014)
Epoch: [148][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4919 (2.3749)	Acc@1 52.344 (56.051)	Acc@5 83.594 (84.892)
Epoch: [148][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3269 (2.3753)	Acc@1 55.469 (56.049)	Acc@5 88.281 (84.914)
Epoch: [148][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.4314 (2.3741)	Acc@1 52.344 (56.140)	Acc@5 83.594 (84.905)
Epoch: [148][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4527 (2.3755)	Acc@1 56.250 (56.114)	Acc@5 86.719 (84.917)
Epoch: [148][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2663 (2.3788)	Acc@1 56.250 (56.027)	Acc@5 84.375 (84.877)
Epoch: [148][300/391]	Time 0.014 (0.011)	Data 0.002 (0.002)	Loss 2.2434 (2.3803)	Acc@1 61.719 (56.011)	Acc@5 87.500 (84.899)
Epoch: [148][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2754 (2.3829)	Acc@1 57.812 (55.944)	Acc@5 91.406 (84.887)
Epoch: [148][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1727 (2.3853)	Acc@1 67.188 (55.931)	Acc@5 89.062 (84.862)
Epoch: [148][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5914 (2.3870)	Acc@1 53.906 (55.936)	Acc@5 81.250 (84.849)
Epoch: [148][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3557 (2.3891)	Acc@1 57.031 (55.886)	Acc@5 86.719 (84.808)
Epoch: [148][350/391]	Time 0.017 (0.011)	Data 0.001 (0.002)	Loss 2.2780 (2.3898)	Acc@1 58.594 (55.901)	Acc@5 83.594 (84.785)
Epoch: [148][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.2482 (2.3897)	Acc@1 59.375 (55.915)	Acc@5 86.719 (84.758)
Epoch: [148][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5255 (2.3929)	Acc@1 51.562 (55.839)	Acc@5 85.938 (84.674)
Epoch: [148][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3402 (2.3927)	Acc@1 54.688 (55.830)	Acc@5 85.156 (84.699)
Epoch: [148][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.6250 (2.3941)	Acc@1 50.000 (55.800)	Acc@5 78.750 (84.680)
num momentum params: 26
[0.1, 2.394084466629028, 2.0445118129253386, 55.8, 47.42, tensor(0.3321, device='cuda:0', grad_fn=<DivBackward0>), 4.283648252487183, 0.3447451591491699]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [149 | 180] LR: 0.100000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [149][0/391]	Time 0.056 (0.056)	Data 0.182 (0.182)	Loss 2.2845 (2.2845)	Acc@1 56.250 (56.250)	Acc@5 90.625 (90.625)
Epoch: [149][10/391]	Time 0.011 (0.016)	Data 0.001 (0.018)	Loss 2.4125 (2.2677)	Acc@1 57.031 (58.239)	Acc@5 83.594 (87.358)
Epoch: [149][20/391]	Time 0.011 (0.014)	Data 0.001 (0.011)	Loss 2.4261 (2.2841)	Acc@1 54.688 (57.366)	Acc@5 82.031 (86.644)
Epoch: [149][30/391]	Time 0.011 (0.013)	Data 0.001 (0.008)	Loss 2.3677 (2.3074)	Acc@1 56.250 (57.359)	Acc@5 88.281 (85.912)
Epoch: [149][40/391]	Time 0.010 (0.012)	Data 0.001 (0.006)	Loss 2.6762 (2.3217)	Acc@1 51.562 (56.955)	Acc@5 79.688 (85.766)
Epoch: [149][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 2.3571 (2.3133)	Acc@1 59.375 (57.491)	Acc@5 84.375 (85.692)
Epoch: [149][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 2.2097 (2.3123)	Acc@1 60.156 (57.505)	Acc@5 86.719 (85.733)
Epoch: [149][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.2820 (2.3148)	Acc@1 60.156 (57.361)	Acc@5 85.938 (85.640)
Epoch: [149][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 2.4588 (2.3229)	Acc@1 50.781 (57.176)	Acc@5 85.938 (85.648)
Epoch: [149][90/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 2.3719 (2.3270)	Acc@1 57.812 (57.160)	Acc@5 85.938 (85.628)
Epoch: [149][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 2.2651 (2.3318)	Acc@1 55.469 (56.993)	Acc@5 85.938 (85.659)
Epoch: [149][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3919 (2.3380)	Acc@1 62.500 (56.841)	Acc@5 83.594 (85.466)
Epoch: [149][120/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 2.4280 (2.3445)	Acc@1 50.000 (56.618)	Acc@5 82.031 (85.356)
Epoch: [149][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3906 (2.3436)	Acc@1 51.562 (56.471)	Acc@5 84.375 (85.383)
Epoch: [149][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 2.2067 (2.3478)	Acc@1 60.938 (56.366)	Acc@5 83.594 (85.262)
Epoch: [149][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.5852 (2.3482)	Acc@1 50.000 (56.441)	Acc@5 81.250 (85.213)
Epoch: [149][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 2.3525 (2.3557)	Acc@1 57.812 (56.274)	Acc@5 82.812 (85.132)
Epoch: [149][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 2.2792 (2.3627)	Acc@1 57.812 (56.136)	Acc@5 87.500 (85.092)
Epoch: [149][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5663 (2.3694)	Acc@1 54.688 (56.013)	Acc@5 81.250 (84.997)
Epoch: [149][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1719 (2.3734)	Acc@1 60.156 (55.919)	Acc@5 88.281 (84.960)
Epoch: [149][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2689 (2.3706)	Acc@1 57.812 (55.974)	Acc@5 87.500 (85.024)
Epoch: [149][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2933 (2.3679)	Acc@1 64.062 (56.124)	Acc@5 83.594 (85.023)
Epoch: [149][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.5235 (2.3740)	Acc@1 52.344 (55.999)	Acc@5 84.375 (84.976)
Epoch: [149][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4214 (2.3780)	Acc@1 57.031 (55.959)	Acc@5 78.125 (84.879)
Epoch: [149][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4766 (2.3761)	Acc@1 54.688 (56.039)	Acc@5 85.156 (84.916)
Epoch: [149][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.3787 (2.3759)	Acc@1 54.688 (55.970)	Acc@5 85.938 (84.985)
Epoch: [149][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.6344 (2.3770)	Acc@1 47.656 (55.951)	Acc@5 82.812 (84.962)
Epoch: [149][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.5974 (2.3812)	Acc@1 54.688 (55.924)	Acc@5 82.031 (84.868)
Epoch: [149][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.0668 (2.3825)	Acc@1 62.500 (55.852)	Acc@5 90.625 (84.881)
Epoch: [149][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4082 (2.3835)	Acc@1 55.469 (55.820)	Acc@5 82.812 (84.837)
Epoch: [149][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.1833 (2.3815)	Acc@1 60.156 (55.861)	Acc@5 87.500 (84.842)
Epoch: [149][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2065 (2.3827)	Acc@1 60.938 (55.881)	Acc@5 85.938 (84.827)
Epoch: [149][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2316 (2.3838)	Acc@1 58.594 (55.856)	Acc@5 88.281 (84.816)
Epoch: [149][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.4967 (2.3836)	Acc@1 50.781 (55.879)	Acc@5 81.250 (84.795)
Epoch: [149][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3184 (2.3841)	Acc@1 55.469 (55.913)	Acc@5 88.281 (84.753)
Epoch: [149][350/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 2.1402 (2.3823)	Acc@1 59.375 (55.947)	Acc@5 90.625 (84.769)
Epoch: [149][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.3637 (2.3820)	Acc@1 58.594 (55.954)	Acc@5 85.938 (84.788)
Epoch: [149][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.8947 (2.3814)	Acc@1 67.969 (55.983)	Acc@5 92.188 (84.819)
Epoch: [149][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 2.2673 (2.3834)	Acc@1 59.375 (55.930)	Acc@5 90.625 (84.795)
Epoch: [149][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 2.3747 (2.3838)	Acc@1 52.500 (55.920)	Acc@5 88.750 (84.806)
num momentum params: 26
[0.1, 2.3837925129699706, 2.002058033943176, 55.92, 47.48, tensor(0.3333, device='cuda:0', grad_fn=<DivBackward0>), 4.215555906295776, 0.34352993965148926]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [357, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [357]
Non Pruning Epoch - module.bn5.bias: [357]
Non Pruning Epoch - module.conv6.weight: [307, 357, 3, 3]
Non Pruning Epoch - module.bn6.weight: [307]
Non Pruning Epoch - module.bn6.bias: [307]
Non Pruning Epoch - module.conv7.weight: [191, 307, 3, 3]
Non Pruning Epoch - module.bn7.weight: [191]
Non Pruning Epoch - module.bn7.bias: [191]
Non Pruning Epoch - module.conv8.weight: [187, 191, 3, 3]
Non Pruning Epoch - module.bn8.weight: [187]
Non Pruning Epoch - module.bn8.bias: [187]
Non Pruning Epoch - module.fc.weight: [100, 187]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [150 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [357, 247, 3, 3]
module.conv6.weight [307, 357, 3, 3]
module.conv7.weight [191, 307, 3, 3]
module.conv8.weight [187, 191, 3, 3]
Epoch: [150][0/391]	Time 0.059 (0.059)	Data 0.196 (0.196)	Loss 2.2780 (2.2780)	Acc@1 64.062 (64.062)	Acc@5 86.719 (86.719)
Epoch: [150][10/391]	Time 0.012 (0.017)	Data 0.002 (0.019)	Loss 2.2299 (2.2456)	Acc@1 57.812 (59.872)	Acc@5 85.938 (87.713)
Epoch: [150][20/391]	Time 0.011 (0.014)	Data 0.001 (0.011)	Loss 2.0858 (2.2007)	Acc@1 62.500 (61.012)	Acc@5 90.625 (88.058)
Epoch: [150][30/391]	Time 0.011 (0.013)	Data 0.001 (0.008)	Loss 2.1724 (2.1700)	Acc@1 62.500 (61.946)	Acc@5 85.938 (88.231)
Epoch: [150][40/391]	Time 0.012 (0.012)	Data 0.001 (0.006)	Loss 2.0954 (2.1351)	Acc@1 63.281 (62.557)	Acc@5 88.281 (88.586)
Epoch: [150][50/391]	Time 0.018 (0.012)	Data 0.001 (0.005)	Loss 1.8619 (2.1076)	Acc@1 71.875 (63.235)	Acc@5 89.844 (88.863)
Epoch: [150][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 1.9412 (2.0789)	Acc@1 64.062 (63.883)	Acc@5 91.406 (89.434)
Epoch: [150][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.9677 (2.0576)	Acc@1 67.188 (64.448)	Acc@5 89.062 (89.657)
Epoch: [150][80/391]	Time 0.012 (0.011)	Data 0.001 (0.004)	Loss 1.8068 (2.0354)	Acc@1 65.625 (64.738)	Acc@5 91.406 (89.931)
Epoch: [150][90/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 1.8371 (2.0163)	Acc@1 67.969 (65.333)	Acc@5 94.531 (90.179)
Epoch: [150][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.9506 (2.0058)	Acc@1 67.969 (65.532)	Acc@5 89.844 (90.231)
Epoch: [150][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.8648 (1.9955)	Acc@1 71.875 (65.773)	Acc@5 89.844 (90.358)
Epoch: [150][120/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 1.8953 (1.9904)	Acc@1 66.406 (65.903)	Acc@5 91.406 (90.399)
Epoch: [150][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.9577 (1.9784)	Acc@1 66.406 (66.144)	Acc@5 89.062 (90.530)
Epoch: [150][140/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 1.9068 (1.9709)	Acc@1 70.312 (66.251)	Acc@5 87.500 (90.642)
Epoch: [150][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.8789 (1.9638)	Acc@1 65.625 (66.489)	Acc@5 92.969 (90.734)
Epoch: [150][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.6141 (1.9565)	Acc@1 76.562 (66.620)	Acc@5 97.656 (90.824)
Epoch: [150][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.8557 (1.9519)	Acc@1 73.438 (66.731)	Acc@5 89.844 (90.835)
Epoch: [150][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.7303 (1.9444)	Acc@1 73.438 (66.890)	Acc@5 94.531 (91.005)
Epoch: [150][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.9765 (1.9367)	Acc@1 64.062 (67.155)	Acc@5 91.406 (91.091)
Epoch: [150][200/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.7182 (1.9316)	Acc@1 71.875 (67.281)	Acc@5 92.188 (91.091)
Epoch: [150][210/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.7204 (1.9231)	Acc@1 69.531 (67.502)	Acc@5 96.875 (91.199)
Epoch: [150][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.8360 (1.9155)	Acc@1 71.094 (67.658)	Acc@5 93.750 (91.297)
Epoch: [150][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.6797 (1.9065)	Acc@1 77.344 (67.962)	Acc@5 92.969 (91.352)
Epoch: [150][240/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 2.0068 (1.9011)	Acc@1 68.750 (68.176)	Acc@5 89.844 (91.397)
Epoch: [150][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.7667 (1.8953)	Acc@1 70.312 (68.317)	Acc@5 91.406 (91.490)
Epoch: [150][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.8199 (1.8914)	Acc@1 69.531 (68.370)	Acc@5 93.750 (91.559)
Epoch: [150][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.7919 (1.8885)	Acc@1 64.844 (68.401)	Acc@5 94.531 (91.617)
Epoch: [150][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.8238 (1.8833)	Acc@1 67.969 (68.553)	Acc@5 94.531 (91.668)
Epoch: [150][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.8802 (1.8813)	Acc@1 67.969 (68.584)	Acc@5 89.844 (91.672)
Epoch: [150][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.7662 (1.8776)	Acc@1 64.844 (68.662)	Acc@5 94.531 (91.713)
Epoch: [150][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.8294 (1.8752)	Acc@1 67.969 (68.685)	Acc@5 91.406 (91.720)
Epoch: [150][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.7996 (1.8705)	Acc@1 67.969 (68.767)	Acc@5 92.188 (91.774)
Epoch: [150][330/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.5338 (1.8671)	Acc@1 78.125 (68.833)	Acc@5 92.969 (91.810)
Epoch: [150][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.7342 (1.8648)	Acc@1 73.438 (68.865)	Acc@5 92.188 (91.835)
Epoch: [150][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.6951 (1.8623)	Acc@1 74.219 (68.975)	Acc@5 93.750 (91.854)
Epoch: [150][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.6992 (1.8601)	Acc@1 71.094 (69.040)	Acc@5 92.188 (91.826)
Epoch: [150][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.6663 (1.8561)	Acc@1 72.656 (69.131)	Acc@5 93.750 (91.907)
Epoch: [150][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4936 (1.8519)	Acc@1 79.688 (69.244)	Acc@5 95.312 (91.937)
Epoch: [150][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 1.8557 (1.8489)	Acc@1 72.500 (69.318)	Acc@5 90.000 (91.972)
num momentum params: 26
[0.010000000000000002, 1.8488972106933594, 1.2448589915037156, 69.318, 65.37, tensor(0.4253, device='cuda:0', grad_fn=<DivBackward0>), 4.228481769561768, 0.3414440155029297]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [108, 32, 3, 3]
Before - module.bn2.weight: [108]
Before - module.bn2.bias: [108]
Before - module.conv3.weight: [202, 108, 3, 3]
Before - module.bn3.weight: [202]
Before - module.bn3.bias: [202]
Before - module.conv4.weight: [247, 202, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [357, 247, 3, 3]
Before - module.bn5.weight: [357]
Before - module.bn5.bias: [357]
Before - module.conv6.weight: [307, 357, 3, 3]
Before - module.bn6.weight: [307]
Before - module.bn6.bias: [307]
Before - module.conv7.weight: [191, 307, 3, 3]
Before - module.bn7.weight: [191]
Before - module.bn7.bias: [191]
Before - module.conv8.weight: [187, 191, 3, 3]
Before - module.bn8.weight: [187]
Before - module.bn8.bias: [187]
Before - module.fc.weight: [100, 187]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [108, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 108 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [202, 108, 3, 3] >> [202, 108, 3, 3]
[module.bn3.weight]: 202 >> 202
running_mean [202]
running_var [202]
num_batches_tracked []
[module.conv4.weight]: [247, 202, 3, 3] >> [247, 202, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [357, 247, 3, 3] >> [356, 247, 3, 3]
[module.bn5.weight]: 357 >> 356
running_mean [356]
running_var [356]
num_batches_tracked []
[module.conv6.weight]: [307, 357, 3, 3] >> [304, 356, 3, 3]
[module.bn6.weight]: 307 >> 304
running_mean [304]
running_var [304]
num_batches_tracked []
[module.conv7.weight]: [191, 307, 3, 3] >> [188, 304, 3, 3]
[module.bn7.weight]: 191 >> 188
running_mean [188]
running_var [188]
num_batches_tracked []
[module.conv8.weight]: [187, 191, 3, 3] >> [186, 188, 3, 3]
[module.bn8.weight]: 187 >> 186
running_mean [186]
running_var [186]
num_batches_tracked []
[module.fc.weight]: [100, 187] >> [100, 186]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [202, 108, 3, 3]
After - module.bn3.weight: [202]
After - module.bn3.bias: [202]
After - module.conv4.weight: [247, 202, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [356, 247, 3, 3]
After - module.bn5.weight: [356]
After - module.bn5.bias: [356]
After - module.conv6.weight: [304, 356, 3, 3]
After - module.bn6.weight: [304]
After - module.bn6.bias: [304]
After - module.conv7.weight: [188, 304, 3, 3]
After - module.bn7.weight: [188]
After - module.bn7.bias: [188]
After - module.conv8.weight: [186, 188, 3, 3]
After - module.bn8.weight: [186]
After - module.bn8.bias: [186]
After - module.fc.weight: [100, 186]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [202, 108, 3, 3]
conv4 --> [247, 202, 3, 3]
conv5 --> [356, 247, 3, 3]
conv6 --> [304, 356, 3, 3]
conv7 --> [188, 304, 3, 3]
conv8 --> [186, 188, 3, 3]
fc --> [186, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5730103296, 12566016, 202
4, 13104958464, 28738944, 247
5, 6888241152, 12662208, 356
6, 8477835264, 15584256, 304
7, 1580138496, 2057472, 188
8, 966795264, 1258848, 186
fc, 7142400, 18600, 0
===================
FLOP REPORT: 15796065600000.0 40483200000.0 81733704 101208 1623 6.279470443725586
[INFO] Storing checkpoint...

Epoch: [151 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [151][0/391]	Time 0.182 (0.182)	Data 0.194 (0.194)	Loss 1.4612 (1.4612)	Acc@1 79.688 (79.688)	Acc@5 96.875 (96.875)
Epoch: [151][10/391]	Time 0.011 (0.028)	Data 0.001 (0.019)	Loss 1.7899 (1.6778)	Acc@1 71.875 (73.935)	Acc@5 93.750 (94.389)
Epoch: [151][20/391]	Time 0.010 (0.020)	Data 0.001 (0.011)	Loss 1.5442 (1.6951)	Acc@1 78.906 (72.693)	Acc@5 94.531 (94.271)
Epoch: [151][30/391]	Time 0.011 (0.017)	Data 0.001 (0.008)	Loss 1.6418 (1.6817)	Acc@1 75.000 (73.059)	Acc@5 93.750 (94.178)
Epoch: [151][40/391]	Time 0.012 (0.015)	Data 0.001 (0.006)	Loss 1.5187 (1.6708)	Acc@1 78.906 (73.533)	Acc@5 92.969 (94.284)
Epoch: [151][50/391]	Time 0.010 (0.015)	Data 0.001 (0.005)	Loss 1.4637 (1.6602)	Acc@1 80.469 (73.897)	Acc@5 96.875 (94.240)
Epoch: [151][60/391]	Time 0.010 (0.014)	Data 0.001 (0.005)	Loss 1.8092 (1.6626)	Acc@1 73.438 (73.899)	Acc@5 91.406 (94.198)
Epoch: [151][70/391]	Time 0.013 (0.014)	Data 0.001 (0.004)	Loss 1.6378 (1.6720)	Acc@1 73.438 (73.592)	Acc@5 94.531 (94.080)
Epoch: [151][80/391]	Time 0.012 (0.013)	Data 0.002 (0.004)	Loss 1.7288 (1.6673)	Acc@1 71.875 (73.650)	Acc@5 95.312 (94.184)
Epoch: [151][90/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 1.8338 (1.6635)	Acc@1 60.156 (73.729)	Acc@5 93.750 (94.248)
Epoch: [151][100/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 1.7096 (1.6638)	Acc@1 75.000 (73.677)	Acc@5 92.188 (94.230)
Epoch: [151][110/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 1.5008 (1.6583)	Acc@1 77.344 (73.761)	Acc@5 93.750 (94.313)
Epoch: [151][120/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 1.5719 (1.6595)	Acc@1 78.125 (73.728)	Acc@5 96.094 (94.299)
Epoch: [151][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.8192 (1.6607)	Acc@1 67.969 (73.706)	Acc@5 91.406 (94.275)
Epoch: [151][140/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.6855 (1.6586)	Acc@1 73.438 (73.737)	Acc@5 92.969 (94.271)
Epoch: [151][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.7182 (1.6592)	Acc@1 71.094 (73.743)	Acc@5 93.750 (94.179)
Epoch: [151][160/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.5536 (1.6569)	Acc@1 76.562 (73.811)	Acc@5 95.312 (94.167)
Epoch: [151][170/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.6721 (1.6546)	Acc@1 73.438 (73.904)	Acc@5 93.750 (94.189)
Epoch: [151][180/391]	Time 0.017 (0.012)	Data 0.001 (0.002)	Loss 1.5792 (1.6536)	Acc@1 78.906 (73.955)	Acc@5 92.188 (94.138)
Epoch: [151][190/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.6094 (1.6556)	Acc@1 71.875 (73.875)	Acc@5 94.531 (94.110)
Epoch: [151][200/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.6151 (1.6562)	Acc@1 78.125 (73.877)	Acc@5 95.312 (94.111)
Epoch: [151][210/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.4777 (1.6564)	Acc@1 80.469 (73.893)	Acc@5 96.094 (94.120)
Epoch: [151][220/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.5663 (1.6531)	Acc@1 71.094 (73.943)	Acc@5 94.531 (94.164)
Epoch: [151][230/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.5661 (1.6541)	Acc@1 72.656 (73.925)	Acc@5 95.312 (94.146)
Epoch: [151][240/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.5692 (1.6516)	Acc@1 74.219 (73.940)	Acc@5 94.531 (94.158)
Epoch: [151][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.5182 (1.6507)	Acc@1 79.688 (74.026)	Acc@5 95.312 (94.127)
Epoch: [151][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.6149 (1.6485)	Acc@1 71.875 (74.072)	Acc@5 92.969 (94.136)
Epoch: [151][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.6590 (1.6479)	Acc@1 76.562 (74.126)	Acc@5 93.750 (94.105)
Epoch: [151][280/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.5076 (1.6466)	Acc@1 75.000 (74.135)	Acc@5 96.094 (94.120)
Epoch: [151][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.8965 (1.6484)	Acc@1 67.188 (74.068)	Acc@5 92.969 (94.107)
Epoch: [151][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.8281 (1.6476)	Acc@1 72.656 (74.092)	Acc@5 87.500 (94.106)
Epoch: [151][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.5706 (1.6456)	Acc@1 75.781 (74.126)	Acc@5 96.094 (94.137)
Epoch: [151][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.6328 (1.6427)	Acc@1 75.781 (74.180)	Acc@5 93.750 (94.173)
Epoch: [151][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4521 (1.6407)	Acc@1 80.469 (74.240)	Acc@5 96.094 (94.203)
Epoch: [151][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4245 (1.6408)	Acc@1 82.812 (74.249)	Acc@5 94.531 (94.192)
Epoch: [151][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.6540 (1.6413)	Acc@1 72.656 (74.239)	Acc@5 95.312 (94.193)
Epoch: [151][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5206 (1.6402)	Acc@1 84.375 (74.275)	Acc@5 93.750 (94.183)
Epoch: [151][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.6923 (1.6400)	Acc@1 74.219 (74.227)	Acc@5 92.188 (94.175)
Epoch: [151][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4869 (1.6402)	Acc@1 76.562 (74.206)	Acc@5 97.656 (94.183)
Epoch: [151][390/391]	Time 0.093 (0.011)	Data 0.000 (0.002)	Loss 1.9018 (1.6407)	Acc@1 70.000 (74.200)	Acc@5 90.000 (94.166)
num momentum params: 26
[0.010000000000000002, 1.6406900359344483, 1.206513113975525, 74.2, 66.12, tensor(0.4659, device='cuda:0', grad_fn=<DivBackward0>), 4.452751159667969, 0.38457608222961426]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [152 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [152][0/391]	Time 0.066 (0.066)	Data 0.197 (0.197)	Loss 1.3424 (1.3424)	Acc@1 85.156 (85.156)	Acc@5 96.094 (96.094)
Epoch: [152][10/391]	Time 0.010 (0.017)	Data 0.001 (0.019)	Loss 1.5265 (1.5423)	Acc@1 79.688 (76.634)	Acc@5 96.875 (95.241)
Epoch: [152][20/391]	Time 0.010 (0.014)	Data 0.001 (0.011)	Loss 1.5814 (1.5456)	Acc@1 78.125 (76.228)	Acc@5 95.312 (95.275)
Epoch: [152][30/391]	Time 0.010 (0.013)	Data 0.001 (0.008)	Loss 1.5977 (1.5365)	Acc@1 74.219 (76.865)	Acc@5 94.531 (95.514)
Epoch: [152][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 1.3632 (1.5407)	Acc@1 82.031 (76.639)	Acc@5 96.875 (95.598)
Epoch: [152][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 1.7823 (1.5511)	Acc@1 73.438 (76.532)	Acc@5 90.625 (95.358)
Epoch: [152][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 1.5575 (1.5528)	Acc@1 76.562 (76.498)	Acc@5 96.094 (95.223)
Epoch: [152][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.5749 (1.5470)	Acc@1 77.344 (76.474)	Acc@5 94.531 (95.268)
Epoch: [152][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.4731 (1.5522)	Acc@1 80.469 (76.427)	Acc@5 92.188 (95.033)
Epoch: [152][90/391]	Time 0.011 (0.011)	Data 0.001 (0.004)	Loss 1.5948 (1.5545)	Acc@1 74.219 (76.356)	Acc@5 91.406 (94.909)
Epoch: [152][100/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.6011 (1.5478)	Acc@1 71.094 (76.547)	Acc@5 95.312 (95.026)
Epoch: [152][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.6907 (1.5481)	Acc@1 72.656 (76.499)	Acc@5 91.406 (94.968)
Epoch: [152][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.6688 (1.5485)	Acc@1 71.875 (76.485)	Acc@5 90.625 (94.990)
Epoch: [152][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.6477 (1.5529)	Acc@1 72.656 (76.407)	Acc@5 92.188 (94.913)
Epoch: [152][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.5209 (1.5509)	Acc@1 77.344 (76.385)	Acc@5 95.312 (94.941)
Epoch: [152][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.4769 (1.5468)	Acc@1 80.469 (76.464)	Acc@5 96.094 (94.992)
Epoch: [152][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.4414 (1.5467)	Acc@1 82.031 (76.485)	Acc@5 92.188 (94.944)
Epoch: [152][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.5823 (1.5452)	Acc@1 75.000 (76.521)	Acc@5 96.875 (94.947)
Epoch: [152][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.6582 (1.5438)	Acc@1 71.094 (76.515)	Acc@5 93.750 (94.989)
Epoch: [152][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.5515 (1.5435)	Acc@1 78.125 (76.538)	Acc@5 94.531 (94.973)
Epoch: [152][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4869 (1.5415)	Acc@1 77.344 (76.547)	Acc@5 96.875 (95.048)
Epoch: [152][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4677 (1.5429)	Acc@1 78.125 (76.444)	Acc@5 96.875 (94.994)
Epoch: [152][220/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.3693 (1.5449)	Acc@1 83.594 (76.389)	Acc@5 97.656 (95.012)
Epoch: [152][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5371 (1.5478)	Acc@1 75.000 (76.275)	Acc@5 92.188 (94.957)
Epoch: [152][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2886 (1.5493)	Acc@1 82.031 (76.229)	Acc@5 98.438 (94.911)
Epoch: [152][250/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.3939 (1.5500)	Acc@1 81.250 (76.211)	Acc@5 96.875 (94.917)
Epoch: [152][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3615 (1.5496)	Acc@1 78.906 (76.215)	Acc@5 98.438 (94.926)
Epoch: [152][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4983 (1.5485)	Acc@1 78.125 (76.274)	Acc@5 96.094 (94.938)
Epoch: [152][280/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.3989 (1.5486)	Acc@1 78.906 (76.254)	Acc@5 99.219 (94.984)
Epoch: [152][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4469 (1.5494)	Acc@1 75.000 (76.168)	Acc@5 98.438 (94.982)
Epoch: [152][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5481 (1.5474)	Acc@1 76.562 (76.254)	Acc@5 96.094 (95.004)
Epoch: [152][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5447 (1.5464)	Acc@1 75.000 (76.301)	Acc@5 94.531 (95.004)
Epoch: [152][320/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.4990 (1.5474)	Acc@1 78.125 (76.266)	Acc@5 94.531 (94.996)
Epoch: [152][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4860 (1.5456)	Acc@1 78.906 (76.296)	Acc@5 95.312 (95.020)
Epoch: [152][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.5657 (1.5459)	Acc@1 75.781 (76.285)	Acc@5 95.312 (94.996)
Epoch: [152][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5138 (1.5466)	Acc@1 76.562 (76.204)	Acc@5 94.531 (94.988)
Epoch: [152][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4046 (1.5463)	Acc@1 78.906 (76.188)	Acc@5 96.875 (94.984)
Epoch: [152][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4916 (1.5455)	Acc@1 80.469 (76.194)	Acc@5 96.094 (94.995)
Epoch: [152][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.5401 (1.5435)	Acc@1 70.312 (76.245)	Acc@5 97.656 (95.015)
Epoch: [152][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 1.6865 (1.5429)	Acc@1 71.250 (76.240)	Acc@5 95.000 (95.036)
num momentum params: 26
[0.010000000000000002, 1.542865535812378, 1.2080889403820039, 76.24, 66.37, tensor(0.4825, device='cuda:0', grad_fn=<DivBackward0>), 4.15947413444519, 0.33480048179626465]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [153 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [153][0/391]	Time 0.056 (0.056)	Data 0.200 (0.200)	Loss 1.2013 (1.2013)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [153][10/391]	Time 0.012 (0.016)	Data 0.001 (0.020)	Loss 1.2737 (1.4642)	Acc@1 87.500 (78.551)	Acc@5 98.438 (94.815)
Epoch: [153][20/391]	Time 0.010 (0.014)	Data 0.001 (0.011)	Loss 1.4368 (1.4513)	Acc@1 73.438 (78.423)	Acc@5 99.219 (95.424)
Epoch: [153][30/391]	Time 0.010 (0.013)	Data 0.001 (0.008)	Loss 1.6224 (1.4519)	Acc@1 75.781 (78.427)	Acc@5 92.188 (95.464)
Epoch: [153][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 1.5060 (1.4535)	Acc@1 75.781 (78.258)	Acc@5 96.875 (95.675)
Epoch: [153][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 1.4055 (1.4544)	Acc@1 77.344 (78.248)	Acc@5 96.094 (95.619)
Epoch: [153][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 1.4458 (1.4502)	Acc@1 78.906 (78.381)	Acc@5 96.875 (95.786)
Epoch: [153][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.4748 (1.4529)	Acc@1 79.688 (78.279)	Acc@5 94.531 (95.819)
Epoch: [153][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 1.3066 (1.4510)	Acc@1 84.375 (78.279)	Acc@5 97.656 (95.862)
Epoch: [153][90/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 1.5356 (1.4543)	Acc@1 75.000 (78.254)	Acc@5 94.531 (95.879)
Epoch: [153][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3284 (1.4543)	Acc@1 82.031 (78.171)	Acc@5 96.875 (95.885)
Epoch: [153][110/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 1.4070 (1.4580)	Acc@1 78.906 (78.026)	Acc@5 96.094 (95.826)
Epoch: [153][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.6414 (1.4595)	Acc@1 71.875 (77.802)	Acc@5 92.969 (95.874)
Epoch: [153][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.6423 (1.4590)	Acc@1 73.438 (77.827)	Acc@5 92.969 (95.897)
Epoch: [153][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.5989 (1.4602)	Acc@1 75.000 (77.848)	Acc@5 95.312 (95.867)
Epoch: [153][150/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.4894 (1.4630)	Acc@1 78.906 (77.851)	Acc@5 96.875 (95.845)
Epoch: [153][160/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 1.3589 (1.4646)	Acc@1 82.812 (77.829)	Acc@5 96.875 (95.807)
Epoch: [153][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.6238 (1.4648)	Acc@1 70.312 (77.769)	Acc@5 95.312 (95.820)
Epoch: [153][180/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.4011 (1.4646)	Acc@1 75.781 (77.737)	Acc@5 96.875 (95.813)
Epoch: [153][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.4936 (1.4677)	Acc@1 72.656 (77.671)	Acc@5 96.094 (95.783)
Epoch: [153][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4383 (1.4695)	Acc@1 77.344 (77.620)	Acc@5 96.094 (95.736)
Epoch: [153][210/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2917 (1.4684)	Acc@1 81.250 (77.584)	Acc@5 97.656 (95.779)
Epoch: [153][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4630 (1.4691)	Acc@1 75.000 (77.612)	Acc@5 95.312 (95.758)
Epoch: [153][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4296 (1.4697)	Acc@1 80.469 (77.608)	Acc@5 95.312 (95.749)
Epoch: [153][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.6358 (1.4695)	Acc@1 75.000 (77.623)	Acc@5 90.625 (95.747)
Epoch: [153][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4664 (1.4697)	Acc@1 75.000 (77.624)	Acc@5 94.531 (95.717)
Epoch: [153][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3762 (1.4679)	Acc@1 79.688 (77.664)	Acc@5 96.875 (95.747)
Epoch: [153][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3579 (1.4669)	Acc@1 80.469 (77.693)	Acc@5 99.219 (95.748)
Epoch: [153][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4641 (1.4639)	Acc@1 77.344 (77.752)	Acc@5 96.094 (95.768)
Epoch: [153][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2778 (1.4636)	Acc@1 82.812 (77.773)	Acc@5 97.656 (95.729)
Epoch: [153][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3156 (1.4621)	Acc@1 83.594 (77.816)	Acc@5 97.656 (95.743)
Epoch: [153][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4925 (1.4627)	Acc@1 78.906 (77.801)	Acc@5 95.312 (95.727)
Epoch: [153][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4271 (1.4616)	Acc@1 77.344 (77.816)	Acc@5 95.312 (95.717)
Epoch: [153][330/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 1.4920 (1.4616)	Acc@1 74.219 (77.802)	Acc@5 97.656 (95.711)
Epoch: [153][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3538 (1.4609)	Acc@1 80.469 (77.816)	Acc@5 94.531 (95.695)
Epoch: [153][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3773 (1.4607)	Acc@1 82.812 (77.813)	Acc@5 94.531 (95.671)
Epoch: [153][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5874 (1.4630)	Acc@1 71.875 (77.746)	Acc@5 93.750 (95.641)
Epoch: [153][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.6039 (1.4627)	Acc@1 75.000 (77.742)	Acc@5 91.406 (95.641)
Epoch: [153][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5434 (1.4624)	Acc@1 73.438 (77.770)	Acc@5 95.312 (95.620)
Epoch: [153][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 1.5687 (1.4633)	Acc@1 75.000 (77.760)	Acc@5 92.500 (95.604)
num momentum params: 26
[0.010000000000000002, 1.4632708363342286, 1.1876977968215943, 77.76, 67.19, tensor(0.4959, device='cuda:0', grad_fn=<DivBackward0>), 4.2142040729522705, 0.33059191703796387]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [154 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [154][0/391]	Time 0.058 (0.058)	Data 0.184 (0.184)	Loss 1.2507 (1.2507)	Acc@1 85.156 (85.156)	Acc@5 97.656 (97.656)
Epoch: [154][10/391]	Time 0.010 (0.016)	Data 0.001 (0.018)	Loss 1.3883 (1.3718)	Acc@1 82.031 (80.540)	Acc@5 95.312 (96.165)
Epoch: [154][20/391]	Time 0.011 (0.014)	Data 0.001 (0.010)	Loss 1.4067 (1.3556)	Acc@1 78.125 (80.952)	Acc@5 97.656 (96.540)
Epoch: [154][30/391]	Time 0.010 (0.013)	Data 0.001 (0.007)	Loss 1.3532 (1.3520)	Acc@1 80.469 (80.973)	Acc@5 96.094 (96.648)
Epoch: [154][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 1.3471 (1.3661)	Acc@1 77.344 (80.469)	Acc@5 98.438 (96.608)
Epoch: [154][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 1.4374 (1.3703)	Acc@1 74.219 (79.979)	Acc@5 97.656 (96.676)
Epoch: [154][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.2283 (1.3702)	Acc@1 82.812 (79.828)	Acc@5 95.312 (96.555)
Epoch: [154][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.4493 (1.3702)	Acc@1 79.688 (79.688)	Acc@5 96.875 (96.655)
Epoch: [154][80/391]	Time 0.010 (0.011)	Data 0.001 (0.004)	Loss 1.2683 (1.3708)	Acc@1 84.375 (79.716)	Acc@5 96.875 (96.595)
Epoch: [154][90/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.4295 (1.3690)	Acc@1 77.344 (79.756)	Acc@5 94.531 (96.575)
Epoch: [154][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.4108 (1.3702)	Acc@1 81.250 (79.873)	Acc@5 92.969 (96.481)
Epoch: [154][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.6653 (1.3722)	Acc@1 69.531 (79.800)	Acc@5 96.094 (96.460)
Epoch: [154][120/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.4696 (1.3789)	Acc@1 78.125 (79.455)	Acc@5 92.969 (96.436)
Epoch: [154][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3179 (1.3816)	Acc@1 81.250 (79.509)	Acc@5 97.656 (96.374)
Epoch: [154][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.4090 (1.3842)	Acc@1 78.125 (79.410)	Acc@5 96.875 (96.321)
Epoch: [154][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.2694 (1.3852)	Acc@1 82.812 (79.393)	Acc@5 96.094 (96.316)
Epoch: [154][160/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3790 (1.3848)	Acc@1 78.906 (79.430)	Acc@5 96.875 (96.336)
Epoch: [154][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3099 (1.3824)	Acc@1 80.469 (79.473)	Acc@5 96.875 (96.363)
Epoch: [154][180/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.3520 (1.3830)	Acc@1 85.156 (79.429)	Acc@5 95.312 (96.366)
Epoch: [154][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.3540 (1.3867)	Acc@1 77.344 (79.352)	Acc@5 96.094 (96.298)
Epoch: [154][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.5281 (1.3879)	Acc@1 75.781 (79.279)	Acc@5 92.188 (96.296)
Epoch: [154][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4891 (1.3891)	Acc@1 76.562 (79.299)	Acc@5 93.750 (96.301)
Epoch: [154][220/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.5106 (1.3901)	Acc@1 77.344 (79.274)	Acc@5 96.094 (96.306)
Epoch: [154][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2293 (1.3902)	Acc@1 82.812 (79.244)	Acc@5 97.656 (96.324)
Epoch: [154][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.5700 (1.3925)	Acc@1 74.219 (79.159)	Acc@5 94.531 (96.317)
Epoch: [154][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3430 (1.3928)	Acc@1 81.250 (79.183)	Acc@5 96.094 (96.305)
Epoch: [154][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.4520 (1.3935)	Acc@1 76.562 (79.161)	Acc@5 98.438 (96.312)
Epoch: [154][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4150 (1.3921)	Acc@1 78.906 (79.189)	Acc@5 94.531 (96.313)
Epoch: [154][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4240 (1.3918)	Acc@1 75.781 (79.179)	Acc@5 96.875 (96.325)
Epoch: [154][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4216 (1.3927)	Acc@1 78.125 (79.172)	Acc@5 96.875 (96.314)
Epoch: [154][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2944 (1.3933)	Acc@1 78.906 (79.142)	Acc@5 97.656 (96.288)
Epoch: [154][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4352 (1.3964)	Acc@1 78.906 (79.042)	Acc@5 95.312 (96.265)
Epoch: [154][320/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.4225 (1.3980)	Acc@1 79.688 (78.977)	Acc@5 93.750 (96.242)
Epoch: [154][330/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.6683 (1.4002)	Acc@1 71.875 (78.932)	Acc@5 90.625 (96.207)
Epoch: [154][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1184 (1.3992)	Acc@1 85.938 (78.936)	Acc@5 97.656 (96.213)
Epoch: [154][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3165 (1.3999)	Acc@1 84.375 (78.935)	Acc@5 97.656 (96.214)
Epoch: [154][360/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3554 (1.3994)	Acc@1 76.562 (78.950)	Acc@5 96.094 (96.215)
Epoch: [154][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2648 (1.3989)	Acc@1 84.375 (78.961)	Acc@5 98.438 (96.229)
Epoch: [154][380/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.4908 (1.3998)	Acc@1 80.469 (78.947)	Acc@5 96.094 (96.223)
Epoch: [154][390/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 1.3433 (1.4010)	Acc@1 81.250 (78.886)	Acc@5 98.750 (96.228)
num momentum params: 26
[0.010000000000000002, 1.4010346036148071, 1.2028857815265654, 78.886, 67.22, tensor(0.5054, device='cuda:0', grad_fn=<DivBackward0>), 4.301366567611694, 0.3492398262023926]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [155 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [155][0/391]	Time 0.056 (0.056)	Data 0.187 (0.187)	Loss 1.3629 (1.3629)	Acc@1 77.344 (77.344)	Acc@5 96.094 (96.094)
Epoch: [155][10/391]	Time 0.010 (0.017)	Data 0.001 (0.018)	Loss 1.3335 (1.3265)	Acc@1 78.906 (79.901)	Acc@5 97.656 (97.372)
Epoch: [155][20/391]	Time 0.010 (0.014)	Data 0.001 (0.010)	Loss 1.1894 (1.3149)	Acc@1 85.156 (81.176)	Acc@5 98.438 (97.210)
Epoch: [155][30/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 1.2951 (1.3148)	Acc@1 83.594 (81.048)	Acc@5 94.531 (97.102)
Epoch: [155][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 1.2964 (1.3061)	Acc@1 82.812 (81.421)	Acc@5 97.656 (97.123)
Epoch: [155][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 1.5820 (1.3071)	Acc@1 77.344 (81.587)	Acc@5 91.406 (97.013)
Epoch: [155][60/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.3101 (1.3081)	Acc@1 82.812 (81.481)	Acc@5 97.656 (97.016)
Epoch: [155][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.1690 (1.3176)	Acc@1 84.375 (81.151)	Acc@5 96.875 (96.886)
Epoch: [155][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.2817 (1.3188)	Acc@1 82.812 (81.115)	Acc@5 97.656 (96.865)
Epoch: [155][90/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.1888 (1.3206)	Acc@1 85.156 (81.216)	Acc@5 100.000 (96.832)
Epoch: [155][100/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3758 (1.3170)	Acc@1 78.125 (81.351)	Acc@5 98.438 (96.875)
Epoch: [155][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3208 (1.3209)	Acc@1 78.125 (81.194)	Acc@5 96.875 (96.868)
Epoch: [155][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3716 (1.3214)	Acc@1 80.469 (81.127)	Acc@5 97.656 (96.875)
Epoch: [155][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3492 (1.3232)	Acc@1 77.344 (81.035)	Acc@5 99.219 (96.863)
Epoch: [155][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.4342 (1.3251)	Acc@1 75.781 (80.973)	Acc@5 95.312 (96.792)
Epoch: [155][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.2861 (1.3265)	Acc@1 83.594 (80.919)	Acc@5 96.094 (96.777)
Epoch: [155][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.3169 (1.3242)	Acc@1 81.250 (80.993)	Acc@5 97.656 (96.793)
Epoch: [155][170/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4130 (1.3249)	Acc@1 78.906 (80.939)	Acc@5 97.656 (96.784)
Epoch: [155][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4226 (1.3267)	Acc@1 78.906 (80.883)	Acc@5 96.094 (96.767)
Epoch: [155][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3053 (1.3289)	Acc@1 81.250 (80.788)	Acc@5 96.875 (96.752)
Epoch: [155][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2919 (1.3287)	Acc@1 82.031 (80.764)	Acc@5 96.094 (96.739)
Epoch: [155][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3003 (1.3279)	Acc@1 81.250 (80.820)	Acc@5 95.312 (96.731)
Epoch: [155][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3083 (1.3281)	Acc@1 82.031 (80.819)	Acc@5 97.656 (96.712)
Epoch: [155][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3157 (1.3270)	Acc@1 83.594 (80.841)	Acc@5 95.312 (96.709)
Epoch: [155][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2699 (1.3250)	Acc@1 84.375 (80.887)	Acc@5 96.875 (96.736)
Epoch: [155][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3745 (1.3241)	Acc@1 78.906 (80.914)	Acc@5 97.656 (96.738)
Epoch: [155][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3248 (1.3238)	Acc@1 82.812 (80.912)	Acc@5 98.438 (96.752)
Epoch: [155][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.4170 (1.3247)	Acc@1 76.562 (80.875)	Acc@5 98.438 (96.748)
Epoch: [155][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4303 (1.3254)	Acc@1 81.250 (80.872)	Acc@5 95.312 (96.730)
Epoch: [155][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3987 (1.3260)	Acc@1 80.469 (80.826)	Acc@5 94.531 (96.733)
Epoch: [155][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3895 (1.3273)	Acc@1 81.250 (80.804)	Acc@5 96.094 (96.717)
Epoch: [155][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1929 (1.3273)	Acc@1 83.594 (80.788)	Acc@5 96.875 (96.714)
Epoch: [155][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3420 (1.3287)	Acc@1 79.688 (80.732)	Acc@5 96.875 (96.712)
Epoch: [155][330/391]	Time 0.011 (0.011)	Data 0.005 (0.002)	Loss 1.3828 (1.3293)	Acc@1 80.469 (80.724)	Acc@5 96.094 (96.715)
Epoch: [155][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3443 (1.3308)	Acc@1 79.688 (80.638)	Acc@5 96.875 (96.692)
Epoch: [155][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3338 (1.3312)	Acc@1 80.469 (80.620)	Acc@5 97.656 (96.697)
Epoch: [155][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1792 (1.3305)	Acc@1 85.938 (80.605)	Acc@5 99.219 (96.708)
Epoch: [155][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3709 (1.3325)	Acc@1 81.250 (80.551)	Acc@5 97.656 (96.683)
Epoch: [155][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2012 (1.3331)	Acc@1 85.156 (80.502)	Acc@5 96.875 (96.678)
Epoch: [155][390/391]	Time 0.015 (0.011)	Data 0.000 (0.002)	Loss 1.3817 (1.3343)	Acc@1 81.250 (80.474)	Acc@5 93.750 (96.654)
num momentum params: 26
[0.010000000000000002, 1.334266530532837, 1.2153103744983673, 80.474, 67.11, tensor(0.5178, device='cuda:0', grad_fn=<DivBackward0>), 4.224013566970825, 0.3317584991455078]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [156 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [156][0/391]	Time 0.059 (0.059)	Data 0.197 (0.197)	Loss 1.3552 (1.3552)	Acc@1 78.906 (78.906)	Acc@5 96.094 (96.094)
Epoch: [156][10/391]	Time 0.012 (0.017)	Data 0.001 (0.019)	Loss 1.2280 (1.2593)	Acc@1 84.375 (82.102)	Acc@5 98.438 (97.514)
Epoch: [156][20/391]	Time 0.011 (0.014)	Data 0.001 (0.011)	Loss 1.3769 (1.2587)	Acc@1 78.906 (82.031)	Acc@5 93.750 (97.321)
Epoch: [156][30/391]	Time 0.011 (0.013)	Data 0.001 (0.008)	Loss 1.3073 (1.2607)	Acc@1 81.250 (82.258)	Acc@5 96.094 (97.102)
Epoch: [156][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 1.3244 (1.2590)	Acc@1 82.031 (82.393)	Acc@5 96.094 (96.951)
Epoch: [156][50/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 1.0568 (1.2568)	Acc@1 86.719 (82.690)	Acc@5 100.000 (97.105)
Epoch: [156][60/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 1.1499 (1.2600)	Acc@1 86.719 (82.569)	Acc@5 97.656 (97.003)
Epoch: [156][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.3462 (1.2607)	Acc@1 78.906 (82.460)	Acc@5 96.094 (97.062)
Epoch: [156][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.1938 (1.2595)	Acc@1 85.156 (82.446)	Acc@5 98.438 (97.155)
Epoch: [156][90/391]	Time 0.017 (0.012)	Data 0.001 (0.004)	Loss 1.2801 (1.2595)	Acc@1 82.031 (82.478)	Acc@5 96.875 (97.167)
Epoch: [156][100/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.3496 (1.2645)	Acc@1 82.031 (82.341)	Acc@5 96.094 (97.076)
Epoch: [156][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.2392 (1.2615)	Acc@1 88.281 (82.482)	Acc@5 95.312 (97.065)
Epoch: [156][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.2555 (1.2626)	Acc@1 78.906 (82.380)	Acc@5 96.875 (97.036)
Epoch: [156][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.3154 (1.2643)	Acc@1 80.469 (82.347)	Acc@5 96.875 (97.036)
Epoch: [156][140/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 1.1095 (1.2628)	Acc@1 85.156 (82.358)	Acc@5 99.219 (97.074)
Epoch: [156][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.2418 (1.2641)	Acc@1 86.719 (82.373)	Acc@5 96.875 (97.087)
Epoch: [156][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.2839 (1.2654)	Acc@1 80.469 (82.230)	Acc@5 98.438 (97.103)
Epoch: [156][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.1379 (1.2664)	Acc@1 85.156 (82.136)	Acc@5 99.219 (97.126)
Epoch: [156][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1885 (1.2651)	Acc@1 82.812 (82.131)	Acc@5 98.438 (97.147)
Epoch: [156][190/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.2884 (1.2651)	Acc@1 81.250 (82.060)	Acc@5 97.656 (97.194)
Epoch: [156][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2859 (1.2629)	Acc@1 85.938 (82.101)	Acc@5 99.219 (97.248)
Epoch: [156][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3160 (1.2675)	Acc@1 80.469 (81.924)	Acc@5 96.875 (97.175)
Epoch: [156][220/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.2034 (1.2663)	Acc@1 82.031 (81.950)	Acc@5 99.219 (97.190)
Epoch: [156][230/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 1.1575 (1.2663)	Acc@1 88.281 (81.930)	Acc@5 97.656 (97.183)
Epoch: [156][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1793 (1.2677)	Acc@1 84.375 (81.895)	Acc@5 97.656 (97.151)
Epoch: [156][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3818 (1.2675)	Acc@1 80.469 (81.913)	Acc@5 96.094 (97.174)
Epoch: [156][260/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.3510 (1.2671)	Acc@1 78.906 (81.906)	Acc@5 95.312 (97.192)
Epoch: [156][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.4304 (1.2677)	Acc@1 77.344 (81.907)	Acc@5 94.531 (97.181)
Epoch: [156][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2677 (1.2681)	Acc@1 82.031 (81.867)	Acc@5 99.219 (97.172)
Epoch: [156][290/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.2956 (1.2688)	Acc@1 79.688 (81.854)	Acc@5 95.312 (97.160)
Epoch: [156][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2455 (1.2692)	Acc@1 81.250 (81.837)	Acc@5 96.094 (97.148)
Epoch: [156][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2042 (1.2702)	Acc@1 85.938 (81.800)	Acc@5 96.875 (97.136)
Epoch: [156][320/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2728 (1.2708)	Acc@1 79.688 (81.773)	Acc@5 97.656 (97.135)
Epoch: [156][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1838 (1.2697)	Acc@1 79.688 (81.757)	Acc@5 97.656 (97.151)
Epoch: [156][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2365 (1.2698)	Acc@1 85.938 (81.733)	Acc@5 94.531 (97.157)
Epoch: [156][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3764 (1.2702)	Acc@1 80.469 (81.735)	Acc@5 95.312 (97.162)
Epoch: [156][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3745 (1.2710)	Acc@1 76.562 (81.704)	Acc@5 97.656 (97.156)
Epoch: [156][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1017 (1.2723)	Acc@1 85.938 (81.656)	Acc@5 98.438 (97.140)
Epoch: [156][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3062 (1.2740)	Acc@1 78.125 (81.592)	Acc@5 96.094 (97.125)
Epoch: [156][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 1.3303 (1.2753)	Acc@1 78.750 (81.552)	Acc@5 92.500 (97.114)
num momentum params: 26
[0.010000000000000002, 1.2753235472106934, 1.2238586908578872, 81.552, 67.37, tensor(0.5292, device='cuda:0', grad_fn=<DivBackward0>), 4.239873886108398, 0.3507561683654785]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [157 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [157][0/391]	Time 0.056 (0.056)	Data 0.196 (0.196)	Loss 1.2352 (1.2352)	Acc@1 82.812 (82.812)	Acc@5 96.875 (96.875)
Epoch: [157][10/391]	Time 0.010 (0.016)	Data 0.001 (0.019)	Loss 1.1135 (1.2002)	Acc@1 86.719 (84.091)	Acc@5 97.656 (97.585)
Epoch: [157][20/391]	Time 0.010 (0.014)	Data 0.001 (0.011)	Loss 1.1009 (1.2004)	Acc@1 87.500 (83.557)	Acc@5 98.438 (97.693)
Epoch: [157][30/391]	Time 0.011 (0.013)	Data 0.001 (0.008)	Loss 1.2499 (1.2002)	Acc@1 82.812 (83.493)	Acc@5 99.219 (97.581)
Epoch: [157][40/391]	Time 0.011 (0.012)	Data 0.001 (0.006)	Loss 1.1680 (1.1917)	Acc@1 88.281 (83.727)	Acc@5 97.656 (97.752)
Epoch: [157][50/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 1.2227 (1.2041)	Acc@1 82.031 (83.180)	Acc@5 98.438 (97.672)
Epoch: [157][60/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 1.1621 (1.2065)	Acc@1 85.938 (83.184)	Acc@5 97.656 (97.605)
Epoch: [157][70/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 1.2181 (1.2091)	Acc@1 83.594 (83.132)	Acc@5 96.875 (97.590)
Epoch: [157][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.0823 (1.2140)	Acc@1 87.500 (82.976)	Acc@5 100.000 (97.598)
Epoch: [157][90/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.3343 (1.2165)	Acc@1 76.562 (82.984)	Acc@5 97.656 (97.570)
Epoch: [157][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.1103 (1.2148)	Acc@1 85.156 (83.060)	Acc@5 96.875 (97.579)
Epoch: [157][110/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1791 (1.2114)	Acc@1 79.688 (83.157)	Acc@5 98.438 (97.607)
Epoch: [157][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.1293 (1.2091)	Acc@1 82.031 (83.213)	Acc@5 99.219 (97.618)
Epoch: [157][130/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1582 (1.2103)	Acc@1 84.375 (83.111)	Acc@5 98.438 (97.632)
Epoch: [157][140/391]	Time 0.012 (0.011)	Data 0.002 (0.003)	Loss 1.2813 (1.2082)	Acc@1 82.812 (83.195)	Acc@5 95.312 (97.656)
Epoch: [157][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.2379 (1.2093)	Acc@1 81.250 (83.133)	Acc@5 97.656 (97.636)
Epoch: [157][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.3076 (1.2118)	Acc@1 78.125 (83.079)	Acc@5 96.094 (97.598)
Epoch: [157][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1550 (1.2130)	Acc@1 86.719 (83.055)	Acc@5 97.656 (97.569)
Epoch: [157][180/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3269 (1.2154)	Acc@1 77.344 (82.981)	Acc@5 96.094 (97.518)
Epoch: [157][190/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2319 (1.2166)	Acc@1 81.250 (82.890)	Acc@5 97.656 (97.525)
Epoch: [157][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1712 (1.2179)	Acc@1 79.688 (82.785)	Acc@5 97.656 (97.532)
Epoch: [157][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1223 (1.2184)	Acc@1 84.375 (82.746)	Acc@5 98.438 (97.549)
Epoch: [157][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1541 (1.2212)	Acc@1 85.938 (82.685)	Acc@5 97.656 (97.515)
Epoch: [157][230/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2291 (1.2224)	Acc@1 82.812 (82.647)	Acc@5 96.094 (97.477)
Epoch: [157][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2409 (1.2242)	Acc@1 81.250 (82.540)	Acc@5 97.656 (97.442)
Epoch: [157][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2653 (1.2251)	Acc@1 80.469 (82.539)	Acc@5 95.312 (97.413)
Epoch: [157][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2614 (1.2269)	Acc@1 82.031 (82.450)	Acc@5 96.094 (97.387)
Epoch: [157][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1543 (1.2275)	Acc@1 83.594 (82.481)	Acc@5 96.875 (97.371)
Epoch: [157][280/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1466 (1.2258)	Acc@1 84.375 (82.518)	Acc@5 98.438 (97.389)
Epoch: [157][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.3638 (1.2270)	Acc@1 76.562 (82.458)	Acc@5 95.312 (97.382)
Epoch: [157][300/391]	Time 0.012 (0.011)	Data 0.000 (0.002)	Loss 1.1716 (1.2268)	Acc@1 85.156 (82.457)	Acc@5 98.438 (97.379)
Epoch: [157][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1389 (1.2266)	Acc@1 83.594 (82.468)	Acc@5 100.000 (97.403)
Epoch: [157][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2122 (1.2273)	Acc@1 78.906 (82.433)	Acc@5 97.656 (97.384)
Epoch: [157][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2940 (1.2281)	Acc@1 78.906 (82.397)	Acc@5 96.094 (97.366)
Epoch: [157][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2323 (1.2277)	Acc@1 80.469 (82.412)	Acc@5 97.656 (97.365)
Epoch: [157][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3196 (1.2290)	Acc@1 77.344 (82.341)	Acc@5 96.094 (97.358)
Epoch: [157][360/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.3364 (1.2301)	Acc@1 82.812 (82.326)	Acc@5 97.656 (97.338)
Epoch: [157][370/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.2279 (1.2299)	Acc@1 82.812 (82.351)	Acc@5 97.656 (97.342)
Epoch: [157][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0926 (1.2309)	Acc@1 90.625 (82.320)	Acc@5 99.219 (97.347)
Epoch: [157][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 1.0032 (1.2322)	Acc@1 92.500 (82.292)	Acc@5 100.000 (97.336)
num momentum params: 26
[0.010000000000000002, 1.2322167140960694, 1.2535585415363313, 82.292, 67.27, tensor(0.5357, device='cuda:0', grad_fn=<DivBackward0>), 4.311000347137451, 0.3457329273223877]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [158 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [158][0/391]	Time 0.060 (0.060)	Data 0.182 (0.182)	Loss 1.2574 (1.2574)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)
Epoch: [158][10/391]	Time 0.011 (0.018)	Data 0.001 (0.018)	Loss 1.1478 (1.1493)	Acc@1 88.281 (84.659)	Acc@5 98.438 (98.224)
Epoch: [158][20/391]	Time 0.013 (0.015)	Data 0.001 (0.010)	Loss 1.1068 (1.1615)	Acc@1 85.938 (84.115)	Acc@5 99.219 (97.954)
Epoch: [158][30/391]	Time 0.010 (0.014)	Data 0.001 (0.007)	Loss 1.1760 (1.1588)	Acc@1 81.250 (83.997)	Acc@5 97.656 (98.059)
Epoch: [158][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 1.1806 (1.1698)	Acc@1 82.812 (83.613)	Acc@5 96.875 (97.828)
Epoch: [158][50/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 1.1835 (1.1735)	Acc@1 82.812 (83.410)	Acc@5 100.000 (97.871)
Epoch: [158][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 1.1892 (1.1784)	Acc@1 85.156 (83.491)	Acc@5 97.656 (97.797)
Epoch: [158][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.1168 (1.1772)	Acc@1 85.156 (83.550)	Acc@5 98.438 (97.832)
Epoch: [158][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.1533 (1.1750)	Acc@1 89.062 (83.681)	Acc@5 96.875 (97.820)
Epoch: [158][90/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.0185 (1.1719)	Acc@1 89.062 (83.774)	Acc@5 99.219 (97.879)
Epoch: [158][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.3894 (1.1760)	Acc@1 76.562 (83.733)	Acc@5 96.094 (97.780)
Epoch: [158][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.1409 (1.1731)	Acc@1 85.938 (83.854)	Acc@5 98.438 (97.825)
Epoch: [158][120/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.1312 (1.1740)	Acc@1 82.812 (83.800)	Acc@5 97.656 (97.811)
Epoch: [158][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.1219 (1.1748)	Acc@1 82.031 (83.755)	Acc@5 100.000 (97.823)
Epoch: [158][140/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 1.1448 (1.1726)	Acc@1 81.250 (83.876)	Acc@5 99.219 (97.828)
Epoch: [158][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1450 (1.1717)	Acc@1 86.719 (83.930)	Acc@5 96.875 (97.811)
Epoch: [158][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1309 (1.1717)	Acc@1 83.594 (83.982)	Acc@5 99.219 (97.797)
Epoch: [158][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.1678 (1.1714)	Acc@1 81.250 (83.982)	Acc@5 98.438 (97.784)
Epoch: [158][180/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 1.2093 (1.1725)	Acc@1 83.594 (83.922)	Acc@5 97.656 (97.777)
Epoch: [158][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1526 (1.1724)	Acc@1 88.281 (83.913)	Acc@5 98.438 (97.804)
Epoch: [158][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2791 (1.1710)	Acc@1 82.812 (83.940)	Acc@5 98.438 (97.835)
Epoch: [158][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3215 (1.1718)	Acc@1 81.250 (83.901)	Acc@5 98.438 (97.849)
Epoch: [158][220/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 1.2184 (1.1715)	Acc@1 83.594 (83.926)	Acc@5 97.656 (97.854)
Epoch: [158][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1207 (1.1722)	Acc@1 85.938 (83.878)	Acc@5 97.656 (97.839)
Epoch: [158][240/391]	Time 0.012 (0.011)	Data 0.003 (0.002)	Loss 1.2811 (1.1731)	Acc@1 81.250 (83.827)	Acc@5 94.531 (97.809)
Epoch: [158][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3822 (1.1768)	Acc@1 78.125 (83.700)	Acc@5 96.875 (97.787)
Epoch: [158][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3641 (1.1779)	Acc@1 77.344 (83.630)	Acc@5 97.656 (97.776)
Epoch: [158][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3604 (1.1791)	Acc@1 75.000 (83.551)	Acc@5 96.875 (97.780)
Epoch: [158][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1611 (1.1792)	Acc@1 86.719 (83.555)	Acc@5 96.875 (97.767)
Epoch: [158][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2021 (1.1789)	Acc@1 82.812 (83.535)	Acc@5 98.438 (97.777)
Epoch: [158][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.3480 (1.1791)	Acc@1 79.688 (83.506)	Acc@5 96.875 (97.778)
Epoch: [158][310/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 1.2714 (1.1786)	Acc@1 82.031 (83.523)	Acc@5 96.875 (97.777)
Epoch: [158][320/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3220 (1.1807)	Acc@1 83.594 (83.477)	Acc@5 96.094 (97.741)
Epoch: [158][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1528 (1.1819)	Acc@1 85.938 (83.426)	Acc@5 96.875 (97.725)
Epoch: [158][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.2061 (1.1851)	Acc@1 82.031 (83.328)	Acc@5 96.875 (97.691)
Epoch: [158][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1210 (1.1847)	Acc@1 83.594 (83.338)	Acc@5 98.438 (97.696)
Epoch: [158][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1599 (1.1860)	Acc@1 82.812 (83.299)	Acc@5 97.656 (97.693)
Epoch: [158][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.3358 (1.1882)	Acc@1 81.250 (83.206)	Acc@5 95.312 (97.673)
Epoch: [158][380/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 1.3185 (1.1870)	Acc@1 77.344 (83.204)	Acc@5 95.312 (97.679)
Epoch: [158][390/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.3389 (1.1873)	Acc@1 78.750 (83.204)	Acc@5 93.750 (97.668)
num momentum params: 26
[0.010000000000000002, 1.187326250076294, 1.2704582279920578, 83.204, 67.26, tensor(0.5444, device='cuda:0', grad_fn=<DivBackward0>), 4.260902404785156, 0.34118223190307617]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [159 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [159][0/391]	Time 0.058 (0.058)	Data 0.196 (0.196)	Loss 1.2596 (1.2596)	Acc@1 78.125 (78.125)	Acc@5 96.094 (96.094)
Epoch: [159][10/391]	Time 0.013 (0.018)	Data 0.001 (0.019)	Loss 1.1478 (1.1242)	Acc@1 85.938 (85.156)	Acc@5 96.875 (97.727)
Epoch: [159][20/391]	Time 0.012 (0.015)	Data 0.001 (0.011)	Loss 1.2174 (1.1280)	Acc@1 85.156 (85.231)	Acc@5 95.312 (97.359)
Epoch: [159][30/391]	Time 0.012 (0.014)	Data 0.001 (0.008)	Loss 1.0115 (1.1222)	Acc@1 89.062 (85.156)	Acc@5 99.219 (97.606)
Epoch: [159][40/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 1.1726 (1.1208)	Acc@1 81.250 (85.118)	Acc@5 96.875 (97.694)
Epoch: [159][50/391]	Time 0.010 (0.013)	Data 0.001 (0.005)	Loss 1.2123 (1.1218)	Acc@1 85.156 (85.126)	Acc@5 95.312 (97.718)
Epoch: [159][60/391]	Time 0.010 (0.012)	Data 0.002 (0.005)	Loss 1.1037 (1.1199)	Acc@1 89.844 (85.246)	Acc@5 96.875 (97.836)
Epoch: [159][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.1851 (1.1211)	Acc@1 85.156 (85.145)	Acc@5 96.875 (97.931)
Epoch: [159][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.1023 (1.1283)	Acc@1 88.281 (84.915)	Acc@5 98.438 (97.946)
Epoch: [159][90/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.1341 (1.1318)	Acc@1 85.156 (84.976)	Acc@5 98.438 (97.914)
Epoch: [159][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.2412 (1.1368)	Acc@1 75.000 (84.731)	Acc@5 99.219 (97.881)
Epoch: [159][110/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.1504 (1.1419)	Acc@1 83.594 (84.565)	Acc@5 96.875 (97.867)
Epoch: [159][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.2753 (1.1433)	Acc@1 83.594 (84.575)	Acc@5 96.094 (97.876)
Epoch: [159][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.0796 (1.1394)	Acc@1 84.375 (84.679)	Acc@5 97.656 (97.948)
Epoch: [159][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1330 (1.1364)	Acc@1 83.594 (84.730)	Acc@5 97.656 (97.978)
Epoch: [159][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1577 (1.1343)	Acc@1 81.250 (84.696)	Acc@5 98.438 (98.034)
Epoch: [159][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.0575 (1.1334)	Acc@1 87.500 (84.700)	Acc@5 98.438 (98.049)
Epoch: [159][170/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.1689 (1.1346)	Acc@1 81.250 (84.649)	Acc@5 97.656 (98.045)
Epoch: [159][180/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1026 (1.1346)	Acc@1 85.156 (84.617)	Acc@5 98.438 (98.032)
Epoch: [159][190/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1614 (1.1358)	Acc@1 79.688 (84.563)	Acc@5 98.438 (98.037)
Epoch: [159][200/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0973 (1.1347)	Acc@1 84.375 (84.585)	Acc@5 98.438 (98.045)
Epoch: [159][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0728 (1.1375)	Acc@1 82.812 (84.453)	Acc@5 98.438 (98.034)
Epoch: [159][220/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1912 (1.1400)	Acc@1 87.500 (84.371)	Acc@5 97.656 (98.006)
Epoch: [159][230/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.1506 (1.1398)	Acc@1 82.031 (84.331)	Acc@5 98.438 (98.018)
Epoch: [159][240/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.1465 (1.1414)	Acc@1 83.594 (84.268)	Acc@5 98.438 (98.013)
Epoch: [159][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2897 (1.1420)	Acc@1 81.250 (84.219)	Acc@5 96.094 (98.017)
Epoch: [159][260/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 1.0876 (1.1442)	Acc@1 87.500 (84.121)	Acc@5 98.438 (98.009)
Epoch: [159][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0812 (1.1449)	Acc@1 86.719 (84.067)	Acc@5 96.875 (98.017)
Epoch: [159][280/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 1.1324 (1.1452)	Acc@1 82.031 (84.047)	Acc@5 97.656 (98.009)
Epoch: [159][290/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.0504 (1.1439)	Acc@1 85.156 (84.074)	Acc@5 98.438 (98.021)
Epoch: [159][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1573 (1.1437)	Acc@1 81.250 (84.071)	Acc@5 99.219 (98.030)
Epoch: [159][310/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 0.9789 (1.1458)	Acc@1 88.281 (84.058)	Acc@5 99.219 (98.008)
Epoch: [159][320/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 1.2229 (1.1467)	Acc@1 84.375 (84.015)	Acc@5 99.219 (98.004)
Epoch: [159][330/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 1.1135 (1.1466)	Acc@1 87.500 (84.033)	Acc@5 98.438 (98.015)
Epoch: [159][340/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0470 (1.1463)	Acc@1 83.594 (84.041)	Acc@5 100.000 (98.014)
Epoch: [159][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.0579 (1.1462)	Acc@1 85.156 (84.014)	Acc@5 99.219 (98.017)
Epoch: [159][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0966 (1.1473)	Acc@1 85.156 (83.977)	Acc@5 99.219 (98.003)
Epoch: [159][370/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2583 (1.1491)	Acc@1 81.250 (83.943)	Acc@5 99.219 (97.966)
Epoch: [159][380/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 1.4554 (1.1503)	Acc@1 77.344 (83.901)	Acc@5 94.531 (97.947)
Epoch: [159][390/391]	Time 0.015 (0.011)	Data 0.000 (0.002)	Loss 1.1577 (1.1509)	Acc@1 81.250 (83.884)	Acc@5 97.500 (97.942)
num momentum params: 26
[0.010000000000000002, 1.1509432224273681, 1.2958849918842317, 83.884, 66.48, tensor(0.5505, device='cuda:0', grad_fn=<DivBackward0>), 4.288671016693115, 0.34273338317871094]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [304, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [304]
Non Pruning Epoch - module.bn6.bias: [304]
Non Pruning Epoch - module.conv7.weight: [188, 304, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [160 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [304, 356, 3, 3]
module.conv7.weight [188, 304, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [160][0/391]	Time 0.055 (0.055)	Data 0.197 (0.197)	Loss 1.0369 (1.0369)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [160][10/391]	Time 0.010 (0.017)	Data 0.001 (0.019)	Loss 0.9412 (1.0978)	Acc@1 91.406 (85.227)	Acc@5 99.219 (98.295)
Epoch: [160][20/391]	Time 0.010 (0.014)	Data 0.001 (0.011)	Loss 1.1221 (1.0941)	Acc@1 83.594 (85.379)	Acc@5 99.219 (98.438)
Epoch: [160][30/391]	Time 0.011 (0.013)	Data 0.001 (0.008)	Loss 0.9528 (1.0887)	Acc@1 89.062 (85.635)	Acc@5 100.000 (98.564)
Epoch: [160][40/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 1.0106 (1.0827)	Acc@1 89.062 (85.785)	Acc@5 98.438 (98.514)
Epoch: [160][50/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 1.1026 (1.0950)	Acc@1 85.938 (85.447)	Acc@5 97.656 (98.453)
Epoch: [160][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 1.1273 (1.0996)	Acc@1 84.375 (85.284)	Acc@5 97.656 (98.361)
Epoch: [160][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.1821 (1.0971)	Acc@1 79.688 (85.376)	Acc@5 99.219 (98.349)
Epoch: [160][80/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 1.0922 (1.0969)	Acc@1 81.250 (85.320)	Acc@5 98.438 (98.312)
Epoch: [160][90/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.0028 (1.0994)	Acc@1 89.844 (85.259)	Acc@5 99.219 (98.309)
Epoch: [160][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.2055 (1.0978)	Acc@1 82.031 (85.288)	Acc@5 96.094 (98.321)
Epoch: [160][110/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.0757 (1.0941)	Acc@1 84.375 (85.374)	Acc@5 97.656 (98.332)
Epoch: [160][120/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.1002 (1.0966)	Acc@1 89.844 (85.285)	Acc@5 97.656 (98.289)
Epoch: [160][130/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.1763 (1.0976)	Acc@1 82.812 (85.258)	Acc@5 97.656 (98.241)
Epoch: [160][140/391]	Time 0.010 (0.011)	Data 0.001 (0.003)	Loss 1.0580 (1.0966)	Acc@1 85.156 (85.273)	Acc@5 98.438 (98.232)
Epoch: [160][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.0276 (1.0981)	Acc@1 88.281 (85.244)	Acc@5 99.219 (98.194)
Epoch: [160][160/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.1801 (1.0995)	Acc@1 83.594 (85.224)	Acc@5 96.094 (98.185)
Epoch: [160][170/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.2718 (1.1015)	Acc@1 80.469 (85.170)	Acc@5 97.656 (98.191)
Epoch: [160][180/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 1.0564 (1.1029)	Acc@1 84.375 (85.104)	Acc@5 99.219 (98.179)
Epoch: [160][190/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.0121 (1.1017)	Acc@1 84.375 (85.152)	Acc@5 99.219 (98.192)
Epoch: [160][200/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.0083 (1.1018)	Acc@1 85.938 (85.113)	Acc@5 100.000 (98.208)
Epoch: [160][210/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1634 (1.1023)	Acc@1 82.031 (85.119)	Acc@5 96.875 (98.197)
Epoch: [160][220/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.0631 (1.1031)	Acc@1 86.719 (85.061)	Acc@5 99.219 (98.204)
Epoch: [160][230/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1025 (1.1038)	Acc@1 85.156 (85.018)	Acc@5 99.219 (98.187)
Epoch: [160][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1965 (1.1038)	Acc@1 78.906 (85.033)	Acc@5 99.219 (98.211)
Epoch: [160][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1683 (1.1042)	Acc@1 82.812 (85.001)	Acc@5 98.438 (98.201)
Epoch: [160][260/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1893 (1.1067)	Acc@1 83.594 (84.923)	Acc@5 96.875 (98.183)
Epoch: [160][270/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1196 (1.1081)	Acc@1 84.375 (84.877)	Acc@5 97.656 (98.167)
Epoch: [160][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2266 (1.1087)	Acc@1 81.250 (84.878)	Acc@5 97.656 (98.146)
Epoch: [160][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.2033 (1.1095)	Acc@1 80.469 (84.850)	Acc@5 98.438 (98.150)
Epoch: [160][300/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1539 (1.1121)	Acc@1 82.812 (84.754)	Acc@5 96.875 (98.108)
Epoch: [160][310/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 0.9807 (1.1121)	Acc@1 90.625 (84.747)	Acc@5 100.000 (98.116)
Epoch: [160][320/391]	Time 0.010 (0.011)	Data 0.003 (0.002)	Loss 1.0967 (1.1143)	Acc@1 84.375 (84.691)	Acc@5 98.438 (98.094)
Epoch: [160][330/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1948 (1.1172)	Acc@1 80.469 (84.597)	Acc@5 97.656 (98.074)
Epoch: [160][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1895 (1.1181)	Acc@1 80.469 (84.593)	Acc@5 98.438 (98.069)
Epoch: [160][350/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0808 (1.1186)	Acc@1 89.062 (84.620)	Acc@5 96.875 (98.052)
Epoch: [160][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0553 (1.1196)	Acc@1 85.156 (84.559)	Acc@5 97.656 (98.044)
Epoch: [160][370/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1953 (1.1208)	Acc@1 83.594 (84.527)	Acc@5 97.656 (98.025)
Epoch: [160][380/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1692 (1.1218)	Acc@1 82.031 (84.496)	Acc@5 97.656 (98.021)
Epoch: [160][390/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 1.0661 (1.1226)	Acc@1 85.000 (84.494)	Acc@5 98.750 (98.010)
num momentum params: 26
[0.010000000000000002, 1.122568756866455, 1.2719432872533798, 84.494, 66.96, tensor(0.5545, device='cuda:0', grad_fn=<DivBackward0>), 4.268827199935913, 0.35328054428100586]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [108, 32, 3, 3]
Before - module.bn2.weight: [108]
Before - module.bn2.bias: [108]
Before - module.conv3.weight: [202, 108, 3, 3]
Before - module.bn3.weight: [202]
Before - module.bn3.bias: [202]
Before - module.conv4.weight: [247, 202, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [356, 247, 3, 3]
Before - module.bn5.weight: [356]
Before - module.bn5.bias: [356]
Before - module.conv6.weight: [304, 356, 3, 3]
Before - module.bn6.weight: [304]
Before - module.bn6.bias: [304]
Before - module.conv7.weight: [188, 304, 3, 3]
Before - module.bn7.weight: [188]
Before - module.bn7.bias: [188]
Before - module.conv8.weight: [186, 188, 3, 3]
Before - module.bn8.weight: [186]
Before - module.bn8.bias: [186]
Before - module.fc.weight: [100, 186]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [108, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 108 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [202, 108, 3, 3] >> [202, 108, 3, 3]
[module.bn3.weight]: 202 >> 202
running_mean [202]
running_var [202]
num_batches_tracked []
[module.conv4.weight]: [247, 202, 3, 3] >> [247, 202, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [356, 247, 3, 3] >> [356, 247, 3, 3]
[module.bn5.weight]: 356 >> 356
running_mean [356]
running_var [356]
num_batches_tracked []
[module.conv6.weight]: [304, 356, 3, 3] >> [303, 356, 3, 3]
[module.bn6.weight]: 304 >> 303
running_mean [303]
running_var [303]
num_batches_tracked []
[module.conv7.weight]: [188, 304, 3, 3] >> [188, 303, 3, 3]
[module.bn7.weight]: 188 >> 188
running_mean [188]
running_var [188]
num_batches_tracked []
[module.conv8.weight]: [186, 188, 3, 3] >> [186, 188, 3, 3]
[module.bn8.weight]: 186 >> 186
running_mean [186]
running_var [186]
num_batches_tracked []
[module.fc.weight]: [100, 186] >> [100, 186]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [202, 108, 3, 3]
After - module.bn3.weight: [202]
After - module.bn3.bias: [202]
After - module.conv4.weight: [247, 202, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [356, 247, 3, 3]
After - module.bn5.weight: [356]
After - module.bn5.bias: [356]
After - module.conv6.weight: [303, 356, 3, 3]
After - module.bn6.weight: [303]
After - module.bn6.bias: [303]
After - module.conv7.weight: [188, 303, 3, 3]
After - module.bn7.weight: [188]
After - module.bn7.bias: [188]
After - module.conv8.weight: [186, 188, 3, 3]
After - module.bn8.weight: [186]
After - module.bn8.bias: [186]
After - module.fc.weight: [100, 186]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [202, 108, 3, 3]
conv4 --> [247, 202, 3, 3]
conv5 --> [356, 247, 3, 3]
conv6 --> [303, 356, 3, 3]
conv7 --> [188, 303, 3, 3]
conv8 --> [186, 188, 3, 3]
fc --> [186, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5730103296, 12566016, 202
4, 13104958464, 28738944, 247
5, 6888241152, 12662208, 356
6, 8449947648, 15532992, 303
7, 1574940672, 2050704, 188
8, 966795264, 1258848, 186
fc, 7142400, 18600, 0
===================
FLOP REPORT: 15783141600000.0 40476800000.0 81675672 101192 1622 6.270130157470703
[INFO] Storing checkpoint...

Epoch: [161 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [161][0/391]	Time 0.130 (0.130)	Data 0.204 (0.204)	Loss 1.0779 (1.0779)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [161][10/391]	Time 0.012 (0.024)	Data 0.002 (0.020)	Loss 1.0804 (1.0399)	Acc@1 84.375 (86.435)	Acc@5 99.219 (99.148)
Epoch: [161][20/391]	Time 0.012 (0.018)	Data 0.002 (0.011)	Loss 1.0305 (1.0402)	Acc@1 85.156 (86.533)	Acc@5 99.219 (99.033)
Epoch: [161][30/391]	Time 0.012 (0.016)	Data 0.002 (0.008)	Loss 1.0298 (1.0486)	Acc@1 88.281 (86.391)	Acc@5 98.438 (98.891)
Epoch: [161][40/391]	Time 0.011 (0.015)	Data 0.002 (0.007)	Loss 1.0579 (1.0545)	Acc@1 85.938 (86.242)	Acc@5 97.656 (98.704)
Epoch: [161][50/391]	Time 0.012 (0.014)	Data 0.001 (0.006)	Loss 1.0333 (1.0563)	Acc@1 85.156 (86.320)	Acc@5 98.438 (98.683)
Epoch: [161][60/391]	Time 0.011 (0.014)	Data 0.001 (0.005)	Loss 1.0709 (1.0592)	Acc@1 88.281 (86.219)	Acc@5 96.875 (98.655)
Epoch: [161][70/391]	Time 0.010 (0.014)	Data 0.002 (0.004)	Loss 1.0328 (1.0557)	Acc@1 86.719 (86.356)	Acc@5 97.656 (98.603)
Epoch: [161][80/391]	Time 0.010 (0.013)	Data 0.002 (0.004)	Loss 1.0343 (1.0601)	Acc@1 88.281 (86.150)	Acc@5 98.438 (98.573)
Epoch: [161][90/391]	Time 0.010 (0.013)	Data 0.001 (0.004)	Loss 1.2418 (1.0625)	Acc@1 80.469 (85.963)	Acc@5 98.438 (98.592)
Epoch: [161][100/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 1.0360 (1.0629)	Acc@1 88.281 (86.069)	Acc@5 98.438 (98.554)
Epoch: [161][110/391]	Time 0.010 (0.013)	Data 0.001 (0.003)	Loss 1.0222 (1.0658)	Acc@1 85.938 (85.867)	Acc@5 100.000 (98.585)
Epoch: [161][120/391]	Time 0.010 (0.013)	Data 0.002 (0.003)	Loss 0.9844 (1.0624)	Acc@1 89.844 (85.989)	Acc@5 98.438 (98.592)
Epoch: [161][130/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 1.0363 (1.0634)	Acc@1 84.375 (86.063)	Acc@5 99.219 (98.575)
Epoch: [161][140/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1864 (1.0666)	Acc@1 85.156 (86.032)	Acc@5 96.875 (98.537)
Epoch: [161][150/391]	Time 0.015 (0.012)	Data 0.001 (0.003)	Loss 1.1297 (1.0675)	Acc@1 84.375 (86.072)	Acc@5 99.219 (98.515)
Epoch: [161][160/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.0413 (1.0670)	Acc@1 87.500 (86.035)	Acc@5 99.219 (98.535)
Epoch: [161][170/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0548 (1.0683)	Acc@1 83.594 (85.906)	Acc@5 100.000 (98.533)
Epoch: [161][180/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9565 (1.0700)	Acc@1 90.625 (85.843)	Acc@5 99.219 (98.545)
Epoch: [161][190/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.0525 (1.0698)	Acc@1 86.719 (85.852)	Acc@5 98.438 (98.540)
Epoch: [161][200/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9881 (1.0699)	Acc@1 87.500 (85.798)	Acc@5 99.219 (98.550)
Epoch: [161][210/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0493 (1.0730)	Acc@1 85.156 (85.678)	Acc@5 98.438 (98.545)
Epoch: [161][220/391]	Time 0.011 (0.012)	Data 0.010 (0.003)	Loss 1.0797 (1.0739)	Acc@1 82.812 (85.619)	Acc@5 99.219 (98.522)
Epoch: [161][230/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.1422 (1.0764)	Acc@1 80.469 (85.532)	Acc@5 98.438 (98.495)
Epoch: [161][240/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9147 (1.0771)	Acc@1 90.625 (85.503)	Acc@5 99.219 (98.457)
Epoch: [161][250/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.2536 (1.0802)	Acc@1 82.031 (85.377)	Acc@5 96.875 (98.438)
Epoch: [161][260/391]	Time 0.016 (0.012)	Data 0.001 (0.002)	Loss 1.1268 (1.0805)	Acc@1 85.156 (85.333)	Acc@5 99.219 (98.438)
Epoch: [161][270/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.1314 (1.0823)	Acc@1 84.375 (85.292)	Acc@5 98.438 (98.417)
Epoch: [161][280/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.0985 (1.0846)	Acc@1 85.156 (85.273)	Acc@5 98.438 (98.365)
Epoch: [161][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.2146 (1.0858)	Acc@1 84.375 (85.256)	Acc@5 96.875 (98.357)
Epoch: [161][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0229 (1.0862)	Acc@1 82.812 (85.195)	Acc@5 100.000 (98.365)
Epoch: [161][310/391]	Time 0.021 (0.012)	Data 0.001 (0.002)	Loss 1.2798 (1.0880)	Acc@1 81.250 (85.126)	Acc@5 98.438 (98.365)
Epoch: [161][320/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0301 (1.0891)	Acc@1 88.281 (85.083)	Acc@5 99.219 (98.350)
Epoch: [161][330/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1595 (1.0907)	Acc@1 83.594 (85.003)	Acc@5 97.656 (98.315)
Epoch: [161][340/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.1194 (1.0920)	Acc@1 82.031 (84.975)	Acc@5 97.656 (98.293)
Epoch: [161][350/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.0600 (1.0927)	Acc@1 85.156 (84.947)	Acc@5 98.438 (98.291)
Epoch: [161][360/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1275 (1.0917)	Acc@1 85.156 (84.977)	Acc@5 98.438 (98.303)
Epoch: [161][370/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0805 (1.0922)	Acc@1 79.688 (84.931)	Acc@5 98.438 (98.294)
Epoch: [161][380/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.2581 (1.0939)	Acc@1 78.906 (84.840)	Acc@5 95.312 (98.282)
Epoch: [161][390/391]	Time 0.057 (0.012)	Data 0.000 (0.002)	Loss 1.1983 (1.0958)	Acc@1 82.500 (84.766)	Acc@5 100.000 (98.268)
num momentum params: 26
[0.010000000000000002, 1.0958409336471557, 1.335990499854088, 84.766, 66.11, tensor(0.5591, device='cuda:0', grad_fn=<DivBackward0>), 4.620204925537109, 0.3928284645080566]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [162 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [162][0/391]	Time 0.062 (0.062)	Data 0.211 (0.211)	Loss 1.0805 (1.0805)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [162][10/391]	Time 0.012 (0.018)	Data 0.002 (0.021)	Loss 1.0796 (1.0503)	Acc@1 83.594 (86.151)	Acc@5 97.656 (98.438)
Epoch: [162][20/391]	Time 0.011 (0.015)	Data 0.001 (0.012)	Loss 1.0490 (1.0483)	Acc@1 87.500 (85.938)	Acc@5 100.000 (98.847)
Epoch: [162][30/391]	Time 0.017 (0.014)	Data 0.002 (0.008)	Loss 1.0142 (1.0401)	Acc@1 89.062 (86.492)	Acc@5 98.438 (98.790)
Epoch: [162][40/391]	Time 0.013 (0.014)	Data 0.002 (0.007)	Loss 1.0950 (1.0426)	Acc@1 86.719 (86.242)	Acc@5 98.438 (98.704)
Epoch: [162][50/391]	Time 0.018 (0.013)	Data 0.002 (0.006)	Loss 1.1443 (1.0514)	Acc@1 82.031 (85.876)	Acc@5 100.000 (98.713)
Epoch: [162][60/391]	Time 0.012 (0.013)	Data 0.002 (0.005)	Loss 1.1264 (1.0537)	Acc@1 85.938 (85.733)	Acc@5 96.875 (98.604)
Epoch: [162][70/391]	Time 0.013 (0.013)	Data 0.002 (0.005)	Loss 1.2143 (1.0478)	Acc@1 85.938 (85.993)	Acc@5 96.875 (98.570)
Epoch: [162][80/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 1.0470 (1.0474)	Acc@1 84.375 (85.995)	Acc@5 97.656 (98.573)
Epoch: [162][90/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 0.9723 (1.0474)	Acc@1 89.062 (85.929)	Acc@5 98.438 (98.583)
Epoch: [162][100/391]	Time 0.010 (0.013)	Data 0.002 (0.004)	Loss 0.9892 (1.0472)	Acc@1 84.375 (85.868)	Acc@5 99.219 (98.561)
Epoch: [162][110/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 1.2196 (1.0457)	Acc@1 81.250 (85.860)	Acc@5 96.875 (98.599)
Epoch: [162][120/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 0.8269 (1.0432)	Acc@1 92.969 (85.950)	Acc@5 100.000 (98.567)
Epoch: [162][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1603 (1.0428)	Acc@1 81.250 (85.997)	Acc@5 96.094 (98.557)
Epoch: [162][140/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.8856 (1.0438)	Acc@1 92.969 (85.938)	Acc@5 99.219 (98.559)
Epoch: [162][150/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9958 (1.0464)	Acc@1 92.188 (85.896)	Acc@5 98.438 (98.505)
Epoch: [162][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0520 (1.0480)	Acc@1 84.375 (85.860)	Acc@5 99.219 (98.476)
Epoch: [162][170/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.1950 (1.0496)	Acc@1 78.125 (85.846)	Acc@5 96.875 (98.479)
Epoch: [162][180/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9502 (1.0530)	Acc@1 91.406 (85.687)	Acc@5 99.219 (98.468)
Epoch: [162][190/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.1279 (1.0535)	Acc@1 81.250 (85.672)	Acc@5 99.219 (98.458)
Epoch: [162][200/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 0.9156 (1.0520)	Acc@1 91.406 (85.735)	Acc@5 99.219 (98.457)
Epoch: [162][210/391]	Time 0.014 (0.012)	Data 0.002 (0.003)	Loss 0.9499 (1.0562)	Acc@1 89.844 (85.638)	Acc@5 99.219 (98.393)
Epoch: [162][220/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0419 (1.0586)	Acc@1 86.719 (85.588)	Acc@5 99.219 (98.402)
Epoch: [162][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0084 (1.0589)	Acc@1 89.062 (85.593)	Acc@5 98.438 (98.390)
Epoch: [162][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0373 (1.0599)	Acc@1 84.375 (85.516)	Acc@5 100.000 (98.405)
Epoch: [162][250/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0781 (1.0604)	Acc@1 85.938 (85.471)	Acc@5 99.219 (98.397)
Epoch: [162][260/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 0.9670 (1.0617)	Acc@1 86.719 (85.396)	Acc@5 100.000 (98.408)
Epoch: [162][270/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.1418 (1.0627)	Acc@1 83.594 (85.375)	Acc@5 98.438 (98.409)
Epoch: [162][280/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1498 (1.0645)	Acc@1 82.812 (85.345)	Acc@5 99.219 (98.385)
Epoch: [162][290/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0175 (1.0636)	Acc@1 85.938 (85.350)	Acc@5 99.219 (98.389)
Epoch: [162][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0859 (1.0682)	Acc@1 84.375 (85.213)	Acc@5 97.656 (98.352)
Epoch: [162][310/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.1329 (1.0694)	Acc@1 83.594 (85.189)	Acc@5 96.094 (98.347)
Epoch: [162][320/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1070 (1.0700)	Acc@1 85.938 (85.190)	Acc@5 96.875 (98.343)
Epoch: [162][330/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.2649 (1.0726)	Acc@1 78.125 (85.111)	Acc@5 95.312 (98.319)
Epoch: [162][340/391]	Time 0.011 (0.012)	Data 0.003 (0.002)	Loss 1.2699 (1.0741)	Acc@1 79.688 (85.037)	Acc@5 96.094 (98.318)
Epoch: [162][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0085 (1.0754)	Acc@1 88.281 (84.980)	Acc@5 97.656 (98.313)
Epoch: [162][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1874 (1.0764)	Acc@1 80.469 (84.927)	Acc@5 100.000 (98.312)
Epoch: [162][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1191 (1.0778)	Acc@1 82.812 (84.874)	Acc@5 97.656 (98.292)
Epoch: [162][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.1842 (1.0791)	Acc@1 75.781 (84.820)	Acc@5 97.656 (98.286)
Epoch: [162][390/391]	Time 0.017 (0.012)	Data 0.000 (0.002)	Loss 1.0869 (1.0799)	Acc@1 82.500 (84.804)	Acc@5 97.500 (98.276)
num momentum params: 26
[0.010000000000000002, 1.0799398644256593, 1.3869355964660643, 84.804, 65.21, tensor(0.5596, device='cuda:0', grad_fn=<DivBackward0>), 4.572735786437988, 0.37059783935546875]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [163 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [163][0/391]	Time 0.051 (0.051)	Data 0.207 (0.207)	Loss 0.9906 (0.9906)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [163][10/391]	Time 0.013 (0.017)	Data 0.002 (0.020)	Loss 0.9984 (1.0007)	Acc@1 87.500 (86.790)	Acc@5 97.656 (99.219)
Epoch: [163][20/391]	Time 0.010 (0.015)	Data 0.002 (0.011)	Loss 0.9783 (1.0308)	Acc@1 89.062 (86.012)	Acc@5 100.000 (98.735)
Epoch: [163][30/391]	Time 0.014 (0.014)	Data 0.001 (0.008)	Loss 1.0725 (1.0278)	Acc@1 86.719 (86.568)	Acc@5 99.219 (98.765)
Epoch: [163][40/391]	Time 0.011 (0.013)	Data 0.002 (0.007)	Loss 0.9768 (1.0271)	Acc@1 85.938 (86.604)	Acc@5 100.000 (98.723)
Epoch: [163][50/391]	Time 0.012 (0.013)	Data 0.002 (0.006)	Loss 1.0452 (1.0287)	Acc@1 85.938 (86.489)	Acc@5 100.000 (98.759)
Epoch: [163][60/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 1.0167 (1.0262)	Acc@1 85.938 (86.437)	Acc@5 98.438 (98.796)
Epoch: [163][70/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 1.0977 (1.0238)	Acc@1 82.812 (86.543)	Acc@5 97.656 (98.746)
Epoch: [163][80/391]	Time 0.010 (0.013)	Data 0.002 (0.004)	Loss 1.0544 (1.0231)	Acc@1 84.375 (86.516)	Acc@5 97.656 (98.775)
Epoch: [163][90/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.9896 (1.0240)	Acc@1 87.500 (86.564)	Acc@5 99.219 (98.695)
Epoch: [163][100/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 1.0336 (1.0200)	Acc@1 86.719 (86.634)	Acc@5 99.219 (98.724)
Epoch: [163][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.1518 (1.0222)	Acc@1 81.250 (86.515)	Acc@5 99.219 (98.733)
Epoch: [163][120/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.8724 (1.0237)	Acc@1 92.188 (86.512)	Acc@5 100.000 (98.696)
Epoch: [163][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0658 (1.0276)	Acc@1 85.938 (86.325)	Acc@5 98.438 (98.682)
Epoch: [163][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.1483 (1.0306)	Acc@1 83.594 (86.292)	Acc@5 97.656 (98.654)
Epoch: [163][150/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.0725 (1.0313)	Acc@1 86.719 (86.367)	Acc@5 98.438 (98.650)
Epoch: [163][160/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0591 (1.0318)	Acc@1 84.375 (86.272)	Acc@5 98.438 (98.670)
Epoch: [163][170/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0980 (1.0311)	Acc@1 83.594 (86.298)	Acc@5 98.438 (98.693)
Epoch: [163][180/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0422 (1.0342)	Acc@1 85.938 (86.222)	Acc@5 99.219 (98.658)
Epoch: [163][190/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9830 (1.0332)	Acc@1 89.844 (86.281)	Acc@5 99.219 (98.667)
Epoch: [163][200/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0632 (1.0336)	Acc@1 86.719 (86.307)	Acc@5 98.438 (98.651)
Epoch: [163][210/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.0968 (1.0347)	Acc@1 85.156 (86.241)	Acc@5 97.656 (98.652)
Epoch: [163][220/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9639 (1.0367)	Acc@1 83.594 (86.139)	Acc@5 100.000 (98.643)
Epoch: [163][230/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9721 (1.0386)	Acc@1 86.719 (86.103)	Acc@5 100.000 (98.600)
Epoch: [163][240/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.0575 (1.0388)	Acc@1 85.156 (86.106)	Acc@5 97.656 (98.593)
Epoch: [163][250/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 1.0838 (1.0383)	Acc@1 85.938 (86.093)	Acc@5 96.875 (98.599)
Epoch: [163][260/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0794 (1.0399)	Acc@1 87.500 (86.057)	Acc@5 99.219 (98.575)
Epoch: [163][270/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9724 (1.0401)	Acc@1 86.719 (86.018)	Acc@5 99.219 (98.585)
Epoch: [163][280/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.0522 (1.0420)	Acc@1 84.375 (85.938)	Acc@5 98.438 (98.568)
Epoch: [163][290/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 0.9675 (1.0429)	Acc@1 91.406 (85.911)	Acc@5 98.438 (98.553)
Epoch: [163][300/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9593 (1.0431)	Acc@1 89.062 (85.922)	Acc@5 99.219 (98.539)
Epoch: [163][310/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 1.1097 (1.0439)	Acc@1 85.938 (85.917)	Acc@5 97.656 (98.528)
Epoch: [163][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0835 (1.0452)	Acc@1 86.719 (85.872)	Acc@5 97.656 (98.518)
Epoch: [163][330/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1058 (1.0458)	Acc@1 79.688 (85.834)	Acc@5 99.219 (98.525)
Epoch: [163][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9895 (1.0461)	Acc@1 91.406 (85.825)	Acc@5 97.656 (98.513)
Epoch: [163][350/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0929 (1.0475)	Acc@1 82.812 (85.757)	Acc@5 97.656 (98.506)
Epoch: [163][360/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.1373 (1.0489)	Acc@1 83.594 (85.695)	Acc@5 97.656 (98.500)
Epoch: [163][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0347 (1.0512)	Acc@1 85.938 (85.626)	Acc@5 96.875 (98.484)
Epoch: [163][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0677 (1.0524)	Acc@1 88.281 (85.616)	Acc@5 97.656 (98.466)
Epoch: [163][390/391]	Time 0.017 (0.012)	Data 0.000 (0.002)	Loss 1.3959 (1.0537)	Acc@1 73.750 (85.570)	Acc@5 95.000 (98.446)
num momentum params: 26
[0.010000000000000002, 1.0536702865600587, 1.3813430434465408, 85.57, 65.86, tensor(0.5655, device='cuda:0', grad_fn=<DivBackward0>), 4.6132097244262695, 0.3713235855102539]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [164 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [164][0/391]	Time 0.065 (0.065)	Data 0.218 (0.218)	Loss 0.9863 (0.9863)	Acc@1 89.844 (89.844)	Acc@5 98.438 (98.438)
Epoch: [164][10/391]	Time 0.012 (0.018)	Data 0.001 (0.021)	Loss 0.9550 (0.9623)	Acc@1 89.844 (88.707)	Acc@5 98.438 (99.077)
Epoch: [164][20/391]	Time 0.012 (0.015)	Data 0.002 (0.012)	Loss 0.9886 (0.9694)	Acc@1 82.031 (88.318)	Acc@5 100.000 (99.144)
Epoch: [164][30/391]	Time 0.012 (0.014)	Data 0.002 (0.008)	Loss 1.0040 (0.9824)	Acc@1 87.500 (87.928)	Acc@5 98.438 (99.093)
Epoch: [164][40/391]	Time 0.010 (0.014)	Data 0.002 (0.007)	Loss 0.9392 (0.9819)	Acc@1 89.844 (88.167)	Acc@5 99.219 (99.085)
Epoch: [164][50/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 0.9847 (0.9832)	Acc@1 86.719 (88.097)	Acc@5 98.438 (99.050)
Epoch: [164][60/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 0.9757 (0.9898)	Acc@1 88.281 (87.705)	Acc@5 98.438 (98.988)
Epoch: [164][70/391]	Time 0.010 (0.013)	Data 0.002 (0.005)	Loss 1.0878 (0.9898)	Acc@1 82.812 (87.687)	Acc@5 99.219 (98.977)
Epoch: [164][80/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 1.1264 (0.9896)	Acc@1 82.031 (87.693)	Acc@5 97.656 (98.949)
Epoch: [164][90/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.9989 (0.9927)	Acc@1 86.719 (87.603)	Acc@5 100.000 (98.918)
Epoch: [164][100/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.1184 (0.9921)	Acc@1 85.156 (87.562)	Acc@5 96.094 (98.902)
Epoch: [164][110/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.1031 (0.9963)	Acc@1 83.594 (87.401)	Acc@5 97.656 (98.853)
Epoch: [164][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0268 (0.9977)	Acc@1 89.844 (87.351)	Acc@5 98.438 (98.831)
Epoch: [164][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9834 (0.9983)	Acc@1 89.062 (87.321)	Acc@5 100.000 (98.849)
Epoch: [164][140/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0978 (1.0016)	Acc@1 85.156 (87.223)	Acc@5 98.438 (98.809)
Epoch: [164][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0791 (1.0039)	Acc@1 85.938 (87.096)	Acc@5 96.875 (98.810)
Epoch: [164][160/391]	Time 0.014 (0.012)	Data 0.002 (0.003)	Loss 0.9848 (1.0055)	Acc@1 87.500 (86.995)	Acc@5 99.219 (98.797)
Epoch: [164][170/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.0804 (1.0072)	Acc@1 84.375 (86.888)	Acc@5 98.438 (98.776)
Epoch: [164][180/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9416 (1.0077)	Acc@1 85.938 (86.835)	Acc@5 99.219 (98.804)
Epoch: [164][190/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.1457 (1.0086)	Acc@1 84.375 (86.801)	Acc@5 98.438 (98.785)
Epoch: [164][200/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9741 (1.0108)	Acc@1 85.938 (86.727)	Acc@5 100.000 (98.776)
Epoch: [164][210/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0903 (1.0122)	Acc@1 82.812 (86.648)	Acc@5 99.219 (98.763)
Epoch: [164][220/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.2297 (1.0138)	Acc@1 80.469 (86.602)	Acc@5 97.656 (98.742)
Epoch: [164][230/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0029 (1.0149)	Acc@1 86.719 (86.529)	Acc@5 99.219 (98.739)
Epoch: [164][240/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.0232 (1.0158)	Acc@1 84.375 (86.489)	Acc@5 98.438 (98.716)
Epoch: [164][250/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0093 (1.0169)	Acc@1 85.938 (86.457)	Acc@5 99.219 (98.715)
Epoch: [164][260/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9504 (1.0199)	Acc@1 89.062 (86.398)	Acc@5 99.219 (98.704)
Epoch: [164][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0141 (1.0208)	Acc@1 89.062 (86.393)	Acc@5 97.656 (98.706)
Epoch: [164][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0062 (1.0237)	Acc@1 88.281 (86.296)	Acc@5 98.438 (98.699)
Epoch: [164][290/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.1338 (1.0272)	Acc@1 79.688 (86.168)	Acc@5 97.656 (98.668)
Epoch: [164][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.1565 (1.0292)	Acc@1 82.031 (86.062)	Acc@5 98.438 (98.669)
Epoch: [164][310/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1013 (1.0303)	Acc@1 85.156 (85.990)	Acc@5 97.656 (98.664)
Epoch: [164][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1437 (1.0327)	Acc@1 87.500 (85.896)	Acc@5 95.312 (98.644)
Epoch: [164][330/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0077 (1.0340)	Acc@1 89.062 (85.862)	Acc@5 98.438 (98.622)
Epoch: [164][340/391]	Time 0.017 (0.012)	Data 0.001 (0.002)	Loss 1.0695 (1.0358)	Acc@1 86.719 (85.814)	Acc@5 96.875 (98.605)
Epoch: [164][350/391]	Time 0.015 (0.012)	Data 0.002 (0.002)	Loss 1.1173 (1.0372)	Acc@1 82.031 (85.757)	Acc@5 98.438 (98.602)
Epoch: [164][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1177 (1.0392)	Acc@1 84.375 (85.695)	Acc@5 97.656 (98.570)
Epoch: [164][370/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0768 (1.0406)	Acc@1 84.375 (85.672)	Acc@5 99.219 (98.547)
Epoch: [164][380/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.2358 (1.0419)	Acc@1 77.344 (85.620)	Acc@5 97.656 (98.530)
Epoch: [164][390/391]	Time 0.017 (0.012)	Data 0.001 (0.002)	Loss 1.1164 (1.0427)	Acc@1 80.000 (85.590)	Acc@5 98.750 (98.524)
num momentum params: 26
[0.010000000000000002, 1.0426787334442138, 1.412081040740013, 85.59, 65.23, tensor(0.5648, device='cuda:0', grad_fn=<DivBackward0>), 4.65533971786499, 0.35346412658691406]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [165 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [165][0/391]	Time 0.057 (0.057)	Data 0.193 (0.193)	Loss 0.9651 (0.9651)	Acc@1 86.719 (86.719)	Acc@5 98.438 (98.438)
Epoch: [165][10/391]	Time 0.010 (0.016)	Data 0.001 (0.019)	Loss 0.9584 (0.9560)	Acc@1 85.938 (87.784)	Acc@5 100.000 (99.148)
Epoch: [165][20/391]	Time 0.010 (0.014)	Data 0.002 (0.011)	Loss 1.1085 (0.9950)	Acc@1 85.156 (86.942)	Acc@5 98.438 (98.735)
Epoch: [165][30/391]	Time 0.013 (0.013)	Data 0.001 (0.008)	Loss 0.9304 (0.9773)	Acc@1 89.844 (87.702)	Acc@5 99.219 (98.942)
Epoch: [165][40/391]	Time 0.011 (0.013)	Data 0.002 (0.006)	Loss 1.0908 (0.9786)	Acc@1 85.938 (87.576)	Acc@5 99.219 (99.066)
Epoch: [165][50/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 1.0723 (0.9753)	Acc@1 82.031 (87.577)	Acc@5 100.000 (99.112)
Epoch: [165][60/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 0.9232 (0.9807)	Acc@1 88.281 (87.334)	Acc@5 98.438 (99.052)
Epoch: [165][70/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 0.9374 (0.9800)	Acc@1 90.625 (87.511)	Acc@5 99.219 (99.043)
Epoch: [165][80/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 0.9666 (0.9827)	Acc@1 89.062 (87.432)	Acc@5 100.000 (98.987)
Epoch: [165][90/391]	Time 0.012 (0.012)	Data 0.001 (0.004)	Loss 0.9307 (0.9832)	Acc@1 89.844 (87.397)	Acc@5 99.219 (98.961)
Epoch: [165][100/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0997 (0.9848)	Acc@1 83.594 (87.245)	Acc@5 100.000 (98.979)
Epoch: [165][110/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0571 (0.9865)	Acc@1 84.375 (87.233)	Acc@5 98.438 (98.972)
Epoch: [165][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0223 (0.9879)	Acc@1 84.375 (87.151)	Acc@5 98.438 (98.941)
Epoch: [165][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0364 (0.9882)	Acc@1 85.938 (87.124)	Acc@5 100.000 (98.962)
Epoch: [165][140/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.1016 (0.9916)	Acc@1 83.594 (86.990)	Acc@5 99.219 (98.958)
Epoch: [165][150/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.1287 (0.9957)	Acc@1 85.156 (86.869)	Acc@5 97.656 (98.898)
Epoch: [165][160/391]	Time 0.016 (0.012)	Data 0.002 (0.003)	Loss 1.0722 (0.9961)	Acc@1 82.031 (86.840)	Acc@5 98.438 (98.889)
Epoch: [165][170/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0120 (0.9973)	Acc@1 88.281 (86.764)	Acc@5 98.438 (98.881)
Epoch: [165][180/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9880 (1.0030)	Acc@1 89.062 (86.589)	Acc@5 96.875 (98.809)
Epoch: [165][190/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.1212 (1.0053)	Acc@1 82.031 (86.535)	Acc@5 98.438 (98.785)
Epoch: [165][200/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0699 (1.0086)	Acc@1 86.719 (86.478)	Acc@5 95.312 (98.756)
Epoch: [165][210/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1777 (1.0105)	Acc@1 80.469 (86.411)	Acc@5 98.438 (98.760)
Epoch: [165][220/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0465 (1.0126)	Acc@1 87.500 (86.386)	Acc@5 100.000 (98.727)
Epoch: [165][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9853 (1.0125)	Acc@1 85.156 (86.394)	Acc@5 97.656 (98.715)
Epoch: [165][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.9842 (1.0117)	Acc@1 82.031 (86.414)	Acc@5 99.219 (98.720)
Epoch: [165][250/391]	Time 0.012 (0.012)	Data 0.004 (0.002)	Loss 0.9849 (1.0146)	Acc@1 87.500 (86.327)	Acc@5 98.438 (98.699)
Epoch: [165][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.8764 (1.0146)	Acc@1 93.750 (86.312)	Acc@5 98.438 (98.692)
Epoch: [165][270/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0365 (1.0153)	Acc@1 86.719 (86.269)	Acc@5 99.219 (98.697)
Epoch: [165][280/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.0406 (1.0164)	Acc@1 85.156 (86.224)	Acc@5 97.656 (98.696)
Epoch: [165][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.9859 (1.0185)	Acc@1 85.938 (86.131)	Acc@5 100.000 (98.698)
Epoch: [165][300/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0034 (1.0178)	Acc@1 85.156 (86.132)	Acc@5 100.000 (98.707)
Epoch: [165][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0580 (1.0191)	Acc@1 84.375 (86.103)	Acc@5 99.219 (98.706)
Epoch: [165][320/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1054 (1.0206)	Acc@1 83.594 (86.040)	Acc@5 97.656 (98.700)
Epoch: [165][330/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9263 (1.0208)	Acc@1 88.281 (86.032)	Acc@5 99.219 (98.709)
Epoch: [165][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0693 (1.0222)	Acc@1 83.594 (86.002)	Acc@5 100.000 (98.701)
Epoch: [165][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.2054 (1.0249)	Acc@1 80.469 (85.933)	Acc@5 96.875 (98.665)
Epoch: [165][360/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0916 (1.0274)	Acc@1 82.031 (85.864)	Acc@5 99.219 (98.643)
Epoch: [165][370/391]	Time 0.019 (0.012)	Data 0.004 (0.002)	Loss 1.0343 (1.0280)	Acc@1 85.156 (85.807)	Acc@5 97.656 (98.631)
Epoch: [165][380/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0082 (1.0288)	Acc@1 85.156 (85.780)	Acc@5 99.219 (98.622)
Epoch: [165][390/391]	Time 0.016 (0.012)	Data 0.001 (0.002)	Loss 1.1851 (1.0300)	Acc@1 87.500 (85.720)	Acc@5 97.500 (98.618)
num momentum params: 26
[0.010000000000000002, 1.0299905193328858, 1.4544578337669372, 85.72, 64.34, tensor(0.5663, device='cuda:0', grad_fn=<DivBackward0>), 4.549242734909058, 0.3653888702392578]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [166 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [166][0/391]	Time 0.055 (0.055)	Data 0.211 (0.211)	Loss 0.9138 (0.9138)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [166][10/391]	Time 0.012 (0.017)	Data 0.002 (0.021)	Loss 0.9233 (0.9609)	Acc@1 89.062 (88.210)	Acc@5 100.000 (99.219)
Epoch: [166][20/391]	Time 0.012 (0.015)	Data 0.002 (0.012)	Loss 1.0289 (0.9636)	Acc@1 88.281 (88.244)	Acc@5 99.219 (99.256)
Epoch: [166][30/391]	Time 0.013 (0.014)	Data 0.002 (0.008)	Loss 0.9138 (0.9744)	Acc@1 92.188 (87.903)	Acc@5 98.438 (99.042)
Epoch: [166][40/391]	Time 0.013 (0.013)	Data 0.002 (0.007)	Loss 1.0589 (0.9802)	Acc@1 85.938 (87.633)	Acc@5 99.219 (99.047)
Epoch: [166][50/391]	Time 0.010 (0.013)	Data 0.001 (0.006)	Loss 0.9348 (0.9816)	Acc@1 88.281 (87.653)	Acc@5 100.000 (98.974)
Epoch: [166][60/391]	Time 0.014 (0.013)	Data 0.002 (0.005)	Loss 0.9036 (0.9768)	Acc@1 89.844 (87.718)	Acc@5 100.000 (99.027)
Epoch: [166][70/391]	Time 0.018 (0.013)	Data 0.002 (0.005)	Loss 0.8657 (0.9738)	Acc@1 90.625 (87.852)	Acc@5 99.219 (99.054)
Epoch: [166][80/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 1.1285 (0.9762)	Acc@1 81.250 (87.722)	Acc@5 98.438 (99.045)
Epoch: [166][90/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 1.0149 (0.9824)	Acc@1 82.812 (87.440)	Acc@5 98.438 (99.013)
Epoch: [166][100/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 1.0893 (0.9869)	Acc@1 82.812 (87.222)	Acc@5 98.438 (98.956)
Epoch: [166][110/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 1.0908 (0.9882)	Acc@1 86.719 (87.268)	Acc@5 98.438 (98.937)
Epoch: [166][120/391]	Time 0.015 (0.012)	Data 0.001 (0.003)	Loss 1.1069 (0.9903)	Acc@1 82.812 (87.171)	Acc@5 98.438 (98.902)
Epoch: [166][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0304 (0.9945)	Acc@1 88.281 (87.059)	Acc@5 97.656 (98.831)
Epoch: [166][140/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1042 (0.9982)	Acc@1 82.031 (86.913)	Acc@5 97.656 (98.820)
Epoch: [166][150/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 0.8767 (0.9964)	Acc@1 89.062 (86.946)	Acc@5 100.000 (98.820)
Epoch: [166][160/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9871 (0.9964)	Acc@1 85.156 (86.898)	Acc@5 98.438 (98.831)
Epoch: [166][170/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9622 (0.9964)	Acc@1 86.719 (86.865)	Acc@5 100.000 (98.844)
Epoch: [166][180/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0601 (0.9994)	Acc@1 82.812 (86.753)	Acc@5 98.438 (98.843)
Epoch: [166][190/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0913 (1.0013)	Acc@1 84.375 (86.653)	Acc@5 97.656 (98.826)
Epoch: [166][200/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9610 (1.0010)	Acc@1 85.938 (86.618)	Acc@5 99.219 (98.838)
Epoch: [166][210/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9334 (1.0017)	Acc@1 90.625 (86.563)	Acc@5 99.219 (98.845)
Epoch: [166][220/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9723 (1.0026)	Acc@1 89.062 (86.514)	Acc@5 100.000 (98.848)
Epoch: [166][230/391]	Time 0.014 (0.012)	Data 0.002 (0.002)	Loss 0.9731 (1.0052)	Acc@1 85.938 (86.431)	Acc@5 99.219 (98.833)
Epoch: [166][240/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 0.9934 (1.0064)	Acc@1 86.719 (86.395)	Acc@5 98.438 (98.830)
Epoch: [166][250/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 0.9483 (1.0077)	Acc@1 88.281 (86.370)	Acc@5 100.000 (98.830)
Epoch: [166][260/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 1.0948 (1.0102)	Acc@1 81.250 (86.282)	Acc@5 100.000 (98.803)
Epoch: [166][270/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 0.8863 (1.0127)	Acc@1 89.844 (86.255)	Acc@5 100.000 (98.760)
Epoch: [166][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 0.9921 (1.0149)	Acc@1 84.375 (86.149)	Acc@5 99.219 (98.757)
Epoch: [166][290/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0520 (1.0157)	Acc@1 83.594 (86.093)	Acc@5 98.438 (98.744)
Epoch: [166][300/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.1974 (1.0169)	Acc@1 82.031 (86.057)	Acc@5 96.875 (98.723)
Epoch: [166][310/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.0554 (1.0185)	Acc@1 85.156 (86.000)	Acc@5 97.656 (98.716)
Epoch: [166][320/391]	Time 0.014 (0.011)	Data 0.002 (0.002)	Loss 1.1430 (1.0205)	Acc@1 82.812 (85.952)	Acc@5 96.875 (98.698)
Epoch: [166][330/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 1.1376 (1.0231)	Acc@1 82.031 (85.848)	Acc@5 99.219 (98.666)
Epoch: [166][340/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 0.9647 (1.0240)	Acc@1 87.500 (85.811)	Acc@5 100.000 (98.655)
Epoch: [166][350/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.0490 (1.0240)	Acc@1 82.031 (85.802)	Acc@5 99.219 (98.649)
Epoch: [166][360/391]	Time 0.013 (0.011)	Data 0.001 (0.002)	Loss 1.0338 (1.0248)	Acc@1 84.375 (85.771)	Acc@5 98.438 (98.641)
Epoch: [166][370/391]	Time 0.011 (0.011)	Data 0.003 (0.002)	Loss 1.0544 (1.0254)	Acc@1 84.375 (85.765)	Acc@5 99.219 (98.638)
Epoch: [166][380/391]	Time 0.013 (0.011)	Data 0.002 (0.002)	Loss 1.0763 (1.0267)	Acc@1 83.594 (85.720)	Acc@5 99.219 (98.624)
Epoch: [166][390/391]	Time 0.018 (0.011)	Data 0.000 (0.002)	Loss 1.1049 (1.0276)	Acc@1 80.000 (85.706)	Acc@5 93.750 (98.608)
num momentum params: 26
[0.010000000000000002, 1.027631418991089, 1.3817865139245986, 85.706, 65.72, tensor(0.5636, device='cuda:0', grad_fn=<DivBackward0>), 4.431212425231934, 0.362518310546875]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [167 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [167][0/391]	Time 0.058 (0.058)	Data 0.203 (0.203)	Loss 1.0404 (1.0404)	Acc@1 88.281 (88.281)	Acc@5 98.438 (98.438)
Epoch: [167][10/391]	Time 0.013 (0.017)	Data 0.002 (0.020)	Loss 1.0244 (1.0049)	Acc@1 85.156 (86.435)	Acc@5 97.656 (98.651)
Epoch: [167][20/391]	Time 0.014 (0.015)	Data 0.001 (0.011)	Loss 0.9990 (0.9907)	Acc@1 84.375 (86.979)	Acc@5 96.875 (98.475)
Epoch: [167][30/391]	Time 0.013 (0.014)	Data 0.002 (0.008)	Loss 0.9339 (0.9869)	Acc@1 87.500 (86.946)	Acc@5 100.000 (98.513)
Epoch: [167][40/391]	Time 0.012 (0.014)	Data 0.002 (0.007)	Loss 0.8798 (0.9833)	Acc@1 89.844 (87.100)	Acc@5 99.219 (98.780)
Epoch: [167][50/391]	Time 0.012 (0.014)	Data 0.002 (0.006)	Loss 1.0891 (0.9831)	Acc@1 85.156 (87.163)	Acc@5 98.438 (98.729)
Epoch: [167][60/391]	Time 0.010 (0.013)	Data 0.001 (0.005)	Loss 0.9531 (0.9796)	Acc@1 89.844 (87.308)	Acc@5 99.219 (98.796)
Epoch: [167][70/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 0.9044 (0.9802)	Acc@1 90.625 (87.302)	Acc@5 99.219 (98.801)
Epoch: [167][80/391]	Time 0.010 (0.013)	Data 0.002 (0.004)	Loss 0.9515 (0.9751)	Acc@1 88.281 (87.529)	Acc@5 99.219 (98.862)
Epoch: [167][90/391]	Time 0.013 (0.013)	Data 0.002 (0.004)	Loss 0.9126 (0.9764)	Acc@1 89.062 (87.328)	Acc@5 100.000 (98.850)
Epoch: [167][100/391]	Time 0.012 (0.013)	Data 0.002 (0.004)	Loss 1.1319 (0.9743)	Acc@1 86.719 (87.485)	Acc@5 98.438 (98.863)
Epoch: [167][110/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 1.0101 (0.9755)	Acc@1 85.156 (87.479)	Acc@5 99.219 (98.832)
Epoch: [167][120/391]	Time 0.013 (0.013)	Data 0.002 (0.003)	Loss 0.9899 (0.9775)	Acc@1 85.938 (87.377)	Acc@5 100.000 (98.857)
Epoch: [167][130/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 1.0565 (0.9778)	Acc@1 82.812 (87.369)	Acc@5 98.438 (98.861)
Epoch: [167][140/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0019 (0.9814)	Acc@1 82.031 (87.179)	Acc@5 100.000 (98.870)
Epoch: [167][150/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.8844 (0.9809)	Acc@1 88.281 (87.153)	Acc@5 99.219 (98.882)
Epoch: [167][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8659 (0.9828)	Acc@1 92.188 (87.117)	Acc@5 99.219 (98.855)
Epoch: [167][170/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0353 (0.9839)	Acc@1 85.938 (87.020)	Acc@5 98.438 (98.858)
Epoch: [167][180/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0015 (0.9868)	Acc@1 83.594 (86.922)	Acc@5 100.000 (98.848)
Epoch: [167][190/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9236 (0.9891)	Acc@1 90.625 (86.833)	Acc@5 99.219 (98.818)
Epoch: [167][200/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9567 (0.9909)	Acc@1 89.062 (86.804)	Acc@5 99.219 (98.795)
Epoch: [167][210/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 0.9407 (0.9920)	Acc@1 84.375 (86.737)	Acc@5 100.000 (98.786)
Epoch: [167][220/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.1266 (0.9942)	Acc@1 80.469 (86.676)	Acc@5 97.656 (98.773)
Epoch: [167][230/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9984 (0.9946)	Acc@1 81.250 (86.627)	Acc@5 100.000 (98.766)
Epoch: [167][240/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9693 (0.9962)	Acc@1 92.969 (86.612)	Acc@5 98.438 (98.739)
Epoch: [167][250/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0996 (0.9982)	Acc@1 83.594 (86.591)	Acc@5 96.875 (98.718)
Epoch: [167][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0911 (0.9993)	Acc@1 83.594 (86.572)	Acc@5 100.000 (98.704)
Epoch: [167][270/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9445 (1.0005)	Acc@1 88.281 (86.546)	Acc@5 100.000 (98.706)
Epoch: [167][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1684 (1.0016)	Acc@1 83.594 (86.507)	Acc@5 98.438 (98.693)
Epoch: [167][290/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9667 (1.0034)	Acc@1 85.938 (86.442)	Acc@5 99.219 (98.698)
Epoch: [167][300/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0975 (1.0064)	Acc@1 84.375 (86.342)	Acc@5 97.656 (98.663)
Epoch: [167][310/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0972 (1.0073)	Acc@1 85.938 (86.314)	Acc@5 97.656 (98.666)
Epoch: [167][320/391]	Time 0.016 (0.012)	Data 0.001 (0.002)	Loss 1.0825 (1.0095)	Acc@1 85.156 (86.244)	Acc@5 98.438 (98.649)
Epoch: [167][330/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0842 (1.0117)	Acc@1 81.250 (86.145)	Acc@5 100.000 (98.633)
Epoch: [167][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.9750 (1.0126)	Acc@1 86.719 (86.112)	Acc@5 99.219 (98.646)
Epoch: [167][350/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0133 (1.0137)	Acc@1 86.719 (86.075)	Acc@5 100.000 (98.640)
Epoch: [167][360/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1015 (1.0150)	Acc@1 82.031 (86.024)	Acc@5 96.875 (98.634)
Epoch: [167][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0558 (1.0163)	Acc@1 83.594 (85.982)	Acc@5 98.438 (98.638)
Epoch: [167][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0252 (1.0181)	Acc@1 81.250 (85.899)	Acc@5 99.219 (98.624)
Epoch: [167][390/391]	Time 0.017 (0.012)	Data 0.001 (0.002)	Loss 1.1915 (1.0189)	Acc@1 81.250 (85.886)	Acc@5 96.250 (98.606)
num momentum params: 26
[0.010000000000000002, 1.0188742740631103, 1.411054379940033, 85.886, 64.92, tensor(0.5640, device='cuda:0', grad_fn=<DivBackward0>), 4.564241647720337, 0.3697173595428467]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [168 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [168][0/391]	Time 0.054 (0.054)	Data 0.193 (0.193)	Loss 0.9102 (0.9102)	Acc@1 88.281 (88.281)	Acc@5 98.438 (98.438)
Epoch: [168][10/391]	Time 0.010 (0.016)	Data 0.001 (0.019)	Loss 1.0136 (0.9345)	Acc@1 88.281 (89.205)	Acc@5 98.438 (99.006)
Epoch: [168][20/391]	Time 0.011 (0.014)	Data 0.002 (0.011)	Loss 0.9523 (0.9455)	Acc@1 90.625 (88.318)	Acc@5 98.438 (99.144)
Epoch: [168][30/391]	Time 0.011 (0.013)	Data 0.002 (0.008)	Loss 0.9188 (0.9592)	Acc@1 88.281 (87.651)	Acc@5 99.219 (98.992)
Epoch: [168][40/391]	Time 0.011 (0.013)	Data 0.002 (0.006)	Loss 0.9217 (0.9534)	Acc@1 89.844 (87.862)	Acc@5 98.438 (99.047)
Epoch: [168][50/391]	Time 0.011 (0.012)	Data 0.001 (0.005)	Loss 1.0029 (0.9598)	Acc@1 83.594 (87.592)	Acc@5 99.219 (99.050)
Epoch: [168][60/391]	Time 0.013 (0.012)	Data 0.002 (0.005)	Loss 0.9239 (0.9651)	Acc@1 85.156 (87.193)	Acc@5 100.000 (98.988)
Epoch: [168][70/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 1.0271 (0.9679)	Acc@1 85.156 (87.192)	Acc@5 99.219 (98.955)
Epoch: [168][80/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 0.9395 (0.9661)	Acc@1 87.500 (87.143)	Acc@5 100.000 (99.007)
Epoch: [168][90/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.9917 (0.9681)	Acc@1 87.500 (87.294)	Acc@5 99.219 (98.953)
Epoch: [168][100/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 1.1247 (0.9691)	Acc@1 83.594 (87.291)	Acc@5 99.219 (99.002)
Epoch: [168][110/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9145 (0.9688)	Acc@1 89.844 (87.289)	Acc@5 100.000 (99.015)
Epoch: [168][120/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0827 (0.9669)	Acc@1 85.938 (87.377)	Acc@5 97.656 (98.980)
Epoch: [168][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9473 (0.9670)	Acc@1 86.719 (87.345)	Acc@5 100.000 (98.986)
Epoch: [168][140/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0032 (0.9681)	Acc@1 82.031 (87.301)	Acc@5 99.219 (99.014)
Epoch: [168][150/391]	Time 0.011 (0.012)	Data 0.003 (0.003)	Loss 1.0080 (0.9685)	Acc@1 82.812 (87.257)	Acc@5 99.219 (99.017)
Epoch: [168][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9642 (0.9724)	Acc@1 86.719 (87.185)	Acc@5 98.438 (99.010)
Epoch: [168][170/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0258 (0.9740)	Acc@1 85.938 (87.166)	Acc@5 99.219 (99.009)
Epoch: [168][180/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.1064 (0.9774)	Acc@1 83.594 (87.047)	Acc@5 99.219 (99.003)
Epoch: [168][190/391]	Time 0.010 (0.012)	Data 0.007 (0.003)	Loss 1.0410 (0.9774)	Acc@1 82.812 (87.026)	Acc@5 98.438 (99.006)
Epoch: [168][200/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9942 (0.9791)	Acc@1 82.031 (86.971)	Acc@5 100.000 (98.997)
Epoch: [168][210/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0804 (0.9814)	Acc@1 82.031 (86.852)	Acc@5 98.438 (98.982)
Epoch: [168][220/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9454 (0.9824)	Acc@1 89.844 (86.821)	Acc@5 99.219 (98.975)
Epoch: [168][230/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8996 (0.9829)	Acc@1 88.281 (86.810)	Acc@5 99.219 (98.958)
Epoch: [168][240/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9512 (0.9818)	Acc@1 83.594 (86.848)	Acc@5 99.219 (98.959)
Epoch: [168][250/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.9449 (0.9836)	Acc@1 86.719 (86.797)	Acc@5 99.219 (98.948)
Epoch: [168][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0292 (0.9851)	Acc@1 84.375 (86.755)	Acc@5 97.656 (98.928)
Epoch: [168][270/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0498 (0.9867)	Acc@1 83.594 (86.701)	Acc@5 99.219 (98.922)
Epoch: [168][280/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 1.0122 (0.9882)	Acc@1 82.812 (86.610)	Acc@5 99.219 (98.927)
Epoch: [168][290/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9996 (0.9902)	Acc@1 86.719 (86.571)	Acc@5 100.000 (98.931)
Epoch: [168][300/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 0.9100 (0.9908)	Acc@1 85.938 (86.555)	Acc@5 100.000 (98.912)
Epoch: [168][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9839 (0.9911)	Acc@1 85.938 (86.550)	Acc@5 100.000 (98.915)
Epoch: [168][320/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.0295 (0.9935)	Acc@1 84.375 (86.446)	Acc@5 99.219 (98.910)
Epoch: [168][330/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0840 (0.9949)	Acc@1 83.594 (86.405)	Acc@5 97.656 (98.905)
Epoch: [168][340/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.0482 (0.9956)	Acc@1 83.594 (86.384)	Acc@5 99.219 (98.912)
Epoch: [168][350/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9556 (0.9961)	Acc@1 88.281 (86.363)	Acc@5 100.000 (98.898)
Epoch: [168][360/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 1.0917 (0.9976)	Acc@1 82.812 (86.301)	Acc@5 99.219 (98.888)
Epoch: [168][370/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0995 (0.9983)	Acc@1 88.281 (86.289)	Acc@5 96.875 (98.876)
Epoch: [168][380/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0890 (0.9989)	Acc@1 82.031 (86.235)	Acc@5 99.219 (98.880)
Epoch: [168][390/391]	Time 0.018 (0.012)	Data 0.000 (0.002)	Loss 1.1426 (1.0000)	Acc@1 81.250 (86.202)	Acc@5 98.750 (98.872)
num momentum params: 26
[0.010000000000000002, 1.0000395613098145, 1.4561641448736191, 86.202, 64.18, tensor(0.5712, device='cuda:0', grad_fn=<DivBackward0>), 4.60652756690979, 0.3615267276763916]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [169 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [169][0/391]	Time 0.056 (0.056)	Data 0.203 (0.203)	Loss 0.9926 (0.9926)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [169][10/391]	Time 0.010 (0.017)	Data 0.001 (0.020)	Loss 0.9381 (0.9315)	Acc@1 89.844 (88.281)	Acc@5 98.438 (99.006)
Epoch: [169][20/391]	Time 0.011 (0.015)	Data 0.001 (0.011)	Loss 1.1016 (0.9541)	Acc@1 82.812 (87.909)	Acc@5 97.656 (98.810)
Epoch: [169][30/391]	Time 0.013 (0.014)	Data 0.002 (0.009)	Loss 1.0060 (0.9577)	Acc@1 87.500 (87.853)	Acc@5 98.438 (98.891)
Epoch: [169][40/391]	Time 0.013 (0.013)	Data 0.002 (0.007)	Loss 1.0211 (0.9536)	Acc@1 86.719 (88.053)	Acc@5 98.438 (98.952)
Epoch: [169][50/391]	Time 0.010 (0.013)	Data 0.002 (0.006)	Loss 1.0157 (0.9660)	Acc@1 82.031 (87.377)	Acc@5 98.438 (98.943)
Epoch: [169][60/391]	Time 0.012 (0.013)	Data 0.002 (0.005)	Loss 1.0634 (0.9638)	Acc@1 83.594 (87.385)	Acc@5 96.875 (98.975)
Epoch: [169][70/391]	Time 0.013 (0.013)	Data 0.002 (0.005)	Loss 1.0685 (0.9676)	Acc@1 87.500 (87.346)	Acc@5 97.656 (98.922)
Epoch: [169][80/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.9323 (0.9593)	Acc@1 85.156 (87.616)	Acc@5 100.000 (98.968)
Epoch: [169][90/391]	Time 0.013 (0.012)	Data 0.002 (0.004)	Loss 0.9027 (0.9558)	Acc@1 88.281 (87.655)	Acc@5 99.219 (99.013)
Epoch: [169][100/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 0.8919 (0.9523)	Acc@1 88.281 (87.724)	Acc@5 100.000 (99.072)
Epoch: [169][110/391]	Time 0.012 (0.012)	Data 0.002 (0.004)	Loss 0.8811 (0.9529)	Acc@1 90.625 (87.718)	Acc@5 100.000 (99.057)
Epoch: [169][120/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0298 (0.9518)	Acc@1 82.812 (87.732)	Acc@5 99.219 (99.070)
Epoch: [169][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9122 (0.9521)	Acc@1 86.719 (87.750)	Acc@5 100.000 (99.076)
Epoch: [169][140/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 0.9537 (0.9528)	Acc@1 87.500 (87.727)	Acc@5 98.438 (99.064)
Epoch: [169][150/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0004 (0.9574)	Acc@1 87.500 (87.531)	Acc@5 98.438 (99.064)
Epoch: [169][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8913 (0.9578)	Acc@1 88.281 (87.568)	Acc@5 100.000 (99.063)
Epoch: [169][170/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9892 (0.9578)	Acc@1 85.938 (87.573)	Acc@5 98.438 (99.077)
Epoch: [169][180/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9903 (0.9610)	Acc@1 89.844 (87.435)	Acc@5 97.656 (99.063)
Epoch: [169][190/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0166 (0.9641)	Acc@1 87.500 (87.316)	Acc@5 99.219 (99.035)
Epoch: [169][200/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.1000 (0.9666)	Acc@1 82.812 (87.201)	Acc@5 98.438 (99.013)
Epoch: [169][210/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0413 (0.9694)	Acc@1 84.375 (87.141)	Acc@5 99.219 (99.004)
Epoch: [169][220/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.1137 (0.9703)	Acc@1 82.812 (87.122)	Acc@5 97.656 (99.007)
Epoch: [169][230/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.8511 (0.9728)	Acc@1 91.406 (87.003)	Acc@5 100.000 (99.002)
Epoch: [169][240/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0049 (0.9790)	Acc@1 87.500 (86.816)	Acc@5 98.438 (98.953)
Epoch: [169][250/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.8961 (0.9795)	Acc@1 91.406 (86.815)	Acc@5 99.219 (98.957)
Epoch: [169][260/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.9151 (0.9827)	Acc@1 89.062 (86.722)	Acc@5 99.219 (98.919)
Epoch: [169][270/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0424 (0.9847)	Acc@1 87.500 (86.664)	Acc@5 97.656 (98.905)
Epoch: [169][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9989 (0.9863)	Acc@1 85.156 (86.610)	Acc@5 99.219 (98.899)
Epoch: [169][290/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 0.8582 (0.9868)	Acc@1 91.406 (86.601)	Acc@5 100.000 (98.891)
Epoch: [169][300/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.9886 (0.9887)	Acc@1 84.375 (86.509)	Acc@5 100.000 (98.884)
Epoch: [169][310/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9377 (0.9890)	Acc@1 90.625 (86.480)	Acc@5 98.438 (98.882)
Epoch: [169][320/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.9794 (0.9916)	Acc@1 82.812 (86.383)	Acc@5 100.000 (98.859)
Epoch: [169][330/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1108 (0.9954)	Acc@1 82.812 (86.287)	Acc@5 100.000 (98.839)
Epoch: [169][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.9709 (0.9966)	Acc@1 87.500 (86.233)	Acc@5 100.000 (98.834)
Epoch: [169][350/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0042 (0.9982)	Acc@1 87.500 (86.205)	Acc@5 96.875 (98.820)
Epoch: [169][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0198 (0.9999)	Acc@1 82.031 (86.145)	Acc@5 100.000 (98.801)
Epoch: [169][370/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.2283 (1.0027)	Acc@1 76.562 (86.051)	Acc@5 96.875 (98.787)
Epoch: [169][380/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0714 (1.0042)	Acc@1 86.719 (86.038)	Acc@5 98.438 (98.764)
Epoch: [169][390/391]	Time 0.019 (0.012)	Data 0.002 (0.002)	Loss 1.0101 (1.0053)	Acc@1 87.500 (86.016)	Acc@5 97.500 (98.748)
num momentum params: 26
[0.010000000000000002, 1.0053089891433715, 1.457399553656578, 86.016, 64.33, tensor(0.5672, device='cuda:0', grad_fn=<DivBackward0>), 4.602785110473633, 0.37201523780822754]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [170 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [170][0/391]	Time 0.056 (0.056)	Data 0.189 (0.189)	Loss 1.0145 (1.0145)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [170][10/391]	Time 0.011 (0.016)	Data 0.001 (0.019)	Loss 1.0283 (1.0005)	Acc@1 89.062 (86.080)	Acc@5 98.438 (99.148)
Epoch: [170][20/391]	Time 0.011 (0.014)	Data 0.002 (0.011)	Loss 1.0056 (0.9981)	Acc@1 84.375 (85.938)	Acc@5 99.219 (99.107)
Epoch: [170][30/391]	Time 0.011 (0.013)	Data 0.002 (0.008)	Loss 1.0189 (0.9849)	Acc@1 85.156 (86.467)	Acc@5 98.438 (99.118)
Epoch: [170][40/391]	Time 0.012 (0.013)	Data 0.001 (0.006)	Loss 0.9599 (0.9880)	Acc@1 85.938 (86.280)	Acc@5 98.438 (99.066)
Epoch: [170][50/391]	Time 0.010 (0.013)	Data 0.002 (0.005)	Loss 0.9548 (0.9832)	Acc@1 88.281 (86.504)	Acc@5 97.656 (98.974)
Epoch: [170][60/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 0.9143 (0.9759)	Acc@1 90.625 (86.885)	Acc@5 97.656 (99.052)
Epoch: [170][70/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.8940 (0.9780)	Acc@1 91.406 (86.741)	Acc@5 100.000 (99.076)
Epoch: [170][80/391]	Time 0.015 (0.012)	Data 0.002 (0.004)	Loss 0.8491 (0.9747)	Acc@1 89.844 (86.844)	Acc@5 99.219 (99.026)
Epoch: [170][90/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.9806 (0.9715)	Acc@1 87.500 (87.036)	Acc@5 100.000 (99.056)
Epoch: [170][100/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.8691 (0.9695)	Acc@1 90.625 (87.098)	Acc@5 99.219 (99.041)
Epoch: [170][110/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9236 (0.9691)	Acc@1 86.719 (87.113)	Acc@5 98.438 (99.008)
Epoch: [170][120/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.8739 (0.9696)	Acc@1 89.062 (87.106)	Acc@5 99.219 (98.980)
Epoch: [170][130/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 0.9185 (0.9681)	Acc@1 89.062 (87.178)	Acc@5 99.219 (98.968)
Epoch: [170][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0777 (0.9693)	Acc@1 83.594 (87.212)	Acc@5 99.219 (98.942)
Epoch: [170][150/391]	Time 0.016 (0.012)	Data 0.001 (0.003)	Loss 1.1740 (0.9733)	Acc@1 84.375 (87.164)	Acc@5 97.656 (98.898)
Epoch: [170][160/391]	Time 0.013 (0.012)	Data 0.001 (0.003)	Loss 0.9539 (0.9727)	Acc@1 85.938 (87.194)	Acc@5 99.219 (98.918)
Epoch: [170][170/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.0236 (0.9718)	Acc@1 82.812 (87.153)	Acc@5 98.438 (98.931)
Epoch: [170][180/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9535 (0.9725)	Acc@1 88.281 (87.168)	Acc@5 97.656 (98.899)
Epoch: [170][190/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9263 (0.9725)	Acc@1 88.281 (87.144)	Acc@5 99.219 (98.904)
Epoch: [170][200/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0525 (0.9747)	Acc@1 85.938 (87.026)	Acc@5 99.219 (98.904)
Epoch: [170][210/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9107 (0.9763)	Acc@1 90.625 (87.019)	Acc@5 98.438 (98.871)
Epoch: [170][220/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0866 (0.9768)	Acc@1 82.031 (86.977)	Acc@5 98.438 (98.862)
Epoch: [170][230/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 0.9614 (0.9776)	Acc@1 84.375 (86.945)	Acc@5 99.219 (98.853)
Epoch: [170][240/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9481 (0.9790)	Acc@1 86.719 (86.900)	Acc@5 99.219 (98.843)
Epoch: [170][250/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.9876 (0.9810)	Acc@1 85.938 (86.849)	Acc@5 100.000 (98.830)
Epoch: [170][260/391]	Time 0.011 (0.012)	Data 0.000 (0.002)	Loss 0.9229 (0.9826)	Acc@1 89.844 (86.806)	Acc@5 99.219 (98.833)
Epoch: [170][270/391]	Time 0.014 (0.012)	Data 0.002 (0.002)	Loss 1.0835 (0.9817)	Acc@1 83.594 (86.825)	Acc@5 97.656 (98.818)
Epoch: [170][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9925 (0.9834)	Acc@1 83.594 (86.722)	Acc@5 100.000 (98.821)
Epoch: [170][290/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.0786 (0.9860)	Acc@1 83.594 (86.654)	Acc@5 99.219 (98.805)
Epoch: [170][300/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1029 (0.9891)	Acc@1 82.812 (86.532)	Acc@5 97.656 (98.806)
Epoch: [170][310/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9758 (0.9911)	Acc@1 87.500 (86.495)	Acc@5 99.219 (98.789)
Epoch: [170][320/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 0.9753 (0.9920)	Acc@1 87.500 (86.478)	Acc@5 99.219 (98.783)
Epoch: [170][330/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 0.9109 (0.9921)	Acc@1 89.062 (86.476)	Acc@5 99.219 (98.787)
Epoch: [170][340/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0302 (0.9927)	Acc@1 83.594 (86.460)	Acc@5 99.219 (98.795)
Epoch: [170][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.8804 (0.9934)	Acc@1 90.625 (86.447)	Acc@5 98.438 (98.789)
Epoch: [170][360/391]	Time 0.010 (0.012)	Data 0.004 (0.002)	Loss 0.9256 (0.9946)	Acc@1 89.062 (86.411)	Acc@5 98.438 (98.788)
Epoch: [170][370/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0652 (0.9947)	Acc@1 78.906 (86.388)	Acc@5 99.219 (98.783)
Epoch: [170][380/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 0.9900 (0.9954)	Acc@1 89.844 (86.380)	Acc@5 99.219 (98.774)
Epoch: [170][390/391]	Time 0.015 (0.012)	Data 0.001 (0.002)	Loss 1.0969 (0.9970)	Acc@1 83.750 (86.328)	Acc@5 97.500 (98.770)
num momentum params: 26
[0.010000000000000002, 0.9969682173919677, 1.4514640748500824, 86.328, 64.72, tensor(0.5693, device='cuda:0', grad_fn=<DivBackward0>), 4.554594278335571, 0.3542203903198242]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [108, 32, 3, 3]
Before - module.bn2.weight: [108]
Before - module.bn2.bias: [108]
Before - module.conv3.weight: [202, 108, 3, 3]
Before - module.bn3.weight: [202]
Before - module.bn3.bias: [202]
Before - module.conv4.weight: [247, 202, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [356, 247, 3, 3]
Before - module.bn5.weight: [356]
Before - module.bn5.bias: [356]
Before - module.conv6.weight: [303, 356, 3, 3]
Before - module.bn6.weight: [303]
Before - module.bn6.bias: [303]
Before - module.conv7.weight: [188, 303, 3, 3]
Before - module.bn7.weight: [188]
Before - module.bn7.bias: [188]
Before - module.conv8.weight: [186, 188, 3, 3]
Before - module.bn8.weight: [186]
Before - module.bn8.bias: [186]
Before - module.fc.weight: [100, 186]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [108, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 108 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [202, 108, 3, 3] >> [202, 108, 3, 3]
[module.bn3.weight]: 202 >> 202
running_mean [202]
running_var [202]
num_batches_tracked []
[module.conv4.weight]: [247, 202, 3, 3] >> [247, 202, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [356, 247, 3, 3] >> [356, 247, 3, 3]
[module.bn5.weight]: 356 >> 356
running_mean [356]
running_var [356]
num_batches_tracked []
[module.conv6.weight]: [303, 356, 3, 3] >> [303, 356, 3, 3]
[module.bn6.weight]: 303 >> 303
running_mean [303]
running_var [303]
num_batches_tracked []
[module.conv7.weight]: [188, 303, 3, 3] >> [188, 303, 3, 3]
[module.bn7.weight]: 188 >> 188
running_mean [188]
running_var [188]
num_batches_tracked []
[module.conv8.weight]: [186, 188, 3, 3] >> [186, 188, 3, 3]
[module.bn8.weight]: 186 >> 186
running_mean [186]
running_var [186]
num_batches_tracked []
[module.fc.weight]: [100, 186] >> [100, 186]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [202, 108, 3, 3]
After - module.bn3.weight: [202]
After - module.bn3.bias: [202]
After - module.conv4.weight: [247, 202, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [356, 247, 3, 3]
After - module.bn5.weight: [356]
After - module.bn5.bias: [356]
After - module.conv6.weight: [303, 356, 3, 3]
After - module.bn6.weight: [303]
After - module.bn6.bias: [303]
After - module.conv7.weight: [188, 303, 3, 3]
After - module.bn7.weight: [188]
After - module.bn7.bias: [188]
After - module.conv8.weight: [186, 188, 3, 3]
After - module.bn8.weight: [186]
After - module.bn8.bias: [186]
After - module.fc.weight: [100, 186]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [202, 108, 3, 3]
conv4 --> [247, 202, 3, 3]
conv5 --> [356, 247, 3, 3]
conv6 --> [303, 356, 3, 3]
conv7 --> [188, 303, 3, 3]
conv8 --> [186, 188, 3, 3]
fc --> [186, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5730103296, 12566016, 202
4, 13104958464, 28738944, 247
5, 6888241152, 12662208, 356
6, 8449947648, 15532992, 303
7, 1574940672, 2050704, 188
8, 966795264, 1258848, 186
fc, 7142400, 18600, 0
===================
FLOP REPORT: 15783141600000.0 40476800000.0 81675672 101192 1622 6.270130157470703
[INFO] Storing checkpoint...

Epoch: [171 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [171][0/391]	Time 0.058 (0.058)	Data 0.208 (0.208)	Loss 0.9700 (0.9700)	Acc@1 88.281 (88.281)	Acc@5 98.438 (98.438)
Epoch: [171][10/391]	Time 0.011 (0.018)	Data 0.002 (0.020)	Loss 1.0289 (0.9962)	Acc@1 86.719 (86.719)	Acc@5 96.875 (98.864)
Epoch: [171][20/391]	Time 0.012 (0.016)	Data 0.001 (0.011)	Loss 1.0152 (0.9660)	Acc@1 84.375 (87.277)	Acc@5 99.219 (99.107)
Epoch: [171][30/391]	Time 0.010 (0.015)	Data 0.002 (0.008)	Loss 0.9274 (0.9532)	Acc@1 89.062 (87.853)	Acc@5 100.000 (99.219)
Epoch: [171][40/391]	Time 0.010 (0.014)	Data 0.003 (0.007)	Loss 1.0209 (0.9550)	Acc@1 86.719 (87.824)	Acc@5 98.438 (99.143)
Epoch: [171][50/391]	Time 0.010 (0.014)	Data 0.002 (0.006)	Loss 0.8967 (0.9494)	Acc@1 86.719 (87.960)	Acc@5 100.000 (99.112)
Epoch: [171][60/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 0.9855 (0.9498)	Acc@1 85.938 (87.769)	Acc@5 98.438 (99.142)
Epoch: [171][70/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 0.9178 (0.9473)	Acc@1 89.844 (87.885)	Acc@5 98.438 (99.087)
Epoch: [171][80/391]	Time 0.012 (0.013)	Data 0.002 (0.004)	Loss 1.0180 (0.9513)	Acc@1 85.938 (87.828)	Acc@5 98.438 (99.045)
Epoch: [171][90/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 0.8804 (0.9527)	Acc@1 92.188 (87.852)	Acc@5 98.438 (98.987)
Epoch: [171][100/391]	Time 0.014 (0.013)	Data 0.002 (0.004)	Loss 1.0076 (0.9554)	Acc@1 84.375 (87.678)	Acc@5 98.438 (99.002)
Epoch: [171][110/391]	Time 0.010 (0.013)	Data 0.002 (0.003)	Loss 0.9490 (0.9599)	Acc@1 88.281 (87.584)	Acc@5 99.219 (98.951)
Epoch: [171][120/391]	Time 0.011 (0.013)	Data 0.002 (0.003)	Loss 0.9787 (0.9609)	Acc@1 85.156 (87.545)	Acc@5 98.438 (98.941)
Epoch: [171][130/391]	Time 0.010 (0.013)	Data 0.002 (0.003)	Loss 1.0894 (0.9615)	Acc@1 83.594 (87.482)	Acc@5 98.438 (98.950)
Epoch: [171][140/391]	Time 0.012 (0.013)	Data 0.002 (0.003)	Loss 0.9879 (0.9600)	Acc@1 86.719 (87.506)	Acc@5 100.000 (98.964)
Epoch: [171][150/391]	Time 0.011 (0.013)	Data 0.001 (0.003)	Loss 1.0726 (0.9650)	Acc@1 84.375 (87.278)	Acc@5 100.000 (98.976)
Epoch: [171][160/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0134 (0.9678)	Acc@1 83.594 (87.107)	Acc@5 98.438 (98.976)
Epoch: [171][170/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0032 (0.9680)	Acc@1 86.719 (87.153)	Acc@5 99.219 (98.986)
Epoch: [171][180/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0736 (0.9702)	Acc@1 84.375 (87.112)	Acc@5 98.438 (98.951)
Epoch: [171][190/391]	Time 0.015 (0.012)	Data 0.001 (0.003)	Loss 1.1232 (0.9707)	Acc@1 81.250 (87.120)	Acc@5 99.219 (98.945)
Epoch: [171][200/391]	Time 0.011 (0.012)	Data 0.009 (0.003)	Loss 1.0653 (0.9697)	Acc@1 83.594 (87.158)	Acc@5 97.656 (98.935)
Epoch: [171][210/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0049 (0.9715)	Acc@1 85.156 (87.096)	Acc@5 99.219 (98.926)
Epoch: [171][220/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0069 (0.9731)	Acc@1 85.156 (87.055)	Acc@5 99.219 (98.918)
Epoch: [171][230/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0242 (0.9751)	Acc@1 85.938 (86.993)	Acc@5 97.656 (98.904)
Epoch: [171][240/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0863 (0.9773)	Acc@1 82.812 (86.926)	Acc@5 99.219 (98.901)
Epoch: [171][250/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0220 (0.9806)	Acc@1 82.031 (86.803)	Acc@5 98.438 (98.867)
Epoch: [171][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0343 (0.9826)	Acc@1 82.812 (86.701)	Acc@5 99.219 (98.854)
Epoch: [171][270/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9157 (0.9828)	Acc@1 86.719 (86.681)	Acc@5 100.000 (98.856)
Epoch: [171][280/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9546 (0.9853)	Acc@1 85.156 (86.560)	Acc@5 100.000 (98.838)
Epoch: [171][290/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.9110 (0.9874)	Acc@1 89.844 (86.496)	Acc@5 99.219 (98.827)
Epoch: [171][300/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9614 (0.9886)	Acc@1 87.500 (86.423)	Acc@5 100.000 (98.835)
Epoch: [171][310/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.8674 (0.9891)	Acc@1 91.406 (86.407)	Acc@5 98.438 (98.814)
Epoch: [171][320/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9497 (0.9887)	Acc@1 89.062 (86.451)	Acc@5 100.000 (98.798)
Epoch: [171][330/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0253 (0.9900)	Acc@1 85.156 (86.386)	Acc@5 99.219 (98.787)
Epoch: [171][340/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0928 (0.9908)	Acc@1 80.469 (86.338)	Acc@5 97.656 (98.781)
Epoch: [171][350/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.0388 (0.9920)	Acc@1 85.938 (86.285)	Acc@5 97.656 (98.762)
Epoch: [171][360/391]	Time 0.015 (0.012)	Data 0.001 (0.002)	Loss 1.0306 (0.9938)	Acc@1 85.938 (86.240)	Acc@5 99.219 (98.753)
Epoch: [171][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1100 (0.9942)	Acc@1 84.375 (86.239)	Acc@5 98.438 (98.747)
Epoch: [171][380/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9311 (0.9955)	Acc@1 89.062 (86.194)	Acc@5 98.438 (98.727)
Epoch: [171][390/391]	Time 0.016 (0.012)	Data 0.000 (0.002)	Loss 1.1789 (0.9974)	Acc@1 83.750 (86.154)	Acc@5 97.500 (98.718)
num momentum params: 26
[0.010000000000000002, 0.9973621431732178, 1.4635732740163803, 86.154, 64.43, tensor(0.5674, device='cuda:0', grad_fn=<DivBackward0>), 4.642757415771484, 0.3706965446472168]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [172 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [172][0/391]	Time 0.053 (0.053)	Data 0.197 (0.197)	Loss 0.9004 (0.9004)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [172][10/391]	Time 0.011 (0.016)	Data 0.001 (0.019)	Loss 1.0389 (0.9674)	Acc@1 84.375 (87.713)	Acc@5 98.438 (98.864)
Epoch: [172][20/391]	Time 0.011 (0.015)	Data 0.002 (0.011)	Loss 1.1278 (0.9728)	Acc@1 82.812 (87.277)	Acc@5 98.438 (98.996)
Epoch: [172][30/391]	Time 0.012 (0.014)	Data 0.002 (0.008)	Loss 0.8578 (0.9589)	Acc@1 91.406 (87.752)	Acc@5 98.438 (98.891)
Epoch: [172][40/391]	Time 0.012 (0.014)	Data 0.001 (0.006)	Loss 1.0146 (0.9559)	Acc@1 85.156 (87.710)	Acc@5 97.656 (98.895)
Epoch: [172][50/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 0.8673 (0.9540)	Acc@1 90.625 (87.760)	Acc@5 99.219 (98.912)
Epoch: [172][60/391]	Time 0.010 (0.013)	Data 0.002 (0.005)	Loss 0.9445 (0.9478)	Acc@1 88.281 (87.871)	Acc@5 99.219 (99.027)
Epoch: [172][70/391]	Time 0.013 (0.013)	Data 0.002 (0.004)	Loss 0.9523 (0.9527)	Acc@1 89.062 (87.643)	Acc@5 98.438 (98.988)
Epoch: [172][80/391]	Time 0.010 (0.013)	Data 0.002 (0.004)	Loss 0.8936 (0.9496)	Acc@1 89.844 (87.799)	Acc@5 99.219 (98.997)
Epoch: [172][90/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 0.9198 (0.9488)	Acc@1 85.156 (87.732)	Acc@5 99.219 (98.996)
Epoch: [172][100/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 0.9933 (0.9492)	Acc@1 86.719 (87.740)	Acc@5 98.438 (98.948)
Epoch: [172][110/391]	Time 0.016 (0.012)	Data 0.001 (0.003)	Loss 1.0008 (0.9521)	Acc@1 86.719 (87.563)	Acc@5 98.438 (98.895)
Epoch: [172][120/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8967 (0.9540)	Acc@1 89.062 (87.481)	Acc@5 99.219 (98.883)
Epoch: [172][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9611 (0.9541)	Acc@1 87.500 (87.518)	Acc@5 99.219 (98.885)
Epoch: [172][140/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0330 (0.9568)	Acc@1 84.375 (87.422)	Acc@5 98.438 (98.886)
Epoch: [172][150/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9442 (0.9540)	Acc@1 89.844 (87.495)	Acc@5 99.219 (98.924)
Epoch: [172][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9065 (0.9522)	Acc@1 87.500 (87.490)	Acc@5 100.000 (98.962)
Epoch: [172][170/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9308 (0.9528)	Acc@1 87.500 (87.445)	Acc@5 99.219 (98.967)
Epoch: [172][180/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9774 (0.9531)	Acc@1 85.938 (87.474)	Acc@5 99.219 (98.990)
Epoch: [172][190/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9987 (0.9536)	Acc@1 84.375 (87.459)	Acc@5 99.219 (98.994)
Epoch: [172][200/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9433 (0.9535)	Acc@1 88.281 (87.469)	Acc@5 98.438 (98.989)
Epoch: [172][210/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.8816 (0.9545)	Acc@1 89.844 (87.441)	Acc@5 100.000 (99.004)
Epoch: [172][220/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.9810 (0.9552)	Acc@1 87.500 (87.405)	Acc@5 99.219 (98.982)
Epoch: [172][230/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0562 (0.9573)	Acc@1 84.375 (87.344)	Acc@5 98.438 (98.962)
Epoch: [172][240/391]	Time 0.011 (0.012)	Data 0.003 (0.002)	Loss 0.9701 (0.9575)	Acc@1 89.062 (87.331)	Acc@5 97.656 (98.956)
Epoch: [172][250/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1249 (0.9596)	Acc@1 83.594 (87.260)	Acc@5 96.875 (98.951)
Epoch: [172][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1123 (0.9619)	Acc@1 81.250 (87.168)	Acc@5 96.875 (98.928)
Epoch: [172][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1571 (0.9641)	Acc@1 85.156 (87.119)	Acc@5 92.969 (98.907)
Epoch: [172][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0285 (0.9665)	Acc@1 85.156 (87.064)	Acc@5 99.219 (98.893)
Epoch: [172][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9464 (0.9682)	Acc@1 89.062 (87.022)	Acc@5 99.219 (98.897)
Epoch: [172][300/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0385 (0.9701)	Acc@1 82.812 (86.971)	Acc@5 98.438 (98.887)
Epoch: [172][310/391]	Time 0.010 (0.012)	Data 0.018 (0.002)	Loss 1.0536 (0.9715)	Acc@1 84.375 (86.917)	Acc@5 97.656 (98.872)
Epoch: [172][320/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0477 (0.9742)	Acc@1 85.938 (86.853)	Acc@5 97.656 (98.876)
Epoch: [172][330/391]	Time 0.010 (0.012)	Data 0.003 (0.002)	Loss 1.2294 (0.9759)	Acc@1 78.906 (86.766)	Acc@5 96.875 (98.865)
Epoch: [172][340/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0599 (0.9781)	Acc@1 76.562 (86.682)	Acc@5 100.000 (98.868)
Epoch: [172][350/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0371 (0.9802)	Acc@1 82.812 (86.625)	Acc@5 98.438 (98.867)
Epoch: [172][360/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0357 (0.9812)	Acc@1 84.375 (86.591)	Acc@5 99.219 (98.864)
Epoch: [172][370/391]	Time 0.014 (0.012)	Data 0.002 (0.002)	Loss 0.8934 (0.9818)	Acc@1 91.406 (86.599)	Acc@5 99.219 (98.852)
Epoch: [172][380/391]	Time 0.016 (0.012)	Data 0.001 (0.002)	Loss 1.1083 (0.9847)	Acc@1 83.594 (86.489)	Acc@5 98.438 (98.844)
Epoch: [172][390/391]	Time 0.016 (0.012)	Data 0.000 (0.002)	Loss 1.0283 (0.9865)	Acc@1 87.500 (86.420)	Acc@5 98.750 (98.836)
num momentum params: 26
[0.010000000000000002, 0.9865018500518798, 1.4376819729804993, 86.42, 65.09, tensor(0.5730, device='cuda:0', grad_fn=<DivBackward0>), 4.538022041320801, 0.36812877655029297]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [173 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [173][0/391]	Time 0.059 (0.059)	Data 0.231 (0.231)	Loss 0.8610 (0.8610)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)
Epoch: [173][10/391]	Time 0.013 (0.018)	Data 0.002 (0.022)	Loss 0.9386 (0.9324)	Acc@1 87.500 (88.849)	Acc@5 100.000 (98.864)
Epoch: [173][20/391]	Time 0.013 (0.015)	Data 0.004 (0.013)	Loss 0.8903 (0.9477)	Acc@1 92.188 (88.170)	Acc@5 96.094 (98.884)
Epoch: [173][30/391]	Time 0.011 (0.014)	Data 0.001 (0.009)	Loss 0.8790 (0.9444)	Acc@1 90.625 (88.130)	Acc@5 99.219 (98.891)
Epoch: [173][40/391]	Time 0.013 (0.013)	Data 0.002 (0.007)	Loss 0.7919 (0.9470)	Acc@1 94.531 (87.862)	Acc@5 99.219 (98.914)
Epoch: [173][50/391]	Time 0.011 (0.013)	Data 0.002 (0.006)	Loss 0.8780 (0.9509)	Acc@1 89.844 (87.791)	Acc@5 100.000 (98.958)
Epoch: [173][60/391]	Time 0.012 (0.013)	Data 0.001 (0.005)	Loss 1.0104 (0.9531)	Acc@1 86.719 (87.731)	Acc@5 98.438 (98.988)
Epoch: [173][70/391]	Time 0.011 (0.013)	Data 0.001 (0.005)	Loss 0.9322 (0.9552)	Acc@1 89.844 (87.775)	Acc@5 100.000 (99.032)
Epoch: [173][80/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 0.8811 (0.9535)	Acc@1 88.281 (87.712)	Acc@5 100.000 (99.074)
Epoch: [173][90/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 0.9615 (0.9501)	Acc@1 89.062 (87.869)	Acc@5 99.219 (99.081)
Epoch: [173][100/391]	Time 0.013 (0.012)	Data 0.001 (0.004)	Loss 0.9624 (0.9502)	Acc@1 88.281 (87.840)	Acc@5 96.875 (99.080)
Epoch: [173][110/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.9854 (0.9497)	Acc@1 87.500 (87.845)	Acc@5 99.219 (99.120)
Epoch: [173][120/391]	Time 0.014 (0.012)	Data 0.002 (0.003)	Loss 1.1039 (0.9525)	Acc@1 81.250 (87.694)	Acc@5 97.656 (99.077)
Epoch: [173][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9418 (0.9520)	Acc@1 85.156 (87.733)	Acc@5 100.000 (99.094)
Epoch: [173][140/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9311 (0.9508)	Acc@1 89.844 (87.688)	Acc@5 97.656 (99.108)
Epoch: [173][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9610 (0.9522)	Acc@1 89.844 (87.702)	Acc@5 98.438 (99.048)
Epoch: [173][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9763 (0.9520)	Acc@1 87.500 (87.665)	Acc@5 99.219 (99.059)
Epoch: [173][170/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.0983 (0.9518)	Acc@1 82.031 (87.724)	Acc@5 96.875 (99.041)
Epoch: [173][180/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.8193 (0.9514)	Acc@1 90.625 (87.716)	Acc@5 100.000 (99.055)
Epoch: [173][190/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8888 (0.9529)	Acc@1 89.062 (87.643)	Acc@5 99.219 (99.055)
Epoch: [173][200/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.0682 (0.9547)	Acc@1 83.594 (87.574)	Acc@5 99.219 (99.075)
Epoch: [173][210/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 0.9662 (0.9588)	Acc@1 89.062 (87.470)	Acc@5 99.219 (99.052)
Epoch: [173][220/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0810 (0.9592)	Acc@1 83.594 (87.461)	Acc@5 98.438 (99.046)
Epoch: [173][230/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.1026 (0.9600)	Acc@1 82.812 (87.409)	Acc@5 98.438 (99.050)
Epoch: [173][240/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0001 (0.9606)	Acc@1 86.719 (87.400)	Acc@5 98.438 (99.044)
Epoch: [173][250/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0224 (0.9609)	Acc@1 82.031 (87.379)	Acc@5 100.000 (99.041)
Epoch: [173][260/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0001 (0.9624)	Acc@1 84.375 (87.317)	Acc@5 98.438 (99.033)
Epoch: [173][270/391]	Time 0.012 (0.012)	Data 0.003 (0.002)	Loss 0.9284 (0.9625)	Acc@1 89.844 (87.336)	Acc@5 99.219 (99.037)
Epoch: [173][280/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0398 (0.9635)	Acc@1 84.375 (87.250)	Acc@5 100.000 (99.044)
Epoch: [173][290/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9699 (0.9637)	Acc@1 89.844 (87.240)	Acc@5 97.656 (99.031)
Epoch: [173][300/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1499 (0.9654)	Acc@1 82.031 (87.196)	Acc@5 97.656 (99.006)
Epoch: [173][310/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0400 (0.9656)	Acc@1 86.719 (87.189)	Acc@5 100.000 (99.018)
Epoch: [173][320/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 0.9790 (0.9667)	Acc@1 89.062 (87.159)	Acc@5 97.656 (99.014)
Epoch: [173][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1582 (0.9689)	Acc@1 78.906 (87.085)	Acc@5 96.875 (98.997)
Epoch: [173][340/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0260 (0.9710)	Acc@1 84.375 (86.982)	Acc@5 99.219 (98.990)
Epoch: [173][350/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.9676 (0.9726)	Acc@1 86.719 (86.952)	Acc@5 99.219 (98.989)
Epoch: [173][360/391]	Time 0.018 (0.012)	Data 0.001 (0.002)	Loss 0.8738 (0.9728)	Acc@1 90.625 (86.965)	Acc@5 99.219 (98.994)
Epoch: [173][370/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.2098 (0.9758)	Acc@1 82.812 (86.872)	Acc@5 96.094 (98.966)
Epoch: [173][380/391]	Time 0.014 (0.012)	Data 0.002 (0.002)	Loss 0.9657 (0.9769)	Acc@1 89.062 (86.856)	Acc@5 99.219 (98.962)
Epoch: [173][390/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 1.0493 (0.9787)	Acc@1 86.250 (86.780)	Acc@5 98.750 (98.964)
num momentum params: 26
[0.010000000000000002, 0.978699514579773, 1.5088799822330474, 86.78, 63.3, tensor(0.5769, device='cuda:0', grad_fn=<DivBackward0>), 4.589292049407959, 0.3515625]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [174 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [174][0/391]	Time 0.061 (0.061)	Data 0.217 (0.217)	Loss 0.9737 (0.9737)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [174][10/391]	Time 0.011 (0.017)	Data 0.002 (0.021)	Loss 0.8556 (0.9422)	Acc@1 89.844 (87.429)	Acc@5 100.000 (98.935)
Epoch: [174][20/391]	Time 0.013 (0.014)	Data 0.002 (0.012)	Loss 0.8475 (0.9198)	Acc@1 92.969 (88.244)	Acc@5 100.000 (99.182)
Epoch: [174][30/391]	Time 0.010 (0.014)	Data 0.002 (0.009)	Loss 0.9066 (0.9198)	Acc@1 91.406 (88.659)	Acc@5 100.000 (99.219)
Epoch: [174][40/391]	Time 0.010 (0.013)	Data 0.002 (0.007)	Loss 0.9653 (0.9271)	Acc@1 85.156 (88.377)	Acc@5 99.219 (99.143)
Epoch: [174][50/391]	Time 0.011 (0.013)	Data 0.002 (0.006)	Loss 0.8547 (0.9342)	Acc@1 89.062 (88.220)	Acc@5 100.000 (99.112)
Epoch: [174][60/391]	Time 0.012 (0.012)	Data 0.001 (0.005)	Loss 0.8759 (0.9444)	Acc@1 91.406 (88.012)	Acc@5 99.219 (99.065)
Epoch: [174][70/391]	Time 0.012 (0.012)	Data 0.002 (0.005)	Loss 0.9944 (0.9509)	Acc@1 85.156 (87.786)	Acc@5 99.219 (99.021)
Epoch: [174][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.8878 (0.9447)	Acc@1 90.625 (87.982)	Acc@5 100.000 (99.084)
Epoch: [174][90/391]	Time 0.014 (0.012)	Data 0.002 (0.004)	Loss 1.1053 (0.9482)	Acc@1 83.594 (87.758)	Acc@5 97.656 (99.056)
Epoch: [174][100/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.7901 (0.9473)	Acc@1 92.188 (87.778)	Acc@5 100.000 (99.080)
Epoch: [174][110/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.8973 (0.9502)	Acc@1 84.375 (87.606)	Acc@5 100.000 (99.064)
Epoch: [174][120/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1384 (0.9538)	Acc@1 83.594 (87.461)	Acc@5 97.656 (99.064)
Epoch: [174][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.8876 (0.9514)	Acc@1 90.625 (87.554)	Acc@5 100.000 (99.094)
Epoch: [174][140/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9643 (0.9519)	Acc@1 88.281 (87.600)	Acc@5 100.000 (99.097)
Epoch: [174][150/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.8317 (0.9505)	Acc@1 91.406 (87.671)	Acc@5 100.000 (99.115)
Epoch: [174][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9190 (0.9531)	Acc@1 87.500 (87.612)	Acc@5 100.000 (99.093)
Epoch: [174][170/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9137 (0.9519)	Acc@1 87.500 (87.642)	Acc@5 100.000 (99.114)
Epoch: [174][180/391]	Time 0.015 (0.012)	Data 0.002 (0.003)	Loss 0.8420 (0.9547)	Acc@1 92.188 (87.560)	Acc@5 100.000 (99.085)
Epoch: [174][190/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9662 (0.9545)	Acc@1 85.938 (87.545)	Acc@5 100.000 (99.092)
Epoch: [174][200/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 1.1280 (0.9581)	Acc@1 78.125 (87.438)	Acc@5 97.656 (99.056)
Epoch: [174][210/391]	Time 0.011 (0.011)	Data 0.008 (0.003)	Loss 0.9842 (0.9609)	Acc@1 85.156 (87.315)	Acc@5 98.438 (99.048)
Epoch: [174][220/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 0.9929 (0.9623)	Acc@1 87.500 (87.299)	Acc@5 98.438 (99.028)
Epoch: [174][230/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 0.9168 (0.9635)	Acc@1 89.844 (87.240)	Acc@5 98.438 (99.033)
Epoch: [174][240/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 0.9701 (0.9643)	Acc@1 87.500 (87.192)	Acc@5 100.000 (99.034)
Epoch: [174][250/391]	Time 0.012 (0.011)	Data 0.002 (0.002)	Loss 0.9284 (0.9655)	Acc@1 86.719 (87.164)	Acc@5 99.219 (99.023)
Epoch: [174][260/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 0.9736 (0.9656)	Acc@1 87.500 (87.117)	Acc@5 100.000 (99.042)
Epoch: [174][270/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 1.0366 (0.9667)	Acc@1 82.812 (87.062)	Acc@5 99.219 (99.031)
Epoch: [174][280/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 0.9733 (0.9683)	Acc@1 85.938 (86.991)	Acc@5 98.438 (99.027)
Epoch: [174][290/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 1.0047 (0.9691)	Acc@1 83.594 (86.971)	Acc@5 96.875 (99.015)
Epoch: [174][300/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.0307 (0.9715)	Acc@1 86.719 (86.919)	Acc@5 98.438 (98.988)
Epoch: [174][310/391]	Time 0.012 (0.011)	Data 0.001 (0.002)	Loss 1.0747 (0.9726)	Acc@1 84.375 (86.874)	Acc@5 98.438 (98.978)
Epoch: [174][320/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.0667 (0.9743)	Acc@1 82.031 (86.811)	Acc@5 99.219 (98.971)
Epoch: [174][330/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 0.9533 (0.9761)	Acc@1 87.500 (86.785)	Acc@5 99.219 (98.954)
Epoch: [174][340/391]	Time 0.020 (0.011)	Data 0.002 (0.002)	Loss 1.0149 (0.9758)	Acc@1 85.938 (86.810)	Acc@5 99.219 (98.967)
Epoch: [174][350/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.0302 (0.9781)	Acc@1 85.938 (86.754)	Acc@5 98.438 (98.949)
Epoch: [174][360/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.1113 (0.9800)	Acc@1 85.156 (86.697)	Acc@5 97.656 (98.942)
Epoch: [174][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9633 (0.9816)	Acc@1 89.062 (86.656)	Acc@5 99.219 (98.943)
Epoch: [174][380/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9735 (0.9830)	Acc@1 85.156 (86.606)	Acc@5 97.656 (98.936)
Epoch: [174][390/391]	Time 0.014 (0.011)	Data 0.000 (0.002)	Loss 1.0833 (0.9842)	Acc@1 85.000 (86.594)	Acc@5 97.500 (98.922)
num momentum params: 26
[0.010000000000000002, 0.9841520011901855, 1.482192148566246, 86.594, 63.72, tensor(0.5735, device='cuda:0', grad_fn=<DivBackward0>), 4.4888176918029785, 0.37761902809143066]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [175 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [175][0/391]	Time 0.057 (0.057)	Data 0.216 (0.216)	Loss 0.8934 (0.8934)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [175][10/391]	Time 0.011 (0.017)	Data 0.001 (0.021)	Loss 0.9564 (0.9263)	Acc@1 88.281 (88.210)	Acc@5 100.000 (99.290)
Epoch: [175][20/391]	Time 0.012 (0.014)	Data 0.002 (0.012)	Loss 1.0073 (0.9428)	Acc@1 85.156 (88.021)	Acc@5 99.219 (99.033)
Epoch: [175][30/391]	Time 0.014 (0.013)	Data 0.002 (0.008)	Loss 0.9707 (0.9499)	Acc@1 87.500 (87.752)	Acc@5 99.219 (99.068)
Epoch: [175][40/391]	Time 0.013 (0.013)	Data 0.001 (0.007)	Loss 0.9713 (0.9507)	Acc@1 86.719 (87.862)	Acc@5 98.438 (99.066)
Epoch: [175][50/391]	Time 0.012 (0.013)	Data 0.002 (0.006)	Loss 0.8414 (0.9431)	Acc@1 92.969 (88.097)	Acc@5 100.000 (99.112)
Epoch: [175][60/391]	Time 0.010 (0.013)	Data 0.001 (0.005)	Loss 0.9545 (0.9427)	Acc@1 87.500 (88.128)	Acc@5 99.219 (99.091)
Epoch: [175][70/391]	Time 0.012 (0.012)	Data 0.002 (0.005)	Loss 0.8715 (0.9458)	Acc@1 91.406 (87.951)	Acc@5 99.219 (99.076)
Epoch: [175][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.0652 (0.9490)	Acc@1 84.375 (87.886)	Acc@5 100.000 (99.064)
Epoch: [175][90/391]	Time 0.015 (0.012)	Data 0.002 (0.004)	Loss 0.9118 (0.9462)	Acc@1 89.062 (87.972)	Acc@5 99.219 (99.107)
Epoch: [175][100/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.8689 (0.9491)	Acc@1 89.844 (87.864)	Acc@5 97.656 (99.103)
Epoch: [175][110/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9611 (0.9503)	Acc@1 82.812 (87.803)	Acc@5 100.000 (99.092)
Epoch: [175][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9123 (0.9506)	Acc@1 88.281 (87.797)	Acc@5 100.000 (99.070)
Epoch: [175][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0698 (0.9551)	Acc@1 85.156 (87.613)	Acc@5 99.219 (99.046)
Epoch: [175][140/391]	Time 0.015 (0.012)	Data 0.002 (0.003)	Loss 0.8954 (0.9554)	Acc@1 89.844 (87.633)	Acc@5 99.219 (99.041)
Epoch: [175][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.0161 (0.9577)	Acc@1 85.938 (87.547)	Acc@5 99.219 (99.043)
Epoch: [175][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1138 (0.9586)	Acc@1 84.375 (87.500)	Acc@5 97.656 (99.025)
Epoch: [175][170/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1037 (0.9609)	Acc@1 85.938 (87.436)	Acc@5 97.656 (98.986)
Epoch: [175][180/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.1262 (0.9631)	Acc@1 83.594 (87.358)	Acc@5 99.219 (98.999)
Epoch: [175][190/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9906 (0.9651)	Acc@1 87.500 (87.255)	Acc@5 99.219 (99.006)
Epoch: [175][200/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8425 (0.9647)	Acc@1 90.625 (87.306)	Acc@5 100.000 (99.013)
Epoch: [175][210/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8742 (0.9673)	Acc@1 90.625 (87.241)	Acc@5 100.000 (99.015)
Epoch: [175][220/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.1138 (0.9702)	Acc@1 83.594 (87.157)	Acc@5 98.438 (98.996)
Epoch: [175][230/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9214 (0.9712)	Acc@1 89.844 (87.077)	Acc@5 100.000 (98.985)
Epoch: [175][240/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9261 (0.9725)	Acc@1 86.719 (87.014)	Acc@5 98.438 (98.959)
Epoch: [175][250/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1186 (0.9736)	Acc@1 82.031 (86.962)	Acc@5 99.219 (98.967)
Epoch: [175][260/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.1420 (0.9760)	Acc@1 82.031 (86.880)	Acc@5 96.875 (98.937)
Epoch: [175][270/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9563 (0.9755)	Acc@1 89.844 (86.929)	Acc@5 98.438 (98.942)
Epoch: [175][280/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0268 (0.9773)	Acc@1 85.156 (86.886)	Acc@5 97.656 (98.927)
Epoch: [175][290/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0749 (0.9770)	Acc@1 85.938 (86.904)	Acc@5 96.094 (98.915)
Epoch: [175][300/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9729 (0.9781)	Acc@1 85.938 (86.874)	Acc@5 100.000 (98.910)
Epoch: [175][310/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0461 (0.9788)	Acc@1 86.719 (86.864)	Acc@5 98.438 (98.910)
Epoch: [175][320/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9104 (0.9794)	Acc@1 88.281 (86.860)	Acc@5 99.219 (98.888)
Epoch: [175][330/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9959 (0.9798)	Acc@1 87.500 (86.823)	Acc@5 98.438 (98.881)
Epoch: [175][340/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.0459 (0.9803)	Acc@1 85.938 (86.804)	Acc@5 98.438 (98.887)
Epoch: [175][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1667 (0.9826)	Acc@1 78.125 (86.708)	Acc@5 96.094 (98.869)
Epoch: [175][360/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9924 (0.9837)	Acc@1 89.062 (86.662)	Acc@5 99.219 (98.868)
Epoch: [175][370/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.0486 (0.9845)	Acc@1 84.375 (86.624)	Acc@5 99.219 (98.865)
Epoch: [175][380/391]	Time 0.014 (0.012)	Data 0.002 (0.002)	Loss 0.9439 (0.9848)	Acc@1 88.281 (86.626)	Acc@5 98.438 (98.856)
Epoch: [175][390/391]	Time 0.014 (0.012)	Data 0.001 (0.002)	Loss 1.0645 (0.9868)	Acc@1 85.000 (86.570)	Acc@5 98.750 (98.846)
num momentum params: 26
[0.010000000000000002, 0.98676137550354, 1.532368117570877, 86.57, 63.49, tensor(0.5719, device='cuda:0', grad_fn=<DivBackward0>), 4.504356861114502, 0.3642241954803467]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [176 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [176][0/391]	Time 0.058 (0.058)	Data 0.225 (0.225)	Loss 0.9133 (0.9133)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [176][10/391]	Time 0.012 (0.017)	Data 0.002 (0.022)	Loss 0.9366 (0.9632)	Acc@1 88.281 (87.358)	Acc@5 99.219 (99.006)
Epoch: [176][20/391]	Time 0.011 (0.015)	Data 0.002 (0.012)	Loss 1.0700 (0.9709)	Acc@1 84.375 (87.277)	Acc@5 97.656 (98.921)
Epoch: [176][30/391]	Time 0.010 (0.014)	Data 0.002 (0.009)	Loss 0.9545 (0.9740)	Acc@1 85.938 (87.097)	Acc@5 99.219 (98.916)
Epoch: [176][40/391]	Time 0.012 (0.013)	Data 0.001 (0.007)	Loss 0.9029 (0.9631)	Acc@1 90.625 (87.443)	Acc@5 99.219 (99.009)
Epoch: [176][50/391]	Time 0.014 (0.013)	Data 0.002 (0.006)	Loss 0.9036 (0.9514)	Acc@1 88.281 (87.806)	Acc@5 100.000 (99.112)
Epoch: [176][60/391]	Time 0.012 (0.013)	Data 0.002 (0.005)	Loss 0.8791 (0.9451)	Acc@1 87.500 (87.871)	Acc@5 100.000 (99.168)
Epoch: [176][70/391]	Time 0.010 (0.013)	Data 0.001 (0.005)	Loss 0.9139 (0.9495)	Acc@1 89.844 (87.764)	Acc@5 100.000 (99.186)
Epoch: [176][80/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 0.8873 (0.9442)	Acc@1 86.719 (87.809)	Acc@5 100.000 (99.238)
Epoch: [176][90/391]	Time 0.012 (0.013)	Data 0.002 (0.004)	Loss 0.9341 (0.9474)	Acc@1 88.281 (87.646)	Acc@5 99.219 (99.184)
Epoch: [176][100/391]	Time 0.012 (0.013)	Data 0.001 (0.004)	Loss 1.0336 (0.9472)	Acc@1 84.375 (87.686)	Acc@5 98.438 (99.180)
Epoch: [176][110/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 1.1046 (0.9485)	Acc@1 85.938 (87.676)	Acc@5 98.438 (99.134)
Epoch: [176][120/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.9882 (0.9502)	Acc@1 89.062 (87.636)	Acc@5 99.219 (99.115)
Epoch: [176][130/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.8322 (0.9522)	Acc@1 92.969 (87.601)	Acc@5 99.219 (99.082)
Epoch: [176][140/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0142 (0.9535)	Acc@1 85.938 (87.500)	Acc@5 99.219 (99.047)
Epoch: [176][150/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0107 (0.9575)	Acc@1 86.719 (87.329)	Acc@5 99.219 (99.043)
Epoch: [176][160/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9644 (0.9596)	Acc@1 86.719 (87.277)	Acc@5 97.656 (99.010)
Epoch: [176][170/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0922 (0.9609)	Acc@1 85.156 (87.217)	Acc@5 99.219 (99.018)
Epoch: [176][180/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 0.8821 (0.9595)	Acc@1 87.500 (87.254)	Acc@5 99.219 (99.037)
Epoch: [176][190/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 0.8871 (0.9603)	Acc@1 89.062 (87.226)	Acc@5 99.219 (99.039)
Epoch: [176][200/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0271 (0.9612)	Acc@1 85.938 (87.201)	Acc@5 99.219 (99.032)
Epoch: [176][210/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9222 (0.9601)	Acc@1 89.844 (87.230)	Acc@5 100.000 (99.037)
Epoch: [176][220/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0550 (0.9628)	Acc@1 85.156 (87.154)	Acc@5 99.219 (99.028)
Epoch: [176][230/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 1.1336 (0.9641)	Acc@1 81.250 (87.104)	Acc@5 98.438 (99.029)
Epoch: [176][240/391]	Time 0.015 (0.012)	Data 0.007 (0.003)	Loss 1.0735 (0.9664)	Acc@1 82.812 (87.085)	Acc@5 97.656 (99.015)
Epoch: [176][250/391]	Time 0.015 (0.012)	Data 0.002 (0.002)	Loss 1.0081 (0.9667)	Acc@1 85.938 (87.067)	Acc@5 97.656 (99.020)
Epoch: [176][260/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0204 (0.9668)	Acc@1 83.594 (87.042)	Acc@5 100.000 (99.033)
Epoch: [176][270/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0670 (0.9682)	Acc@1 85.938 (86.975)	Acc@5 98.438 (99.020)
Epoch: [176][280/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 0.9234 (0.9703)	Acc@1 88.281 (86.897)	Acc@5 99.219 (99.024)
Epoch: [176][290/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 0.9365 (0.9708)	Acc@1 87.500 (86.861)	Acc@5 99.219 (99.020)
Epoch: [176][300/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 0.9889 (0.9709)	Acc@1 87.500 (86.867)	Acc@5 99.219 (99.027)
Epoch: [176][310/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9927 (0.9729)	Acc@1 86.719 (86.807)	Acc@5 99.219 (99.023)
Epoch: [176][320/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.1017 (0.9752)	Acc@1 82.812 (86.750)	Acc@5 99.219 (99.012)
Epoch: [176][330/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0587 (0.9759)	Acc@1 85.156 (86.721)	Acc@5 99.219 (99.018)
Epoch: [176][340/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0182 (0.9774)	Acc@1 86.719 (86.712)	Acc@5 100.000 (98.994)
Epoch: [176][350/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9784 (0.9788)	Acc@1 88.281 (86.712)	Acc@5 99.219 (98.983)
Epoch: [176][360/391]	Time 0.012 (0.012)	Data 0.004 (0.002)	Loss 0.9169 (0.9799)	Acc@1 90.625 (86.680)	Acc@5 99.219 (98.968)
Epoch: [176][370/391]	Time 0.012 (0.012)	Data 0.001 (0.002)	Loss 1.0831 (0.9816)	Acc@1 83.594 (86.630)	Acc@5 98.438 (98.951)
Epoch: [176][380/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1346 (0.9835)	Acc@1 79.688 (86.573)	Acc@5 99.219 (98.938)
Epoch: [176][390/391]	Time 0.015 (0.012)	Data 0.000 (0.002)	Loss 0.9781 (0.9851)	Acc@1 88.750 (86.534)	Acc@5 98.750 (98.928)
num momentum params: 26
[0.010000000000000002, 0.9850556353378296, 1.5148706138134003, 86.534, 63.71, tensor(0.5730, device='cuda:0', grad_fn=<DivBackward0>), 4.555705547332764, 0.3465564250946045]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [177 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [177][0/391]	Time 0.056 (0.056)	Data 0.209 (0.209)	Loss 0.8107 (0.8107)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [177][10/391]	Time 0.011 (0.017)	Data 0.001 (0.021)	Loss 0.9035 (0.9495)	Acc@1 89.062 (88.068)	Acc@5 99.219 (99.077)
Epoch: [177][20/391]	Time 0.011 (0.015)	Data 0.002 (0.011)	Loss 0.8882 (0.9342)	Acc@1 86.719 (88.132)	Acc@5 99.219 (99.144)
Epoch: [177][30/391]	Time 0.011 (0.014)	Data 0.001 (0.008)	Loss 0.9443 (0.9389)	Acc@1 87.500 (88.180)	Acc@5 98.438 (99.168)
Epoch: [177][40/391]	Time 0.011 (0.013)	Data 0.002 (0.007)	Loss 0.8985 (0.9346)	Acc@1 89.062 (88.300)	Acc@5 99.219 (99.219)
Epoch: [177][50/391]	Time 0.011 (0.013)	Data 0.001 (0.006)	Loss 0.8140 (0.9377)	Acc@1 96.094 (88.312)	Acc@5 100.000 (99.234)
Epoch: [177][60/391]	Time 0.011 (0.013)	Data 0.002 (0.005)	Loss 0.8741 (0.9339)	Acc@1 90.625 (88.358)	Acc@5 100.000 (99.283)
Epoch: [177][70/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 0.8434 (0.9303)	Acc@1 91.406 (88.435)	Acc@5 99.219 (99.329)
Epoch: [177][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.9053 (0.9306)	Acc@1 91.406 (88.484)	Acc@5 98.438 (99.267)
Epoch: [177][90/391]	Time 0.023 (0.012)	Data 0.002 (0.004)	Loss 0.9657 (0.9265)	Acc@1 88.281 (88.565)	Acc@5 100.000 (99.296)
Epoch: [177][100/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.0096 (0.9274)	Acc@1 85.156 (88.529)	Acc@5 99.219 (99.319)
Epoch: [177][110/391]	Time 0.010 (0.012)	Data 0.002 (0.003)	Loss 1.0383 (0.9286)	Acc@1 85.156 (88.478)	Acc@5 98.438 (99.331)
Epoch: [177][120/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 1.0643 (0.9329)	Acc@1 82.031 (88.288)	Acc@5 97.656 (99.296)
Epoch: [177][130/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9169 (0.9311)	Acc@1 85.938 (88.359)	Acc@5 99.219 (99.284)
Epoch: [177][140/391]	Time 0.017 (0.012)	Data 0.001 (0.003)	Loss 0.9916 (0.9315)	Acc@1 83.594 (88.314)	Acc@5 99.219 (99.296)
Epoch: [177][150/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 0.8848 (0.9329)	Acc@1 89.062 (88.224)	Acc@5 100.000 (99.291)
Epoch: [177][160/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.8880 (0.9364)	Acc@1 91.406 (88.102)	Acc@5 99.219 (99.296)
Epoch: [177][170/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 0.9235 (0.9375)	Acc@1 89.062 (88.080)	Acc@5 99.219 (99.292)
Epoch: [177][180/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.8722 (0.9376)	Acc@1 88.281 (88.091)	Acc@5 100.000 (99.292)
Epoch: [177][190/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0042 (0.9380)	Acc@1 85.938 (88.109)	Acc@5 100.000 (99.292)
Epoch: [177][200/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0791 (0.9390)	Acc@1 85.156 (88.087)	Acc@5 98.438 (99.277)
Epoch: [177][210/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9335 (0.9401)	Acc@1 86.719 (88.070)	Acc@5 100.000 (99.278)
Epoch: [177][220/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.8721 (0.9405)	Acc@1 90.625 (88.037)	Acc@5 99.219 (99.275)
Epoch: [177][230/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9161 (0.9418)	Acc@1 91.406 (87.960)	Acc@5 98.438 (99.259)
Epoch: [177][240/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0464 (0.9433)	Acc@1 82.031 (87.899)	Acc@5 99.219 (99.245)
Epoch: [177][250/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0413 (0.9441)	Acc@1 84.375 (87.874)	Acc@5 99.219 (99.253)
Epoch: [177][260/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9663 (0.9466)	Acc@1 86.719 (87.778)	Acc@5 99.219 (99.222)
Epoch: [177][270/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 0.9524 (0.9479)	Acc@1 88.281 (87.728)	Acc@5 100.000 (99.210)
Epoch: [177][280/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0458 (0.9491)	Acc@1 82.812 (87.667)	Acc@5 98.438 (99.208)
Epoch: [177][290/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9192 (0.9511)	Acc@1 91.406 (87.610)	Acc@5 100.000 (99.195)
Epoch: [177][300/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.1361 (0.9533)	Acc@1 85.938 (87.534)	Acc@5 96.094 (99.172)
Epoch: [177][310/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0376 (0.9551)	Acc@1 82.812 (87.457)	Acc@5 99.219 (99.171)
Epoch: [177][320/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0635 (0.9589)	Acc@1 85.938 (87.352)	Acc@5 97.656 (99.136)
Epoch: [177][330/391]	Time 0.012 (0.012)	Data 0.000 (0.002)	Loss 1.1249 (0.9615)	Acc@1 85.156 (87.266)	Acc@5 98.438 (99.115)
Epoch: [177][340/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.9986 (0.9631)	Acc@1 87.500 (87.223)	Acc@5 97.656 (99.093)
Epoch: [177][350/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9319 (0.9661)	Acc@1 87.500 (87.115)	Acc@5 99.219 (99.058)
Epoch: [177][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0505 (0.9685)	Acc@1 84.375 (87.030)	Acc@5 96.875 (99.037)
Epoch: [177][370/391]	Time 0.014 (0.012)	Data 0.002 (0.002)	Loss 0.9425 (0.9693)	Acc@1 90.625 (87.005)	Acc@5 97.656 (99.027)
Epoch: [177][380/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9035 (0.9708)	Acc@1 87.500 (86.979)	Acc@5 99.219 (99.020)
Epoch: [177][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 1.3863 (0.9735)	Acc@1 78.750 (86.894)	Acc@5 95.000 (99.002)
num momentum params: 26
[0.010000000000000002, 0.9734698080062866, 1.554138400554657, 86.894, 62.83, tensor(0.5807, device='cuda:0', grad_fn=<DivBackward0>), 4.529982328414917, 0.38923072814941406]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [178 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [178][0/391]	Time 0.059 (0.059)	Data 0.199 (0.199)	Loss 0.9383 (0.9383)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [178][10/391]	Time 0.010 (0.017)	Data 0.002 (0.020)	Loss 0.8938 (0.9633)	Acc@1 91.406 (87.074)	Acc@5 99.219 (99.432)
Epoch: [178][20/391]	Time 0.011 (0.015)	Data 0.001 (0.011)	Loss 0.8453 (0.9464)	Acc@1 91.406 (87.388)	Acc@5 99.219 (99.442)
Epoch: [178][30/391]	Time 0.011 (0.013)	Data 0.002 (0.008)	Loss 0.8475 (0.9375)	Acc@1 89.062 (87.853)	Acc@5 100.000 (99.420)
Epoch: [178][40/391]	Time 0.010 (0.013)	Data 0.003 (0.006)	Loss 1.0116 (0.9381)	Acc@1 85.156 (87.710)	Acc@5 98.438 (99.371)
Epoch: [178][50/391]	Time 0.011 (0.012)	Data 0.002 (0.005)	Loss 0.9631 (0.9377)	Acc@1 86.719 (87.806)	Acc@5 98.438 (99.341)
Epoch: [178][60/391]	Time 0.010 (0.012)	Data 0.001 (0.005)	Loss 0.9857 (0.9370)	Acc@1 88.281 (87.743)	Acc@5 99.219 (99.372)
Epoch: [178][70/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.0164 (0.9402)	Acc@1 86.719 (87.654)	Acc@5 99.219 (99.351)
Epoch: [178][80/391]	Time 0.010 (0.012)	Data 0.002 (0.004)	Loss 0.8304 (0.9403)	Acc@1 92.188 (87.760)	Acc@5 100.000 (99.354)
Epoch: [178][90/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.8265 (0.9458)	Acc@1 90.625 (87.594)	Acc@5 100.000 (99.322)
Epoch: [178][100/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.8971 (0.9487)	Acc@1 88.281 (87.407)	Acc@5 100.000 (99.296)
Epoch: [178][110/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.1578 (0.9506)	Acc@1 80.469 (87.296)	Acc@5 98.438 (99.289)
Epoch: [178][120/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9469 (0.9494)	Acc@1 88.281 (87.351)	Acc@5 98.438 (99.270)
Epoch: [178][130/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 1.0857 (0.9509)	Acc@1 85.938 (87.428)	Acc@5 97.656 (99.237)
Epoch: [178][140/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.0419 (0.9555)	Acc@1 81.250 (87.245)	Acc@5 100.000 (99.208)
Epoch: [178][150/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 0.8813 (0.9560)	Acc@1 90.625 (87.241)	Acc@5 99.219 (99.188)
Epoch: [178][160/391]	Time 0.011 (0.011)	Data 0.002 (0.003)	Loss 0.9843 (0.9599)	Acc@1 87.500 (87.151)	Acc@5 98.438 (99.156)
Epoch: [178][170/391]	Time 0.013 (0.011)	Data 0.002 (0.003)	Loss 1.0059 (0.9628)	Acc@1 85.156 (87.034)	Acc@5 98.438 (99.141)
Epoch: [178][180/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 0.9512 (0.9637)	Acc@1 89.844 (86.986)	Acc@5 97.656 (99.137)
Epoch: [178][190/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 1.0337 (0.9663)	Acc@1 84.375 (86.919)	Acc@5 97.656 (99.092)
Epoch: [178][200/391]	Time 0.012 (0.011)	Data 0.001 (0.003)	Loss 0.9973 (0.9666)	Acc@1 87.500 (86.929)	Acc@5 98.438 (99.059)
Epoch: [178][210/391]	Time 0.010 (0.011)	Data 0.002 (0.003)	Loss 1.0950 (0.9694)	Acc@1 87.500 (86.826)	Acc@5 99.219 (99.067)
Epoch: [178][220/391]	Time 0.011 (0.011)	Data 0.001 (0.003)	Loss 0.9348 (0.9700)	Acc@1 87.500 (86.828)	Acc@5 100.000 (99.074)
Epoch: [178][230/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 0.9468 (0.9706)	Acc@1 87.500 (86.830)	Acc@5 100.000 (99.067)
Epoch: [178][240/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.0438 (0.9697)	Acc@1 85.938 (86.878)	Acc@5 99.219 (99.070)
Epoch: [178][250/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 1.0701 (0.9692)	Acc@1 84.375 (86.937)	Acc@5 100.000 (99.063)
Epoch: [178][260/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 0.9606 (0.9694)	Acc@1 87.500 (86.955)	Acc@5 99.219 (99.060)
Epoch: [178][270/391]	Time 0.010 (0.011)	Data 0.005 (0.002)	Loss 1.0380 (0.9709)	Acc@1 85.156 (86.895)	Acc@5 99.219 (99.060)
Epoch: [178][280/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 0.8703 (0.9719)	Acc@1 90.625 (86.866)	Acc@5 98.438 (99.041)
Epoch: [178][290/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.0057 (0.9728)	Acc@1 84.375 (86.840)	Acc@5 99.219 (99.034)
Epoch: [178][300/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 0.9990 (0.9741)	Acc@1 85.156 (86.807)	Acc@5 100.000 (99.024)
Epoch: [178][310/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0100 (0.9743)	Acc@1 85.156 (86.809)	Acc@5 98.438 (99.023)
Epoch: [178][320/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.0242 (0.9767)	Acc@1 85.938 (86.728)	Acc@5 97.656 (99.007)
Epoch: [178][330/391]	Time 0.010 (0.011)	Data 0.002 (0.002)	Loss 1.0913 (0.9788)	Acc@1 82.812 (86.662)	Acc@5 97.656 (98.987)
Epoch: [178][340/391]	Time 0.011 (0.011)	Data 0.001 (0.002)	Loss 0.9312 (0.9806)	Acc@1 88.281 (86.597)	Acc@5 100.000 (98.980)
Epoch: [178][350/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 1.0811 (0.9808)	Acc@1 82.812 (86.592)	Acc@5 96.094 (98.963)
Epoch: [178][360/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 0.9715 (0.9828)	Acc@1 85.938 (86.524)	Acc@5 100.000 (98.966)
Epoch: [178][370/391]	Time 0.011 (0.011)	Data 0.002 (0.002)	Loss 0.9894 (0.9832)	Acc@1 85.156 (86.510)	Acc@5 98.438 (98.962)
Epoch: [178][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.1619 (0.9850)	Acc@1 78.125 (86.432)	Acc@5 97.656 (98.956)
Epoch: [178][390/391]	Time 0.014 (0.011)	Data 0.001 (0.002)	Loss 0.9520 (0.9858)	Acc@1 88.750 (86.404)	Acc@5 98.750 (98.962)
num momentum params: 26
[0.010000000000000002, 0.9857722864341736, 1.5886929500102998, 86.404, 62.37, tensor(0.5734, device='cuda:0', grad_fn=<DivBackward0>), 4.346934080123901, 0.36650848388671875]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [179 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [179][0/391]	Time 0.061 (0.061)	Data 0.204 (0.204)	Loss 0.8947 (0.8947)	Acc@1 89.844 (89.844)	Acc@5 99.219 (99.219)
Epoch: [179][10/391]	Time 0.014 (0.018)	Data 0.002 (0.020)	Loss 0.8488 (0.9190)	Acc@1 90.625 (88.778)	Acc@5 99.219 (99.219)
Epoch: [179][20/391]	Time 0.011 (0.015)	Data 0.000 (0.011)	Loss 0.9462 (0.9172)	Acc@1 90.625 (88.765)	Acc@5 99.219 (99.330)
Epoch: [179][30/391]	Time 0.012 (0.014)	Data 0.002 (0.008)	Loss 0.8891 (0.9329)	Acc@1 91.406 (88.130)	Acc@5 99.219 (99.269)
Epoch: [179][40/391]	Time 0.010 (0.014)	Data 0.002 (0.007)	Loss 0.9150 (0.9315)	Acc@1 88.281 (88.224)	Acc@5 100.000 (99.181)
Epoch: [179][50/391]	Time 0.012 (0.013)	Data 0.002 (0.006)	Loss 0.9224 (0.9297)	Acc@1 86.719 (88.082)	Acc@5 100.000 (99.219)
Epoch: [179][60/391]	Time 0.010 (0.013)	Data 0.002 (0.005)	Loss 0.9760 (0.9349)	Acc@1 86.719 (87.974)	Acc@5 99.219 (99.116)
Epoch: [179][70/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 0.9702 (0.9381)	Acc@1 89.844 (88.017)	Acc@5 99.219 (99.087)
Epoch: [179][80/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 1.0783 (0.9378)	Acc@1 82.031 (87.973)	Acc@5 100.000 (99.084)
Epoch: [179][90/391]	Time 0.010 (0.012)	Data 0.001 (0.004)	Loss 0.9065 (0.9355)	Acc@1 89.062 (88.041)	Acc@5 99.219 (99.124)
Epoch: [179][100/391]	Time 0.011 (0.012)	Data 0.002 (0.004)	Loss 0.9579 (0.9353)	Acc@1 85.938 (88.065)	Acc@5 97.656 (99.087)
Epoch: [179][110/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.0825 (0.9385)	Acc@1 84.375 (88.000)	Acc@5 97.656 (99.064)
Epoch: [179][120/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9619 (0.9357)	Acc@1 89.062 (88.029)	Acc@5 100.000 (99.128)
Epoch: [179][130/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9872 (0.9358)	Acc@1 85.156 (88.043)	Acc@5 97.656 (99.123)
Epoch: [179][140/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.8418 (0.9333)	Acc@1 92.188 (88.220)	Acc@5 99.219 (99.147)
Epoch: [179][150/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.9704 (0.9331)	Acc@1 86.719 (88.307)	Acc@5 97.656 (99.131)
Epoch: [179][160/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9487 (0.9351)	Acc@1 85.938 (88.204)	Acc@5 100.000 (99.151)
Epoch: [179][170/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9336 (0.9353)	Acc@1 87.500 (88.217)	Acc@5 100.000 (99.155)
Epoch: [179][180/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0139 (0.9373)	Acc@1 84.375 (88.152)	Acc@5 99.219 (99.150)
Epoch: [179][190/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 1.0480 (0.9390)	Acc@1 81.250 (88.056)	Acc@5 98.438 (99.141)
Epoch: [179][200/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0116 (0.9391)	Acc@1 86.719 (88.040)	Acc@5 97.656 (99.129)
Epoch: [179][210/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9562 (0.9416)	Acc@1 86.719 (87.996)	Acc@5 98.438 (99.111)
Epoch: [179][220/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8839 (0.9420)	Acc@1 88.281 (87.974)	Acc@5 99.219 (99.113)
Epoch: [179][230/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 0.9747 (0.9432)	Acc@1 87.500 (87.916)	Acc@5 99.219 (99.111)
Epoch: [179][240/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 0.9945 (0.9451)	Acc@1 86.719 (87.876)	Acc@5 97.656 (99.089)
Epoch: [179][250/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 0.8316 (0.9481)	Acc@1 92.969 (87.777)	Acc@5 99.219 (99.057)
Epoch: [179][260/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 0.9101 (0.9482)	Acc@1 91.406 (87.802)	Acc@5 98.438 (99.063)
Epoch: [179][270/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 0.9099 (0.9493)	Acc@1 90.625 (87.771)	Acc@5 98.438 (99.054)
Epoch: [179][280/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 0.8625 (0.9508)	Acc@1 91.406 (87.756)	Acc@5 99.219 (99.041)
Epoch: [179][290/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1232 (0.9535)	Acc@1 82.031 (87.656)	Acc@5 96.875 (99.020)
Epoch: [179][300/391]	Time 0.015 (0.012)	Data 0.002 (0.002)	Loss 0.8691 (0.9555)	Acc@1 91.406 (87.619)	Acc@5 100.000 (99.006)
Epoch: [179][310/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1024 (0.9576)	Acc@1 81.250 (87.530)	Acc@5 97.656 (98.998)
Epoch: [179][320/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 1.0854 (0.9593)	Acc@1 83.594 (87.478)	Acc@5 96.875 (98.988)
Epoch: [179][330/391]	Time 0.012 (0.012)	Data 0.002 (0.002)	Loss 1.1206 (0.9607)	Acc@1 81.250 (87.474)	Acc@5 97.656 (98.976)
Epoch: [179][340/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.2031 (0.9630)	Acc@1 80.469 (87.385)	Acc@5 96.875 (98.974)
Epoch: [179][350/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 1.0923 (0.9649)	Acc@1 80.469 (87.344)	Acc@5 98.438 (98.965)
Epoch: [179][360/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0031 (0.9661)	Acc@1 84.375 (87.303)	Acc@5 98.438 (98.963)
Epoch: [179][370/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 1.0629 (0.9689)	Acc@1 84.375 (87.230)	Acc@5 98.438 (98.966)
Epoch: [179][380/391]	Time 0.010 (0.011)	Data 0.001 (0.002)	Loss 1.0390 (0.9720)	Acc@1 83.594 (87.121)	Acc@5 100.000 (98.960)
Epoch: [179][390/391]	Time 0.015 (0.011)	Data 0.000 (0.002)	Loss 1.1367 (0.9739)	Acc@1 78.750 (87.046)	Acc@5 98.750 (98.956)
num momentum params: 26
[0.010000000000000002, 0.9738847759628296, 1.5131166768074036, 87.046, 63.39, tensor(0.5815, device='cuda:0', grad_fn=<DivBackward0>), 4.482492923736572, 0.3609001636505127]
Non Pruning Epoch - module.conv1.weight: [32, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [32]
Non Pruning Epoch - module.bn1.bias: [32]
Non Pruning Epoch - module.conv2.weight: [108, 32, 3, 3]
Non Pruning Epoch - module.bn2.weight: [108]
Non Pruning Epoch - module.bn2.bias: [108]
Non Pruning Epoch - module.conv3.weight: [202, 108, 3, 3]
Non Pruning Epoch - module.bn3.weight: [202]
Non Pruning Epoch - module.bn3.bias: [202]
Non Pruning Epoch - module.conv4.weight: [247, 202, 3, 3]
Non Pruning Epoch - module.bn4.weight: [247]
Non Pruning Epoch - module.bn4.bias: [247]
Non Pruning Epoch - module.conv5.weight: [356, 247, 3, 3]
Non Pruning Epoch - module.bn5.weight: [356]
Non Pruning Epoch - module.bn5.bias: [356]
Non Pruning Epoch - module.conv6.weight: [303, 356, 3, 3]
Non Pruning Epoch - module.bn6.weight: [303]
Non Pruning Epoch - module.bn6.bias: [303]
Non Pruning Epoch - module.conv7.weight: [188, 303, 3, 3]
Non Pruning Epoch - module.bn7.weight: [188]
Non Pruning Epoch - module.bn7.bias: [188]
Non Pruning Epoch - module.conv8.weight: [186, 188, 3, 3]
Non Pruning Epoch - module.bn8.weight: [186]
Non Pruning Epoch - module.bn8.bias: [186]
Non Pruning Epoch - module.fc.weight: [100, 186]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [180 | 180] LR: 0.010000
module.conv1.weight [32, 3, 3, 3]
module.conv2.weight [108, 32, 3, 3]
module.conv3.weight [202, 108, 3, 3]
module.conv4.weight [247, 202, 3, 3]
module.conv5.weight [356, 247, 3, 3]
module.conv6.weight [303, 356, 3, 3]
module.conv7.weight [188, 303, 3, 3]
module.conv8.weight [186, 188, 3, 3]
Epoch: [180][0/391]	Time 0.068 (0.068)	Data 0.210 (0.210)	Loss 0.9776 (0.9776)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [180][10/391]	Time 0.011 (0.018)	Data 0.002 (0.021)	Loss 0.9442 (0.9112)	Acc@1 83.594 (87.642)	Acc@5 99.219 (99.219)
Epoch: [180][20/391]	Time 0.010 (0.015)	Data 0.002 (0.012)	Loss 0.8564 (0.9236)	Acc@1 90.625 (88.690)	Acc@5 99.219 (99.070)
Epoch: [180][30/391]	Time 0.012 (0.014)	Data 0.002 (0.008)	Loss 0.8639 (0.9092)	Acc@1 89.062 (89.037)	Acc@5 99.219 (99.269)
Epoch: [180][40/391]	Time 0.012 (0.014)	Data 0.001 (0.007)	Loss 0.9372 (0.9088)	Acc@1 89.844 (88.967)	Acc@5 98.438 (99.333)
Epoch: [180][50/391]	Time 0.011 (0.014)	Data 0.001 (0.006)	Loss 0.9718 (0.9012)	Acc@1 85.156 (89.154)	Acc@5 99.219 (99.372)
Epoch: [180][60/391]	Time 0.013 (0.013)	Data 0.001 (0.005)	Loss 0.9358 (0.9030)	Acc@1 85.156 (89.037)	Acc@5 99.219 (99.347)
Epoch: [180][70/391]	Time 0.012 (0.013)	Data 0.002 (0.005)	Loss 0.9265 (0.9046)	Acc@1 85.156 (88.963)	Acc@5 100.000 (99.329)
Epoch: [180][80/391]	Time 0.011 (0.013)	Data 0.001 (0.004)	Loss 0.8847 (0.9021)	Acc@1 92.188 (89.149)	Acc@5 100.000 (99.334)
Epoch: [180][90/391]	Time 0.011 (0.013)	Data 0.002 (0.004)	Loss 0.9304 (0.9067)	Acc@1 89.062 (88.882)	Acc@5 100.000 (99.330)
Epoch: [180][100/391]	Time 0.011 (0.012)	Data 0.001 (0.004)	Loss 0.8524 (0.9088)	Acc@1 90.625 (88.892)	Acc@5 100.000 (99.358)
Epoch: [180][110/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.8552 (0.9083)	Acc@1 89.062 (88.936)	Acc@5 100.000 (99.360)
Epoch: [180][120/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 0.8822 (0.9133)	Acc@1 90.625 (88.785)	Acc@5 99.219 (99.329)
Epoch: [180][130/391]	Time 0.010 (0.012)	Data 0.001 (0.003)	Loss 1.0031 (0.9154)	Acc@1 84.375 (88.711)	Acc@5 99.219 (99.332)
Epoch: [180][140/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9859 (0.9176)	Acc@1 85.938 (88.647)	Acc@5 100.000 (99.330)
Epoch: [180][150/391]	Time 0.011 (0.012)	Data 0.001 (0.003)	Loss 0.9378 (0.9175)	Acc@1 88.281 (88.669)	Acc@5 99.219 (99.317)
Epoch: [180][160/391]	Time 0.011 (0.012)	Data 0.002 (0.003)	Loss 0.9412 (0.9203)	Acc@1 85.938 (88.626)	Acc@5 99.219 (99.292)
Epoch: [180][170/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.8777 (0.9219)	Acc@1 90.625 (88.560)	Acc@5 99.219 (99.269)
Epoch: [180][180/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 0.9131 (0.9237)	Acc@1 89.844 (88.532)	Acc@5 100.000 (99.271)
Epoch: [180][190/391]	Time 0.013 (0.012)	Data 0.002 (0.003)	Loss 0.9975 (0.9238)	Acc@1 86.719 (88.527)	Acc@5 98.438 (99.264)
Epoch: [180][200/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 1.0217 (0.9263)	Acc@1 84.375 (88.444)	Acc@5 98.438 (99.254)
Epoch: [180][210/391]	Time 0.014 (0.012)	Data 0.001 (0.003)	Loss 0.9523 (0.9280)	Acc@1 88.281 (88.355)	Acc@5 98.438 (99.237)
Epoch: [180][220/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9821 (0.9284)	Acc@1 89.062 (88.302)	Acc@5 98.438 (99.247)
Epoch: [180][230/391]	Time 0.012 (0.012)	Data 0.002 (0.003)	Loss 0.9730 (0.9285)	Acc@1 85.156 (88.329)	Acc@5 100.000 (99.253)
Epoch: [180][240/391]	Time 0.012 (0.012)	Data 0.001 (0.003)	Loss 1.0407 (0.9315)	Acc@1 84.375 (88.249)	Acc@5 98.438 (99.238)
Epoch: [180][250/391]	Time 0.013 (0.012)	Data 0.000 (0.002)	Loss 0.9691 (0.9355)	Acc@1 89.062 (88.091)	Acc@5 99.219 (99.241)
Epoch: [180][260/391]	Time 0.016 (0.012)	Data 0.002 (0.002)	Loss 1.1056 (0.9376)	Acc@1 83.594 (88.018)	Acc@5 97.656 (99.225)
Epoch: [180][270/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.8949 (0.9396)	Acc@1 89.844 (87.970)	Acc@5 100.000 (99.219)
Epoch: [180][280/391]	Time 0.016 (0.012)	Data 0.002 (0.002)	Loss 0.9663 (0.9405)	Acc@1 88.281 (87.900)	Acc@5 99.219 (99.219)
Epoch: [180][290/391]	Time 0.016 (0.012)	Data 0.002 (0.002)	Loss 1.0988 (0.9435)	Acc@1 81.250 (87.830)	Acc@5 98.438 (99.203)
Epoch: [180][300/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 1.0962 (0.9458)	Acc@1 81.250 (87.752)	Acc@5 98.438 (99.182)
Epoch: [180][310/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0097 (0.9486)	Acc@1 84.375 (87.656)	Acc@5 98.438 (99.138)
Epoch: [180][320/391]	Time 0.012 (0.012)	Data 0.000 (0.002)	Loss 0.9987 (0.9510)	Acc@1 87.500 (87.612)	Acc@5 98.438 (99.134)
Epoch: [180][330/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.1905 (0.9533)	Acc@1 78.906 (87.559)	Acc@5 98.438 (99.124)
Epoch: [180][340/391]	Time 0.013 (0.012)	Data 0.002 (0.002)	Loss 1.0985 (0.9551)	Acc@1 83.594 (87.511)	Acc@5 100.000 (99.116)
Epoch: [180][350/391]	Time 0.011 (0.012)	Data 0.001 (0.002)	Loss 0.9854 (0.9572)	Acc@1 87.500 (87.478)	Acc@5 98.438 (99.096)
Epoch: [180][360/391]	Time 0.010 (0.012)	Data 0.001 (0.002)	Loss 1.0621 (0.9583)	Acc@1 88.281 (87.455)	Acc@5 97.656 (99.089)
Epoch: [180][370/391]	Time 0.010 (0.012)	Data 0.002 (0.002)	Loss 0.9337 (0.9599)	Acc@1 87.500 (87.376)	Acc@5 100.000 (99.090)
Epoch: [180][380/391]	Time 0.011 (0.012)	Data 0.002 (0.002)	Loss 1.0122 (0.9612)	Acc@1 83.594 (87.322)	Acc@5 99.219 (99.073)
Epoch: [180][390/391]	Time 0.013 (0.012)	Data 0.001 (0.002)	Loss 1.1107 (0.9632)	Acc@1 78.750 (87.260)	Acc@5 98.750 (99.056)
num momentum params: 26
[0.010000000000000002, 0.9631825969696045, 1.570161761045456, 87.26, 62.81, tensor(0.5885, device='cuda:0', grad_fn=<DivBackward0>), 4.617851257324219, 0.35480523109436035]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [32, 3, 3, 3]
Before - module.bn1.weight: [32]
Before - module.bn1.bias: [32]
Before - module.conv2.weight: [108, 32, 3, 3]
Before - module.bn2.weight: [108]
Before - module.bn2.bias: [108]
Before - module.conv3.weight: [202, 108, 3, 3]
Before - module.bn3.weight: [202]
Before - module.bn3.bias: [202]
Before - module.conv4.weight: [247, 202, 3, 3]
Before - module.bn4.weight: [247]
Before - module.bn4.bias: [247]
Before - module.conv5.weight: [356, 247, 3, 3]
Before - module.bn5.weight: [356]
Before - module.bn5.bias: [356]
Before - module.conv6.weight: [303, 356, 3, 3]
Before - module.bn6.weight: [303]
Before - module.bn6.bias: [303]
Before - module.conv7.weight: [188, 303, 3, 3]
Before - module.bn7.weight: [188]
Before - module.bn7.bias: [188]
Before - module.conv8.weight: [186, 188, 3, 3]
Before - module.bn8.weight: [186]
Before - module.bn8.bias: [186]
Before - module.fc.weight: [100, 186]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [32, 3, 3, 3] >> [32, 3, 3, 3]
[module.bn1.weight]: 32 >> 32
running_mean [32]
running_var [32]
num_batches_tracked []
[module.conv2.weight]: [108, 32, 3, 3] >> [108, 32, 3, 3]
[module.bn2.weight]: 108 >> 108
running_mean [108]
running_var [108]
num_batches_tracked []
[module.conv3.weight]: [202, 108, 3, 3] >> [202, 108, 3, 3]
[module.bn3.weight]: 202 >> 202
running_mean [202]
running_var [202]
num_batches_tracked []
[module.conv4.weight]: [247, 202, 3, 3] >> [247, 202, 3, 3]
[module.bn4.weight]: 247 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv5.weight]: [356, 247, 3, 3] >> [356, 247, 3, 3]
[module.bn5.weight]: 356 >> 356
running_mean [356]
running_var [356]
num_batches_tracked []
[module.conv6.weight]: [303, 356, 3, 3] >> [303, 356, 3, 3]
[module.bn6.weight]: 303 >> 303
running_mean [303]
running_var [303]
num_batches_tracked []
[module.conv7.weight]: [188, 303, 3, 3] >> [188, 303, 3, 3]
[module.bn7.weight]: 188 >> 188
running_mean [188]
running_var [188]
num_batches_tracked []
[module.conv8.weight]: [186, 188, 3, 3] >> [186, 188, 3, 3]
[module.bn8.weight]: 186 >> 186
running_mean [186]
running_var [186]
num_batches_tracked []
[module.fc.weight]: [100, 186] >> [100, 186]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [32, 3, 3, 3]
After - module.bn1.weight: [32]
After - module.bn1.bias: [32]
After - module.conv2.weight: [108, 32, 3, 3]
After - module.bn2.weight: [108]
After - module.bn2.bias: [108]
After - module.conv3.weight: [202, 108, 3, 3]
After - module.bn3.weight: [202]
After - module.bn3.bias: [202]
After - module.conv4.weight: [247, 202, 3, 3]
After - module.bn4.weight: [247]
After - module.bn4.bias: [247]
After - module.conv5.weight: [356, 247, 3, 3]
After - module.bn5.weight: [356]
After - module.bn5.bias: [356]
After - module.conv6.weight: [303, 356, 3, 3]
After - module.bn6.weight: [303]
After - module.bn6.bias: [303]
After - module.conv7.weight: [188, 303, 3, 3]
After - module.bn7.weight: [188]
After - module.bn7.bias: [188]
After - module.conv8.weight: [186, 188, 3, 3]
After - module.bn8.weight: [186]
After - module.bn8.bias: [186]
After - module.fc.weight: [100, 186]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [32, 3, 3, 3]
conv2 --> [108, 32, 3, 3]
conv3 --> [202, 108, 3, 3]
conv4 --> [247, 202, 3, 3]
conv5 --> [356, 247, 3, 3]
conv6 --> [303, 356, 3, 3]
conv7 --> [188, 303, 3, 3]
conv8 --> [186, 188, 3, 3]
fc --> [186, 100]
1, 354336768, 884736, 32
2, 3328376832, 7962624, 108
3, 5730103296, 12566016, 202
4, 13104958464, 28738944, 247
5, 6888241152, 12662208, 356
6, 8449947648, 15532992, 303
7, 1574940672, 2050704, 188
8, 966795264, 1258848, 186
fc, 7142400, 18600, 0
===================
FLOP REPORT: 15783141600000.0 40476800000.0 81675672 101192 1622 6.270130157470703
[INFO] Storing checkpoint...
Best acc:
67.37

Total time:
1549.297494
[2021-06-20T05:02:56.156753] Command finished with return code 0


[2021-06-20T05:02:56.157190] The experiment completed successfully. Finalizing run...
Cleaning up all outstanding Run operations, waiting 900.0 seconds
1 items cleaning up...
Cleanup took 0.08638763427734375 seconds
[2021-06-20T05:02:56.430901] Finished context manager injector.
2021/06/20 05:02:57 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status
2021/06/20 05:02:57 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 2
FilteredData: 0.
2021/06/20 05:02:57 Process Exiting with Code:  0
2021/06/20 05:02:57 All App Insights Logs was send successfully
