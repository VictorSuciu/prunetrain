bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 02:15:59 Starting App Insight Logger for task:  runTaskLet
2021/06/20 02:15:59 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612
2021/06/20 02:15:59 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info
bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 02:15:59 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status
[2021-06-20T02:15:59.523709] Entering context manager injector.
[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['python run-script.py --data-path PLACEHOLDER --dataset cifar100 --model vgg11 --num-gpus 1'])
Script type = COMMAND
[2021-06-20T02:16:00.137126] Command=python run-script.py --data-path PLACEHOLDER --dataset cifar100 --model vgg11 --num-gpus 1
[2021-06-20T02:16:00.137414] Entering Run History Context Manager.
[2021-06-20T02:16:00.705017] Command Working Directory=/mnt/batch/tasks/shared/LS_root/jobs/prunetrain/azureml/dynamic-tensor1/wd/azureml/Dynamic-Tensor1
[2021-06-20T02:16:00.705274] Starting Linux command : python run-script.py --data-path PLACEHOLDER --dataset cifar100 --model vgg11 --num-gpus 1
python src/cifar.py --workers 4 --dataset cifar100 --epochs 10 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_10.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/data/torch/cifar-100-python.tar.gz
0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%2021/06/20 02:16:04 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 1
FilteredData: 0.
Extracting ./dataset/data/torch/cifar-100-python.tar.gz to ./dataset/data/torch
==> creating model 'vgg11_bn_flat'
    Total params: 9.27M
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375

Epoch: [1 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [1][0/196]	Time 0.624 (0.624)	Data 0.166 (0.166)	Loss 6.0462 (6.0462)	Acc@1 0.391 (0.391)	Acc@5 7.422 (7.422)
Epoch: [1][10/196]	Time 0.015 (0.072)	Data 0.005 (0.017)	Loss 6.2906 (6.2328)	Acc@1 4.297 (2.379)	Acc@5 11.719 (9.943)
Epoch: [1][20/196]	Time 0.019 (0.047)	Data 0.001 (0.010)	Loss 7.7679 (6.6311)	Acc@1 4.297 (2.641)	Acc@5 7.812 (10.063)
Epoch: [1][30/196]	Time 0.015 (0.037)	Data 0.002 (0.009)	Loss 6.5262 (6.7583)	Acc@1 3.906 (2.344)	Acc@5 8.203 (9.186)
Epoch: [1][40/196]	Time 0.015 (0.032)	Data 0.002 (0.007)	Loss 6.1016 (6.6182)	Acc@1 1.953 (2.172)	Acc@5 8.984 (8.956)
Epoch: [1][50/196]	Time 0.015 (0.029)	Data 0.002 (0.006)	Loss 5.6756 (6.4686)	Acc@1 1.953 (2.160)	Acc@5 9.375 (9.007)
Epoch: [1][60/196]	Time 0.015 (0.027)	Data 0.002 (0.006)	Loss 5.7043 (6.3515)	Acc@1 5.469 (2.222)	Acc@5 12.500 (9.125)
Epoch: [1][70/196]	Time 0.011 (0.025)	Data 0.012 (0.005)	Loss 5.6327 (6.2628)	Acc@1 0.781 (2.184)	Acc@5 13.281 (9.336)
Epoch: [1][80/196]	Time 0.017 (0.024)	Data 0.001 (0.005)	Loss 5.7001 (6.1850)	Acc@1 2.344 (2.223)	Acc@5 9.766 (9.732)
Epoch: [1][90/196]	Time 0.015 (0.023)	Data 0.002 (0.005)	Loss 5.5422 (6.1170)	Acc@1 3.516 (2.284)	Acc@5 12.109 (10.195)
Epoch: [1][100/196]	Time 0.017 (0.022)	Data 0.001 (0.005)	Loss 5.4629 (6.0568)	Acc@1 5.078 (2.460)	Acc@5 18.750 (10.794)
Epoch: [1][110/196]	Time 0.016 (0.021)	Data 0.002 (0.005)	Loss 5.3885 (5.9963)	Acc@1 7.422 (2.657)	Acc@5 18.750 (11.536)
Epoch: [1][120/196]	Time 0.015 (0.021)	Data 0.002 (0.004)	Loss 5.4128 (5.9399)	Acc@1 1.562 (2.880)	Acc@5 21.094 (12.268)
Epoch: [1][130/196]	Time 0.015 (0.020)	Data 0.002 (0.004)	Loss 5.2652 (5.8922)	Acc@1 6.250 (3.042)	Acc@5 20.312 (12.968)
Epoch: [1][140/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 5.1811 (5.8467)	Acc@1 5.078 (3.205)	Acc@5 24.219 (13.594)
Epoch: [1][150/196]	Time 0.012 (0.020)	Data 0.002 (0.004)	Loss 5.1963 (5.8045)	Acc@1 6.250 (3.391)	Acc@5 24.219 (14.176)
Epoch: [1][160/196]	Time 0.016 (0.019)	Data 0.002 (0.004)	Loss 5.2141 (5.7649)	Acc@1 2.344 (3.545)	Acc@5 19.922 (14.679)
Epoch: [1][170/196]	Time 0.015 (0.019)	Data 0.003 (0.004)	Loss 5.0423 (5.7274)	Acc@1 7.422 (3.733)	Acc@5 29.688 (15.319)
Epoch: [1][180/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 5.0192 (5.6892)	Acc@1 7.422 (3.936)	Acc@5 25.391 (15.972)
Epoch: [1][190/196]	Time 0.015 (0.019)	Data 0.003 (0.004)	Loss 5.1553 (5.6580)	Acc@1 4.688 (4.047)	Acc@5 21.484 (16.394)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [2 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [2][0/196]	Time 0.031 (0.031)	Data 0.156 (0.156)	Loss 5.1420 (5.1420)	Acc@1 6.250 (6.250)	Acc@5 22.656 (22.656)
Epoch: [2][10/196]	Time 0.017 (0.017)	Data 0.001 (0.017)	Loss 5.1031 (5.0155)	Acc@1 7.031 (7.102)	Acc@5 25.000 (27.734)
Epoch: [2][20/196]	Time 0.013 (0.016)	Data 0.004 (0.010)	Loss 4.9745 (5.0140)	Acc@1 8.203 (7.385)	Acc@5 28.125 (26.990)
Epoch: [2][30/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 4.9671 (4.9926)	Acc@1 8.984 (7.460)	Acc@5 26.172 (27.281)
Epoch: [2][40/196]	Time 0.014 (0.015)	Data 0.004 (0.006)	Loss 4.8962 (4.9619)	Acc@1 7.031 (7.851)	Acc@5 30.469 (28.382)
Epoch: [2][50/196]	Time 0.016 (0.015)	Data 0.004 (0.006)	Loss 4.9786 (4.9428)	Acc@1 8.203 (8.104)	Acc@5 30.078 (29.174)
Epoch: [2][60/196]	Time 0.019 (0.016)	Data 0.003 (0.005)	Loss 4.9007 (4.9402)	Acc@1 8.203 (8.145)	Acc@5 33.984 (29.355)
Epoch: [2][70/196]	Time 0.029 (0.016)	Data 0.001 (0.005)	Loss 4.8077 (4.9184)	Acc@1 9.766 (8.511)	Acc@5 31.641 (29.985)
Epoch: [2][80/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 4.7875 (4.8978)	Acc@1 5.859 (8.777)	Acc@5 31.250 (30.464)
Epoch: [2][90/196]	Time 0.019 (0.016)	Data 0.003 (0.004)	Loss 4.6327 (4.8791)	Acc@1 14.453 (9.079)	Acc@5 36.328 (30.885)
Epoch: [2][100/196]	Time 0.011 (0.016)	Data 0.001 (0.004)	Loss 4.6100 (4.8592)	Acc@1 12.500 (9.286)	Acc@5 37.500 (31.389)
Epoch: [2][110/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 4.7300 (4.8411)	Acc@1 9.375 (9.607)	Acc@5 33.594 (31.926)
Epoch: [2][120/196]	Time 0.015 (0.016)	Data 0.015 (0.004)	Loss 4.6784 (4.8258)	Acc@1 13.672 (9.843)	Acc@5 37.891 (32.396)
Epoch: [2][130/196]	Time 0.017 (0.016)	Data 0.004 (0.004)	Loss 4.6466 (4.8103)	Acc@1 14.844 (9.989)	Acc@5 37.109 (32.801)
Epoch: [2][140/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 4.5701 (4.7892)	Acc@1 10.938 (10.217)	Acc@5 37.109 (33.259)
Epoch: [2][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 4.4887 (4.7724)	Acc@1 10.938 (10.405)	Acc@5 38.672 (33.604)
Epoch: [2][160/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 4.5544 (4.7553)	Acc@1 11.719 (10.678)	Acc@5 41.406 (34.072)
Epoch: [2][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 4.5111 (4.7418)	Acc@1 15.234 (10.887)	Acc@5 37.500 (34.402)
Epoch: [2][180/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 4.5612 (4.7273)	Acc@1 15.625 (11.132)	Acc@5 37.891 (34.774)
Epoch: [2][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 4.5012 (4.7119)	Acc@1 13.672 (11.332)	Acc@5 39.844 (35.164)
[INFO] Storing checkpoint...

Epoch: [3 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [3][0/196]	Time 0.030 (0.030)	Data 0.157 (0.157)	Loss 4.5212 (4.5212)	Acc@1 15.234 (15.234)	Acc@5 39.062 (39.062)
Epoch: [3][10/196]	Time 0.015 (0.016)	Data 0.002 (0.016)	Loss 4.2889 (4.4184)	Acc@1 16.406 (15.092)	Acc@5 45.703 (42.862)
Epoch: [3][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 4.3770 (4.3886)	Acc@1 18.750 (15.978)	Acc@5 42.969 (43.452)
Epoch: [3][30/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 4.4297 (4.3667)	Acc@1 13.672 (16.230)	Acc@5 41.406 (43.914)
Epoch: [3][40/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 4.3161 (4.3441)	Acc@1 19.531 (16.635)	Acc@5 46.875 (44.293)
Epoch: [3][50/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 4.2381 (4.3231)	Acc@1 17.578 (16.743)	Acc@5 44.141 (44.799)
Epoch: [3][60/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 4.3665 (4.3133)	Acc@1 15.625 (16.835)	Acc@5 44.531 (45.056)
Epoch: [3][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 4.0997 (4.2990)	Acc@1 23.828 (17.160)	Acc@5 50.391 (45.456)
Epoch: [3][80/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 4.2363 (4.2840)	Acc@1 17.578 (17.467)	Acc@5 45.703 (45.795)
Epoch: [3][90/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 4.0749 (4.2689)	Acc@1 24.219 (17.698)	Acc@5 48.828 (46.184)
Epoch: [3][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 4.0942 (4.2475)	Acc@1 18.359 (18.031)	Acc@5 47.656 (46.631)
Epoch: [3][110/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 4.0524 (4.2276)	Acc@1 23.047 (18.377)	Acc@5 50.391 (46.995)
Epoch: [3][120/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 4.0722 (4.2129)	Acc@1 18.750 (18.472)	Acc@5 51.172 (47.295)
Epoch: [3][130/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 4.0101 (4.1984)	Acc@1 21.094 (18.634)	Acc@5 51.953 (47.656)
Epoch: [3][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 3.9371 (4.1817)	Acc@1 22.266 (18.908)	Acc@5 52.344 (48.097)
Epoch: [3][150/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 3.9909 (4.1666)	Acc@1 25.781 (19.210)	Acc@5 50.000 (48.414)
Epoch: [3][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 3.7646 (4.1515)	Acc@1 24.219 (19.393)	Acc@5 60.547 (48.721)
Epoch: [3][170/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 3.5014 (4.1345)	Acc@1 32.031 (19.616)	Acc@5 62.109 (49.045)
Epoch: [3][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 3.8608 (4.1218)	Acc@1 23.047 (19.810)	Acc@5 55.078 (49.320)
Epoch: [3][190/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 3.9822 (4.1115)	Acc@1 19.141 (19.934)	Acc@5 53.125 (49.577)
[INFO] Storing checkpoint...

Epoch: [4 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [4][0/196]	Time 0.031 (0.031)	Data 0.159 (0.159)	Loss 3.9024 (3.9024)	Acc@1 23.047 (23.047)	Acc@5 52.344 (52.344)
Epoch: [4][10/196]	Time 0.015 (0.016)	Data 0.002 (0.017)	Loss 3.8114 (3.8347)	Acc@1 22.266 (23.153)	Acc@5 53.125 (54.510)
Epoch: [4][20/196]	Time 0.015 (0.016)	Data 0.003 (0.010)	Loss 3.6411 (3.7856)	Acc@1 24.219 (23.884)	Acc@5 62.109 (56.622)
Epoch: [4][30/196]	Time 0.017 (0.015)	Data 0.001 (0.007)	Loss 3.6700 (3.7894)	Acc@1 27.344 (24.206)	Acc@5 59.375 (56.489)
Epoch: [4][40/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 3.6282 (3.7703)	Acc@1 26.953 (24.381)	Acc@5 62.500 (57.012)
Epoch: [4][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 3.6902 (3.7609)	Acc@1 27.734 (24.885)	Acc@5 59.375 (57.138)
Epoch: [4][60/196]	Time 0.010 (0.015)	Data 0.007 (0.005)	Loss 3.8215 (3.7468)	Acc@1 24.219 (25.141)	Acc@5 50.000 (57.454)
Epoch: [4][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 3.6758 (3.7353)	Acc@1 30.078 (25.572)	Acc@5 59.375 (57.592)
Epoch: [4][80/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 3.5661 (3.7275)	Acc@1 23.828 (25.805)	Acc@5 59.766 (57.803)
Epoch: [4][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.5457 (3.7201)	Acc@1 26.172 (25.979)	Acc@5 61.719 (57.933)
Epoch: [4][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 3.6104 (3.7086)	Acc@1 29.688 (26.218)	Acc@5 60.938 (58.242)
Epoch: [4][110/196]	Time 0.019 (0.015)	Data 0.001 (0.004)	Loss 3.4538 (3.6964)	Acc@1 30.469 (26.415)	Acc@5 64.453 (58.495)
Epoch: [4][120/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 3.5877 (3.6874)	Acc@1 26.172 (26.466)	Acc@5 58.984 (58.687)
Epoch: [4][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.6059 (3.6746)	Acc@1 26.562 (26.673)	Acc@5 62.500 (58.934)
Epoch: [4][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 3.5202 (3.6632)	Acc@1 32.422 (27.003)	Acc@5 60.938 (59.156)
Epoch: [4][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.3331 (3.6514)	Acc@1 35.938 (27.227)	Acc@5 63.281 (59.437)
Epoch: [4][160/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 3.5128 (3.6419)	Acc@1 30.078 (27.392)	Acc@5 61.328 (59.598)
Epoch: [4][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.5319 (3.6314)	Acc@1 26.562 (27.515)	Acc@5 61.328 (59.782)
Epoch: [4][180/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 3.4933 (3.6229)	Acc@1 27.734 (27.624)	Acc@5 63.672 (59.992)
Epoch: [4][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.3684 (3.6138)	Acc@1 31.250 (27.767)	Acc@5 66.797 (60.183)
[INFO] Storing checkpoint...

Epoch: [5 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [5][0/196]	Time 0.029 (0.029)	Data 0.165 (0.165)	Loss 3.4646 (3.4646)	Acc@1 28.125 (28.125)	Acc@5 61.719 (61.719)
Epoch: [5][10/196]	Time 0.017 (0.017)	Data 0.001 (0.017)	Loss 3.3637 (3.4290)	Acc@1 31.641 (29.901)	Acc@5 63.672 (64.134)
Epoch: [5][20/196]	Time 0.011 (0.016)	Data 0.006 (0.010)	Loss 3.5708 (3.4192)	Acc@1 23.047 (30.246)	Acc@5 63.281 (64.137)
Epoch: [5][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 3.2986 (3.3816)	Acc@1 32.031 (31.225)	Acc@5 65.234 (64.541)
Epoch: [5][40/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 3.4026 (3.3738)	Acc@1 30.469 (31.517)	Acc@5 62.500 (64.539)
Epoch: [5][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 3.1824 (3.3547)	Acc@1 36.719 (31.832)	Acc@5 66.016 (65.150)
Epoch: [5][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 3.2924 (3.3362)	Acc@1 32.812 (32.108)	Acc@5 64.453 (65.580)
Epoch: [5][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 3.0708 (3.3302)	Acc@1 37.891 (32.295)	Acc@5 71.094 (65.735)
Epoch: [5][80/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 3.5313 (3.3208)	Acc@1 27.734 (32.509)	Acc@5 59.766 (65.900)
Epoch: [5][90/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 3.2015 (3.3139)	Acc@1 30.078 (32.611)	Acc@5 69.531 (66.110)
Epoch: [5][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 3.2879 (3.3088)	Acc@1 33.203 (32.673)	Acc@5 64.062 (66.290)
Epoch: [5][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.0011 (3.3007)	Acc@1 33.594 (32.693)	Acc@5 73.047 (66.480)
Epoch: [5][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 3.2421 (3.2938)	Acc@1 33.984 (32.874)	Acc@5 64.062 (66.616)
Epoch: [5][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.2281 (3.2893)	Acc@1 35.156 (32.908)	Acc@5 66.016 (66.615)
Epoch: [5][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 3.1958 (3.2842)	Acc@1 37.500 (33.026)	Acc@5 68.750 (66.642)
Epoch: [5][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.1264 (3.2767)	Acc@1 35.938 (33.144)	Acc@5 69.141 (66.825)
Epoch: [5][160/196]	Time 0.011 (0.015)	Data 0.007 (0.003)	Loss 3.2269 (3.2725)	Acc@1 35.156 (33.276)	Acc@5 69.141 (66.870)
Epoch: [5][170/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.9618 (3.2653)	Acc@1 44.141 (33.486)	Acc@5 73.828 (66.975)
Epoch: [5][180/196]	Time 0.011 (0.015)	Data 0.006 (0.003)	Loss 3.0876 (3.2609)	Acc@1 39.453 (33.620)	Acc@5 71.484 (67.073)
Epoch: [5][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 3.0791 (3.2516)	Acc@1 35.547 (33.778)	Acc@5 68.359 (67.218)
[INFO] Storing checkpoint...

Epoch: [6 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [6][0/196]	Time 0.030 (0.030)	Data 0.166 (0.166)	Loss 3.1586 (3.1586)	Acc@1 34.766 (34.766)	Acc@5 68.359 (68.359)
Epoch: [6][10/196]	Time 0.018 (0.017)	Data 0.000 (0.017)	Loss 3.2027 (3.0469)	Acc@1 33.984 (38.139)	Acc@5 66.406 (71.200)
Epoch: [6][20/196]	Time 0.011 (0.016)	Data 0.007 (0.010)	Loss 3.1364 (3.0626)	Acc@1 32.812 (37.388)	Acc@5 68.359 (70.815)
Epoch: [6][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.9899 (3.0409)	Acc@1 39.062 (37.891)	Acc@5 74.609 (71.472)
Epoch: [6][40/196]	Time 0.011 (0.016)	Data 0.009 (0.006)	Loss 2.9734 (3.0250)	Acc@1 39.453 (38.300)	Acc@5 74.219 (71.704)
Epoch: [6][50/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.9990 (3.0286)	Acc@1 44.531 (38.434)	Acc@5 70.312 (71.446)
Epoch: [6][60/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.8950 (3.0280)	Acc@1 45.703 (38.550)	Acc@5 74.219 (71.350)
Epoch: [6][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.9308 (3.0341)	Acc@1 36.719 (38.287)	Acc@5 71.875 (71.094)
Epoch: [6][80/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.9300 (3.0303)	Acc@1 40.625 (38.276)	Acc@5 75.391 (71.142)
Epoch: [6][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.9586 (3.0216)	Acc@1 40.234 (38.530)	Acc@5 76.953 (71.407)
Epoch: [6][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.8740 (3.0184)	Acc@1 37.109 (38.537)	Acc@5 76.562 (71.438)
Epoch: [6][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.2177 (3.0168)	Acc@1 35.547 (38.492)	Acc@5 64.453 (71.460)
Epoch: [6][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 3.1598 (3.0142)	Acc@1 37.891 (38.488)	Acc@5 67.969 (71.459)
Epoch: [6][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.0110 (3.0105)	Acc@1 38.672 (38.508)	Acc@5 71.484 (71.499)
Epoch: [6][140/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 3.0452 (3.0139)	Acc@1 34.375 (38.290)	Acc@5 67.578 (71.476)
Epoch: [6][150/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.7428 (3.0081)	Acc@1 41.016 (38.434)	Acc@5 78.125 (71.565)
Epoch: [6][160/196]	Time 0.014 (0.015)	Data 0.004 (0.003)	Loss 2.9039 (2.9996)	Acc@1 42.578 (38.667)	Acc@5 73.438 (71.725)
Epoch: [6][170/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.8859 (2.9950)	Acc@1 39.453 (38.807)	Acc@5 70.703 (71.768)
Epoch: [6][180/196]	Time 0.015 (0.015)	Data 0.003 (0.003)	Loss 2.8577 (2.9907)	Acc@1 39.844 (38.853)	Acc@5 73.047 (71.832)
Epoch: [6][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 3.0145 (2.9857)	Acc@1 38.281 (38.932)	Acc@5 71.484 (71.924)
[INFO] Storing checkpoint...

Epoch: [7 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [7][0/196]	Time 0.030 (0.030)	Data 0.165 (0.165)	Loss 2.8548 (2.8548)	Acc@1 39.453 (39.453)	Acc@5 75.781 (75.781)
Epoch: [7][10/196]	Time 0.015 (0.016)	Data 0.002 (0.017)	Loss 2.8364 (2.8488)	Acc@1 40.625 (41.016)	Acc@5 73.828 (74.609)
Epoch: [7][20/196]	Time 0.015 (0.016)	Data 0.003 (0.010)	Loss 2.7367 (2.8604)	Acc@1 44.531 (41.220)	Acc@5 76.172 (74.070)
Epoch: [7][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 3.1120 (2.8860)	Acc@1 34.375 (40.423)	Acc@5 71.875 (73.664)
Epoch: [7][40/196]	Time 0.014 (0.015)	Data 0.004 (0.006)	Loss 2.7368 (2.8789)	Acc@1 47.266 (40.482)	Acc@5 72.266 (73.761)
Epoch: [7][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 3.0335 (2.8609)	Acc@1 36.719 (40.855)	Acc@5 70.703 (74.112)
Epoch: [7][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.7894 (2.8598)	Acc@1 42.578 (40.958)	Acc@5 74.219 (74.129)
Epoch: [7][70/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.8151 (2.8504)	Acc@1 46.094 (41.252)	Acc@5 75.781 (74.345)
Epoch: [7][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.6742 (2.8391)	Acc@1 44.531 (41.464)	Acc@5 79.688 (74.609)
Epoch: [7][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.6337 (2.8308)	Acc@1 43.750 (41.625)	Acc@5 76.172 (74.717)
Epoch: [7][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.9269 (2.8220)	Acc@1 39.062 (41.820)	Acc@5 74.609 (74.965)
Epoch: [7][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.8555 (2.8153)	Acc@1 40.625 (42.043)	Acc@5 73.438 (75.028)
Epoch: [7][120/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.8631 (2.8098)	Acc@1 42.969 (42.139)	Acc@5 74.219 (75.110)
Epoch: [7][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.8985 (2.8094)	Acc@1 40.234 (42.188)	Acc@5 74.219 (75.063)
Epoch: [7][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.7112 (2.8080)	Acc@1 40.234 (42.176)	Acc@5 80.859 (75.122)
Epoch: [7][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.7331 (2.8089)	Acc@1 45.703 (42.208)	Acc@5 75.781 (75.145)
Epoch: [7][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.8060 (2.8075)	Acc@1 43.750 (42.192)	Acc@5 70.703 (75.180)
Epoch: [7][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.7657 (2.8003)	Acc@1 41.797 (42.311)	Acc@5 73.438 (75.274)
Epoch: [7][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.7881 (2.8014)	Acc@1 41.016 (42.250)	Acc@5 75.000 (75.186)
Epoch: [7][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.8611 (2.8020)	Acc@1 41.406 (42.214)	Acc@5 73.047 (75.135)
[INFO] Storing checkpoint...

Epoch: [8 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [8][0/196]	Time 0.027 (0.027)	Data 0.166 (0.166)	Loss 2.6813 (2.6813)	Acc@1 43.359 (43.359)	Acc@5 82.422 (82.422)
Epoch: [8][10/196]	Time 0.016 (0.016)	Data 0.002 (0.018)	Loss 2.5992 (2.7338)	Acc@1 48.047 (43.182)	Acc@5 81.250 (76.847)
Epoch: [8][20/196]	Time 0.012 (0.016)	Data 0.006 (0.010)	Loss 2.6482 (2.6646)	Acc@1 47.266 (44.940)	Acc@5 76.953 (78.404)
Epoch: [8][30/196]	Time 0.016 (0.016)	Data 0.002 (0.008)	Loss 2.5877 (2.6558)	Acc@1 47.266 (45.136)	Acc@5 75.781 (78.226)
Epoch: [8][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 2.7313 (2.6578)	Acc@1 45.312 (45.055)	Acc@5 76.562 (78.106)
Epoch: [8][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.5049 (2.6456)	Acc@1 44.141 (45.335)	Acc@5 81.641 (78.485)
Epoch: [8][60/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.5125 (2.6373)	Acc@1 48.047 (45.402)	Acc@5 80.859 (78.567)
Epoch: [8][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.7634 (2.6346)	Acc@1 42.188 (45.516)	Acc@5 76.953 (78.466)
Epoch: [8][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.6399 (2.6388)	Acc@1 45.703 (45.414)	Acc@5 79.297 (78.443)
Epoch: [8][90/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.7581 (2.6428)	Acc@1 42.188 (45.218)	Acc@5 75.391 (78.232)
Epoch: [8][100/196]	Time 0.010 (0.015)	Data 0.008 (0.004)	Loss 2.6423 (2.6434)	Acc@1 46.094 (45.239)	Acc@5 80.078 (78.187)
Epoch: [8][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.6169 (2.6456)	Acc@1 45.312 (45.320)	Acc@5 79.297 (78.093)
Epoch: [8][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.8511 (2.6490)	Acc@1 41.406 (45.287)	Acc@5 73.828 (78.006)
Epoch: [8][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.5893 (2.6503)	Acc@1 48.438 (45.268)	Acc@5 78.516 (77.943)
Epoch: [8][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.6663 (2.6544)	Acc@1 45.312 (45.168)	Acc@5 75.000 (77.823)
Epoch: [8][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.7190 (2.6554)	Acc@1 44.922 (45.175)	Acc@5 77.734 (77.802)
Epoch: [8][160/196]	Time 0.010 (0.015)	Data 0.007 (0.003)	Loss 2.5400 (2.6536)	Acc@1 46.484 (45.167)	Acc@5 76.172 (77.773)
Epoch: [8][170/196]	Time 0.015 (0.015)	Data 0.003 (0.003)	Loss 2.5660 (2.6505)	Acc@1 43.750 (45.299)	Acc@5 79.297 (77.782)
Epoch: [8][180/196]	Time 0.014 (0.015)	Data 0.002 (0.003)	Loss 2.6519 (2.6440)	Acc@1 41.797 (45.423)	Acc@5 79.297 (77.894)
Epoch: [8][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.6098 (2.6432)	Acc@1 46.875 (45.462)	Acc@5 80.078 (77.933)
[INFO] Storing checkpoint...

Epoch: [9 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [9][0/196]	Time 0.029 (0.029)	Data 0.166 (0.166)	Loss 2.5931 (2.5931)	Acc@1 45.703 (45.703)	Acc@5 79.688 (79.688)
Epoch: [9][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 2.4703 (2.5347)	Acc@1 48.828 (47.763)	Acc@5 81.641 (80.078)
Epoch: [9][20/196]	Time 0.011 (0.015)	Data 0.009 (0.011)	Loss 2.4752 (2.5155)	Acc@1 52.734 (48.140)	Acc@5 78.906 (80.339)
Epoch: [9][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.5188 (2.5200)	Acc@1 44.922 (48.097)	Acc@5 80.469 (80.280)
Epoch: [9][40/196]	Time 0.014 (0.015)	Data 0.004 (0.007)	Loss 2.4414 (2.5245)	Acc@1 51.562 (48.199)	Acc@5 81.641 (80.183)
Epoch: [9][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.5179 (2.5269)	Acc@1 47.266 (48.055)	Acc@5 81.250 (80.047)
Epoch: [9][60/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 2.5248 (2.5246)	Acc@1 43.750 (48.169)	Acc@5 80.859 (80.065)
Epoch: [9][70/196]	Time 0.019 (0.015)	Data 0.000 (0.005)	Loss 2.4807 (2.5271)	Acc@1 45.703 (48.019)	Acc@5 80.859 (80.095)
Epoch: [9][80/196]	Time 0.015 (0.015)	Data 0.004 (0.005)	Loss 2.3168 (2.5198)	Acc@1 51.172 (48.249)	Acc@5 82.031 (80.141)
Epoch: [9][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4557 (2.5194)	Acc@1 49.609 (48.347)	Acc@5 80.859 (79.992)
Epoch: [9][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.5875 (2.5201)	Acc@1 45.703 (48.271)	Acc@5 75.781 (79.916)
Epoch: [9][110/196]	Time 0.019 (0.015)	Data 0.001 (0.004)	Loss 2.5472 (2.5198)	Acc@1 45.703 (48.283)	Acc@5 81.250 (79.934)
Epoch: [9][120/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.5285 (2.5211)	Acc@1 45.312 (48.144)	Acc@5 80.469 (79.901)
Epoch: [9][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.6017 (2.5227)	Acc@1 43.750 (48.035)	Acc@5 78.125 (79.971)
Epoch: [9][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.5843 (2.5220)	Acc@1 50.000 (48.077)	Acc@5 76.172 (79.978)
Epoch: [9][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.6089 (2.5274)	Acc@1 48.438 (48.003)	Acc@5 76.172 (79.856)
Epoch: [9][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.5264 (2.5310)	Acc@1 51.172 (48.047)	Acc@5 78.125 (79.736)
Epoch: [9][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.4754 (2.5334)	Acc@1 49.609 (48.047)	Acc@5 81.250 (79.676)
Epoch: [9][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.6184 (2.5356)	Acc@1 46.484 (48.008)	Acc@5 80.469 (79.644)
Epoch: [9][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.5041 (2.5370)	Acc@1 49.609 (47.930)	Acc@5 77.344 (79.598)
[INFO] Storing checkpoint...

Epoch: [10 | 10] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [10][0/196]	Time 0.028 (0.028)	Data 0.181 (0.181)	Loss 2.3984 (2.3984)	Acc@1 53.125 (53.125)	Acc@5 81.641 (81.641)
Epoch: [10][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 2.7054 (2.4566)	Acc@1 50.391 (50.604)	Acc@5 75.391 (80.753)
Epoch: [10][20/196]	Time 0.013 (0.016)	Data 0.005 (0.011)	Loss 2.6089 (2.4803)	Acc@1 46.875 (49.535)	Acc@5 79.297 (80.766)
Epoch: [10][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 2.3954 (2.4563)	Acc@1 51.172 (50.063)	Acc@5 83.594 (81.250)
Epoch: [10][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 2.3072 (2.4438)	Acc@1 50.781 (50.248)	Acc@5 83.984 (81.469)
Epoch: [10][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 2.3612 (2.4408)	Acc@1 49.219 (49.946)	Acc@5 84.375 (81.702)
Epoch: [10][60/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.3878 (2.4371)	Acc@1 48.828 (50.019)	Acc@5 82.031 (81.801)
Epoch: [10][70/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.5219 (2.4446)	Acc@1 48.047 (49.961)	Acc@5 81.641 (81.729)
Epoch: [10][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.4988 (2.4464)	Acc@1 50.000 (50.106)	Acc@5 82.031 (81.510)
Epoch: [10][90/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 2.3227 (2.4453)	Acc@1 52.344 (50.137)	Acc@5 80.469 (81.409)
Epoch: [10][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.5412 (2.4484)	Acc@1 49.609 (50.070)	Acc@5 79.688 (81.397)
Epoch: [10][110/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.4212 (2.4484)	Acc@1 50.391 (50.025)	Acc@5 81.250 (81.334)
Epoch: [10][120/196]	Time 0.017 (0.015)	Data 0.007 (0.004)	Loss 2.4622 (2.4453)	Acc@1 46.875 (50.071)	Acc@5 83.984 (81.463)
Epoch: [10][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3497 (2.4448)	Acc@1 52.344 (50.098)	Acc@5 84.766 (81.500)
Epoch: [10][140/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.4826 (2.4444)	Acc@1 46.875 (50.086)	Acc@5 82.422 (81.571)
Epoch: [10][150/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.2832 (2.4467)	Acc@1 51.172 (50.031)	Acc@5 81.641 (81.522)
Epoch: [10][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.4566 (2.4485)	Acc@1 49.219 (50.000)	Acc@5 82.422 (81.468)
Epoch: [10][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.4621 (2.4535)	Acc@1 48.438 (49.858)	Acc@5 79.688 (81.405)
Epoch: [10][180/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.4338 (2.4533)	Acc@1 55.078 (49.855)	Acc@5 82.422 (81.427)
Epoch: [10][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.3806 (2.4548)	Acc@1 51.953 (49.814)	Acc@5 85.547 (81.436)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [64, 3, 3, 3] >> [64, 3, 3, 3]
[module.conv2.weight]: [128, 64, 3, 3] >> [128, 64, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [256, 128, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [512, 256, 3, 3]
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv7.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv8.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.fc.weight]: [100, 512] >> [100, 512]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
41.79
python src/cifar.py --workers 4 --dataset cifar100 --epochs 20 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_20.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 9.27M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375

Epoch: [11 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [11][0/196]	Time 0.604 (0.604)	Data 0.156 (0.156)	Loss 2.4435 (2.4435)	Acc@1 47.266 (47.266)	Acc@5 80.859 (80.859)
Epoch: [11][10/196]	Time 0.017 (0.070)	Data 0.002 (0.016)	Loss 2.3385 (2.3528)	Acc@1 52.344 (52.450)	Acc@5 82.422 (82.848)
Epoch: [11][20/196]	Time 0.016 (0.044)	Data 0.002 (0.009)	Loss 2.3079 (2.3628)	Acc@1 50.781 (51.990)	Acc@5 85.156 (82.906)
Epoch: [11][30/196]	Time 0.017 (0.035)	Data 0.002 (0.007)	Loss 2.3840 (2.3567)	Acc@1 51.562 (52.218)	Acc@5 83.594 (83.115)
Epoch: [11][40/196]	Time 0.016 (0.031)	Data 0.002 (0.005)	Loss 2.3920 (2.3591)	Acc@1 51.172 (52.220)	Acc@5 82.422 (83.060)
Epoch: [11][50/196]	Time 0.016 (0.028)	Data 0.002 (0.005)	Loss 2.3022 (2.3757)	Acc@1 51.172 (51.769)	Acc@5 82.812 (82.782)
Epoch: [11][60/196]	Time 0.017 (0.026)	Data 0.001 (0.004)	Loss 2.4348 (2.3826)	Acc@1 48.828 (51.678)	Acc@5 79.688 (82.576)
Epoch: [11][70/196]	Time 0.017 (0.025)	Data 0.002 (0.004)	Loss 2.4518 (2.3830)	Acc@1 50.000 (51.750)	Acc@5 83.594 (82.664)
Epoch: [11][80/196]	Time 0.017 (0.024)	Data 0.002 (0.004)	Loss 2.3351 (2.3828)	Acc@1 55.859 (51.775)	Acc@5 83.594 (82.769)
Epoch: [11][90/196]	Time 0.016 (0.023)	Data 0.002 (0.003)	Loss 2.3765 (2.3827)	Acc@1 50.000 (51.751)	Acc@5 83.984 (82.765)
Epoch: [11][100/196]	Time 0.016 (0.022)	Data 0.002 (0.003)	Loss 2.2571 (2.3821)	Acc@1 55.859 (51.752)	Acc@5 87.891 (82.851)
Epoch: [11][110/196]	Time 0.016 (0.022)	Data 0.002 (0.003)	Loss 2.2228 (2.3900)	Acc@1 55.859 (51.531)	Acc@5 84.766 (82.784)
Epoch: [11][120/196]	Time 0.016 (0.021)	Data 0.002 (0.003)	Loss 2.4061 (2.3912)	Acc@1 49.219 (51.514)	Acc@5 83.203 (82.745)
Epoch: [11][130/196]	Time 0.017 (0.021)	Data 0.002 (0.003)	Loss 2.4474 (2.3960)	Acc@1 49.219 (51.360)	Acc@5 83.984 (82.717)
Epoch: [11][140/196]	Time 0.016 (0.021)	Data 0.002 (0.003)	Loss 2.3629 (2.3981)	Acc@1 53.516 (51.321)	Acc@5 80.469 (82.613)
Epoch: [11][150/196]	Time 0.016 (0.020)	Data 0.002 (0.003)	Loss 2.5715 (2.4006)	Acc@1 44.531 (51.268)	Acc@5 80.469 (82.577)
Epoch: [11][160/196]	Time 0.016 (0.020)	Data 0.002 (0.003)	Loss 2.2570 (2.4011)	Acc@1 61.328 (51.327)	Acc@5 83.594 (82.577)
Epoch: [11][170/196]	Time 0.016 (0.020)	Data 0.001 (0.003)	Loss 2.4828 (2.4005)	Acc@1 51.953 (51.377)	Acc@5 82.812 (82.554)
Epoch: [11][180/196]	Time 0.017 (0.020)	Data 0.002 (0.003)	Loss 2.2728 (2.4002)	Acc@1 55.469 (51.433)	Acc@5 85.547 (82.595)
Epoch: [11][190/196]	Time 0.017 (0.020)	Data 0.001 (0.003)	Loss 2.6060 (2.4020)	Acc@1 49.219 (51.419)	Acc@5 79.688 (82.592)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [12 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [12][0/196]	Time 0.034 (0.034)	Data 0.160 (0.160)	Loss 2.2095 (2.2095)	Acc@1 56.641 (56.641)	Acc@5 86.719 (86.719)
Epoch: [12][10/196]	Time 0.018 (0.018)	Data 0.000 (0.016)	Loss 2.3084 (2.3235)	Acc@1 53.516 (54.119)	Acc@5 83.594 (84.375)
Epoch: [12][20/196]	Time 0.014 (0.017)	Data 0.004 (0.010)	Loss 2.3058 (2.3019)	Acc@1 53.516 (54.520)	Acc@5 86.328 (84.505)
Epoch: [12][30/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 2.4169 (2.3032)	Acc@1 50.781 (54.259)	Acc@5 82.422 (84.463)
Epoch: [12][40/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 2.3559 (2.3114)	Acc@1 52.344 (54.030)	Acc@5 86.328 (84.194)
Epoch: [12][50/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.2163 (2.3238)	Acc@1 57.422 (53.791)	Acc@5 84.375 (83.946)
Epoch: [12][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.3304 (2.3321)	Acc@1 57.812 (53.810)	Acc@5 81.250 (83.747)
Epoch: [12][70/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.3739 (2.3363)	Acc@1 46.875 (53.631)	Acc@5 84.766 (83.665)
Epoch: [12][80/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.3037 (2.3402)	Acc@1 55.078 (53.530)	Acc@5 83.984 (83.661)
Epoch: [12][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3238 (2.3415)	Acc@1 52.734 (53.498)	Acc@5 82.812 (83.589)
Epoch: [12][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1341 (2.3450)	Acc@1 58.203 (53.384)	Acc@5 87.109 (83.513)
Epoch: [12][110/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3196 (2.3489)	Acc@1 53.516 (53.255)	Acc@5 84.375 (83.390)
Epoch: [12][120/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3857 (2.3507)	Acc@1 55.859 (53.202)	Acc@5 81.641 (83.448)
Epoch: [12][130/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.4139 (2.3506)	Acc@1 53.516 (53.176)	Acc@5 82.422 (83.471)
Epoch: [12][140/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3407 (2.3554)	Acc@1 54.297 (53.125)	Acc@5 84.375 (83.392)
Epoch: [12][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.4491 (2.3545)	Acc@1 49.219 (53.179)	Acc@5 83.594 (83.407)
Epoch: [12][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2824 (2.3544)	Acc@1 55.859 (53.110)	Acc@5 81.250 (83.426)
Epoch: [12][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3680 (2.3552)	Acc@1 51.562 (53.086)	Acc@5 83.203 (83.420)
Epoch: [12][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2456 (2.3551)	Acc@1 53.125 (53.101)	Acc@5 89.062 (83.449)
Epoch: [12][190/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.3633 (2.3567)	Acc@1 52.734 (53.094)	Acc@5 85.547 (83.434)
[INFO] Storing checkpoint...

Epoch: [13 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [13][0/196]	Time 0.032 (0.032)	Data 0.163 (0.163)	Loss 2.1510 (2.1510)	Acc@1 59.766 (59.766)	Acc@5 88.281 (88.281)
Epoch: [13][10/196]	Time 0.018 (0.018)	Data 0.000 (0.017)	Loss 2.2035 (2.2839)	Acc@1 58.594 (53.800)	Acc@5 87.109 (85.511)
Epoch: [13][20/196]	Time 0.012 (0.017)	Data 0.006 (0.010)	Loss 2.3028 (2.3158)	Acc@1 56.641 (53.330)	Acc@5 85.156 (84.766)
Epoch: [13][30/196]	Time 0.018 (0.017)	Data 0.001 (0.007)	Loss 2.2127 (2.3147)	Acc@1 53.125 (53.478)	Acc@5 88.281 (84.703)
Epoch: [13][40/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 2.2583 (2.3213)	Acc@1 53.125 (53.382)	Acc@5 84.375 (84.499)
Epoch: [13][50/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 2.3269 (2.3279)	Acc@1 52.344 (53.462)	Acc@5 84.375 (84.329)
Epoch: [13][60/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.3778 (2.3153)	Acc@1 53.125 (53.925)	Acc@5 83.984 (84.471)
Epoch: [13][70/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3013 (2.3114)	Acc@1 55.078 (54.044)	Acc@5 84.375 (84.551)
Epoch: [13][80/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.3764 (2.3138)	Acc@1 53.906 (54.109)	Acc@5 83.594 (84.529)
Epoch: [13][90/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.2985 (2.3133)	Acc@1 53.516 (54.194)	Acc@5 85.938 (84.577)
Epoch: [13][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.3260 (2.3199)	Acc@1 54.297 (54.011)	Acc@5 83.984 (84.472)
Epoch: [13][110/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2482 (2.3157)	Acc@1 58.203 (54.198)	Acc@5 86.328 (84.488)
Epoch: [13][120/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.4099 (2.3202)	Acc@1 54.297 (54.113)	Acc@5 82.422 (84.381)
Epoch: [13][130/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2499 (2.3204)	Acc@1 54.297 (54.163)	Acc@5 85.938 (84.366)
Epoch: [13][140/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3929 (2.3223)	Acc@1 53.906 (54.095)	Acc@5 81.641 (84.339)
Epoch: [13][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2581 (2.3234)	Acc@1 55.859 (54.142)	Acc@5 83.984 (84.264)
Epoch: [13][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2585 (2.3270)	Acc@1 55.859 (54.035)	Acc@5 87.891 (84.242)
Epoch: [13][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2530 (2.3310)	Acc@1 54.688 (53.902)	Acc@5 87.500 (84.217)
Epoch: [13][180/196]	Time 0.018 (0.016)	Data 0.002 (0.003)	Loss 2.2478 (2.3291)	Acc@1 56.641 (54.036)	Acc@5 87.891 (84.256)
Epoch: [13][190/196]	Time 0.016 (0.016)	Data 0.001 (0.003)	Loss 2.3691 (2.3327)	Acc@1 54.297 (54.000)	Acc@5 83.594 (84.185)
[INFO] Storing checkpoint...

Epoch: [14 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [14][0/196]	Time 0.033 (0.033)	Data 0.162 (0.162)	Loss 2.2346 (2.2346)	Acc@1 58.203 (58.203)	Acc@5 87.109 (87.109)
Epoch: [14][10/196]	Time 0.017 (0.018)	Data 0.001 (0.017)	Loss 2.2902 (2.2625)	Acc@1 55.078 (55.433)	Acc@5 84.766 (85.263)
Epoch: [14][20/196]	Time 0.014 (0.017)	Data 0.005 (0.010)	Loss 2.2420 (2.2551)	Acc@1 58.203 (55.990)	Acc@5 83.594 (85.640)
Epoch: [14][30/196]	Time 0.016 (0.017)	Data 0.003 (0.008)	Loss 2.2256 (2.2421)	Acc@1 55.078 (56.515)	Acc@5 86.328 (85.648)
Epoch: [14][40/196]	Time 0.012 (0.016)	Data 0.006 (0.006)	Loss 2.4614 (2.2454)	Acc@1 53.516 (56.469)	Acc@5 82.812 (85.661)
Epoch: [14][50/196]	Time 0.011 (0.016)	Data 0.002 (0.005)	Loss 2.3575 (2.2449)	Acc@1 55.469 (56.740)	Acc@5 81.250 (85.585)
Epoch: [14][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.3104 (2.2336)	Acc@1 55.078 (56.999)	Acc@5 85.156 (85.803)
Epoch: [14][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1711 (2.2392)	Acc@1 57.812 (56.751)	Acc@5 91.406 (85.882)
Epoch: [14][80/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.2737 (2.2498)	Acc@1 55.469 (56.245)	Acc@5 83.594 (85.774)
Epoch: [14][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2934 (2.2551)	Acc@1 56.250 (56.083)	Acc@5 85.156 (85.719)
Epoch: [14][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2796 (2.2603)	Acc@1 56.641 (55.952)	Acc@5 82.812 (85.616)
Epoch: [14][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.4207 (2.2665)	Acc@1 52.344 (55.909)	Acc@5 84.766 (85.476)
Epoch: [14][120/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.4123 (2.2688)	Acc@1 51.562 (55.853)	Acc@5 82.031 (85.447)
Epoch: [14][130/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3479 (2.2736)	Acc@1 55.859 (55.818)	Acc@5 82.812 (85.353)
Epoch: [14][140/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2881 (2.2756)	Acc@1 53.516 (55.785)	Acc@5 85.156 (85.303)
Epoch: [14][150/196]	Time 0.018 (0.016)	Data 0.002 (0.003)	Loss 2.3972 (2.2827)	Acc@1 54.297 (55.660)	Acc@5 84.375 (85.156)
Epoch: [14][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2430 (2.2842)	Acc@1 60.156 (55.653)	Acc@5 86.328 (85.207)
Epoch: [14][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3634 (2.2840)	Acc@1 52.734 (55.702)	Acc@5 87.891 (85.227)
Epoch: [14][180/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2523 (2.2853)	Acc@1 56.250 (55.732)	Acc@5 86.328 (85.210)
Epoch: [14][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2864 (2.2895)	Acc@1 58.203 (55.669)	Acc@5 83.203 (85.115)
[INFO] Storing checkpoint...

Epoch: [15 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [15][0/196]	Time 0.034 (0.034)	Data 0.165 (0.165)	Loss 2.3593 (2.3593)	Acc@1 53.125 (53.125)	Acc@5 83.594 (83.594)
Epoch: [15][10/196]	Time 0.016 (0.017)	Data 0.002 (0.017)	Loss 2.3966 (2.3011)	Acc@1 52.734 (55.043)	Acc@5 83.594 (85.085)
Epoch: [15][20/196]	Time 0.013 (0.017)	Data 0.005 (0.010)	Loss 2.1996 (2.2519)	Acc@1 59.375 (56.417)	Acc@5 88.672 (86.365)
Epoch: [15][30/196]	Time 0.019 (0.016)	Data 0.001 (0.008)	Loss 2.2888 (2.2374)	Acc@1 58.594 (56.691)	Acc@5 86.328 (86.820)
Epoch: [15][40/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 2.1112 (2.2336)	Acc@1 63.281 (56.974)	Acc@5 89.844 (86.871)
Epoch: [15][50/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.4343 (2.2367)	Acc@1 52.344 (56.878)	Acc@5 82.422 (86.765)
Epoch: [15][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0378 (2.2378)	Acc@1 62.891 (57.038)	Acc@5 88.672 (86.591)
Epoch: [15][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.3484 (2.2397)	Acc@1 57.422 (57.130)	Acc@5 84.375 (86.416)
Epoch: [15][80/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3421 (2.2433)	Acc@1 55.469 (57.128)	Acc@5 88.281 (86.323)
Epoch: [15][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2517 (2.2499)	Acc@1 57.422 (56.993)	Acc@5 84.766 (86.208)
Epoch: [15][100/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2360 (2.2545)	Acc@1 59.375 (56.973)	Acc@5 83.984 (86.007)
Epoch: [15][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3257 (2.2555)	Acc@1 57.031 (57.010)	Acc@5 84.766 (86.018)
Epoch: [15][120/196]	Time 0.019 (0.016)	Data 0.002 (0.003)	Loss 2.2882 (2.2569)	Acc@1 55.859 (56.989)	Acc@5 84.766 (86.034)
Epoch: [15][130/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.4779 (2.2622)	Acc@1 57.031 (56.763)	Acc@5 81.641 (86.012)
Epoch: [15][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.5240 (2.2706)	Acc@1 52.734 (56.552)	Acc@5 81.641 (85.846)
Epoch: [15][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3850 (2.2750)	Acc@1 53.516 (56.428)	Acc@5 84.375 (85.684)
Epoch: [15][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.4438 (2.2805)	Acc@1 51.172 (56.325)	Acc@5 80.859 (85.615)
Epoch: [15][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3871 (2.2833)	Acc@1 57.422 (56.236)	Acc@5 82.422 (85.595)
Epoch: [15][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3107 (2.2839)	Acc@1 55.859 (56.218)	Acc@5 84.375 (85.612)
Epoch: [15][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2210 (2.2872)	Acc@1 59.375 (56.256)	Acc@5 85.156 (85.486)
[INFO] Storing checkpoint...

Epoch: [16 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [16][0/196]	Time 0.032 (0.032)	Data 0.168 (0.168)	Loss 2.0289 (2.0289)	Acc@1 63.281 (63.281)	Acc@5 91.406 (91.406)
Epoch: [16][10/196]	Time 0.018 (0.017)	Data 0.000 (0.018)	Loss 2.2813 (2.2343)	Acc@1 57.031 (57.670)	Acc@5 83.203 (87.074)
Epoch: [16][20/196]	Time 0.014 (0.017)	Data 0.004 (0.010)	Loss 2.1471 (2.2244)	Acc@1 58.594 (57.850)	Acc@5 89.453 (87.202)
Epoch: [16][30/196]	Time 0.016 (0.016)	Data 0.002 (0.008)	Loss 2.0837 (2.2050)	Acc@1 62.891 (58.354)	Acc@5 87.891 (87.172)
Epoch: [16][40/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.2978 (2.2255)	Acc@1 55.469 (58.089)	Acc@5 85.938 (86.928)
Epoch: [16][50/196]	Time 0.017 (0.016)	Data 0.002 (0.006)	Loss 2.2681 (2.2279)	Acc@1 56.250 (58.081)	Acc@5 85.938 (86.772)
Epoch: [16][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.1749 (2.2208)	Acc@1 62.891 (58.126)	Acc@5 87.500 (86.789)
Epoch: [16][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1635 (2.2258)	Acc@1 58.984 (57.950)	Acc@5 87.891 (86.768)
Epoch: [16][80/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.4430 (2.2354)	Acc@1 51.172 (57.711)	Acc@5 85.156 (86.627)
Epoch: [16][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2672 (2.2376)	Acc@1 57.812 (57.632)	Acc@5 85.156 (86.611)
Epoch: [16][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3804 (2.2457)	Acc@1 54.297 (57.345)	Acc@5 86.719 (86.522)
Epoch: [16][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3475 (2.2473)	Acc@1 54.688 (57.373)	Acc@5 87.109 (86.451)
Epoch: [16][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.5355 (2.2536)	Acc@1 50.781 (57.290)	Acc@5 82.422 (86.354)
Epoch: [16][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3634 (2.2544)	Acc@1 57.422 (57.306)	Acc@5 83.984 (86.340)
Epoch: [16][140/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 2.2609 (2.2531)	Acc@1 57.812 (57.369)	Acc@5 85.156 (86.350)
Epoch: [16][150/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.2475 (2.2518)	Acc@1 58.594 (57.497)	Acc@5 86.328 (86.362)
Epoch: [16][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2839 (2.2536)	Acc@1 58.984 (57.466)	Acc@5 86.719 (86.321)
Epoch: [16][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2967 (2.2522)	Acc@1 57.422 (57.516)	Acc@5 85.156 (86.298)
Epoch: [16][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3141 (2.2542)	Acc@1 54.688 (57.472)	Acc@5 87.109 (86.259)
Epoch: [16][190/196]	Time 0.016 (0.016)	Data 0.001 (0.003)	Loss 2.3623 (2.2576)	Acc@1 57.031 (57.389)	Acc@5 84.766 (86.236)
[INFO] Storing checkpoint...

Epoch: [17 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [17][0/196]	Time 0.034 (0.034)	Data 0.178 (0.178)	Loss 2.1951 (2.1951)	Acc@1 59.766 (59.766)	Acc@5 88.672 (88.672)
Epoch: [17][10/196]	Time 0.018 (0.018)	Data 0.001 (0.018)	Loss 2.2957 (2.2017)	Acc@1 58.203 (59.517)	Acc@5 83.984 (86.825)
Epoch: [17][20/196]	Time 0.015 (0.017)	Data 0.004 (0.010)	Loss 2.0824 (2.1932)	Acc@1 62.891 (59.282)	Acc@5 89.062 (87.165)
Epoch: [17][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 2.2668 (2.2079)	Acc@1 57.031 (58.896)	Acc@5 85.547 (86.895)
Epoch: [17][40/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 2.2312 (2.2063)	Acc@1 57.422 (58.822)	Acc@5 87.500 (87.138)
Epoch: [17][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 2.0952 (2.2109)	Acc@1 63.281 (58.732)	Acc@5 89.062 (87.040)
Epoch: [17][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.1197 (2.2083)	Acc@1 61.328 (58.741)	Acc@5 90.234 (87.103)
Epoch: [17][70/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 2.0895 (2.2048)	Acc@1 64.844 (58.902)	Acc@5 87.500 (87.159)
Epoch: [17][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 2.4024 (2.2065)	Acc@1 53.906 (58.806)	Acc@5 83.594 (87.076)
Epoch: [17][90/196]	Time 0.017 (0.017)	Data 0.002 (0.004)	Loss 2.1881 (2.2015)	Acc@1 63.281 (58.989)	Acc@5 86.719 (87.144)
Epoch: [17][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1928 (2.2081)	Acc@1 59.766 (58.756)	Acc@5 87.500 (87.125)
Epoch: [17][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.3394 (2.2160)	Acc@1 58.203 (58.654)	Acc@5 82.812 (87.053)
Epoch: [17][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.3014 (2.2186)	Acc@1 55.078 (58.671)	Acc@5 84.375 (87.029)
Epoch: [17][130/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1386 (2.2241)	Acc@1 60.156 (58.573)	Acc@5 87.891 (86.936)
Epoch: [17][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.4202 (2.2269)	Acc@1 52.734 (58.572)	Acc@5 85.156 (86.907)
Epoch: [17][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1936 (2.2286)	Acc@1 55.469 (58.457)	Acc@5 91.016 (86.902)
Epoch: [17][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2491 (2.2295)	Acc@1 58.984 (58.477)	Acc@5 85.156 (86.859)
Epoch: [17][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3626 (2.2315)	Acc@1 53.516 (58.393)	Acc@5 84.375 (86.840)
Epoch: [17][180/196]	Time 0.020 (0.016)	Data 0.002 (0.003)	Loss 2.3147 (2.2360)	Acc@1 58.203 (58.348)	Acc@5 85.938 (86.725)
Epoch: [17][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1924 (2.2376)	Acc@1 64.453 (58.348)	Acc@5 86.719 (86.723)
[INFO] Storing checkpoint...

Epoch: [18 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [18][0/196]	Time 0.034 (0.034)	Data 0.180 (0.180)	Loss 2.0487 (2.0487)	Acc@1 63.281 (63.281)	Acc@5 89.453 (89.453)
Epoch: [18][10/196]	Time 0.019 (0.018)	Data 0.001 (0.018)	Loss 2.0809 (2.1241)	Acc@1 60.938 (60.511)	Acc@5 90.625 (89.524)
Epoch: [18][20/196]	Time 0.012 (0.017)	Data 0.007 (0.011)	Loss 2.2489 (2.1272)	Acc@1 57.031 (60.082)	Acc@5 87.109 (89.211)
Epoch: [18][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 2.0976 (2.1246)	Acc@1 62.109 (60.559)	Acc@5 89.453 (89.025)
Epoch: [18][40/196]	Time 0.015 (0.017)	Data 0.004 (0.006)	Loss 2.2556 (2.1356)	Acc@1 60.547 (60.480)	Acc@5 85.938 (88.700)
Epoch: [18][50/196]	Time 0.018 (0.017)	Data 0.001 (0.006)	Loss 2.1563 (2.1514)	Acc@1 59.766 (60.409)	Acc@5 87.500 (88.258)
Epoch: [18][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.3160 (2.1631)	Acc@1 55.078 (60.342)	Acc@5 86.328 (88.038)
Epoch: [18][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1151 (2.1656)	Acc@1 62.109 (60.255)	Acc@5 86.719 (88.056)
Epoch: [18][80/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.3058 (2.1730)	Acc@1 57.812 (60.166)	Acc@5 86.328 (87.968)
Epoch: [18][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0878 (2.1812)	Acc@1 65.625 (59.937)	Acc@5 90.234 (87.882)
Epoch: [18][100/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.2617 (2.1872)	Acc@1 58.594 (59.653)	Acc@5 84.766 (87.836)
Epoch: [18][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2889 (2.1929)	Acc@1 56.641 (59.533)	Acc@5 87.891 (87.715)
Epoch: [18][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.3596 (2.2004)	Acc@1 58.984 (59.391)	Acc@5 84.375 (87.642)
Epoch: [18][130/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.2985 (2.2059)	Acc@1 54.297 (59.315)	Acc@5 87.109 (87.548)
Epoch: [18][140/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.3932 (2.2115)	Acc@1 55.469 (59.092)	Acc@5 83.203 (87.472)
Epoch: [18][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2703 (2.2158)	Acc@1 58.203 (58.969)	Acc@5 86.719 (87.448)
Epoch: [18][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.4091 (2.2215)	Acc@1 50.781 (58.807)	Acc@5 84.766 (87.379)
Epoch: [18][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3382 (2.2245)	Acc@1 53.906 (58.774)	Acc@5 87.500 (87.294)
Epoch: [18][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1822 (2.2273)	Acc@1 59.766 (58.702)	Acc@5 88.672 (87.312)
Epoch: [18][190/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.1158 (2.2267)	Acc@1 62.500 (58.727)	Acc@5 88.281 (87.326)
[INFO] Storing checkpoint...

Epoch: [19 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [19][0/196]	Time 0.035 (0.035)	Data 0.177 (0.177)	Loss 2.1083 (2.1083)	Acc@1 57.812 (57.812)	Acc@5 92.188 (92.188)
Epoch: [19][10/196]	Time 0.018 (0.018)	Data 0.001 (0.018)	Loss 2.0760 (2.0882)	Acc@1 62.891 (62.429)	Acc@5 90.234 (89.737)
Epoch: [19][20/196]	Time 0.011 (0.017)	Data 0.007 (0.011)	Loss 2.2158 (2.1493)	Acc@1 59.766 (61.179)	Acc@5 87.109 (89.044)
Epoch: [19][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 2.0664 (2.1497)	Acc@1 61.719 (60.925)	Acc@5 89.453 (88.936)
Epoch: [19][40/196]	Time 0.011 (0.017)	Data 0.008 (0.007)	Loss 2.2077 (2.1644)	Acc@1 61.328 (60.718)	Acc@5 89.844 (88.748)
Epoch: [19][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.0436 (2.1557)	Acc@1 60.938 (60.899)	Acc@5 89.844 (88.940)
Epoch: [19][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1723 (2.1626)	Acc@1 65.234 (60.809)	Acc@5 89.844 (88.710)
Epoch: [19][70/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 2.1329 (2.1656)	Acc@1 57.812 (60.745)	Acc@5 91.406 (88.595)
Epoch: [19][80/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 2.0683 (2.1712)	Acc@1 63.281 (60.581)	Acc@5 86.719 (88.455)
Epoch: [19][90/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.2504 (2.1767)	Acc@1 57.812 (60.435)	Acc@5 86.328 (88.397)
Epoch: [19][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2247 (2.1850)	Acc@1 58.594 (60.257)	Acc@5 89.062 (88.219)
Epoch: [19][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2122 (2.1884)	Acc@1 60.547 (60.244)	Acc@5 88.672 (88.148)
Epoch: [19][120/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.2588 (2.1902)	Acc@1 58.594 (60.253)	Acc@5 88.672 (88.146)
Epoch: [19][130/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.2082 (2.1923)	Acc@1 61.328 (60.183)	Acc@5 87.500 (88.138)
Epoch: [19][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2469 (2.1928)	Acc@1 59.375 (60.156)	Acc@5 90.625 (88.082)
Epoch: [19][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2687 (2.1979)	Acc@1 57.812 (59.993)	Acc@5 85.156 (87.984)
Epoch: [19][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3154 (2.2014)	Acc@1 54.297 (59.909)	Acc@5 83.203 (87.912)
Epoch: [19][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3035 (2.2052)	Acc@1 56.250 (59.766)	Acc@5 87.109 (87.845)
Epoch: [19][180/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2836 (2.2100)	Acc@1 58.984 (59.653)	Acc@5 87.109 (87.774)
Epoch: [19][190/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.2290 (2.2122)	Acc@1 61.328 (59.680)	Acc@5 88.281 (87.737)
[INFO] Storing checkpoint...

Epoch: [20 | 20] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [20][0/196]	Time 0.033 (0.033)	Data 0.180 (0.180)	Loss 2.0547 (2.0547)	Acc@1 65.625 (65.625)	Acc@5 89.844 (89.844)
Epoch: [20][10/196]	Time 0.018 (0.017)	Data 0.000 (0.019)	Loss 1.9815 (2.1286)	Acc@1 64.844 (61.506)	Acc@5 88.672 (88.175)
Epoch: [20][20/196]	Time 0.016 (0.017)	Data 0.003 (0.011)	Loss 2.1800 (2.1014)	Acc@1 58.203 (61.868)	Acc@5 89.062 (89.100)
Epoch: [20][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.2290 (2.1239)	Acc@1 58.594 (61.555)	Acc@5 90.234 (88.987)
Epoch: [20][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 2.2007 (2.1270)	Acc@1 62.109 (61.709)	Acc@5 86.719 (88.720)
Epoch: [20][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9541 (2.1250)	Acc@1 67.578 (61.841)	Acc@5 91.016 (88.840)
Epoch: [20][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1591 (2.1384)	Acc@1 59.375 (61.623)	Acc@5 87.891 (88.697)
Epoch: [20][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1945 (2.1434)	Acc@1 64.453 (61.592)	Acc@5 87.891 (88.666)
Epoch: [20][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.2858 (2.1505)	Acc@1 57.812 (61.381)	Acc@5 88.281 (88.580)
Epoch: [20][90/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1691 (2.1557)	Acc@1 61.328 (61.375)	Acc@5 89.062 (88.466)
Epoch: [20][100/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1819 (2.1628)	Acc@1 61.328 (61.181)	Acc@5 89.062 (88.382)
Epoch: [20][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0621 (2.1674)	Acc@1 59.766 (61.057)	Acc@5 90.625 (88.355)
Epoch: [20][120/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0884 (2.1723)	Acc@1 60.938 (60.915)	Acc@5 92.188 (88.346)
Epoch: [20][130/196]	Time 0.019 (0.016)	Data 0.002 (0.004)	Loss 2.2923 (2.1791)	Acc@1 58.984 (60.717)	Acc@5 87.891 (88.281)
Epoch: [20][140/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1069 (2.1799)	Acc@1 63.281 (60.721)	Acc@5 91.797 (88.231)
Epoch: [20][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2193 (2.1832)	Acc@1 62.891 (60.570)	Acc@5 87.109 (88.211)
Epoch: [20][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2719 (2.1872)	Acc@1 55.469 (60.484)	Acc@5 86.719 (88.187)
Epoch: [20][170/196]	Time 0.018 (0.016)	Data 0.002 (0.003)	Loss 2.2550 (2.1904)	Acc@1 62.500 (60.467)	Acc@5 85.156 (88.151)
Epoch: [20][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3675 (2.1928)	Acc@1 58.203 (60.437)	Acc@5 84.766 (88.115)
Epoch: [20][190/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.2158 (2.1974)	Acc@1 60.938 (60.287)	Acc@5 89.844 (88.083)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [64, 3, 3, 3] >> [64, 3, 3, 3]
[module.conv2.weight]: [128, 64, 3, 3] >> [128, 64, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [256, 128, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [512, 256, 3, 3]
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv7.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv8.weight]: [512, 512, 3, 3] >> [490, 512, 3, 3]
[module.fc.weight]: [100, 512] >> [100, 490]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
52.31
python src/cifar.py --workers 4 --dataset cifar100 --epochs 30 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_30.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 9.17M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [490, 512, 3, 3]
fc --> [490, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 6936330240, 9031680, 490
fc, 18816000, 49000, 0
===================
FLOP REPORT: 31025065200000.0 60585600000.0 152407912 151464 2730 17.48766326904297

Epoch: [21 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [21][0/196]	Time 0.731 (0.731)	Data 0.162 (0.162)	Loss 2.0844 (2.0844)	Acc@1 62.891 (62.891)	Acc@5 89.844 (89.844)
Epoch: [21][10/196]	Time 0.014 (0.081)	Data 0.003 (0.017)	Loss 2.1494 (2.1848)	Acc@1 62.500 (61.506)	Acc@5 90.234 (88.814)
Epoch: [21][20/196]	Time 0.016 (0.050)	Data 0.003 (0.010)	Loss 2.0210 (2.1391)	Acc@1 65.234 (62.277)	Acc@5 91.406 (89.472)
Epoch: [21][30/196]	Time 0.016 (0.039)	Data 0.003 (0.007)	Loss 2.2566 (2.1361)	Acc@1 59.375 (62.450)	Acc@5 86.719 (89.352)
Epoch: [21][40/196]	Time 0.015 (0.033)	Data 0.002 (0.006)	Loss 2.0132 (2.1337)	Acc@1 64.453 (62.691)	Acc@5 89.453 (89.024)
Epoch: [21][50/196]	Time 0.016 (0.030)	Data 0.002 (0.005)	Loss 2.0002 (2.1244)	Acc@1 64.844 (62.852)	Acc@5 91.797 (89.254)
Epoch: [21][60/196]	Time 0.015 (0.027)	Data 0.003 (0.005)	Loss 2.1644 (2.1169)	Acc@1 57.422 (62.929)	Acc@5 89.453 (89.402)
Epoch: [21][70/196]	Time 0.015 (0.026)	Data 0.002 (0.005)	Loss 2.0477 (2.1121)	Acc@1 62.109 (62.880)	Acc@5 91.016 (89.514)
Epoch: [21][80/196]	Time 0.016 (0.024)	Data 0.002 (0.004)	Loss 2.2023 (2.1321)	Acc@1 63.281 (62.413)	Acc@5 88.672 (89.140)
Epoch: [21][90/196]	Time 0.015 (0.023)	Data 0.003 (0.004)	Loss 2.0718 (2.1352)	Acc@1 65.625 (62.328)	Acc@5 89.453 (89.153)
Epoch: [21][100/196]	Time 0.015 (0.023)	Data 0.003 (0.004)	Loss 2.3281 (2.1438)	Acc@1 57.812 (62.125)	Acc@5 89.453 (89.043)
Epoch: [21][110/196]	Time 0.015 (0.022)	Data 0.002 (0.004)	Loss 2.2502 (2.1514)	Acc@1 55.078 (61.775)	Acc@5 91.406 (88.978)
Epoch: [21][120/196]	Time 0.015 (0.021)	Data 0.003 (0.004)	Loss 2.2781 (2.1569)	Acc@1 55.859 (61.648)	Acc@5 88.672 (88.882)
Epoch: [21][130/196]	Time 0.016 (0.021)	Data 0.003 (0.004)	Loss 2.3400 (2.1605)	Acc@1 57.422 (61.531)	Acc@5 84.375 (88.815)
Epoch: [21][140/196]	Time 0.013 (0.020)	Data 0.006 (0.004)	Loss 2.2195 (2.1642)	Acc@1 61.719 (61.436)	Acc@5 87.500 (88.785)
Epoch: [21][150/196]	Time 0.015 (0.020)	Data 0.002 (0.004)	Loss 2.0322 (2.1656)	Acc@1 64.062 (61.344)	Acc@5 92.188 (88.788)
Epoch: [21][160/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 2.2411 (2.1705)	Acc@1 61.719 (61.231)	Acc@5 88.672 (88.679)
Epoch: [21][170/196]	Time 0.013 (0.019)	Data 0.005 (0.004)	Loss 2.3843 (2.1726)	Acc@1 51.953 (61.173)	Acc@5 86.328 (88.672)
Epoch: [21][180/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 2.2400 (2.1746)	Acc@1 60.156 (61.147)	Acc@5 88.672 (88.629)
Epoch: [21][190/196]	Time 0.015 (0.019)	Data 0.003 (0.004)	Loss 2.4754 (2.1821)	Acc@1 54.688 (60.974)	Acc@5 83.594 (88.486)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [22 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [22][0/196]	Time 0.028 (0.028)	Data 0.166 (0.166)	Loss 2.0419 (2.0419)	Acc@1 62.109 (62.109)	Acc@5 90.625 (90.625)
Epoch: [22][10/196]	Time 0.013 (0.016)	Data 0.005 (0.018)	Loss 2.0627 (2.1303)	Acc@1 64.453 (61.612)	Acc@5 90.625 (88.956)
Epoch: [22][20/196]	Time 0.012 (0.016)	Data 0.006 (0.010)	Loss 2.1566 (2.1455)	Acc@1 58.203 (61.514)	Acc@5 91.016 (88.690)
Epoch: [22][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0353 (2.1359)	Acc@1 62.891 (61.769)	Acc@5 89.453 (88.886)
Epoch: [22][40/196]	Time 0.012 (0.016)	Data 0.010 (0.007)	Loss 2.0263 (2.1210)	Acc@1 65.625 (62.605)	Acc@5 89.453 (89.062)
Epoch: [22][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1358 (2.1179)	Acc@1 62.500 (62.791)	Acc@5 88.672 (89.139)
Epoch: [22][60/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 2.2292 (2.1212)	Acc@1 59.375 (62.641)	Acc@5 87.500 (89.210)
Epoch: [22][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1113 (2.1237)	Acc@1 64.453 (62.506)	Acc@5 89.062 (89.200)
Epoch: [22][80/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 2.3251 (2.1326)	Acc@1 57.031 (62.307)	Acc@5 87.891 (89.082)
Epoch: [22][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2385 (2.1369)	Acc@1 60.156 (62.230)	Acc@5 88.281 (89.093)
Epoch: [22][100/196]	Time 0.011 (0.016)	Data 0.012 (0.004)	Loss 2.0991 (2.1394)	Acc@1 65.234 (62.345)	Acc@5 89.844 (89.097)
Epoch: [22][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2209 (2.1436)	Acc@1 62.109 (62.299)	Acc@5 88.672 (89.041)
Epoch: [22][120/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.2303 (2.1462)	Acc@1 60.547 (62.193)	Acc@5 87.500 (89.024)
Epoch: [22][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3849 (2.1552)	Acc@1 53.516 (62.002)	Acc@5 88.281 (88.881)
Epoch: [22][140/196]	Time 0.011 (0.016)	Data 0.014 (0.004)	Loss 2.1047 (2.1604)	Acc@1 63.672 (61.860)	Acc@5 90.234 (88.841)
Epoch: [22][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2094 (2.1623)	Acc@1 58.203 (61.820)	Acc@5 88.672 (88.778)
Epoch: [22][160/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.3455 (2.1643)	Acc@1 57.422 (61.779)	Acc@5 88.281 (88.793)
Epoch: [22][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2385 (2.1684)	Acc@1 57.812 (61.673)	Acc@5 89.062 (88.731)
Epoch: [22][180/196]	Time 0.011 (0.016)	Data 0.010 (0.004)	Loss 2.1528 (2.1743)	Acc@1 62.500 (61.553)	Acc@5 87.500 (88.637)
Epoch: [22][190/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2746 (2.1805)	Acc@1 55.078 (61.369)	Acc@5 91.016 (88.608)
[INFO] Storing checkpoint...

Epoch: [23 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [23][0/196]	Time 0.025 (0.025)	Data 0.162 (0.162)	Loss 2.1311 (2.1311)	Acc@1 61.328 (61.328)	Acc@5 89.453 (89.453)
Epoch: [23][10/196]	Time 0.016 (0.016)	Data 0.003 (0.017)	Loss 2.2407 (2.1421)	Acc@1 62.109 (63.104)	Acc@5 87.891 (90.163)
Epoch: [23][20/196]	Time 0.014 (0.016)	Data 0.004 (0.010)	Loss 1.9071 (2.1301)	Acc@1 69.141 (62.723)	Acc@5 92.188 (89.918)
Epoch: [23][30/196]	Time 0.015 (0.016)	Data 0.003 (0.008)	Loss 2.1230 (2.1080)	Acc@1 62.109 (63.319)	Acc@5 88.672 (89.970)
Epoch: [23][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 2.2394 (2.1141)	Acc@1 57.812 (63.005)	Acc@5 87.891 (89.949)
Epoch: [23][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.0768 (2.1103)	Acc@1 63.672 (63.021)	Acc@5 90.625 (89.982)
Epoch: [23][60/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.0836 (2.1042)	Acc@1 62.891 (63.307)	Acc@5 91.406 (90.036)
Epoch: [23][70/196]	Time 0.012 (0.015)	Data 0.010 (0.006)	Loss 2.1979 (2.1102)	Acc@1 59.766 (63.133)	Acc@5 88.281 (89.932)
Epoch: [23][80/196]	Time 0.019 (0.015)	Data 0.003 (0.005)	Loss 2.0932 (2.1140)	Acc@1 62.500 (63.055)	Acc@5 92.188 (89.955)
Epoch: [23][90/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.1777 (2.1170)	Acc@1 61.719 (63.011)	Acc@5 86.328 (89.857)
Epoch: [23][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1927 (2.1241)	Acc@1 63.281 (62.956)	Acc@5 87.109 (89.735)
Epoch: [23][110/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.4715 (2.1279)	Acc@1 54.688 (62.750)	Acc@5 82.812 (89.731)
Epoch: [23][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2339 (2.1370)	Acc@1 59.375 (62.600)	Acc@5 85.938 (89.624)
Epoch: [23][130/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.1980 (2.1484)	Acc@1 63.281 (62.354)	Acc@5 87.109 (89.432)
Epoch: [23][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3884 (2.1514)	Acc@1 55.859 (62.206)	Acc@5 87.109 (89.370)
Epoch: [23][150/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.0959 (2.1534)	Acc@1 63.281 (62.182)	Acc@5 89.453 (89.329)
Epoch: [23][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2629 (2.1573)	Acc@1 58.984 (62.088)	Acc@5 87.891 (89.247)
Epoch: [23][170/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3872 (2.1664)	Acc@1 56.641 (61.874)	Acc@5 88.281 (89.131)
Epoch: [23][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9834 (2.1692)	Acc@1 67.969 (61.822)	Acc@5 92.188 (89.082)
Epoch: [23][190/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.3648 (2.1723)	Acc@1 55.859 (61.784)	Acc@5 87.109 (89.030)
[INFO] Storing checkpoint...

Epoch: [24 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [24][0/196]	Time 0.026 (0.026)	Data 0.163 (0.163)	Loss 1.9405 (1.9405)	Acc@1 66.016 (66.016)	Acc@5 90.625 (90.625)
Epoch: [24][10/196]	Time 0.015 (0.017)	Data 0.002 (0.017)	Loss 2.0692 (2.0623)	Acc@1 63.281 (64.240)	Acc@5 90.625 (90.128)
Epoch: [24][20/196]	Time 0.014 (0.016)	Data 0.002 (0.010)	Loss 2.0945 (2.0789)	Acc@1 64.062 (64.025)	Acc@5 89.453 (90.160)
Epoch: [24][30/196]	Time 0.015 (0.016)	Data 0.003 (0.008)	Loss 1.9874 (2.0857)	Acc@1 68.359 (63.848)	Acc@5 89.844 (90.146)
Epoch: [24][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 2.0692 (2.0869)	Acc@1 65.234 (63.729)	Acc@5 89.453 (90.111)
Epoch: [24][50/196]	Time 0.014 (0.015)	Data 0.005 (0.006)	Loss 2.2291 (2.0921)	Acc@1 60.156 (63.718)	Acc@5 87.109 (90.035)
Epoch: [24][60/196]	Time 0.012 (0.015)	Data 0.008 (0.006)	Loss 2.0502 (2.0943)	Acc@1 65.234 (63.717)	Acc@5 89.453 (90.138)
Epoch: [24][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2286 (2.1099)	Acc@1 59.375 (63.232)	Acc@5 87.891 (89.959)
Epoch: [24][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.0193 (2.1121)	Acc@1 66.406 (63.190)	Acc@5 89.453 (89.849)
Epoch: [24][90/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0664 (2.1260)	Acc@1 64.844 (62.916)	Acc@5 90.234 (89.560)
Epoch: [24][100/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.4432 (2.1322)	Acc@1 54.688 (62.840)	Acc@5 85.547 (89.449)
Epoch: [24][110/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.2352 (2.1396)	Acc@1 61.719 (62.676)	Acc@5 89.062 (89.425)
Epoch: [24][120/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.3341 (2.1463)	Acc@1 56.250 (62.590)	Acc@5 87.109 (89.305)
Epoch: [24][130/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.2217 (2.1477)	Acc@1 62.500 (62.581)	Acc@5 89.062 (89.361)
Epoch: [24][140/196]	Time 0.013 (0.015)	Data 0.006 (0.005)	Loss 2.3370 (2.1475)	Acc@1 60.156 (62.589)	Acc@5 84.375 (89.323)
Epoch: [24][150/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.2947 (2.1489)	Acc@1 62.109 (62.575)	Acc@5 86.328 (89.355)
Epoch: [24][160/196]	Time 0.015 (0.015)	Data 0.008 (0.004)	Loss 2.2552 (2.1525)	Acc@1 62.891 (62.478)	Acc@5 87.500 (89.325)
Epoch: [24][170/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1745 (2.1574)	Acc@1 63.672 (62.310)	Acc@5 87.109 (89.280)
Epoch: [24][180/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.1630 (2.1593)	Acc@1 61.719 (62.213)	Acc@5 91.016 (89.280)
Epoch: [24][190/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.2133 (2.1646)	Acc@1 60.938 (62.101)	Acc@5 88.672 (89.181)
[INFO] Storing checkpoint...

Epoch: [25 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [25][0/196]	Time 0.026 (0.026)	Data 0.178 (0.178)	Loss 2.0716 (2.0716)	Acc@1 64.062 (64.062)	Acc@5 91.797 (91.797)
Epoch: [25][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 2.0495 (2.0686)	Acc@1 62.891 (64.382)	Acc@5 90.234 (91.584)
Epoch: [25][20/196]	Time 0.014 (0.016)	Data 0.004 (0.011)	Loss 2.1483 (2.0952)	Acc@1 62.500 (63.821)	Acc@5 90.625 (90.551)
Epoch: [25][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9009 (2.0879)	Acc@1 68.750 (64.126)	Acc@5 93.359 (90.688)
Epoch: [25][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 2.2084 (2.1029)	Acc@1 62.891 (63.872)	Acc@5 87.891 (90.339)
Epoch: [25][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.2148 (2.1156)	Acc@1 62.500 (63.664)	Acc@5 87.891 (90.066)
Epoch: [25][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1465 (2.1268)	Acc@1 60.938 (63.416)	Acc@5 89.453 (89.869)
Epoch: [25][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2227 (2.1280)	Acc@1 61.719 (63.375)	Acc@5 87.891 (89.932)
Epoch: [25][80/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 2.2543 (2.1312)	Acc@1 59.766 (63.334)	Acc@5 90.625 (89.959)
Epoch: [25][90/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.0102 (2.1295)	Acc@1 64.844 (63.517)	Acc@5 93.359 (89.981)
Epoch: [25][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1019 (2.1301)	Acc@1 64.844 (63.513)	Acc@5 90.625 (89.960)
Epoch: [25][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1836 (2.1277)	Acc@1 62.109 (63.601)	Acc@5 88.281 (89.946)
Epoch: [25][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1990 (2.1277)	Acc@1 61.328 (63.678)	Acc@5 89.453 (89.915)
Epoch: [25][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1865 (2.1321)	Acc@1 62.109 (63.663)	Acc@5 90.625 (89.808)
Epoch: [25][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2189 (2.1376)	Acc@1 62.500 (63.528)	Acc@5 89.062 (89.683)
Epoch: [25][150/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1619 (2.1448)	Acc@1 63.281 (63.297)	Acc@5 85.938 (89.601)
Epoch: [25][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3222 (2.1507)	Acc@1 58.203 (63.206)	Acc@5 85.156 (89.502)
Epoch: [25][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2476 (2.1562)	Acc@1 60.547 (63.057)	Acc@5 87.500 (89.412)
Epoch: [25][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3693 (2.1588)	Acc@1 59.766 (63.029)	Acc@5 86.719 (89.347)
Epoch: [25][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3858 (2.1627)	Acc@1 56.641 (62.946)	Acc@5 88.281 (89.330)
[INFO] Storing checkpoint...

Epoch: [26 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [26][0/196]	Time 0.026 (0.026)	Data 0.169 (0.169)	Loss 2.0156 (2.0156)	Acc@1 65.625 (65.625)	Acc@5 90.625 (90.625)
Epoch: [26][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 2.0396 (2.0560)	Acc@1 61.328 (64.950)	Acc@5 92.188 (90.696)
Epoch: [26][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 2.2524 (2.0802)	Acc@1 61.719 (64.602)	Acc@5 88.672 (90.476)
Epoch: [26][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.1338 (2.0846)	Acc@1 61.328 (64.491)	Acc@5 91.406 (90.524)
Epoch: [26][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 2.0000 (2.0728)	Acc@1 66.016 (64.844)	Acc@5 90.625 (90.635)
Epoch: [26][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0822 (2.0700)	Acc@1 67.578 (65.005)	Acc@5 87.891 (90.610)
Epoch: [26][60/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 1.9872 (2.0792)	Acc@1 66.406 (64.709)	Acc@5 92.578 (90.580)
Epoch: [26][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2198 (2.0902)	Acc@1 59.766 (64.338)	Acc@5 90.234 (90.416)
Epoch: [26][80/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.1001 (2.0945)	Acc@1 60.156 (64.347)	Acc@5 90.234 (90.384)
Epoch: [26][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1920 (2.0981)	Acc@1 60.938 (64.342)	Acc@5 89.453 (90.376)
Epoch: [26][100/196]	Time 0.012 (0.016)	Data 0.012 (0.004)	Loss 1.9857 (2.0971)	Acc@1 66.406 (64.391)	Acc@5 91.797 (90.424)
Epoch: [26][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0403 (2.0977)	Acc@1 65.234 (64.450)	Acc@5 91.406 (90.460)
Epoch: [26][120/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2078 (2.1049)	Acc@1 62.500 (64.282)	Acc@5 88.672 (90.360)
Epoch: [26][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1207 (2.1071)	Acc@1 62.500 (64.179)	Acc@5 92.578 (90.288)
Epoch: [26][140/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1510 (2.1122)	Acc@1 65.234 (64.057)	Acc@5 89.062 (90.190)
Epoch: [26][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1185 (2.1115)	Acc@1 65.625 (64.083)	Acc@5 89.844 (90.229)
Epoch: [26][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0488 (2.1172)	Acc@1 64.453 (63.939)	Acc@5 89.453 (90.135)
Epoch: [26][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1119 (2.1203)	Acc@1 65.234 (63.882)	Acc@5 89.453 (90.095)
Epoch: [26][180/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.1120 (2.1249)	Acc@1 63.672 (63.778)	Acc@5 91.797 (90.047)
Epoch: [26][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.4148 (2.1289)	Acc@1 57.422 (63.662)	Acc@5 85.938 (89.973)
[INFO] Storing checkpoint...

Epoch: [27 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [27][0/196]	Time 0.026 (0.026)	Data 0.187 (0.187)	Loss 2.0385 (2.0385)	Acc@1 67.969 (67.969)	Acc@5 90.234 (90.234)
Epoch: [27][10/196]	Time 0.018 (0.017)	Data 0.000 (0.019)	Loss 2.0643 (2.0697)	Acc@1 62.500 (64.950)	Acc@5 92.188 (91.051)
Epoch: [27][20/196]	Time 0.012 (0.017)	Data 0.008 (0.011)	Loss 1.9938 (2.0644)	Acc@1 67.578 (65.402)	Acc@5 92.188 (91.127)
Epoch: [27][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.7766 (2.0411)	Acc@1 73.047 (65.789)	Acc@5 93.750 (91.255)
Epoch: [27][40/196]	Time 0.013 (0.016)	Data 0.009 (0.007)	Loss 2.0471 (2.0479)	Acc@1 65.625 (65.682)	Acc@5 90.234 (91.168)
Epoch: [27][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0230 (2.0417)	Acc@1 67.578 (65.847)	Acc@5 89.062 (91.230)
Epoch: [27][60/196]	Time 0.011 (0.016)	Data 0.009 (0.006)	Loss 2.1725 (2.0544)	Acc@1 62.891 (65.567)	Acc@5 89.453 (91.028)
Epoch: [27][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2597 (2.0710)	Acc@1 62.500 (65.185)	Acc@5 86.328 (90.763)
Epoch: [27][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.2043 (2.0791)	Acc@1 62.109 (65.013)	Acc@5 87.891 (90.625)
Epoch: [27][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0454 (2.0799)	Acc@1 64.844 (64.942)	Acc@5 91.797 (90.651)
Epoch: [27][100/196]	Time 0.011 (0.016)	Data 0.011 (0.004)	Loss 2.1806 (2.0884)	Acc@1 63.672 (64.828)	Acc@5 89.062 (90.490)
Epoch: [27][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.3331 (2.1011)	Acc@1 59.766 (64.509)	Acc@5 89.062 (90.386)
Epoch: [27][120/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.2518 (2.1086)	Acc@1 60.156 (64.327)	Acc@5 88.672 (90.322)
Epoch: [27][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1819 (2.1167)	Acc@1 61.719 (64.131)	Acc@5 89.844 (90.231)
Epoch: [27][140/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1525 (2.1158)	Acc@1 65.234 (64.135)	Acc@5 87.109 (90.251)
Epoch: [27][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1340 (2.1237)	Acc@1 62.891 (63.946)	Acc@5 90.625 (90.185)
Epoch: [27][160/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.1780 (2.1332)	Acc@1 64.062 (63.788)	Acc@5 91.016 (90.055)
Epoch: [27][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.3266 (2.1388)	Acc@1 58.984 (63.692)	Acc@5 86.328 (89.942)
Epoch: [27][180/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1870 (2.1378)	Acc@1 62.109 (63.704)	Acc@5 89.453 (89.997)
Epoch: [27][190/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.3390 (2.1442)	Acc@1 58.203 (63.523)	Acc@5 86.328 (89.934)
[INFO] Storing checkpoint...

Epoch: [28 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [28][0/196]	Time 0.026 (0.026)	Data 0.177 (0.177)	Loss 1.8626 (1.8626)	Acc@1 71.484 (71.484)	Acc@5 94.531 (94.531)
Epoch: [28][10/196]	Time 0.018 (0.017)	Data 0.000 (0.018)	Loss 2.2339 (2.0915)	Acc@1 63.672 (65.625)	Acc@5 89.844 (90.909)
Epoch: [28][20/196]	Time 0.013 (0.016)	Data 0.005 (0.011)	Loss 2.0377 (2.0808)	Acc@1 67.578 (65.867)	Acc@5 91.016 (91.016)
Epoch: [28][30/196]	Time 0.013 (0.015)	Data 0.005 (0.008)	Loss 2.0680 (2.0683)	Acc@1 64.062 (66.079)	Acc@5 90.234 (91.255)
Epoch: [28][40/196]	Time 0.012 (0.015)	Data 0.009 (0.007)	Loss 1.9839 (2.0645)	Acc@1 69.922 (66.111)	Acc@5 89.844 (91.292)
Epoch: [28][50/196]	Time 0.014 (0.015)	Data 0.004 (0.007)	Loss 2.1149 (2.0725)	Acc@1 63.672 (65.870)	Acc@5 90.625 (91.153)
Epoch: [28][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 2.1058 (2.0886)	Acc@1 62.109 (65.106)	Acc@5 90.625 (91.003)
Epoch: [28][70/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.2600 (2.0934)	Acc@1 59.375 (65.014)	Acc@5 88.281 (90.840)
Epoch: [28][80/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.1138 (2.0992)	Acc@1 65.234 (64.897)	Acc@5 91.016 (90.794)
Epoch: [28][90/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 2.0400 (2.1034)	Acc@1 71.484 (64.754)	Acc@5 90.234 (90.741)
Epoch: [28][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.1292 (2.1117)	Acc@1 66.797 (64.414)	Acc@5 89.453 (90.656)
Epoch: [28][110/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0833 (2.1187)	Acc@1 66.406 (64.284)	Acc@5 91.016 (90.488)
Epoch: [28][120/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.2749 (2.1286)	Acc@1 60.547 (64.040)	Acc@5 88.672 (90.373)
Epoch: [28][130/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.2717 (2.1333)	Acc@1 63.281 (64.006)	Acc@5 88.672 (90.255)
Epoch: [28][140/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.3433 (2.1342)	Acc@1 59.766 (63.999)	Acc@5 88.672 (90.259)
Epoch: [28][150/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.2704 (2.1344)	Acc@1 60.938 (64.008)	Acc@5 87.891 (90.250)
Epoch: [28][160/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 2.2626 (2.1372)	Acc@1 63.281 (63.980)	Acc@5 87.109 (90.179)
Epoch: [28][170/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 2.2198 (2.1388)	Acc@1 57.422 (63.935)	Acc@5 87.500 (90.132)
Epoch: [28][180/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.1647 (2.1430)	Acc@1 64.453 (63.845)	Acc@5 89.062 (90.040)
Epoch: [28][190/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.2603 (2.1467)	Acc@1 59.375 (63.782)	Acc@5 91.016 (90.028)
[INFO] Storing checkpoint...

Epoch: [29 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [29][0/196]	Time 0.026 (0.026)	Data 0.176 (0.176)	Loss 2.0872 (2.0872)	Acc@1 64.844 (64.844)	Acc@5 91.016 (91.016)
Epoch: [29][10/196]	Time 0.013 (0.016)	Data 0.004 (0.018)	Loss 2.3434 (2.1088)	Acc@1 60.938 (65.305)	Acc@5 87.109 (90.589)
Epoch: [29][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 1.9566 (2.0837)	Acc@1 65.625 (65.346)	Acc@5 93.750 (91.239)
Epoch: [29][30/196]	Time 0.013 (0.016)	Data 0.005 (0.008)	Loss 2.0940 (2.0697)	Acc@1 64.844 (65.675)	Acc@5 91.797 (91.583)
Epoch: [29][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 2.1854 (2.0626)	Acc@1 68.750 (65.949)	Acc@5 89.062 (91.587)
Epoch: [29][50/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 2.0653 (2.0604)	Acc@1 67.188 (65.939)	Acc@5 87.891 (91.452)
Epoch: [29][60/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 2.1636 (2.0591)	Acc@1 63.281 (66.041)	Acc@5 93.359 (91.470)
Epoch: [29][70/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.0328 (2.0588)	Acc@1 63.281 (65.906)	Acc@5 92.578 (91.489)
Epoch: [29][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0457 (2.0657)	Acc@1 67.969 (65.721)	Acc@5 92.969 (91.329)
Epoch: [29][90/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.1052 (2.0747)	Acc@1 65.234 (65.462)	Acc@5 93.750 (91.222)
Epoch: [29][100/196]	Time 0.013 (0.015)	Data 0.006 (0.005)	Loss 2.0005 (2.0776)	Acc@1 69.141 (65.443)	Acc@5 94.531 (91.224)
Epoch: [29][110/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0849 (2.0798)	Acc@1 64.062 (65.414)	Acc@5 91.797 (91.241)
Epoch: [29][120/196]	Time 0.014 (0.015)	Data 0.005 (0.005)	Loss 2.0273 (2.0837)	Acc@1 67.578 (65.334)	Acc@5 89.844 (91.096)
Epoch: [29][130/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.2117 (2.0894)	Acc@1 62.500 (65.222)	Acc@5 89.062 (90.962)
Epoch: [29][140/196]	Time 0.018 (0.015)	Data 0.010 (0.005)	Loss 2.0944 (2.0938)	Acc@1 66.016 (65.104)	Acc@5 91.797 (90.908)
Epoch: [29][150/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.2318 (2.0940)	Acc@1 58.984 (65.100)	Acc@5 85.938 (90.853)
Epoch: [29][160/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0418 (2.0951)	Acc@1 66.406 (65.079)	Acc@5 90.234 (90.807)
Epoch: [29][170/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.2340 (2.0992)	Acc@1 62.891 (64.985)	Acc@5 89.062 (90.762)
Epoch: [29][180/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.1433 (2.1031)	Acc@1 64.453 (64.904)	Acc@5 89.453 (90.716)
Epoch: [29][190/196]	Time 0.019 (0.015)	Data 0.002 (0.005)	Loss 2.2933 (2.1051)	Acc@1 58.594 (64.872)	Acc@5 87.891 (90.650)
[INFO] Storing checkpoint...

Epoch: [30 | 30] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [490, 512, 3, 3]
Epoch: [30][0/196]	Time 0.026 (0.026)	Data 0.183 (0.183)	Loss 2.0388 (2.0388)	Acc@1 64.453 (64.453)	Acc@5 92.969 (92.969)
Epoch: [30][10/196]	Time 0.017 (0.017)	Data 0.000 (0.019)	Loss 2.0633 (2.0949)	Acc@1 69.531 (65.128)	Acc@5 90.234 (90.909)
Epoch: [30][20/196]	Time 0.013 (0.016)	Data 0.005 (0.011)	Loss 2.1517 (2.0886)	Acc@1 62.109 (65.086)	Acc@5 91.016 (90.923)
Epoch: [30][30/196]	Time 0.016 (0.016)	Data 0.002 (0.008)	Loss 1.9873 (2.0633)	Acc@1 64.844 (65.713)	Acc@5 92.969 (91.356)
Epoch: [30][40/196]	Time 0.012 (0.016)	Data 0.009 (0.007)	Loss 2.1204 (2.0600)	Acc@1 66.797 (65.930)	Acc@5 90.234 (91.349)
Epoch: [30][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.1692 (2.0611)	Acc@1 63.672 (65.893)	Acc@5 91.016 (91.445)
Epoch: [30][60/196]	Time 0.015 (0.015)	Data 0.009 (0.006)	Loss 2.1935 (2.0792)	Acc@1 62.109 (65.484)	Acc@5 89.844 (91.221)
Epoch: [30][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.0473 (2.0828)	Acc@1 66.016 (65.614)	Acc@5 93.750 (91.164)
Epoch: [30][80/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 2.2049 (2.0889)	Acc@1 62.891 (65.456)	Acc@5 90.625 (91.093)
Epoch: [30][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.3665 (2.0988)	Acc@1 61.328 (65.200)	Acc@5 85.938 (90.934)
Epoch: [30][100/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.2102 (2.1041)	Acc@1 64.844 (65.080)	Acc@5 89.062 (90.834)
Epoch: [30][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1650 (2.1055)	Acc@1 66.016 (65.027)	Acc@5 90.234 (90.804)
Epoch: [30][120/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.1725 (2.1085)	Acc@1 61.719 (64.941)	Acc@5 89.453 (90.806)
Epoch: [30][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1270 (2.1068)	Acc@1 64.062 (65.023)	Acc@5 90.625 (90.846)
Epoch: [30][140/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.2398 (2.1112)	Acc@1 64.453 (64.924)	Acc@5 89.062 (90.777)
Epoch: [30][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2467 (2.1174)	Acc@1 59.766 (64.753)	Acc@5 87.500 (90.679)
Epoch: [30][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.3929 (2.1191)	Acc@1 58.203 (64.688)	Acc@5 86.328 (90.620)
Epoch: [30][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2106 (2.1252)	Acc@1 62.109 (64.506)	Acc@5 88.672 (90.540)
Epoch: [30][180/196]	Time 0.013 (0.015)	Data 0.009 (0.004)	Loss 2.1280 (2.1263)	Acc@1 65.234 (64.429)	Acc@5 92.969 (90.539)
Epoch: [30][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0859 (2.1276)	Acc@1 65.234 (64.433)	Acc@5 89.844 (90.492)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [64, 3, 3, 3] >> [63, 3, 3, 3]
[module.conv2.weight]: [128, 64, 3, 3] >> [128, 63, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [256, 128, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [512, 256, 3, 3]
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv7.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv8.weight]: [490, 512, 3, 3] >> [463, 512, 3, 3]
[module.fc.weight]: [100, 490] >> [100, 463]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
52.47
python src/cifar.py --workers 4 --dataset cifar100 --epochs 40 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_40.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 9.04M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [63, 3, 3, 3]
conv2 --> [128, 63, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [463, 512, 3, 3]
fc --> [463, 100]
1, 697600512, 1741824, 63
2, 7766212608, 18579456, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 6554124288, 8534016, 463
fc, 17779200, 46300, 0
===================
FLOP REPORT: 30822882000000.0 60132800000.0 151584988 150332 2702 17.242855072021484

Epoch: [31 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [31][0/196]	Time 0.740 (0.740)	Data 0.169 (0.169)	Loss 2.3027 (2.3027)	Acc@1 60.547 (60.547)	Acc@5 87.109 (87.109)
Epoch: [31][10/196]	Time 0.015 (0.081)	Data 0.003 (0.017)	Loss 2.0813 (2.0399)	Acc@1 63.281 (66.300)	Acc@5 91.797 (92.188)
Epoch: [31][20/196]	Time 0.016 (0.050)	Data 0.002 (0.010)	Loss 2.1323 (2.0692)	Acc@1 60.938 (65.774)	Acc@5 91.016 (91.667)
Epoch: [31][30/196]	Time 0.015 (0.039)	Data 0.002 (0.007)	Loss 2.1320 (2.0628)	Acc@1 62.109 (65.789)	Acc@5 91.797 (92.011)
Epoch: [31][40/196]	Time 0.015 (0.033)	Data 0.002 (0.006)	Loss 1.9697 (2.0571)	Acc@1 70.312 (65.806)	Acc@5 91.406 (91.968)
Epoch: [31][50/196]	Time 0.015 (0.030)	Data 0.003 (0.005)	Loss 2.0120 (2.0582)	Acc@1 67.969 (65.916)	Acc@5 91.797 (91.843)
Epoch: [31][60/196]	Time 0.016 (0.027)	Data 0.002 (0.005)	Loss 2.1192 (2.0582)	Acc@1 63.672 (66.003)	Acc@5 91.016 (91.803)
Epoch: [31][70/196]	Time 0.012 (0.026)	Data 0.006 (0.004)	Loss 2.1283 (2.0673)	Acc@1 64.844 (65.812)	Acc@5 91.016 (91.626)
Epoch: [31][80/196]	Time 0.017 (0.024)	Data 0.001 (0.004)	Loss 2.2570 (2.0786)	Acc@1 62.891 (65.673)	Acc@5 89.844 (91.493)
Epoch: [31][90/196]	Time 0.012 (0.023)	Data 0.006 (0.004)	Loss 1.9750 (2.0826)	Acc@1 69.531 (65.672)	Acc@5 92.188 (91.320)
Epoch: [31][100/196]	Time 0.017 (0.023)	Data 0.001 (0.004)	Loss 2.0206 (2.0789)	Acc@1 68.359 (65.830)	Acc@5 90.234 (91.360)
Epoch: [31][110/196]	Time 0.011 (0.022)	Data 0.006 (0.004)	Loss 2.1400 (2.0854)	Acc@1 63.281 (65.709)	Acc@5 92.188 (91.304)
Epoch: [31][120/196]	Time 0.017 (0.021)	Data 0.000 (0.004)	Loss 2.4728 (2.0899)	Acc@1 55.078 (65.531)	Acc@5 86.328 (91.251)
Epoch: [31][130/196]	Time 0.011 (0.021)	Data 0.007 (0.003)	Loss 2.2847 (2.0961)	Acc@1 64.453 (65.428)	Acc@5 89.453 (91.153)
Epoch: [31][140/196]	Time 0.017 (0.020)	Data 0.001 (0.003)	Loss 2.1003 (2.1024)	Acc@1 62.891 (65.298)	Acc@5 90.625 (91.063)
Epoch: [31][150/196]	Time 0.012 (0.020)	Data 0.006 (0.003)	Loss 2.2129 (2.1033)	Acc@1 64.844 (65.320)	Acc@5 88.281 (91.021)
Epoch: [31][160/196]	Time 0.019 (0.020)	Data 0.001 (0.003)	Loss 2.0799 (2.1087)	Acc@1 66.797 (65.208)	Acc@5 91.406 (90.948)
Epoch: [31][170/196]	Time 0.015 (0.020)	Data 0.004 (0.003)	Loss 2.0687 (2.1125)	Acc@1 64.453 (65.125)	Acc@5 90.625 (90.879)
Epoch: [31][180/196]	Time 0.021 (0.019)	Data 0.001 (0.003)	Loss 2.1839 (2.1157)	Acc@1 64.453 (64.999)	Acc@5 90.625 (90.811)
Epoch: [31][190/196]	Time 0.012 (0.019)	Data 0.006 (0.003)	Loss 2.1907 (2.1167)	Acc@1 64.453 (64.971)	Acc@5 86.719 (90.772)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [32 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [32][0/196]	Time 0.027 (0.027)	Data 0.170 (0.170)	Loss 1.9492 (1.9492)	Acc@1 68.750 (68.750)	Acc@5 92.969 (92.969)
Epoch: [32][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 2.1313 (2.0395)	Acc@1 62.500 (67.472)	Acc@5 88.281 (91.513)
Epoch: [32][20/196]	Time 0.010 (0.016)	Data 0.008 (0.011)	Loss 2.1278 (2.0563)	Acc@1 62.500 (66.704)	Acc@5 91.016 (91.685)
Epoch: [32][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.8652 (2.0495)	Acc@1 71.094 (66.822)	Acc@5 95.312 (91.759)
Epoch: [32][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.0248 (2.0586)	Acc@1 68.359 (66.683)	Acc@5 92.188 (91.473)
Epoch: [32][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9287 (2.0523)	Acc@1 68.359 (66.912)	Acc@5 93.750 (91.720)
Epoch: [32][60/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.0368 (2.0487)	Acc@1 66.406 (67.027)	Acc@5 91.797 (91.803)
Epoch: [32][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0425 (2.0456)	Acc@1 65.234 (67.116)	Acc@5 94.531 (91.808)
Epoch: [32][80/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.1265 (2.0453)	Acc@1 66.016 (67.144)	Acc@5 91.406 (91.845)
Epoch: [32][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9968 (2.0454)	Acc@1 70.703 (67.235)	Acc@5 90.234 (91.720)
Epoch: [32][100/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 1.9827 (2.0471)	Acc@1 70.703 (67.226)	Acc@5 92.188 (91.681)
Epoch: [32][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0959 (2.0565)	Acc@1 65.234 (66.990)	Acc@5 89.062 (91.519)
Epoch: [32][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0786 (2.0664)	Acc@1 65.625 (66.690)	Acc@5 91.797 (91.374)
Epoch: [32][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1734 (2.0699)	Acc@1 60.547 (66.487)	Acc@5 91.406 (91.350)
Epoch: [32][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0961 (2.0759)	Acc@1 65.234 (66.298)	Acc@5 91.406 (91.318)
Epoch: [32][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2133 (2.0839)	Acc@1 66.016 (66.088)	Acc@5 85.938 (91.207)
Epoch: [32][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1441 (2.0913)	Acc@1 67.969 (65.933)	Acc@5 90.234 (91.137)
Epoch: [32][170/196]	Time 0.017 (0.015)	Data 0.001 (0.003)	Loss 2.1938 (2.0965)	Acc@1 66.406 (65.792)	Acc@5 86.719 (91.043)
Epoch: [32][180/196]	Time 0.011 (0.015)	Data 0.006 (0.003)	Loss 2.2707 (2.0995)	Acc@1 62.109 (65.690)	Acc@5 87.891 (90.985)
Epoch: [32][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.1628 (2.1062)	Acc@1 62.891 (65.502)	Acc@5 87.500 (90.897)
[INFO] Storing checkpoint...

Epoch: [33 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [33][0/196]	Time 0.024 (0.024)	Data 0.168 (0.168)	Loss 1.9466 (1.9466)	Acc@1 71.094 (71.094)	Acc@5 93.359 (93.359)
Epoch: [33][10/196]	Time 0.015 (0.016)	Data 0.003 (0.017)	Loss 2.0502 (2.0389)	Acc@1 69.141 (67.543)	Acc@5 91.016 (91.974)
Epoch: [33][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 2.0430 (2.0294)	Acc@1 67.578 (67.801)	Acc@5 90.625 (91.908)
Epoch: [33][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.9699 (2.0324)	Acc@1 68.359 (67.679)	Acc@5 92.969 (91.898)
Epoch: [33][40/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.0139 (2.0388)	Acc@1 67.578 (67.511)	Acc@5 90.625 (91.806)
Epoch: [33][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.0200 (2.0378)	Acc@1 71.875 (67.578)	Acc@5 92.578 (91.904)
Epoch: [33][60/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.1966 (2.0493)	Acc@1 66.406 (67.277)	Acc@5 90.234 (91.829)
Epoch: [33][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1551 (2.0517)	Acc@1 64.844 (67.248)	Acc@5 87.500 (91.769)
Epoch: [33][80/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1311 (2.0532)	Acc@1 64.844 (67.221)	Acc@5 90.625 (91.691)
Epoch: [33][90/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0426 (2.0605)	Acc@1 67.578 (66.930)	Acc@5 91.797 (91.647)
Epoch: [33][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3428 (2.0676)	Acc@1 61.328 (66.739)	Acc@5 86.328 (91.646)
Epoch: [33][110/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0098 (2.0690)	Acc@1 67.578 (66.716)	Acc@5 91.016 (91.617)
Epoch: [33][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3125 (2.0732)	Acc@1 57.812 (66.535)	Acc@5 91.016 (91.661)
Epoch: [33][130/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1694 (2.0783)	Acc@1 61.328 (66.451)	Acc@5 92.188 (91.576)
Epoch: [33][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2164 (2.0855)	Acc@1 62.891 (66.262)	Acc@5 90.625 (91.495)
Epoch: [33][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2196 (2.0903)	Acc@1 61.719 (66.104)	Acc@5 88.281 (91.393)
Epoch: [33][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0387 (2.0980)	Acc@1 65.625 (65.931)	Acc@5 92.578 (91.249)
Epoch: [33][170/196]	Time 0.017 (0.015)	Data 0.002 (0.003)	Loss 2.3572 (2.1035)	Acc@1 57.031 (65.785)	Acc@5 86.328 (91.171)
Epoch: [33][180/196]	Time 0.017 (0.015)	Data 0.001 (0.003)	Loss 2.0939 (2.1059)	Acc@1 66.016 (65.651)	Acc@5 89.453 (91.126)
Epoch: [33][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.2075 (2.1105)	Acc@1 64.453 (65.549)	Acc@5 91.797 (91.089)
[INFO] Storing checkpoint...

Epoch: [34 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [34][0/196]	Time 0.024 (0.024)	Data 0.170 (0.170)	Loss 2.0296 (2.0296)	Acc@1 67.578 (67.578)	Acc@5 92.969 (92.969)
Epoch: [34][10/196]	Time 0.013 (0.016)	Data 0.004 (0.017)	Loss 2.0095 (2.1341)	Acc@1 68.750 (65.518)	Acc@5 92.969 (91.016)
Epoch: [34][20/196]	Time 0.010 (0.016)	Data 0.006 (0.010)	Loss 1.9401 (2.1015)	Acc@1 69.141 (66.220)	Acc@5 93.359 (91.053)
Epoch: [34][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9884 (2.0867)	Acc@1 70.703 (66.520)	Acc@5 91.797 (91.205)
Epoch: [34][40/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.1064 (2.0672)	Acc@1 64.062 (67.026)	Acc@5 90.234 (91.530)
Epoch: [34][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0085 (2.0553)	Acc@1 71.875 (67.371)	Acc@5 91.797 (91.690)
Epoch: [34][60/196]	Time 0.010 (0.015)	Data 0.009 (0.005)	Loss 2.0723 (2.0516)	Acc@1 67.188 (67.540)	Acc@5 92.188 (91.797)
Epoch: [34][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1498 (2.0493)	Acc@1 62.891 (67.347)	Acc@5 91.406 (91.868)
Epoch: [34][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 1.9600 (2.0554)	Acc@1 69.922 (67.236)	Acc@5 93.750 (91.807)
Epoch: [34][90/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0858 (2.0633)	Acc@1 68.359 (66.986)	Acc@5 91.016 (91.741)
Epoch: [34][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0169 (2.0627)	Acc@1 69.922 (66.967)	Acc@5 92.969 (91.832)
Epoch: [34][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1253 (2.0623)	Acc@1 64.062 (67.008)	Acc@5 89.844 (91.839)
Epoch: [34][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1514 (2.0738)	Acc@1 63.672 (66.755)	Acc@5 91.797 (91.687)
Epoch: [34][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3835 (2.0831)	Acc@1 60.156 (66.529)	Acc@5 88.281 (91.564)
Epoch: [34][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2779 (2.0903)	Acc@1 60.156 (66.284)	Acc@5 88.672 (91.467)
Epoch: [34][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0587 (2.0967)	Acc@1 66.797 (66.070)	Acc@5 91.797 (91.404)
Epoch: [34][160/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.3228 (2.1046)	Acc@1 63.281 (65.906)	Acc@5 88.281 (91.241)
Epoch: [34][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2244 (2.1109)	Acc@1 60.938 (65.714)	Acc@5 89.062 (91.153)
Epoch: [34][180/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.1926 (2.1124)	Acc@1 62.891 (65.668)	Acc@5 90.234 (91.126)
Epoch: [34][190/196]	Time 0.017 (0.015)	Data 0.001 (0.003)	Loss 2.2728 (2.1170)	Acc@1 62.500 (65.558)	Acc@5 88.281 (91.044)
[INFO] Storing checkpoint...

Epoch: [35 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [35][0/196]	Time 0.025 (0.025)	Data 0.173 (0.173)	Loss 2.1090 (2.1090)	Acc@1 63.672 (63.672)	Acc@5 88.281 (88.281)
Epoch: [35][10/196]	Time 0.014 (0.016)	Data 0.005 (0.018)	Loss 1.9487 (2.0259)	Acc@1 70.703 (67.720)	Acc@5 93.359 (91.974)
Epoch: [35][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 1.9602 (2.0268)	Acc@1 70.703 (67.950)	Acc@5 94.141 (92.281)
Epoch: [35][30/196]	Time 0.013 (0.015)	Data 0.005 (0.008)	Loss 2.0770 (2.0269)	Acc@1 63.672 (67.717)	Acc@5 94.141 (92.351)
Epoch: [35][40/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.0479 (2.0141)	Acc@1 62.891 (68.007)	Acc@5 92.578 (92.550)
Epoch: [35][50/196]	Time 0.014 (0.015)	Data 0.005 (0.006)	Loss 2.0606 (2.0146)	Acc@1 68.750 (67.946)	Acc@5 92.578 (92.540)
Epoch: [35][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 1.8641 (2.0215)	Acc@1 71.094 (67.713)	Acc@5 92.969 (92.277)
Epoch: [35][70/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.2494 (2.0345)	Acc@1 62.109 (67.342)	Acc@5 89.844 (92.088)
Epoch: [35][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9981 (2.0385)	Acc@1 70.703 (67.192)	Acc@5 92.578 (92.052)
Epoch: [35][90/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1099 (2.0392)	Acc@1 69.922 (67.205)	Acc@5 90.234 (91.964)
Epoch: [35][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2241 (2.0468)	Acc@1 64.453 (67.029)	Acc@5 91.406 (91.967)
Epoch: [35][110/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1766 (2.0515)	Acc@1 62.500 (66.913)	Acc@5 91.016 (91.913)
Epoch: [35][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2017 (2.0566)	Acc@1 57.812 (66.716)	Acc@5 92.578 (91.913)
Epoch: [35][130/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0840 (2.0606)	Acc@1 67.969 (66.722)	Acc@5 91.406 (91.821)
Epoch: [35][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 1.9057 (2.0657)	Acc@1 70.312 (66.689)	Acc@5 94.922 (91.766)
Epoch: [35][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0998 (2.0732)	Acc@1 66.016 (66.515)	Acc@5 90.625 (91.631)
Epoch: [35][160/196]	Time 0.013 (0.015)	Data 0.007 (0.004)	Loss 2.0340 (2.0765)	Acc@1 71.094 (66.515)	Acc@5 91.406 (91.574)
Epoch: [35][170/196]	Time 0.015 (0.015)	Data 0.003 (0.003)	Loss 2.1495 (2.0840)	Acc@1 67.188 (66.402)	Acc@5 92.969 (91.450)
Epoch: [35][180/196]	Time 0.011 (0.015)	Data 0.007 (0.003)	Loss 2.0713 (2.0903)	Acc@1 67.578 (66.206)	Acc@5 91.797 (91.372)
Epoch: [35][190/196]	Time 0.014 (0.015)	Data 0.004 (0.003)	Loss 2.3129 (2.0956)	Acc@1 62.891 (66.075)	Acc@5 87.500 (91.292)
[INFO] Storing checkpoint...

Epoch: [36 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [36][0/196]	Time 0.025 (0.025)	Data 0.168 (0.168)	Loss 1.9500 (1.9500)	Acc@1 71.094 (71.094)	Acc@5 93.359 (93.359)
Epoch: [36][10/196]	Time 0.017 (0.015)	Data 0.000 (0.018)	Loss 2.1039 (2.0342)	Acc@1 66.406 (67.507)	Acc@5 89.844 (92.365)
Epoch: [36][20/196]	Time 0.015 (0.015)	Data 0.003 (0.011)	Loss 2.1343 (2.0472)	Acc@1 66.406 (67.485)	Acc@5 91.797 (91.760)
Epoch: [36][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 1.9104 (2.0320)	Acc@1 72.266 (67.843)	Acc@5 94.141 (92.137)
Epoch: [36][40/196]	Time 0.013 (0.015)	Data 0.005 (0.007)	Loss 2.0725 (2.0282)	Acc@1 69.531 (67.931)	Acc@5 91.797 (92.235)
Epoch: [36][50/196]	Time 0.017 (0.015)	Data 0.003 (0.006)	Loss 2.0216 (2.0370)	Acc@1 66.406 (67.632)	Acc@5 91.797 (92.073)
Epoch: [36][60/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0818 (2.0437)	Acc@1 68.359 (67.450)	Acc@5 91.797 (92.098)
Epoch: [36][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1844 (2.0534)	Acc@1 58.203 (66.984)	Acc@5 93.359 (92.050)
Epoch: [36][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.4318 (2.0633)	Acc@1 54.688 (66.647)	Acc@5 87.500 (91.922)
Epoch: [36][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0033 (2.0651)	Acc@1 68.359 (66.728)	Acc@5 91.797 (91.831)
Epoch: [36][100/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1906 (2.0727)	Acc@1 66.406 (66.468)	Acc@5 88.672 (91.720)
Epoch: [36][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2013 (2.0815)	Acc@1 64.844 (66.297)	Acc@5 92.969 (91.551)
Epoch: [36][120/196]	Time 0.017 (0.015)	Data 0.003 (0.004)	Loss 2.0778 (2.0891)	Acc@1 64.844 (65.980)	Acc@5 91.406 (91.516)
Epoch: [36][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9913 (2.0945)	Acc@1 69.922 (65.911)	Acc@5 94.922 (91.490)
Epoch: [36][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2886 (2.1000)	Acc@1 62.109 (65.838)	Acc@5 87.500 (91.359)
Epoch: [36][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9569 (2.1025)	Acc@1 70.312 (65.850)	Acc@5 93.750 (91.295)
Epoch: [36][160/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.2455 (2.1038)	Acc@1 62.109 (65.802)	Acc@5 85.156 (91.261)
Epoch: [36][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1733 (2.1024)	Acc@1 64.062 (65.821)	Acc@5 89.062 (91.262)
Epoch: [36][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1373 (2.1010)	Acc@1 65.625 (65.908)	Acc@5 91.406 (91.279)
Epoch: [36][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0730 (2.1020)	Acc@1 68.750 (65.893)	Acc@5 90.234 (91.230)
[INFO] Storing checkpoint...

Epoch: [37 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [37][0/196]	Time 0.024 (0.024)	Data 0.182 (0.182)	Loss 1.9445 (1.9445)	Acc@1 67.969 (67.969)	Acc@5 92.969 (92.969)
Epoch: [37][10/196]	Time 0.013 (0.015)	Data 0.005 (0.019)	Loss 2.0088 (2.0392)	Acc@1 65.625 (66.903)	Acc@5 93.750 (93.217)
Epoch: [37][20/196]	Time 0.010 (0.015)	Data 0.008 (0.011)	Loss 1.9323 (2.0047)	Acc@1 72.266 (67.857)	Acc@5 92.188 (93.155)
Epoch: [37][30/196]	Time 0.015 (0.015)	Data 0.003 (0.008)	Loss 1.8626 (2.0185)	Acc@1 71.094 (67.641)	Acc@5 94.531 (92.616)
Epoch: [37][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 2.1116 (2.0120)	Acc@1 67.969 (68.093)	Acc@5 91.797 (92.635)
Epoch: [37][50/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 2.0776 (2.0143)	Acc@1 67.969 (68.030)	Acc@5 92.578 (92.555)
Epoch: [37][60/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0472 (2.0074)	Acc@1 67.578 (68.289)	Acc@5 91.797 (92.559)
Epoch: [37][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9354 (2.0104)	Acc@1 71.094 (68.189)	Acc@5 90.625 (92.534)
Epoch: [37][80/196]	Time 0.010 (0.015)	Data 0.008 (0.005)	Loss 2.0457 (2.0121)	Acc@1 67.969 (68.133)	Acc@5 93.359 (92.540)
Epoch: [37][90/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1914 (2.0271)	Acc@1 65.234 (67.797)	Acc@5 90.234 (92.359)
Epoch: [37][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0625 (2.0376)	Acc@1 66.797 (67.605)	Acc@5 91.016 (92.195)
Epoch: [37][110/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1946 (2.0512)	Acc@1 61.719 (67.385)	Acc@5 91.406 (92.029)
Epoch: [37][120/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.2376 (2.0606)	Acc@1 59.766 (67.055)	Acc@5 88.672 (91.955)
Epoch: [37][130/196]	Time 0.013 (0.015)	Data 0.006 (0.004)	Loss 2.1926 (2.0658)	Acc@1 64.453 (66.976)	Acc@5 89.453 (91.848)
Epoch: [37][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.2997 (2.0718)	Acc@1 63.281 (66.861)	Acc@5 86.719 (91.753)
Epoch: [37][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 1.9733 (2.0746)	Acc@1 69.922 (66.799)	Acc@5 92.969 (91.701)
Epoch: [37][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1690 (2.0795)	Acc@1 64.062 (66.683)	Acc@5 89.453 (91.617)
Epoch: [37][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2112 (2.0795)	Acc@1 62.109 (66.676)	Acc@5 92.578 (91.644)
Epoch: [37][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0800 (2.0833)	Acc@1 69.922 (66.607)	Acc@5 90.625 (91.544)
Epoch: [37][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.0518 (2.0894)	Acc@1 71.875 (66.464)	Acc@5 89.062 (91.441)
[INFO] Storing checkpoint...

Epoch: [38 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [38][0/196]	Time 0.025 (0.025)	Data 0.181 (0.181)	Loss 1.8903 (1.8903)	Acc@1 71.484 (71.484)	Acc@5 93.750 (93.750)
Epoch: [38][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 2.0802 (1.9656)	Acc@1 66.016 (69.567)	Acc@5 92.578 (93.288)
Epoch: [38][20/196]	Time 0.014 (0.016)	Data 0.005 (0.011)	Loss 2.0061 (1.9921)	Acc@1 69.531 (69.438)	Acc@5 92.578 (92.783)
Epoch: [38][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9492 (1.9862)	Acc@1 70.312 (69.367)	Acc@5 94.922 (92.792)
Epoch: [38][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 2.2329 (2.0108)	Acc@1 62.891 (68.931)	Acc@5 89.062 (92.426)
Epoch: [38][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.2420 (2.0259)	Acc@1 63.672 (68.581)	Acc@5 90.625 (92.295)
Epoch: [38][60/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.1042 (2.0224)	Acc@1 66.797 (68.577)	Acc@5 89.062 (92.354)
Epoch: [38][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1670 (2.0322)	Acc@1 67.188 (68.271)	Acc@5 91.406 (92.309)
Epoch: [38][80/196]	Time 0.013 (0.015)	Data 0.008 (0.005)	Loss 2.0065 (2.0442)	Acc@1 69.141 (67.896)	Acc@5 94.531 (92.183)
Epoch: [38][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0953 (2.0496)	Acc@1 63.281 (67.690)	Acc@5 91.016 (92.136)
Epoch: [38][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1323 (2.0537)	Acc@1 64.453 (67.582)	Acc@5 91.016 (92.137)
Epoch: [38][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0232 (2.0612)	Acc@1 68.750 (67.409)	Acc@5 92.969 (92.022)
Epoch: [38][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0658 (2.0658)	Acc@1 68.359 (67.310)	Acc@5 91.797 (91.939)
Epoch: [38][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2303 (2.0751)	Acc@1 61.719 (67.056)	Acc@5 92.578 (91.871)
Epoch: [38][140/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.2409 (2.0810)	Acc@1 65.625 (66.888)	Acc@5 87.891 (91.777)
Epoch: [38][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2081 (2.0857)	Acc@1 63.672 (66.766)	Acc@5 91.016 (91.699)
Epoch: [38][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1744 (2.0908)	Acc@1 64.453 (66.603)	Acc@5 92.188 (91.651)
Epoch: [38][170/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.2516 (2.0945)	Acc@1 63.672 (66.543)	Acc@5 91.406 (91.596)
Epoch: [38][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1788 (2.0966)	Acc@1 64.844 (66.424)	Acc@5 89.453 (91.629)
Epoch: [38][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0048 (2.0980)	Acc@1 67.188 (66.408)	Acc@5 90.234 (91.578)
[INFO] Storing checkpoint...

Epoch: [39 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [39][0/196]	Time 0.027 (0.027)	Data 0.179 (0.179)	Loss 1.9469 (1.9469)	Acc@1 72.656 (72.656)	Acc@5 92.969 (92.969)
Epoch: [39][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 2.0572 (2.0781)	Acc@1 68.750 (66.690)	Acc@5 91.797 (92.045)
Epoch: [39][20/196]	Time 0.014 (0.015)	Data 0.002 (0.011)	Loss 2.0011 (2.0494)	Acc@1 66.406 (67.485)	Acc@5 93.359 (92.355)
Epoch: [39][30/196]	Time 0.020 (0.015)	Data 0.000 (0.008)	Loss 1.9295 (2.0274)	Acc@1 71.875 (68.246)	Acc@5 93.359 (92.629)
Epoch: [39][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 1.9234 (2.0302)	Acc@1 68.359 (68.216)	Acc@5 93.359 (92.530)
Epoch: [39][50/196]	Time 0.016 (0.015)	Data 0.002 (0.006)	Loss 1.9601 (2.0363)	Acc@1 70.312 (68.076)	Acc@5 94.922 (92.402)
Epoch: [39][60/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.1200 (2.0454)	Acc@1 66.406 (67.879)	Acc@5 89.453 (92.239)
Epoch: [39][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9950 (2.0478)	Acc@1 68.750 (67.749)	Acc@5 92.188 (92.226)
Epoch: [39][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0164 (2.0521)	Acc@1 68.359 (67.646)	Acc@5 90.625 (92.106)
Epoch: [39][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1205 (2.0532)	Acc@1 67.578 (67.651)	Acc@5 88.672 (92.020)
Epoch: [39][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9289 (2.0511)	Acc@1 71.875 (67.725)	Acc@5 94.922 (92.083)
Epoch: [39][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1637 (2.0528)	Acc@1 63.672 (67.691)	Acc@5 90.234 (92.071)
Epoch: [39][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1769 (2.0556)	Acc@1 64.453 (67.633)	Acc@5 92.188 (92.055)
Epoch: [39][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1530 (2.0608)	Acc@1 67.578 (67.495)	Acc@5 89.453 (91.964)
Epoch: [39][140/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.0302 (2.0662)	Acc@1 65.625 (67.312)	Acc@5 93.750 (91.960)
Epoch: [39][150/196]	Time 0.019 (0.015)	Data 0.001 (0.004)	Loss 2.0582 (2.0681)	Acc@1 69.141 (67.294)	Acc@5 91.797 (91.960)
Epoch: [39][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1668 (2.0714)	Acc@1 64.453 (67.185)	Acc@5 92.188 (91.930)
Epoch: [39][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0978 (2.0725)	Acc@1 66.406 (67.158)	Acc@5 92.188 (91.872)
Epoch: [39][180/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.1466 (2.0775)	Acc@1 64.844 (66.998)	Acc@5 91.016 (91.801)
Epoch: [39][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.2719 (2.0809)	Acc@1 62.500 (66.911)	Acc@5 88.281 (91.733)
[INFO] Storing checkpoint...

Epoch: [40 | 40] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [463, 512, 3, 3]
Epoch: [40][0/196]	Time 0.027 (0.027)	Data 0.189 (0.189)	Loss 2.1072 (2.1072)	Acc@1 65.234 (65.234)	Acc@5 91.016 (91.016)
Epoch: [40][10/196]	Time 0.017 (0.016)	Data 0.000 (0.020)	Loss 2.0666 (2.0456)	Acc@1 69.922 (67.365)	Acc@5 91.016 (92.791)
Epoch: [40][20/196]	Time 0.011 (0.016)	Data 0.006 (0.012)	Loss 1.9828 (2.0120)	Acc@1 69.141 (68.452)	Acc@5 94.531 (93.006)
Epoch: [40][30/196]	Time 0.015 (0.015)	Data 0.002 (0.009)	Loss 1.8753 (2.0005)	Acc@1 69.922 (68.750)	Acc@5 94.531 (93.145)
Epoch: [40][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 2.1435 (2.0101)	Acc@1 65.625 (68.655)	Acc@5 89.453 (92.845)
Epoch: [40][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.0437 (1.9998)	Acc@1 66.016 (68.781)	Acc@5 93.359 (92.961)
Epoch: [40][60/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 2.0999 (2.0030)	Acc@1 66.016 (68.603)	Acc@5 92.578 (92.943)
Epoch: [40][70/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 1.9947 (2.0064)	Acc@1 71.094 (68.563)	Acc@5 92.969 (92.815)
Epoch: [40][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.1679 (2.0136)	Acc@1 66.016 (68.490)	Acc@5 89.453 (92.732)
Epoch: [40][90/196]	Time 0.010 (0.015)	Data 0.009 (0.005)	Loss 2.0724 (2.0258)	Acc@1 65.234 (68.243)	Acc@5 92.188 (92.518)
Epoch: [40][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 1.9840 (2.0353)	Acc@1 69.922 (68.038)	Acc@5 93.750 (92.427)
Epoch: [40][110/196]	Time 0.010 (0.015)	Data 0.007 (0.005)	Loss 2.0907 (2.0410)	Acc@1 64.453 (67.807)	Acc@5 90.234 (92.360)
Epoch: [40][120/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.1553 (2.0450)	Acc@1 65.625 (67.723)	Acc@5 90.234 (92.304)
Epoch: [40][130/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 1.9951 (2.0480)	Acc@1 69.141 (67.617)	Acc@5 93.750 (92.277)
Epoch: [40][140/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.1744 (2.0535)	Acc@1 63.281 (67.545)	Acc@5 91.406 (92.188)
Epoch: [40][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1540 (2.0532)	Acc@1 64.453 (67.596)	Acc@5 92.578 (92.182)
Epoch: [40][160/196]	Time 0.011 (0.015)	Data 0.005 (0.004)	Loss 2.0835 (2.0552)	Acc@1 69.531 (67.503)	Acc@5 89.844 (92.107)
Epoch: [40][170/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.0093 (2.0587)	Acc@1 70.703 (67.382)	Acc@5 91.797 (91.996)
Epoch: [40][180/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 1.9966 (2.0618)	Acc@1 69.531 (67.347)	Acc@5 92.188 (91.941)
Epoch: [40][190/196]	Time 0.010 (0.015)	Data 0.013 (0.004)	Loss 2.1722 (2.0650)	Acc@1 60.938 (67.259)	Acc@5 92.578 (91.954)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [63, 3, 3, 3] >> [62, 3, 3, 3]
[module.conv2.weight]: [128, 63, 3, 3] >> [128, 62, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [256, 128, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [512, 256, 3, 3]
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv7.weight]: [512, 512, 3, 3] >> [509, 512, 3, 3]
[module.conv8.weight]: [463, 512, 3, 3] >> [432, 509, 3, 3]
[module.fc.weight]: [100, 463] >> [100, 432]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
57.53
python src/cifar.py --workers 4 --dataset cifar100 --epochs 50 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_50.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 8.87M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [62, 3, 3, 3]
conv2 --> [128, 62, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [509, 512, 3, 3]
conv8 --> [432, 509, 3, 3]
fc --> [432, 100]
1, 686527488, 1714176, 62
2, 7642939392, 18284544, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7205289984, 9381888, 509
8, 6079463424, 7915968, 432
fc, 16588800, 43200, 0
===================
FLOP REPORT: 30567934800000.0 59668800000.0 150585984 149172 2667 16.91349220275879

Epoch: [41 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [41][0/196]	Time 0.767 (0.767)	Data 0.179 (0.179)	Loss 1.9647 (1.9647)	Acc@1 70.703 (70.703)	Acc@5 92.969 (92.969)
Epoch: [41][10/196]	Time 0.016 (0.084)	Data 0.002 (0.018)	Loss 2.1067 (2.0003)	Acc@1 63.672 (68.821)	Acc@5 91.797 (92.614)
Epoch: [41][20/196]	Time 0.016 (0.051)	Data 0.002 (0.010)	Loss 2.0361 (2.0018)	Acc@1 66.797 (69.457)	Acc@5 94.141 (92.727)
Epoch: [41][30/196]	Time 0.015 (0.040)	Data 0.003 (0.008)	Loss 2.0041 (2.0041)	Acc@1 70.312 (69.254)	Acc@5 91.016 (92.692)
Epoch: [41][40/196]	Time 0.015 (0.034)	Data 0.002 (0.006)	Loss 2.0377 (2.0085)	Acc@1 66.016 (68.855)	Acc@5 92.188 (92.731)
Epoch: [41][50/196]	Time 0.016 (0.031)	Data 0.002 (0.005)	Loss 2.0209 (2.0061)	Acc@1 67.188 (68.827)	Acc@5 92.188 (92.831)
Epoch: [41][60/196]	Time 0.015 (0.028)	Data 0.003 (0.005)	Loss 2.0653 (2.0174)	Acc@1 67.578 (68.468)	Acc@5 93.359 (92.713)
Epoch: [41][70/196]	Time 0.016 (0.027)	Data 0.002 (0.004)	Loss 2.1638 (2.0237)	Acc@1 61.328 (68.299)	Acc@5 93.359 (92.677)
Epoch: [41][80/196]	Time 0.016 (0.025)	Data 0.002 (0.004)	Loss 2.1539 (2.0311)	Acc@1 63.672 (68.099)	Acc@5 91.797 (92.655)
Epoch: [41][90/196]	Time 0.018 (0.024)	Data 0.002 (0.004)	Loss 2.0651 (2.0330)	Acc@1 65.234 (68.016)	Acc@5 92.578 (92.643)
Epoch: [41][100/196]	Time 0.016 (0.023)	Data 0.002 (0.004)	Loss 1.9354 (2.0367)	Acc@1 69.141 (68.050)	Acc@5 94.141 (92.586)
Epoch: [41][110/196]	Time 0.016 (0.023)	Data 0.002 (0.003)	Loss 2.1703 (2.0396)	Acc@1 65.234 (67.895)	Acc@5 89.453 (92.529)
Epoch: [41][120/196]	Time 0.015 (0.022)	Data 0.002 (0.003)	Loss 2.2978 (2.0487)	Acc@1 64.844 (67.652)	Acc@5 87.109 (92.394)
Epoch: [41][130/196]	Time 0.016 (0.022)	Data 0.002 (0.003)	Loss 2.1074 (2.0578)	Acc@1 67.969 (67.447)	Acc@5 91.406 (92.271)
Epoch: [41][140/196]	Time 0.015 (0.021)	Data 0.002 (0.003)	Loss 2.1187 (2.0667)	Acc@1 62.891 (67.232)	Acc@5 91.016 (92.102)
Epoch: [41][150/196]	Time 0.015 (0.021)	Data 0.002 (0.003)	Loss 2.1274 (2.0768)	Acc@1 66.797 (66.978)	Acc@5 89.453 (91.991)
Epoch: [41][160/196]	Time 0.016 (0.021)	Data 0.002 (0.003)	Loss 2.0640 (2.0802)	Acc@1 67.578 (66.889)	Acc@5 92.969 (91.969)
Epoch: [41][170/196]	Time 0.016 (0.020)	Data 0.002 (0.003)	Loss 2.0875 (2.0840)	Acc@1 66.797 (66.763)	Acc@5 92.578 (91.932)
Epoch: [41][180/196]	Time 0.016 (0.020)	Data 0.002 (0.003)	Loss 2.0469 (2.0863)	Acc@1 69.141 (66.745)	Acc@5 91.797 (91.907)
Epoch: [41][190/196]	Time 0.016 (0.020)	Data 0.002 (0.003)	Loss 2.1343 (2.0896)	Acc@1 66.797 (66.672)	Acc@5 89.062 (91.891)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [42 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [42][0/196]	Time 0.035 (0.035)	Data 0.161 (0.161)	Loss 1.9538 (1.9538)	Acc@1 71.875 (71.875)	Acc@5 91.016 (91.016)
Epoch: [42][10/196]	Time 0.017 (0.017)	Data 0.000 (0.017)	Loss 1.9400 (2.0094)	Acc@1 68.359 (68.537)	Acc@5 94.922 (92.507)
Epoch: [42][20/196]	Time 0.012 (0.016)	Data 0.005 (0.010)	Loss 2.0627 (1.9778)	Acc@1 67.188 (69.773)	Acc@5 92.578 (93.025)
Epoch: [42][30/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.7244 (1.9697)	Acc@1 76.953 (70.287)	Acc@5 95.703 (92.918)
Epoch: [42][40/196]	Time 0.013 (0.016)	Data 0.008 (0.006)	Loss 1.8007 (1.9710)	Acc@1 74.609 (70.112)	Acc@5 96.094 (93.121)
Epoch: [42][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.0737 (1.9848)	Acc@1 66.797 (69.792)	Acc@5 93.750 (93.053)
Epoch: [42][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9211 (1.9918)	Acc@1 72.266 (69.570)	Acc@5 91.797 (93.001)
Epoch: [42][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9738 (1.9926)	Acc@1 69.922 (69.680)	Acc@5 93.750 (92.980)
Epoch: [42][80/196]	Time 0.014 (0.016)	Data 0.006 (0.004)	Loss 2.1724 (2.0065)	Acc@1 64.453 (69.295)	Acc@5 91.406 (92.795)
Epoch: [42][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2591 (2.0170)	Acc@1 65.234 (69.025)	Acc@5 88.281 (92.634)
Epoch: [42][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 1.9647 (2.0205)	Acc@1 71.094 (68.847)	Acc@5 94.922 (92.586)
Epoch: [42][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1605 (2.0298)	Acc@1 62.109 (68.514)	Acc@5 90.625 (92.511)
Epoch: [42][120/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.3643 (2.0413)	Acc@1 59.766 (68.263)	Acc@5 90.625 (92.410)
Epoch: [42][130/196]	Time 0.020 (0.016)	Data 0.001 (0.003)	Loss 2.0086 (2.0469)	Acc@1 72.266 (68.136)	Acc@5 92.969 (92.384)
Epoch: [42][140/196]	Time 0.011 (0.016)	Data 0.006 (0.003)	Loss 2.1898 (2.0545)	Acc@1 63.281 (67.949)	Acc@5 91.016 (92.279)
Epoch: [42][150/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 1.9956 (2.0611)	Acc@1 67.578 (67.793)	Acc@5 91.406 (92.146)
Epoch: [42][160/196]	Time 0.012 (0.016)	Data 0.006 (0.003)	Loss 2.0744 (2.0673)	Acc@1 69.922 (67.663)	Acc@5 92.188 (92.044)
Epoch: [42][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.3204 (2.0716)	Acc@1 61.719 (67.578)	Acc@5 87.891 (91.977)
Epoch: [42][180/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.2833 (2.0753)	Acc@1 60.156 (67.524)	Acc@5 90.234 (91.916)
Epoch: [42][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1341 (2.0800)	Acc@1 66.797 (67.421)	Acc@5 88.281 (91.834)
[INFO] Storing checkpoint...

Epoch: [43 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [43][0/196]	Time 0.035 (0.035)	Data 0.161 (0.161)	Loss 1.9282 (1.9282)	Acc@1 73.828 (73.828)	Acc@5 94.141 (94.141)
Epoch: [43][10/196]	Time 0.015 (0.017)	Data 0.002 (0.017)	Loss 2.0042 (1.9865)	Acc@1 64.453 (69.531)	Acc@5 94.141 (93.217)
Epoch: [43][20/196]	Time 0.014 (0.016)	Data 0.003 (0.010)	Loss 1.9781 (1.9826)	Acc@1 68.750 (69.401)	Acc@5 92.578 (92.950)
Epoch: [43][30/196]	Time 0.018 (0.016)	Data 0.000 (0.008)	Loss 1.9409 (1.9917)	Acc@1 69.531 (69.204)	Acc@5 94.141 (93.019)
Epoch: [43][40/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 1.8938 (1.9894)	Acc@1 71.875 (69.379)	Acc@5 92.969 (92.959)
Epoch: [43][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1267 (1.9940)	Acc@1 67.969 (69.256)	Acc@5 91.797 (92.953)
Epoch: [43][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9908 (2.0052)	Acc@1 67.578 (68.981)	Acc@5 93.750 (92.809)
Epoch: [43][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1865 (2.0218)	Acc@1 63.281 (68.590)	Acc@5 91.406 (92.606)
Epoch: [43][80/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0270 (2.0292)	Acc@1 68.750 (68.383)	Acc@5 92.578 (92.520)
Epoch: [43][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0042 (2.0283)	Acc@1 67.188 (68.437)	Acc@5 91.406 (92.527)
Epoch: [43][100/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.9959 (2.0296)	Acc@1 68.750 (68.464)	Acc@5 93.359 (92.543)
Epoch: [43][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9744 (2.0351)	Acc@1 69.531 (68.324)	Acc@5 91.797 (92.525)
Epoch: [43][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9847 (2.0435)	Acc@1 71.875 (68.040)	Acc@5 93.750 (92.443)
Epoch: [43][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0417 (2.0439)	Acc@1 67.188 (68.049)	Acc@5 92.578 (92.423)
Epoch: [43][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1589 (2.0505)	Acc@1 67.578 (67.944)	Acc@5 90.625 (92.334)
Epoch: [43][150/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.1518 (2.0556)	Acc@1 62.891 (67.811)	Acc@5 91.797 (92.309)
Epoch: [43][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0726 (2.0604)	Acc@1 70.312 (67.682)	Acc@5 92.188 (92.272)
Epoch: [43][170/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0176 (2.0662)	Acc@1 68.359 (67.560)	Acc@5 93.359 (92.201)
Epoch: [43][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1555 (2.0715)	Acc@1 63.672 (67.423)	Acc@5 90.625 (92.129)
Epoch: [43][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.3776 (2.0778)	Acc@1 60.547 (67.269)	Acc@5 85.938 (92.016)
[INFO] Storing checkpoint...

Epoch: [44 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [44][0/196]	Time 0.033 (0.033)	Data 0.167 (0.167)	Loss 1.8959 (1.8959)	Acc@1 73.047 (73.047)	Acc@5 94.531 (94.531)
Epoch: [44][10/196]	Time 0.013 (0.017)	Data 0.004 (0.017)	Loss 2.0281 (1.9687)	Acc@1 67.578 (69.993)	Acc@5 95.312 (93.786)
Epoch: [44][20/196]	Time 0.020 (0.017)	Data 0.003 (0.010)	Loss 1.9641 (1.9567)	Acc@1 68.750 (70.647)	Acc@5 94.141 (94.159)
Epoch: [44][30/196]	Time 0.013 (0.016)	Data 0.005 (0.008)	Loss 2.0007 (1.9524)	Acc@1 69.141 (70.489)	Acc@5 91.797 (93.939)
Epoch: [44][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 1.9963 (1.9587)	Acc@1 70.703 (70.351)	Acc@5 94.141 (93.779)
Epoch: [44][50/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.9341 (1.9673)	Acc@1 67.578 (70.052)	Acc@5 96.094 (93.750)
Epoch: [44][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1652 (1.9826)	Acc@1 66.016 (69.730)	Acc@5 92.578 (93.596)
Epoch: [44][70/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9752 (1.9923)	Acc@1 70.312 (69.553)	Acc@5 92.188 (93.458)
Epoch: [44][80/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0699 (1.9976)	Acc@1 64.453 (69.449)	Acc@5 94.141 (93.355)
Epoch: [44][90/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 1.8570 (1.9970)	Acc@1 74.219 (69.527)	Acc@5 94.141 (93.355)
Epoch: [44][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2500 (2.0065)	Acc@1 63.281 (69.191)	Acc@5 89.844 (93.209)
Epoch: [44][110/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2287 (2.0143)	Acc@1 65.625 (68.986)	Acc@5 89.453 (93.057)
Epoch: [44][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1442 (2.0226)	Acc@1 66.406 (68.756)	Acc@5 91.016 (92.975)
Epoch: [44][130/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0904 (2.0255)	Acc@1 68.359 (68.702)	Acc@5 91.797 (92.960)
Epoch: [44][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1334 (2.0308)	Acc@1 63.281 (68.506)	Acc@5 91.797 (92.863)
Epoch: [44][150/196]	Time 0.012 (0.016)	Data 0.004 (0.003)	Loss 2.3709 (2.0401)	Acc@1 60.547 (68.248)	Acc@5 89.453 (92.811)
Epoch: [44][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2738 (2.0494)	Acc@1 61.328 (68.042)	Acc@5 88.281 (92.634)
Epoch: [44][170/196]	Time 0.013 (0.016)	Data 0.006 (0.003)	Loss 2.2149 (2.0584)	Acc@1 61.719 (67.793)	Acc@5 90.625 (92.496)
Epoch: [44][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0155 (2.0614)	Acc@1 64.453 (67.688)	Acc@5 91.797 (92.416)
Epoch: [44][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0899 (2.0653)	Acc@1 68.359 (67.580)	Acc@5 90.234 (92.357)
[INFO] Storing checkpoint...

Epoch: [45 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [45][0/196]	Time 0.033 (0.033)	Data 0.175 (0.175)	Loss 2.0614 (2.0614)	Acc@1 69.531 (69.531)	Acc@5 92.578 (92.578)
Epoch: [45][10/196]	Time 0.017 (0.017)	Data 0.000 (0.017)	Loss 2.0783 (2.0413)	Acc@1 65.625 (68.679)	Acc@5 93.750 (92.933)
Epoch: [45][20/196]	Time 0.011 (0.016)	Data 0.006 (0.010)	Loss 2.0093 (2.0189)	Acc@1 67.969 (69.327)	Acc@5 92.578 (93.099)
Epoch: [45][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.1351 (2.0164)	Acc@1 63.672 (69.456)	Acc@5 92.578 (93.082)
Epoch: [45][40/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.1088 (2.0245)	Acc@1 69.141 (69.169)	Acc@5 92.188 (92.893)
Epoch: [45][50/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.1817 (2.0262)	Acc@1 64.844 (68.911)	Acc@5 91.406 (92.900)
Epoch: [45][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9797 (2.0303)	Acc@1 71.875 (68.654)	Acc@5 93.359 (92.950)
Epoch: [45][70/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0511 (2.0293)	Acc@1 69.531 (68.645)	Acc@5 90.625 (92.919)
Epoch: [45][80/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.2068 (2.0309)	Acc@1 62.500 (68.591)	Acc@5 91.016 (92.882)
Epoch: [45][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1173 (2.0335)	Acc@1 67.578 (68.617)	Acc@5 90.234 (92.767)
Epoch: [45][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0707 (2.0394)	Acc@1 67.969 (68.522)	Acc@5 91.406 (92.675)
Epoch: [45][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9786 (2.0412)	Acc@1 70.312 (68.359)	Acc@5 94.531 (92.649)
Epoch: [45][120/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.2056 (2.0486)	Acc@1 62.109 (68.201)	Acc@5 91.797 (92.514)
Epoch: [45][130/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 2.1058 (2.0562)	Acc@1 63.672 (67.927)	Acc@5 91.406 (92.405)
Epoch: [45][140/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0534 (2.0626)	Acc@1 70.312 (67.769)	Acc@5 94.141 (92.301)
Epoch: [45][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.9955 (2.0637)	Acc@1 67.578 (67.762)	Acc@5 93.750 (92.304)
Epoch: [45][160/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.2344 (2.0688)	Acc@1 60.156 (67.600)	Acc@5 92.188 (92.255)
Epoch: [45][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0561 (2.0733)	Acc@1 68.750 (67.510)	Acc@5 91.797 (92.192)
Epoch: [45][180/196]	Time 0.011 (0.016)	Data 0.008 (0.003)	Loss 2.0718 (2.0762)	Acc@1 68.750 (67.423)	Acc@5 91.797 (92.151)
Epoch: [45][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9512 (2.0755)	Acc@1 70.312 (67.427)	Acc@5 93.359 (92.190)
[INFO] Storing checkpoint...

Epoch: [46 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [46][0/196]	Time 0.033 (0.033)	Data 0.175 (0.175)	Loss 1.9907 (1.9907)	Acc@1 71.094 (71.094)	Acc@5 93.359 (93.359)
Epoch: [46][10/196]	Time 0.015 (0.017)	Data 0.003 (0.018)	Loss 2.0303 (1.9929)	Acc@1 68.359 (69.815)	Acc@5 92.578 (93.324)
Epoch: [46][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 1.7758 (1.9805)	Acc@1 76.953 (69.885)	Acc@5 95.312 (93.620)
Epoch: [46][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.8421 (1.9477)	Acc@1 73.047 (70.817)	Acc@5 95.312 (93.863)
Epoch: [46][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 1.9539 (1.9498)	Acc@1 71.484 (70.951)	Acc@5 91.797 (93.693)
Epoch: [46][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0925 (1.9644)	Acc@1 66.797 (70.427)	Acc@5 93.359 (93.604)
Epoch: [46][60/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.1596 (1.9780)	Acc@1 63.672 (69.947)	Acc@5 91.797 (93.398)
Epoch: [46][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9872 (1.9874)	Acc@1 70.703 (69.680)	Acc@5 93.359 (93.277)
Epoch: [46][80/196]	Time 0.010 (0.015)	Data 0.009 (0.005)	Loss 2.1274 (1.9946)	Acc@1 65.234 (69.478)	Acc@5 91.797 (93.263)
Epoch: [46][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0113 (1.9958)	Acc@1 68.750 (69.501)	Acc@5 91.797 (93.243)
Epoch: [46][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0515 (2.0054)	Acc@1 67.578 (69.291)	Acc@5 92.578 (93.069)
Epoch: [46][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1807 (2.0139)	Acc@1 64.453 (69.074)	Acc@5 91.016 (92.976)
Epoch: [46][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0495 (2.0186)	Acc@1 67.578 (68.966)	Acc@5 92.969 (92.956)
Epoch: [46][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1915 (2.0277)	Acc@1 62.891 (68.729)	Acc@5 93.359 (92.873)
Epoch: [46][140/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0419 (2.0333)	Acc@1 66.797 (68.617)	Acc@5 91.016 (92.744)
Epoch: [46][150/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1630 (2.0429)	Acc@1 65.234 (68.365)	Acc@5 92.578 (92.617)
Epoch: [46][160/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.2259 (2.0477)	Acc@1 62.500 (68.233)	Acc@5 88.281 (92.576)
Epoch: [46][170/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.1617 (2.0529)	Acc@1 66.016 (68.049)	Acc@5 92.578 (92.526)
Epoch: [46][180/196]	Time 0.012 (0.015)	Data 0.006 (0.003)	Loss 2.0959 (2.0592)	Acc@1 66.797 (67.893)	Acc@5 92.188 (92.434)
Epoch: [46][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.0894 (2.0656)	Acc@1 65.234 (67.795)	Acc@5 91.406 (92.355)
[INFO] Storing checkpoint...

Epoch: [47 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [47][0/196]	Time 0.034 (0.034)	Data 0.176 (0.176)	Loss 1.9008 (1.9008)	Acc@1 74.219 (74.219)	Acc@5 95.703 (95.703)
Epoch: [47][10/196]	Time 0.013 (0.017)	Data 0.005 (0.018)	Loss 2.0066 (1.9868)	Acc@1 66.406 (70.561)	Acc@5 92.969 (93.359)
Epoch: [47][20/196]	Time 0.010 (0.016)	Data 0.007 (0.011)	Loss 2.0235 (1.9896)	Acc@1 65.234 (70.331)	Acc@5 93.359 (93.359)
Epoch: [47][30/196]	Time 0.018 (0.016)	Data 0.000 (0.008)	Loss 1.9300 (1.9852)	Acc@1 73.828 (70.401)	Acc@5 95.703 (93.460)
Epoch: [47][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 1.9650 (1.9860)	Acc@1 67.969 (70.265)	Acc@5 95.312 (93.283)
Epoch: [47][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8953 (1.9971)	Acc@1 70.312 (69.738)	Acc@5 93.359 (93.168)
Epoch: [47][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0400 (1.9964)	Acc@1 71.094 (69.883)	Acc@5 92.188 (93.103)
Epoch: [47][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1008 (2.0022)	Acc@1 67.969 (69.696)	Acc@5 92.969 (93.095)
Epoch: [47][80/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.0570 (2.0093)	Acc@1 65.234 (69.420)	Acc@5 93.750 (93.041)
Epoch: [47][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9797 (2.0137)	Acc@1 71.875 (69.347)	Acc@5 93.359 (93.003)
Epoch: [47][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0946 (2.0241)	Acc@1 67.969 (69.121)	Acc@5 89.453 (92.864)
Epoch: [47][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0650 (2.0304)	Acc@1 71.484 (69.056)	Acc@5 91.016 (92.751)
Epoch: [47][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0499 (2.0357)	Acc@1 69.531 (68.944)	Acc@5 91.406 (92.688)
Epoch: [47][130/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0101 (2.0394)	Acc@1 70.703 (68.878)	Acc@5 93.750 (92.620)
Epoch: [47][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0457 (2.0440)	Acc@1 67.578 (68.761)	Acc@5 91.406 (92.559)
Epoch: [47][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1413 (2.0460)	Acc@1 62.500 (68.654)	Acc@5 92.578 (92.607)
Epoch: [47][160/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.3532 (2.0479)	Acc@1 60.547 (68.573)	Acc@5 88.672 (92.585)
Epoch: [47][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1547 (2.0514)	Acc@1 66.016 (68.519)	Acc@5 89.844 (92.514)
Epoch: [47][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3522 (2.0543)	Acc@1 62.109 (68.439)	Acc@5 90.234 (92.470)
Epoch: [47][190/196]	Time 0.016 (0.016)	Data 0.001 (0.003)	Loss 2.0832 (2.0600)	Acc@1 68.359 (68.265)	Acc@5 91.016 (92.388)
[INFO] Storing checkpoint...

Epoch: [48 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [48][0/196]	Time 0.034 (0.034)	Data 0.174 (0.174)	Loss 1.9657 (1.9657)	Acc@1 70.703 (70.703)	Acc@5 94.922 (94.922)
Epoch: [48][10/196]	Time 0.019 (0.017)	Data 0.001 (0.018)	Loss 2.1416 (2.0314)	Acc@1 66.797 (69.531)	Acc@5 89.844 (93.075)
Epoch: [48][20/196]	Time 0.013 (0.016)	Data 0.005 (0.011)	Loss 1.9292 (2.0261)	Acc@1 69.141 (68.694)	Acc@5 94.141 (93.099)
Epoch: [48][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 1.9231 (2.0158)	Acc@1 71.094 (68.926)	Acc@5 93.359 (93.145)
Epoch: [48][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 1.9577 (2.0167)	Acc@1 71.094 (69.007)	Acc@5 93.359 (93.150)
Epoch: [48][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8918 (2.0110)	Acc@1 73.438 (69.439)	Acc@5 93.750 (93.114)
Epoch: [48][60/196]	Time 0.017 (0.016)	Data 0.004 (0.005)	Loss 2.0274 (2.0136)	Acc@1 71.875 (69.378)	Acc@5 91.406 (93.026)
Epoch: [48][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1360 (2.0198)	Acc@1 67.969 (69.185)	Acc@5 89.844 (93.018)
Epoch: [48][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0291 (2.0241)	Acc@1 66.797 (68.914)	Acc@5 94.141 (93.007)
Epoch: [48][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0772 (2.0306)	Acc@1 68.359 (68.698)	Acc@5 92.969 (92.969)
Epoch: [48][100/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9893 (2.0349)	Acc@1 69.141 (68.630)	Acc@5 95.312 (92.876)
Epoch: [48][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9016 (2.0382)	Acc@1 71.094 (68.504)	Acc@5 92.969 (92.835)
Epoch: [48][120/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1124 (2.0419)	Acc@1 65.625 (68.372)	Acc@5 91.406 (92.762)
Epoch: [48][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9812 (2.0414)	Acc@1 71.484 (68.443)	Acc@5 92.969 (92.685)
Epoch: [48][140/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.2399 (2.0450)	Acc@1 62.891 (68.373)	Acc@5 90.234 (92.645)
Epoch: [48][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0622 (2.0474)	Acc@1 67.188 (68.411)	Acc@5 92.578 (92.635)
Epoch: [48][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0558 (2.0497)	Acc@1 66.016 (68.328)	Acc@5 92.969 (92.619)
Epoch: [48][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0557 (2.0523)	Acc@1 66.797 (68.275)	Acc@5 93.359 (92.574)
Epoch: [48][180/196]	Time 0.011 (0.016)	Data 0.008 (0.003)	Loss 2.2892 (2.0601)	Acc@1 63.281 (68.057)	Acc@5 87.500 (92.468)
Epoch: [48][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0265 (2.0625)	Acc@1 71.875 (68.034)	Acc@5 92.969 (92.433)
[INFO] Storing checkpoint...

Epoch: [49 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [49][0/196]	Time 0.034 (0.034)	Data 0.181 (0.181)	Loss 1.8457 (1.8457)	Acc@1 73.438 (73.438)	Acc@5 94.531 (94.531)
Epoch: [49][10/196]	Time 0.016 (0.017)	Data 0.003 (0.019)	Loss 1.9478 (1.9902)	Acc@1 71.094 (70.241)	Acc@5 95.312 (93.821)
Epoch: [49][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 1.8644 (1.9680)	Acc@1 71.484 (70.610)	Acc@5 93.750 (93.917)
Epoch: [49][30/196]	Time 0.013 (0.016)	Data 0.005 (0.008)	Loss 1.9270 (1.9611)	Acc@1 70.703 (70.980)	Acc@5 93.359 (93.725)
Epoch: [49][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 2.0665 (1.9741)	Acc@1 69.922 (70.722)	Acc@5 92.188 (93.417)
Epoch: [49][50/196]	Time 0.018 (0.016)	Data 0.002 (0.006)	Loss 1.8957 (1.9780)	Acc@1 73.047 (70.749)	Acc@5 94.141 (93.451)
Epoch: [49][60/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9358 (1.9869)	Acc@1 70.703 (70.473)	Acc@5 93.750 (93.404)
Epoch: [49][70/196]	Time 0.017 (0.016)	Data 0.003 (0.005)	Loss 2.1080 (2.0008)	Acc@1 66.016 (69.993)	Acc@5 91.797 (93.194)
Epoch: [49][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0278 (2.0055)	Acc@1 68.359 (69.869)	Acc@5 94.141 (93.176)
Epoch: [49][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1371 (2.0049)	Acc@1 64.062 (69.793)	Acc@5 93.359 (93.188)
Epoch: [49][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1665 (2.0102)	Acc@1 63.672 (69.570)	Acc@5 93.359 (93.189)
Epoch: [49][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9132 (2.0115)	Acc@1 71.875 (69.570)	Acc@5 94.531 (93.187)
Epoch: [49][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0111 (2.0206)	Acc@1 69.531 (69.347)	Acc@5 93.359 (93.053)
Epoch: [49][130/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0669 (2.0230)	Acc@1 68.750 (69.215)	Acc@5 92.578 (93.028)
Epoch: [49][140/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9982 (2.0296)	Acc@1 66.406 (68.947)	Acc@5 94.531 (92.966)
Epoch: [49][150/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1683 (2.0392)	Acc@1 64.844 (68.685)	Acc@5 92.578 (92.876)
Epoch: [49][160/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2662 (2.0465)	Acc@1 64.844 (68.563)	Acc@5 88.672 (92.741)
Epoch: [49][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1683 (2.0539)	Acc@1 65.625 (68.373)	Acc@5 93.359 (92.699)
Epoch: [49][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2312 (2.0583)	Acc@1 61.719 (68.267)	Acc@5 89.844 (92.617)
Epoch: [49][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1405 (2.0605)	Acc@1 65.625 (68.218)	Acc@5 90.234 (92.590)
[INFO] Storing checkpoint...

Epoch: [50 | 50] LR: 0.100000
module.conv1.weight [62, 3, 3, 3]
module.conv2.weight [128, 62, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [509, 512, 3, 3]
module.conv8.weight [432, 509, 3, 3]
Epoch: [50][0/196]	Time 0.035 (0.035)	Data 0.176 (0.176)	Loss 2.1223 (2.1223)	Acc@1 70.312 (70.312)	Acc@5 89.844 (89.844)
Epoch: [50][10/196]	Time 0.014 (0.017)	Data 0.004 (0.018)	Loss 2.0069 (2.0552)	Acc@1 70.312 (68.714)	Acc@5 92.969 (92.152)
Epoch: [50][20/196]	Time 0.015 (0.016)	Data 0.002 (0.011)	Loss 1.9671 (2.0201)	Acc@1 70.312 (69.494)	Acc@5 94.531 (92.783)
Epoch: [50][30/196]	Time 0.018 (0.016)	Data 0.000 (0.008)	Loss 2.0001 (2.0054)	Acc@1 69.531 (69.733)	Acc@5 92.188 (92.906)
Epoch: [50][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 1.9618 (2.0050)	Acc@1 69.922 (69.722)	Acc@5 93.359 (92.740)
Epoch: [50][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0296 (2.0004)	Acc@1 66.016 (69.861)	Acc@5 93.750 (92.953)
Epoch: [50][60/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9380 (2.0034)	Acc@1 69.531 (69.723)	Acc@5 93.750 (92.950)
Epoch: [50][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.3014 (2.0108)	Acc@1 62.891 (69.581)	Acc@5 91.016 (92.870)
Epoch: [50][80/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9300 (2.0190)	Acc@1 74.219 (69.382)	Acc@5 92.578 (92.814)
Epoch: [50][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0228 (2.0263)	Acc@1 67.969 (69.179)	Acc@5 93.750 (92.776)
Epoch: [50][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9858 (2.0296)	Acc@1 70.312 (69.052)	Acc@5 93.750 (92.756)
Epoch: [50][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1248 (2.0329)	Acc@1 66.016 (68.979)	Acc@5 92.578 (92.758)
Epoch: [50][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0340 (2.0366)	Acc@1 68.750 (68.866)	Acc@5 93.750 (92.740)
Epoch: [50][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0524 (2.0376)	Acc@1 68.750 (68.783)	Acc@5 95.703 (92.790)
Epoch: [50][140/196]	Time 0.011 (0.016)	Data 0.005 (0.004)	Loss 2.0801 (2.0404)	Acc@1 70.312 (68.786)	Acc@5 91.016 (92.747)
Epoch: [50][150/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 2.2919 (2.0443)	Acc@1 65.234 (68.703)	Acc@5 88.672 (92.695)
Epoch: [50][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1238 (2.0477)	Acc@1 68.750 (68.549)	Acc@5 92.188 (92.653)
Epoch: [50][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2166 (2.0526)	Acc@1 65.625 (68.423)	Acc@5 88.281 (92.542)
Epoch: [50][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2439 (2.0556)	Acc@1 65.234 (68.379)	Acc@5 88.281 (92.505)
Epoch: [50][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2364 (2.0628)	Acc@1 62.891 (68.241)	Acc@5 88.672 (92.406)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [62, 3, 3, 3] >> [57, 3, 3, 3]
[module.conv2.weight]: [128, 62, 3, 3] >> [128, 57, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [256, 128, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [512, 256, 3, 3]
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 512, 3, 3]
[module.conv7.weight]: [509, 512, 3, 3] >> [506, 512, 3, 3]
[module.conv8.weight]: [432, 509, 3, 3] >> [384, 506, 3, 3]
[module.fc.weight]: [100, 432] >> [100, 384]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
57.53
python src/cifar.py --workers 4 --dataset cifar100 --epochs 60 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_60.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 8.61M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [57, 3, 3, 3]
conv2 --> [128, 57, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [506, 512, 3, 3]
conv8 --> [384, 506, 3, 3]
fc --> [384, 100]
1, 631162368, 1575936, 57
2, 7026573312, 16809984, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7162822656, 9326592, 506
8, 5372116992, 6994944, 384
fc, 14745600, 38400, 0
===================
FLOP REPORT: 30011923800000.0 57539200000.0 147992064 143848 2611 16.427349090576172

Epoch: [51 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [51][0/196]	Time 0.697 (0.697)	Data 0.167 (0.167)	Loss 1.9551 (1.9551)	Acc@1 71.094 (71.094)	Acc@5 91.797 (91.797)
Epoch: [51][10/196]	Time 0.015 (0.077)	Data 0.002 (0.017)	Loss 1.9718 (2.0385)	Acc@1 68.359 (68.714)	Acc@5 93.359 (92.685)
Epoch: [51][20/196]	Time 0.014 (0.047)	Data 0.003 (0.010)	Loss 1.8651 (1.9920)	Acc@1 72.656 (69.587)	Acc@5 95.312 (93.266)
Epoch: [51][30/196]	Time 0.015 (0.037)	Data 0.002 (0.007)	Loss 1.8816 (1.9842)	Acc@1 73.438 (70.199)	Acc@5 94.531 (93.309)
Epoch: [51][40/196]	Time 0.011 (0.031)	Data 0.006 (0.006)	Loss 1.8841 (1.9787)	Acc@1 77.344 (70.436)	Acc@5 94.141 (93.455)
Epoch: [51][50/196]	Time 0.016 (0.028)	Data 0.001 (0.005)	Loss 1.9782 (1.9729)	Acc@1 67.969 (70.558)	Acc@5 93.750 (93.513)
Epoch: [51][60/196]	Time 0.011 (0.026)	Data 0.006 (0.005)	Loss 2.0099 (1.9686)	Acc@1 69.141 (70.601)	Acc@5 92.188 (93.519)
Epoch: [51][70/196]	Time 0.016 (0.024)	Data 0.001 (0.005)	Loss 2.0039 (1.9750)	Acc@1 69.141 (70.412)	Acc@5 93.750 (93.502)
Epoch: [51][80/196]	Time 0.011 (0.023)	Data 0.006 (0.004)	Loss 2.0868 (1.9834)	Acc@1 64.453 (70.163)	Acc@5 94.141 (93.514)
Epoch: [51][90/196]	Time 0.017 (0.022)	Data 0.001 (0.004)	Loss 1.8514 (1.9907)	Acc@1 75.000 (69.995)	Acc@5 95.312 (93.445)
Epoch: [51][100/196]	Time 0.011 (0.022)	Data 0.007 (0.004)	Loss 1.9961 (1.9974)	Acc@1 71.484 (69.752)	Acc@5 95.312 (93.406)
Epoch: [51][110/196]	Time 0.016 (0.021)	Data 0.001 (0.004)	Loss 2.1288 (1.9988)	Acc@1 64.844 (69.672)	Acc@5 91.797 (93.366)
Epoch: [51][120/196]	Time 0.010 (0.020)	Data 0.008 (0.004)	Loss 2.0346 (2.0073)	Acc@1 69.141 (69.509)	Acc@5 93.750 (93.269)
Epoch: [51][130/196]	Time 0.016 (0.020)	Data 0.001 (0.004)	Loss 2.2402 (2.0129)	Acc@1 66.016 (69.314)	Acc@5 89.453 (93.195)
Epoch: [51][140/196]	Time 0.011 (0.020)	Data 0.008 (0.004)	Loss 2.1287 (2.0201)	Acc@1 65.234 (69.141)	Acc@5 92.188 (93.080)
Epoch: [51][150/196]	Time 0.016 (0.019)	Data 0.001 (0.003)	Loss 2.0052 (2.0256)	Acc@1 70.312 (69.006)	Acc@5 92.188 (93.023)
Epoch: [51][160/196]	Time 0.011 (0.019)	Data 0.009 (0.003)	Loss 1.8365 (2.0283)	Acc@1 75.000 (68.966)	Acc@5 96.094 (93.012)
Epoch: [51][170/196]	Time 0.016 (0.019)	Data 0.000 (0.003)	Loss 2.1436 (2.0355)	Acc@1 62.891 (68.812)	Acc@5 92.188 (92.905)
Epoch: [51][180/196]	Time 0.013 (0.019)	Data 0.009 (0.003)	Loss 2.0857 (2.0387)	Acc@1 69.531 (68.726)	Acc@5 93.359 (92.885)
Epoch: [51][190/196]	Time 0.016 (0.019)	Data 0.000 (0.003)	Loss 2.0004 (2.0430)	Acc@1 69.531 (68.621)	Acc@5 94.141 (92.848)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [52 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [52][0/196]	Time 0.026 (0.026)	Data 0.161 (0.161)	Loss 2.1018 (2.1018)	Acc@1 67.969 (67.969)	Acc@5 92.188 (92.188)
Epoch: [52][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 1.8864 (2.0286)	Acc@1 75.000 (69.531)	Acc@5 93.359 (92.969)
Epoch: [52][20/196]	Time 0.013 (0.015)	Data 0.004 (0.010)	Loss 2.0440 (2.0141)	Acc@1 70.703 (70.554)	Acc@5 91.016 (93.285)
Epoch: [52][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 2.0130 (1.9880)	Acc@1 71.094 (71.132)	Acc@5 91.797 (93.523)
Epoch: [52][40/196]	Time 0.012 (0.015)	Data 0.006 (0.007)	Loss 1.8381 (1.9797)	Acc@1 70.703 (70.989)	Acc@5 94.531 (93.436)
Epoch: [52][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.0249 (1.9894)	Acc@1 66.016 (70.450)	Acc@5 92.969 (93.336)
Epoch: [52][60/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 2.0018 (2.0029)	Acc@1 70.312 (70.114)	Acc@5 92.578 (93.212)
Epoch: [52][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0624 (2.0120)	Acc@1 67.969 (69.949)	Acc@5 92.188 (93.106)
Epoch: [52][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.2051 (2.0169)	Acc@1 66.406 (69.840)	Acc@5 89.844 (92.998)
Epoch: [52][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0353 (2.0204)	Acc@1 67.188 (69.630)	Acc@5 93.359 (92.977)
Epoch: [52][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 1.9870 (2.0192)	Acc@1 69.531 (69.624)	Acc@5 93.359 (92.984)
Epoch: [52][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0106 (2.0210)	Acc@1 72.656 (69.528)	Acc@5 91.797 (92.923)
Epoch: [52][120/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.0566 (2.0259)	Acc@1 69.141 (69.438)	Acc@5 93.750 (92.904)
Epoch: [52][130/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0718 (2.0313)	Acc@1 67.969 (69.224)	Acc@5 92.188 (92.841)
Epoch: [52][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1380 (2.0372)	Acc@1 64.453 (69.035)	Acc@5 93.359 (92.786)
Epoch: [52][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1289 (2.0433)	Acc@1 68.750 (68.869)	Acc@5 90.234 (92.700)
Epoch: [52][160/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 2.1757 (2.0457)	Acc@1 61.719 (68.752)	Acc@5 91.016 (92.668)
Epoch: [52][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1468 (2.0501)	Acc@1 64.844 (68.656)	Acc@5 92.578 (92.654)
Epoch: [52][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.2201 (2.0557)	Acc@1 63.672 (68.495)	Acc@5 91.406 (92.578)
Epoch: [52][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1480 (2.0575)	Acc@1 63.672 (68.447)	Acc@5 92.578 (92.568)
[INFO] Storing checkpoint...

Epoch: [53 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [53][0/196]	Time 0.025 (0.025)	Data 0.176 (0.176)	Loss 1.8835 (1.8835)	Acc@1 74.219 (74.219)	Acc@5 95.312 (95.312)
Epoch: [53][10/196]	Time 0.015 (0.016)	Data 0.001 (0.018)	Loss 1.9521 (1.9856)	Acc@1 71.094 (70.561)	Acc@5 92.969 (94.212)
Epoch: [53][20/196]	Time 0.011 (0.015)	Data 0.004 (0.011)	Loss 1.8970 (1.9798)	Acc@1 72.656 (70.424)	Acc@5 94.531 (93.787)
Epoch: [53][30/196]	Time 0.015 (0.015)	Data 0.003 (0.008)	Loss 2.0188 (1.9682)	Acc@1 68.359 (70.754)	Acc@5 93.750 (93.838)
Epoch: [53][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 2.1248 (1.9790)	Acc@1 67.188 (70.455)	Acc@5 91.016 (93.693)
Epoch: [53][50/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.9814 (1.9717)	Acc@1 70.703 (70.496)	Acc@5 95.703 (93.758)
Epoch: [53][60/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0699 (1.9730)	Acc@1 67.578 (70.530)	Acc@5 93.359 (93.718)
Epoch: [53][70/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9058 (1.9701)	Acc@1 72.656 (70.643)	Acc@5 92.969 (93.662)
Epoch: [53][80/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0322 (1.9737)	Acc@1 65.625 (70.539)	Acc@5 95.703 (93.673)
Epoch: [53][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9962 (1.9772)	Acc@1 70.312 (70.343)	Acc@5 93.359 (93.690)
Epoch: [53][100/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.1060 (1.9824)	Acc@1 71.094 (70.227)	Acc@5 90.234 (93.649)
Epoch: [53][110/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0426 (1.9885)	Acc@1 69.531 (70.073)	Acc@5 92.188 (93.616)
Epoch: [53][120/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3273 (2.0016)	Acc@1 64.062 (69.760)	Acc@5 90.234 (93.392)
Epoch: [53][130/196]	Time 0.013 (0.015)	Data 0.003 (0.004)	Loss 2.2305 (2.0100)	Acc@1 61.328 (69.525)	Acc@5 89.062 (93.267)
Epoch: [53][140/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2218 (2.0214)	Acc@1 62.891 (69.285)	Acc@5 92.969 (93.132)
Epoch: [53][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.3653 (2.0334)	Acc@1 61.719 (68.934)	Acc@5 87.891 (92.961)
Epoch: [53][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1019 (2.0436)	Acc@1 66.797 (68.648)	Acc@5 92.188 (92.813)
Epoch: [53][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2178 (2.0508)	Acc@1 63.672 (68.499)	Acc@5 89.453 (92.727)
Epoch: [53][180/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2220 (2.0533)	Acc@1 62.500 (68.450)	Acc@5 92.578 (92.688)
Epoch: [53][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0322 (2.0552)	Acc@1 67.969 (68.398)	Acc@5 92.969 (92.664)
[INFO] Storing checkpoint...

Epoch: [54 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [54][0/196]	Time 0.025 (0.025)	Data 0.170 (0.170)	Loss 1.9813 (1.9813)	Acc@1 69.922 (69.922)	Acc@5 94.141 (94.141)
Epoch: [54][10/196]	Time 0.013 (0.015)	Data 0.004 (0.018)	Loss 1.8462 (1.9462)	Acc@1 75.781 (71.413)	Acc@5 92.969 (94.425)
Epoch: [54][20/196]	Time 0.014 (0.015)	Data 0.003 (0.010)	Loss 1.9012 (1.9754)	Acc@1 75.000 (71.001)	Acc@5 94.531 (93.564)
Epoch: [54][30/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 1.8636 (1.9748)	Acc@1 76.172 (70.615)	Acc@5 95.312 (93.674)
Epoch: [54][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 2.0296 (1.9636)	Acc@1 70.312 (71.065)	Acc@5 91.016 (93.769)
Epoch: [54][50/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.8526 (1.9562)	Acc@1 75.391 (71.347)	Acc@5 95.312 (93.773)
Epoch: [54][60/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.8642 (1.9507)	Acc@1 73.828 (71.465)	Acc@5 92.188 (93.859)
Epoch: [54][70/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.0976 (1.9557)	Acc@1 69.531 (71.396)	Acc@5 91.797 (93.778)
Epoch: [54][80/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 1.8948 (1.9624)	Acc@1 72.266 (71.118)	Acc@5 95.312 (93.750)
Epoch: [54][90/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 1.9619 (1.9697)	Acc@1 67.969 (70.858)	Acc@5 94.141 (93.724)
Epoch: [54][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0345 (1.9749)	Acc@1 71.094 (70.692)	Acc@5 92.188 (93.638)
Epoch: [54][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0080 (1.9798)	Acc@1 72.266 (70.594)	Acc@5 93.359 (93.585)
Epoch: [54][120/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0054 (1.9844)	Acc@1 70.312 (70.471)	Acc@5 91.797 (93.527)
Epoch: [54][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9380 (1.9904)	Acc@1 70.703 (70.238)	Acc@5 92.969 (93.431)
Epoch: [54][140/196]	Time 0.012 (0.015)	Data 0.003 (0.004)	Loss 2.0992 (1.9976)	Acc@1 66.016 (70.088)	Acc@5 92.578 (93.351)
Epoch: [54][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0823 (2.0073)	Acc@1 67.969 (69.880)	Acc@5 94.141 (93.274)
Epoch: [54][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0889 (2.0128)	Acc@1 68.359 (69.691)	Acc@5 92.969 (93.207)
Epoch: [54][170/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1370 (2.0166)	Acc@1 68.750 (69.570)	Acc@5 90.234 (93.174)
Epoch: [54][180/196]	Time 0.014 (0.015)	Data 0.013 (0.004)	Loss 2.0795 (2.0218)	Acc@1 66.406 (69.451)	Acc@5 92.188 (93.113)
Epoch: [54][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1282 (2.0259)	Acc@1 63.281 (69.347)	Acc@5 91.797 (93.073)
[INFO] Storing checkpoint...

Epoch: [55 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [55][0/196]	Time 0.025 (0.025)	Data 0.169 (0.169)	Loss 1.9175 (1.9175)	Acc@1 72.266 (72.266)	Acc@5 95.312 (95.312)
Epoch: [55][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 1.8831 (2.0107)	Acc@1 74.609 (70.135)	Acc@5 94.922 (93.501)
Epoch: [55][20/196]	Time 0.012 (0.015)	Data 0.005 (0.011)	Loss 1.9037 (1.9946)	Acc@1 72.266 (70.387)	Acc@5 91.406 (93.620)
Epoch: [55][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.8725 (1.9744)	Acc@1 73.438 (70.955)	Acc@5 95.312 (93.838)
Epoch: [55][40/196]	Time 0.015 (0.015)	Data 0.003 (0.007)	Loss 2.0180 (1.9688)	Acc@1 70.703 (71.094)	Acc@5 94.141 (93.902)
Epoch: [55][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.9281 (1.9644)	Acc@1 71.875 (71.347)	Acc@5 93.359 (93.926)
Epoch: [55][60/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 1.8506 (1.9676)	Acc@1 74.219 (71.183)	Acc@5 95.703 (93.859)
Epoch: [55][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.8895 (1.9751)	Acc@1 70.703 (70.890)	Acc@5 94.922 (93.827)
Epoch: [55][80/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0858 (1.9875)	Acc@1 70.312 (70.655)	Acc@5 92.969 (93.620)
Epoch: [55][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9740 (1.9923)	Acc@1 68.750 (70.471)	Acc@5 94.922 (93.531)
Epoch: [55][100/196]	Time 0.015 (0.015)	Data 0.004 (0.005)	Loss 2.2536 (1.9991)	Acc@1 64.844 (70.305)	Acc@5 91.016 (93.383)
Epoch: [55][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0335 (2.0043)	Acc@1 69.141 (70.165)	Acc@5 93.359 (93.349)
Epoch: [55][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0010 (2.0106)	Acc@1 69.531 (70.009)	Acc@5 92.188 (93.250)
Epoch: [55][130/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 2.0232 (2.0166)	Acc@1 69.922 (69.829)	Acc@5 94.141 (93.166)
Epoch: [55][140/196]	Time 0.013 (0.014)	Data 0.004 (0.004)	Loss 2.0264 (2.0217)	Acc@1 71.484 (69.758)	Acc@5 90.625 (93.102)
Epoch: [55][150/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 2.0638 (2.0255)	Acc@1 66.797 (69.614)	Acc@5 94.141 (93.067)
Epoch: [55][160/196]	Time 0.012 (0.014)	Data 0.005 (0.004)	Loss 2.2934 (2.0306)	Acc@1 64.844 (69.480)	Acc@5 91.406 (93.059)
Epoch: [55][170/196]	Time 0.016 (0.014)	Data 0.001 (0.004)	Loss 2.1893 (2.0357)	Acc@1 64.453 (69.346)	Acc@5 93.359 (92.976)
Epoch: [55][180/196]	Time 0.011 (0.014)	Data 0.006 (0.004)	Loss 1.9885 (2.0414)	Acc@1 73.438 (69.188)	Acc@5 91.406 (92.930)
Epoch: [55][190/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 2.0580 (2.0430)	Acc@1 66.797 (69.073)	Acc@5 92.578 (92.905)
[INFO] Storing checkpoint...

Epoch: [56 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [56][0/196]	Time 0.026 (0.026)	Data 0.174 (0.174)	Loss 1.9568 (1.9568)	Acc@1 70.312 (70.312)	Acc@5 94.141 (94.141)
Epoch: [56][10/196]	Time 0.016 (0.015)	Data 0.000 (0.018)	Loss 2.0860 (2.0056)	Acc@1 66.797 (69.354)	Acc@5 94.922 (93.679)
Epoch: [56][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 2.0528 (2.0023)	Acc@1 68.359 (69.736)	Acc@5 90.234 (93.583)
Epoch: [56][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 2.0135 (1.9958)	Acc@1 73.047 (70.376)	Acc@5 92.578 (93.700)
Epoch: [56][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 1.8635 (1.9829)	Acc@1 73.828 (70.589)	Acc@5 93.750 (93.893)
Epoch: [56][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0219 (1.9801)	Acc@1 68.750 (70.826)	Acc@5 91.406 (93.896)
Epoch: [56][60/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 1.8595 (1.9743)	Acc@1 75.781 (70.895)	Acc@5 94.531 (93.949)
Epoch: [56][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0020 (1.9763)	Acc@1 71.094 (70.797)	Acc@5 93.750 (93.844)
Epoch: [56][80/196]	Time 0.011 (0.015)	Data 0.005 (0.005)	Loss 2.0164 (1.9769)	Acc@1 69.922 (70.713)	Acc@5 92.969 (93.846)
Epoch: [56][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9524 (1.9884)	Acc@1 73.438 (70.450)	Acc@5 92.969 (93.686)
Epoch: [56][100/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 1.9221 (1.9942)	Acc@1 75.000 (70.312)	Acc@5 91.797 (93.568)
Epoch: [56][110/196]	Time 0.016 (0.014)	Data 0.001 (0.004)	Loss 2.0892 (2.0043)	Acc@1 68.359 (70.094)	Acc@5 92.578 (93.476)
Epoch: [56][120/196]	Time 0.013 (0.014)	Data 0.004 (0.004)	Loss 2.1420 (2.0146)	Acc@1 66.797 (69.786)	Acc@5 92.969 (93.363)
Epoch: [56][130/196]	Time 0.017 (0.014)	Data 0.001 (0.004)	Loss 2.3117 (2.0206)	Acc@1 62.109 (69.633)	Acc@5 89.453 (93.333)
Epoch: [56][140/196]	Time 0.013 (0.014)	Data 0.006 (0.004)	Loss 2.1775 (2.0239)	Acc@1 62.109 (69.548)	Acc@5 93.750 (93.301)
Epoch: [56][150/196]	Time 0.016 (0.014)	Data 0.001 (0.004)	Loss 2.0462 (2.0285)	Acc@1 66.797 (69.412)	Acc@5 91.797 (93.225)
Epoch: [56][160/196]	Time 0.012 (0.014)	Data 0.010 (0.004)	Loss 2.1292 (2.0346)	Acc@1 66.797 (69.250)	Acc@5 92.578 (93.151)
Epoch: [56][170/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 2.2720 (2.0383)	Acc@1 63.672 (69.218)	Acc@5 90.625 (93.078)
Epoch: [56][180/196]	Time 0.012 (0.014)	Data 0.005 (0.004)	Loss 2.2426 (2.0441)	Acc@1 64.062 (69.095)	Acc@5 91.016 (92.990)
Epoch: [56][190/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 2.0588 (2.0510)	Acc@1 68.750 (68.920)	Acc@5 94.531 (92.916)
[INFO] Storing checkpoint...

Epoch: [57 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [57][0/196]	Time 0.027 (0.027)	Data 0.174 (0.174)	Loss 1.8831 (1.8831)	Acc@1 75.391 (75.391)	Acc@5 96.484 (96.484)
Epoch: [57][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 2.0311 (2.0084)	Acc@1 70.703 (70.561)	Acc@5 91.797 (93.750)
Epoch: [57][20/196]	Time 0.014 (0.015)	Data 0.002 (0.011)	Loss 2.0498 (1.9945)	Acc@1 68.359 (70.926)	Acc@5 91.797 (93.601)
Epoch: [57][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.9057 (1.9863)	Acc@1 73.828 (71.258)	Acc@5 95.312 (93.637)
Epoch: [57][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 2.0122 (1.9851)	Acc@1 70.312 (71.446)	Acc@5 92.969 (93.655)
Epoch: [57][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.9782 (1.9847)	Acc@1 72.656 (71.461)	Acc@5 94.531 (93.719)
Epoch: [57][60/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 1.9529 (1.9848)	Acc@1 72.656 (71.305)	Acc@5 92.578 (93.692)
Epoch: [57][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9375 (1.9852)	Acc@1 69.531 (71.253)	Acc@5 93.359 (93.623)
Epoch: [57][80/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9918 (1.9948)	Acc@1 66.797 (70.964)	Acc@5 93.359 (93.412)
Epoch: [57][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9246 (1.9966)	Acc@1 71.094 (70.849)	Acc@5 92.969 (93.415)
Epoch: [57][100/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0504 (1.9989)	Acc@1 68.359 (70.804)	Acc@5 91.797 (93.344)
Epoch: [57][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9950 (2.0021)	Acc@1 70.312 (70.682)	Acc@5 92.969 (93.300)
Epoch: [57][120/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.2071 (2.0106)	Acc@1 64.844 (70.390)	Acc@5 90.234 (93.237)
Epoch: [57][130/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0253 (2.0201)	Acc@1 69.141 (70.035)	Acc@5 92.969 (93.172)
Epoch: [57][140/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 2.0859 (2.0248)	Acc@1 68.359 (69.938)	Acc@5 92.969 (93.107)
Epoch: [57][150/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0480 (2.0286)	Acc@1 67.969 (69.754)	Acc@5 93.750 (93.093)
Epoch: [57][160/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 1.9660 (2.0330)	Acc@1 70.703 (69.558)	Acc@5 93.359 (93.056)
Epoch: [57][170/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1910 (2.0344)	Acc@1 67.578 (69.556)	Acc@5 91.406 (93.035)
Epoch: [57][180/196]	Time 0.012 (0.014)	Data 0.008 (0.004)	Loss 2.0874 (2.0402)	Acc@1 69.141 (69.387)	Acc@5 93.359 (92.990)
Epoch: [57][190/196]	Time 0.011 (0.014)	Data 0.003 (0.004)	Loss 2.0424 (2.0414)	Acc@1 70.312 (69.310)	Acc@5 92.188 (92.979)
[INFO] Storing checkpoint...

Epoch: [58 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [58][0/196]	Time 0.025 (0.025)	Data 0.190 (0.190)	Loss 1.9457 (1.9457)	Acc@1 72.656 (72.656)	Acc@5 95.312 (95.312)
Epoch: [58][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 2.1409 (1.9999)	Acc@1 64.453 (70.810)	Acc@5 93.359 (93.537)
Epoch: [58][20/196]	Time 0.011 (0.015)	Data 0.008 (0.011)	Loss 2.0846 (1.9919)	Acc@1 67.969 (70.740)	Acc@5 91.797 (93.508)
Epoch: [58][30/196]	Time 0.017 (0.015)	Data 0.001 (0.008)	Loss 2.0957 (1.9910)	Acc@1 67.578 (70.602)	Acc@5 91.016 (93.599)
Epoch: [58][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 2.1074 (1.9944)	Acc@1 66.797 (70.274)	Acc@5 90.234 (93.569)
Epoch: [58][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.9341 (1.9887)	Acc@1 73.828 (70.627)	Acc@5 94.531 (93.650)
Epoch: [58][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 1.9273 (1.9817)	Acc@1 71.094 (70.665)	Acc@5 96.094 (93.692)
Epoch: [58][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9804 (1.9868)	Acc@1 71.875 (70.483)	Acc@5 94.922 (93.717)
Epoch: [58][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.1030 (1.9967)	Acc@1 68.750 (70.308)	Acc@5 94.141 (93.649)
Epoch: [58][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1186 (2.0098)	Acc@1 65.234 (69.991)	Acc@5 92.188 (93.458)
Epoch: [58][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0166 (2.0165)	Acc@1 69.531 (69.787)	Acc@5 91.406 (93.317)
Epoch: [58][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1850 (2.0245)	Acc@1 62.891 (69.535)	Acc@5 92.188 (93.233)
Epoch: [58][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0338 (2.0288)	Acc@1 69.922 (69.376)	Acc@5 91.797 (93.169)
Epoch: [58][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9602 (2.0321)	Acc@1 73.828 (69.311)	Acc@5 94.531 (93.139)
Epoch: [58][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1692 (2.0351)	Acc@1 62.891 (69.168)	Acc@5 93.750 (93.096)
Epoch: [58][150/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0908 (2.0393)	Acc@1 69.141 (69.128)	Acc@5 91.406 (93.028)
Epoch: [58][160/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.1746 (2.0458)	Acc@1 65.234 (68.971)	Acc@5 91.406 (92.918)
Epoch: [58][170/196]	Time 0.013 (0.015)	Data 0.004 (0.003)	Loss 2.0065 (2.0485)	Acc@1 67.578 (68.873)	Acc@5 94.531 (92.905)
Epoch: [58][180/196]	Time 0.013 (0.015)	Data 0.004 (0.003)	Loss 2.1418 (2.0490)	Acc@1 67.188 (68.901)	Acc@5 91.797 (92.880)
Epoch: [58][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.1355 (2.0499)	Acc@1 67.578 (68.907)	Acc@5 94.141 (92.858)
[INFO] Storing checkpoint...

Epoch: [59 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [59][0/196]	Time 0.025 (0.025)	Data 0.179 (0.179)	Loss 1.9748 (1.9748)	Acc@1 69.531 (69.531)	Acc@5 93.750 (93.750)
Epoch: [59][10/196]	Time 0.016 (0.016)	Data 0.000 (0.019)	Loss 1.8769 (1.9939)	Acc@1 74.219 (70.241)	Acc@5 96.484 (94.141)
Epoch: [59][20/196]	Time 0.015 (0.015)	Data 0.002 (0.011)	Loss 1.9008 (1.9856)	Acc@1 71.875 (70.052)	Acc@5 95.312 (94.289)
Epoch: [59][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.9765 (1.9831)	Acc@1 71.094 (70.401)	Acc@5 92.969 (93.977)
Epoch: [59][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 1.9732 (1.9757)	Acc@1 71.094 (70.722)	Acc@5 92.969 (94.112)
Epoch: [59][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0545 (1.9864)	Acc@1 69.922 (70.450)	Acc@5 95.312 (94.064)
Epoch: [59][60/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 1.9862 (1.9873)	Acc@1 70.312 (70.473)	Acc@5 94.141 (93.884)
Epoch: [59][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0369 (1.9836)	Acc@1 71.094 (70.681)	Acc@5 92.969 (93.849)
Epoch: [59][80/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 1.8901 (1.9828)	Acc@1 73.828 (70.761)	Acc@5 94.141 (93.803)
Epoch: [59][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.8695 (1.9798)	Acc@1 72.656 (70.720)	Acc@5 96.094 (93.849)
Epoch: [59][100/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1168 (1.9881)	Acc@1 67.578 (70.599)	Acc@5 91.797 (93.692)
Epoch: [59][110/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0687 (1.9955)	Acc@1 68.750 (70.425)	Acc@5 94.531 (93.627)
Epoch: [59][120/196]	Time 0.013 (0.015)	Data 0.003 (0.004)	Loss 2.1957 (2.0062)	Acc@1 65.625 (70.119)	Acc@5 90.625 (93.534)
Epoch: [59][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1285 (2.0127)	Acc@1 66.016 (69.949)	Acc@5 92.188 (93.431)
Epoch: [59][140/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.0487 (2.0167)	Acc@1 66.406 (69.828)	Acc@5 93.750 (93.354)
Epoch: [59][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0449 (2.0198)	Acc@1 65.625 (69.699)	Acc@5 92.578 (93.315)
Epoch: [59][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0499 (2.0247)	Acc@1 69.922 (69.660)	Acc@5 90.234 (93.214)
Epoch: [59][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9434 (2.0281)	Acc@1 72.656 (69.545)	Acc@5 94.531 (93.188)
Epoch: [59][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0595 (2.0328)	Acc@1 69.922 (69.423)	Acc@5 92.578 (93.133)
Epoch: [59][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1037 (2.0370)	Acc@1 66.797 (69.312)	Acc@5 93.359 (93.102)
[INFO] Storing checkpoint...

Epoch: [60 | 60] LR: 0.100000
module.conv1.weight [57, 3, 3, 3]
module.conv2.weight [128, 57, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [506, 512, 3, 3]
module.conv8.weight [384, 506, 3, 3]
Epoch: [60][0/196]	Time 0.026 (0.026)	Data 0.191 (0.191)	Loss 2.0437 (2.0437)	Acc@1 69.922 (69.922)	Acc@5 91.797 (91.797)
Epoch: [60][10/196]	Time 0.017 (0.016)	Data 0.001 (0.019)	Loss 2.0131 (1.9991)	Acc@1 70.703 (70.348)	Acc@5 93.359 (93.821)
Epoch: [60][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 2.0465 (1.9790)	Acc@1 69.531 (70.666)	Acc@5 95.312 (94.308)
Epoch: [60][30/196]	Time 0.020 (0.016)	Data 0.000 (0.008)	Loss 2.1387 (1.9515)	Acc@1 66.406 (71.585)	Acc@5 93.359 (94.506)
Epoch: [60][40/196]	Time 0.014 (0.016)	Data 0.005 (0.007)	Loss 2.0000 (1.9428)	Acc@1 71.094 (71.751)	Acc@5 91.016 (94.436)
Epoch: [60][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0343 (1.9446)	Acc@1 67.969 (71.607)	Acc@5 92.578 (94.409)
Epoch: [60][60/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 1.9898 (1.9505)	Acc@1 72.656 (71.382)	Acc@5 94.141 (94.403)
Epoch: [60][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1728 (1.9635)	Acc@1 66.797 (71.066)	Acc@5 92.188 (94.190)
Epoch: [60][80/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.0511 (1.9648)	Acc@1 69.922 (71.070)	Acc@5 93.359 (94.165)
Epoch: [60][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0253 (1.9710)	Acc@1 69.922 (70.845)	Acc@5 93.359 (94.055)
Epoch: [60][100/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 1.9757 (1.9804)	Acc@1 69.922 (70.525)	Acc@5 94.922 (93.990)
Epoch: [60][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1191 (1.9857)	Acc@1 65.234 (70.383)	Acc@5 92.969 (93.958)
Epoch: [60][120/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 1.9760 (1.9895)	Acc@1 71.484 (70.290)	Acc@5 94.141 (93.892)
Epoch: [60][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9131 (1.9949)	Acc@1 73.047 (70.163)	Acc@5 94.531 (93.822)
Epoch: [60][140/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.0009 (1.9980)	Acc@1 68.359 (70.041)	Acc@5 93.359 (93.786)
Epoch: [60][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0781 (2.0019)	Acc@1 66.406 (69.987)	Acc@5 93.750 (93.691)
Epoch: [60][160/196]	Time 0.012 (0.015)	Data 0.013 (0.004)	Loss 2.1907 (2.0082)	Acc@1 66.016 (69.856)	Acc@5 89.062 (93.602)
Epoch: [60][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0464 (2.0130)	Acc@1 67.188 (69.741)	Acc@5 93.750 (93.503)
Epoch: [60][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1893 (2.0177)	Acc@1 64.844 (69.656)	Acc@5 91.406 (93.422)
Epoch: [60][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0604 (2.0229)	Acc@1 69.531 (69.501)	Acc@5 89.844 (93.331)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [57, 3, 3, 3] >> [51, 3, 3, 3]
[module.conv2.weight]: [128, 57, 3, 3] >> [127, 51, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [256, 127, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [511, 256, 3, 3]
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 511, 3, 3]
[module.conv7.weight]: [506, 512, 3, 3] >> [493, 512, 3, 3]
[module.conv8.weight]: [384, 506, 3, 3] >> [359, 493, 3, 3]
[module.fc.weight]: [100, 384] >> [100, 359]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
57.53
python src/cifar.py --workers 4 --dataset cifar100 --epochs 70 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_70.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 8.38M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [51, 3, 3, 3]
conv2 --> [127, 51, 3, 3]
conv3 --> [256, 127, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [511, 256, 3, 3]
conv6 --> [512, 511, 3, 3]
conv7 --> [493, 512, 3, 3]
conv8 --> [359, 493, 3, 3]
fc --> [359, 100]
1, 564724224, 1410048, 51
2, 6237817344, 14923008, 127
3, 8539471872, 18726912, 256
4, 17213423616, 37748736, 256
5, 10247602176, 18837504, 511
6, 20495204352, 37675008, 512
7, 6978797568, 9086976, 493
8, 4893336576, 6371532, 359
fc, 13785600, 35900, 0
===================
FLOP REPORT: 29368813800000.0 54912000000.0 144815624 137280 2565 15.978975296020508

Epoch: [61 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [61][0/196]	Time 0.728 (0.728)	Data 0.159 (0.159)	Loss 2.0229 (2.0229)	Acc@1 68.359 (68.359)	Acc@5 93.359 (93.359)
Epoch: [61][10/196]	Time 0.017 (0.080)	Data 0.003 (0.016)	Loss 1.9610 (1.9410)	Acc@1 72.266 (72.159)	Acc@5 91.406 (93.999)
Epoch: [61][20/196]	Time 0.023 (0.049)	Data 0.003 (0.010)	Loss 1.8358 (1.9326)	Acc@1 74.609 (71.819)	Acc@5 96.484 (94.420)
Epoch: [61][30/196]	Time 0.014 (0.038)	Data 0.003 (0.007)	Loss 1.8610 (1.9321)	Acc@1 74.609 (72.177)	Acc@5 95.703 (94.229)
Epoch: [61][40/196]	Time 0.015 (0.032)	Data 0.002 (0.006)	Loss 1.8337 (1.9342)	Acc@1 75.781 (72.037)	Acc@5 96.484 (94.350)
Epoch: [61][50/196]	Time 0.015 (0.029)	Data 0.003 (0.005)	Loss 1.9549 (1.9447)	Acc@1 73.438 (71.844)	Acc@5 94.141 (94.332)
Epoch: [61][60/196]	Time 0.014 (0.027)	Data 0.002 (0.005)	Loss 1.9024 (1.9468)	Acc@1 71.875 (71.849)	Acc@5 94.141 (94.237)
Epoch: [61][70/196]	Time 0.014 (0.025)	Data 0.003 (0.005)	Loss 2.0786 (1.9531)	Acc@1 71.094 (71.704)	Acc@5 91.016 (94.146)
Epoch: [61][80/196]	Time 0.012 (0.024)	Data 0.005 (0.004)	Loss 2.0467 (1.9569)	Acc@1 64.844 (71.489)	Acc@5 94.141 (94.131)
Epoch: [61][90/196]	Time 0.013 (0.023)	Data 0.004 (0.004)	Loss 2.1162 (1.9679)	Acc@1 69.922 (71.248)	Acc@5 93.359 (93.978)
Epoch: [61][100/196]	Time 0.011 (0.022)	Data 0.008 (0.004)	Loss 1.8550 (1.9738)	Acc@1 73.828 (71.082)	Acc@5 95.312 (93.901)
Epoch: [61][110/196]	Time 0.013 (0.021)	Data 0.006 (0.004)	Loss 2.0740 (1.9817)	Acc@1 71.094 (70.960)	Acc@5 92.969 (93.792)
Epoch: [61][120/196]	Time 0.011 (0.021)	Data 0.008 (0.004)	Loss 2.1067 (1.9880)	Acc@1 68.750 (70.826)	Acc@5 92.188 (93.724)
Epoch: [61][130/196]	Time 0.013 (0.020)	Data 0.005 (0.004)	Loss 2.0961 (1.9957)	Acc@1 67.578 (70.593)	Acc@5 94.141 (93.669)
Epoch: [61][140/196]	Time 0.011 (0.020)	Data 0.008 (0.004)	Loss 1.9772 (2.0009)	Acc@1 70.312 (70.518)	Acc@5 94.922 (93.581)
Epoch: [61][150/196]	Time 0.013 (0.019)	Data 0.004 (0.004)	Loss 2.0816 (2.0050)	Acc@1 66.406 (70.416)	Acc@5 92.578 (93.512)
Epoch: [61][160/196]	Time 0.011 (0.019)	Data 0.007 (0.004)	Loss 2.0587 (2.0118)	Acc@1 67.188 (70.220)	Acc@5 94.141 (93.437)
Epoch: [61][170/196]	Time 0.011 (0.019)	Data 0.006 (0.004)	Loss 2.0490 (2.0208)	Acc@1 70.312 (70.022)	Acc@5 92.578 (93.323)
Epoch: [61][180/196]	Time 0.011 (0.018)	Data 0.008 (0.004)	Loss 2.2405 (2.0272)	Acc@1 66.406 (69.874)	Acc@5 89.844 (93.249)
Epoch: [61][190/196]	Time 0.013 (0.018)	Data 0.003 (0.004)	Loss 2.1172 (2.0322)	Acc@1 66.797 (69.689)	Acc@5 91.406 (93.190)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [62 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [62][0/196]	Time 0.028 (0.028)	Data 0.161 (0.161)	Loss 1.9416 (1.9416)	Acc@1 72.656 (72.656)	Acc@5 92.969 (92.969)
Epoch: [62][10/196]	Time 0.016 (0.017)	Data 0.000 (0.017)	Loss 1.9449 (1.9457)	Acc@1 71.484 (71.662)	Acc@5 94.141 (94.638)
Epoch: [62][20/196]	Time 0.012 (0.016)	Data 0.006 (0.010)	Loss 2.0309 (1.9513)	Acc@1 66.406 (71.243)	Acc@5 93.359 (94.401)
Epoch: [62][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 1.9229 (1.9505)	Acc@1 71.484 (71.295)	Acc@5 94.141 (94.405)
Epoch: [62][40/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 1.9355 (1.9640)	Acc@1 75.000 (71.189)	Acc@5 93.359 (94.150)
Epoch: [62][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.9856 (1.9692)	Acc@1 70.703 (71.178)	Acc@5 94.531 (93.903)
Epoch: [62][60/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 2.0532 (1.9716)	Acc@1 70.312 (71.241)	Acc@5 93.750 (93.814)
Epoch: [62][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.8306 (1.9729)	Acc@1 75.781 (71.116)	Acc@5 93.359 (93.805)
Epoch: [62][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0304 (1.9805)	Acc@1 69.922 (70.901)	Acc@5 92.188 (93.678)
Epoch: [62][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0541 (1.9851)	Acc@1 68.750 (70.750)	Acc@5 91.016 (93.634)
Epoch: [62][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0740 (1.9908)	Acc@1 66.797 (70.614)	Acc@5 94.922 (93.661)
Epoch: [62][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0758 (1.9953)	Acc@1 70.703 (70.520)	Acc@5 91.406 (93.563)
Epoch: [62][120/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.3354 (2.0060)	Acc@1 59.766 (70.261)	Acc@5 90.234 (93.398)
Epoch: [62][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1559 (2.0156)	Acc@1 66.406 (70.017)	Acc@5 89.453 (93.279)
Epoch: [62][140/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.1829 (2.0243)	Acc@1 65.625 (69.764)	Acc@5 91.797 (93.188)
Epoch: [62][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0377 (2.0303)	Acc@1 67.188 (69.596)	Acc@5 94.141 (93.150)
Epoch: [62][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1411 (2.0329)	Acc@1 65.234 (69.488)	Acc@5 92.578 (93.160)
Epoch: [62][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9994 (2.0360)	Acc@1 71.484 (69.428)	Acc@5 92.188 (93.115)
Epoch: [62][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1449 (2.0409)	Acc@1 66.406 (69.322)	Acc@5 94.141 (93.051)
Epoch: [62][190/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2914 (2.0475)	Acc@1 59.375 (69.116)	Acc@5 88.672 (92.967)
[INFO] Storing checkpoint...

Epoch: [63 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [63][0/196]	Time 0.029 (0.029)	Data 0.165 (0.165)	Loss 2.0068 (2.0068)	Acc@1 71.094 (71.094)	Acc@5 91.797 (91.797)
Epoch: [63][10/196]	Time 0.017 (0.016)	Data 0.001 (0.017)	Loss 1.9417 (2.0262)	Acc@1 70.312 (70.810)	Acc@5 93.359 (93.217)
Epoch: [63][20/196]	Time 0.011 (0.016)	Data 0.007 (0.010)	Loss 1.9988 (2.0097)	Acc@1 66.797 (70.275)	Acc@5 95.312 (93.694)
Epoch: [63][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.9977 (1.9879)	Acc@1 72.266 (70.766)	Acc@5 93.750 (93.977)
Epoch: [63][40/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 1.9522 (1.9742)	Acc@1 72.266 (71.218)	Acc@5 94.922 (94.055)
Epoch: [63][50/196]	Time 0.014 (0.015)	Data 0.000 (0.006)	Loss 1.9733 (1.9753)	Acc@1 69.922 (71.147)	Acc@5 93.359 (94.102)
Epoch: [63][60/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.0002 (1.9725)	Acc@1 71.484 (71.145)	Acc@5 94.141 (94.115)
Epoch: [63][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1797 (1.9769)	Acc@1 63.281 (71.017)	Acc@5 91.406 (94.102)
Epoch: [63][80/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.0300 (1.9886)	Acc@1 70.312 (70.824)	Acc@5 92.969 (93.890)
Epoch: [63][90/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2481 (2.0011)	Acc@1 62.891 (70.368)	Acc@5 88.672 (93.741)
Epoch: [63][100/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0134 (2.0060)	Acc@1 69.141 (70.289)	Acc@5 92.969 (93.630)
Epoch: [63][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1292 (2.0143)	Acc@1 69.141 (70.034)	Acc@5 91.016 (93.472)
Epoch: [63][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1341 (2.0174)	Acc@1 68.359 (69.974)	Acc@5 93.750 (93.408)
Epoch: [63][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9974 (2.0217)	Acc@1 72.656 (69.958)	Acc@5 93.359 (93.330)
Epoch: [63][140/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.1581 (2.0218)	Acc@1 67.969 (69.997)	Acc@5 91.016 (93.318)
Epoch: [63][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.8262 (2.0194)	Acc@1 76.562 (70.051)	Acc@5 95.312 (93.370)
Epoch: [63][160/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 1.9882 (2.0201)	Acc@1 68.359 (69.995)	Acc@5 93.750 (93.362)
Epoch: [63][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0174 (2.0229)	Acc@1 72.266 (69.913)	Acc@5 91.797 (93.309)
Epoch: [63][180/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.1404 (2.0274)	Acc@1 66.016 (69.803)	Acc@5 92.578 (93.217)
Epoch: [63][190/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 2.0197 (2.0270)	Acc@1 68.359 (69.846)	Acc@5 94.531 (93.243)
[INFO] Storing checkpoint...

Epoch: [64 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [64][0/196]	Time 0.029 (0.029)	Data 0.166 (0.166)	Loss 1.8749 (1.8749)	Acc@1 77.344 (77.344)	Acc@5 93.359 (93.359)
Epoch: [64][10/196]	Time 0.016 (0.015)	Data 0.000 (0.018)	Loss 1.9341 (1.9668)	Acc@1 73.828 (72.301)	Acc@5 94.922 (94.176)
Epoch: [64][20/196]	Time 0.015 (0.015)	Data 0.003 (0.011)	Loss 1.9878 (1.9607)	Acc@1 70.312 (71.596)	Acc@5 94.531 (94.252)
Epoch: [64][30/196]	Time 0.014 (0.015)	Data 0.002 (0.008)	Loss 1.9381 (1.9522)	Acc@1 74.609 (71.900)	Acc@5 94.922 (94.519)
Epoch: [64][40/196]	Time 0.012 (0.015)	Data 0.007 (0.007)	Loss 1.8529 (1.9418)	Acc@1 74.219 (71.932)	Acc@5 96.484 (94.579)
Epoch: [64][50/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 2.0026 (1.9466)	Acc@1 68.750 (71.944)	Acc@5 94.922 (94.347)
Epoch: [64][60/196]	Time 0.012 (0.015)	Data 0.004 (0.006)	Loss 1.9122 (1.9501)	Acc@1 69.531 (71.766)	Acc@5 94.922 (94.269)
Epoch: [64][70/196]	Time 0.011 (0.014)	Data 0.008 (0.006)	Loss 2.1307 (1.9568)	Acc@1 62.891 (71.605)	Acc@5 93.359 (94.218)
Epoch: [64][80/196]	Time 0.013 (0.014)	Data 0.003 (0.005)	Loss 1.9903 (1.9662)	Acc@1 69.141 (71.229)	Acc@5 95.312 (94.112)
Epoch: [64][90/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 2.1690 (1.9705)	Acc@1 67.188 (71.064)	Acc@5 91.406 (94.003)
Epoch: [64][100/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 2.0255 (1.9774)	Acc@1 67.578 (70.897)	Acc@5 93.359 (93.940)
Epoch: [64][110/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 2.1504 (1.9813)	Acc@1 66.797 (70.742)	Acc@5 91.406 (93.863)
Epoch: [64][120/196]	Time 0.017 (0.014)	Data 0.000 (0.005)	Loss 1.9303 (1.9888)	Acc@1 74.609 (70.610)	Acc@5 92.188 (93.721)
Epoch: [64][130/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 2.0678 (1.9933)	Acc@1 69.531 (70.497)	Acc@5 90.625 (93.669)
Epoch: [64][140/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 2.1537 (2.0000)	Acc@1 69.531 (70.346)	Acc@5 91.016 (93.611)
Epoch: [64][150/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 2.0321 (2.0031)	Acc@1 68.750 (70.263)	Acc@5 91.016 (93.584)
Epoch: [64][160/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 1.9655 (2.0069)	Acc@1 71.484 (70.191)	Acc@5 94.141 (93.520)
Epoch: [64][170/196]	Time 0.012 (0.014)	Data 0.008 (0.005)	Loss 2.1718 (2.0119)	Acc@1 63.672 (70.036)	Acc@5 89.844 (93.428)
Epoch: [64][180/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 2.1648 (2.0149)	Acc@1 65.234 (69.946)	Acc@5 90.625 (93.407)
Epoch: [64][190/196]	Time 0.012 (0.014)	Data 0.012 (0.004)	Loss 2.0271 (2.0183)	Acc@1 67.969 (69.867)	Acc@5 94.531 (93.390)
[INFO] Storing checkpoint...

Epoch: [65 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [65][0/196]	Time 0.031 (0.031)	Data 0.178 (0.178)	Loss 1.9688 (1.9688)	Acc@1 71.094 (71.094)	Acc@5 93.750 (93.750)
Epoch: [65][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 2.0057 (1.9633)	Acc@1 71.094 (72.195)	Acc@5 94.531 (94.105)
Epoch: [65][20/196]	Time 0.015 (0.016)	Data 0.003 (0.010)	Loss 1.9664 (1.9654)	Acc@1 73.047 (72.173)	Acc@5 94.531 (93.973)
Epoch: [65][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9372 (1.9679)	Acc@1 71.094 (71.724)	Acc@5 93.359 (93.952)
Epoch: [65][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.8653 (1.9591)	Acc@1 75.391 (71.865)	Acc@5 95.312 (94.055)
Epoch: [65][50/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.9441 (1.9618)	Acc@1 71.094 (71.921)	Acc@5 93.359 (93.941)
Epoch: [65][60/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0246 (1.9598)	Acc@1 69.531 (71.843)	Acc@5 92.578 (93.974)
Epoch: [65][70/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9336 (1.9636)	Acc@1 71.875 (71.677)	Acc@5 91.797 (93.932)
Epoch: [65][80/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 1.9501 (1.9711)	Acc@1 69.531 (71.465)	Acc@5 94.922 (93.899)
Epoch: [65][90/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2032 (1.9860)	Acc@1 66.016 (71.038)	Acc@5 92.188 (93.698)
Epoch: [65][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0458 (1.9912)	Acc@1 69.141 (70.838)	Acc@5 92.188 (93.673)
Epoch: [65][110/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1377 (2.0047)	Acc@1 64.062 (70.457)	Acc@5 92.188 (93.493)
Epoch: [65][120/196]	Time 0.012 (0.015)	Data 0.002 (0.004)	Loss 2.2186 (2.0101)	Acc@1 62.500 (70.280)	Acc@5 91.016 (93.421)
Epoch: [65][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2389 (2.0161)	Acc@1 64.453 (70.143)	Acc@5 91.016 (93.359)
Epoch: [65][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9783 (2.0187)	Acc@1 71.875 (70.008)	Acc@5 94.141 (93.343)
Epoch: [65][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9763 (2.0229)	Acc@1 69.531 (69.862)	Acc@5 94.531 (93.310)
Epoch: [65][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9823 (2.0262)	Acc@1 75.000 (69.774)	Acc@5 94.141 (93.279)
Epoch: [65][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0337 (2.0279)	Acc@1 71.484 (69.762)	Acc@5 94.922 (93.247)
Epoch: [65][180/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2037 (2.0321)	Acc@1 67.578 (69.665)	Acc@5 90.625 (93.241)
Epoch: [65][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9926 (2.0369)	Acc@1 72.266 (69.607)	Acc@5 93.750 (93.147)
[INFO] Storing checkpoint...

Epoch: [66 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [66][0/196]	Time 0.030 (0.030)	Data 0.174 (0.174)	Loss 1.8640 (1.8640)	Acc@1 72.266 (72.266)	Acc@5 94.531 (94.531)
Epoch: [66][10/196]	Time 0.015 (0.016)	Data 0.003 (0.018)	Loss 1.9375 (1.9446)	Acc@1 74.219 (72.372)	Acc@5 92.969 (94.673)
Epoch: [66][20/196]	Time 0.014 (0.015)	Data 0.002 (0.010)	Loss 1.8738 (1.9466)	Acc@1 74.219 (72.303)	Acc@5 95.703 (94.457)
Epoch: [66][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.9495 (1.9550)	Acc@1 69.922 (72.077)	Acc@5 92.969 (94.279)
Epoch: [66][40/196]	Time 0.015 (0.015)	Data 0.003 (0.007)	Loss 1.9785 (1.9545)	Acc@1 67.969 (71.989)	Acc@5 95.703 (94.255)
Epoch: [66][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.0217 (1.9599)	Acc@1 65.625 (71.615)	Acc@5 93.750 (94.171)
Epoch: [66][60/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.9292 (1.9636)	Acc@1 73.438 (71.529)	Acc@5 94.141 (94.115)
Epoch: [66][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9518 (1.9695)	Acc@1 72.656 (71.402)	Acc@5 92.578 (93.987)
Epoch: [66][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9265 (1.9661)	Acc@1 73.047 (71.533)	Acc@5 92.969 (93.972)
Epoch: [66][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0890 (1.9627)	Acc@1 67.188 (71.532)	Acc@5 92.578 (93.960)
Epoch: [66][100/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.9288 (1.9677)	Acc@1 69.531 (71.496)	Acc@5 96.094 (93.843)
Epoch: [66][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0618 (1.9712)	Acc@1 70.312 (71.351)	Acc@5 91.406 (93.863)
Epoch: [66][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0534 (1.9757)	Acc@1 72.656 (71.258)	Acc@5 92.578 (93.779)
Epoch: [66][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0474 (1.9811)	Acc@1 71.094 (71.121)	Acc@5 91.797 (93.640)
Epoch: [66][140/196]	Time 0.011 (0.014)	Data 0.007 (0.004)	Loss 2.2395 (1.9865)	Acc@1 63.672 (70.991)	Acc@5 90.625 (93.553)
Epoch: [66][150/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 2.0722 (1.9936)	Acc@1 68.359 (70.835)	Acc@5 93.359 (93.476)
Epoch: [66][160/196]	Time 0.011 (0.014)	Data 0.005 (0.004)	Loss 2.1830 (2.0004)	Acc@1 65.625 (70.652)	Acc@5 89.844 (93.369)
Epoch: [66][170/196]	Time 0.016 (0.014)	Data 0.001 (0.004)	Loss 2.0350 (2.0066)	Acc@1 72.656 (70.527)	Acc@5 93.359 (93.305)
Epoch: [66][180/196]	Time 0.011 (0.014)	Data 0.009 (0.004)	Loss 2.1344 (2.0117)	Acc@1 64.844 (70.332)	Acc@5 94.531 (93.256)
Epoch: [66][190/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 2.0194 (2.0161)	Acc@1 68.359 (70.171)	Acc@5 94.531 (93.224)
[INFO] Storing checkpoint...

Epoch: [67 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [67][0/196]	Time 0.030 (0.030)	Data 0.175 (0.175)	Loss 1.9471 (1.9471)	Acc@1 73.438 (73.438)	Acc@5 91.797 (91.797)
Epoch: [67][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 1.9241 (1.9466)	Acc@1 72.266 (71.804)	Acc@5 93.750 (94.070)
Epoch: [67][20/196]	Time 0.015 (0.016)	Data 0.002 (0.011)	Loss 1.9094 (1.9466)	Acc@1 72.266 (71.968)	Acc@5 95.312 (94.141)
Epoch: [67][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 1.9523 (1.9292)	Acc@1 71.094 (72.656)	Acc@5 94.922 (94.317)
Epoch: [67][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 1.9358 (1.9307)	Acc@1 73.047 (72.342)	Acc@5 94.922 (94.446)
Epoch: [67][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 1.8808 (1.9273)	Acc@1 76.172 (72.495)	Acc@5 94.922 (94.432)
Epoch: [67][60/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.0867 (1.9312)	Acc@1 64.844 (72.272)	Acc@5 92.969 (94.358)
Epoch: [67][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2045 (1.9455)	Acc@1 62.500 (71.919)	Acc@5 92.188 (94.256)
Epoch: [67][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0796 (1.9558)	Acc@1 69.141 (71.672)	Acc@5 93.359 (94.131)
Epoch: [67][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.8877 (1.9635)	Acc@1 75.781 (71.570)	Acc@5 95.312 (94.008)
Epoch: [67][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.9799 (1.9746)	Acc@1 71.484 (71.159)	Acc@5 93.359 (93.916)
Epoch: [67][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0981 (1.9832)	Acc@1 67.188 (70.964)	Acc@5 91.797 (93.817)
Epoch: [67][120/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1806 (1.9936)	Acc@1 68.359 (70.819)	Acc@5 91.406 (93.689)
Epoch: [67][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9719 (1.9986)	Acc@1 71.094 (70.658)	Acc@5 95.312 (93.664)
Epoch: [67][140/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.2005 (2.0037)	Acc@1 67.188 (70.534)	Acc@5 90.625 (93.589)
Epoch: [67][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0405 (2.0088)	Acc@1 70.703 (70.354)	Acc@5 92.578 (93.512)
Epoch: [67][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0211 (2.0138)	Acc@1 67.188 (70.201)	Acc@5 95.312 (93.427)
Epoch: [67][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9853 (2.0159)	Acc@1 73.438 (70.148)	Acc@5 92.578 (93.357)
Epoch: [67][180/196]	Time 0.012 (0.015)	Data 0.008 (0.003)	Loss 2.3649 (2.0226)	Acc@1 63.281 (69.954)	Acc@5 89.062 (93.282)
Epoch: [67][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.2383 (2.0301)	Acc@1 60.156 (69.752)	Acc@5 91.016 (93.220)
[INFO] Storing checkpoint...

Epoch: [68 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [68][0/196]	Time 0.031 (0.031)	Data 0.179 (0.179)	Loss 1.8547 (1.8547)	Acc@1 75.000 (75.000)	Acc@5 95.703 (95.703)
Epoch: [68][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 1.9474 (1.9894)	Acc@1 74.219 (71.626)	Acc@5 94.531 (93.643)
Epoch: [68][20/196]	Time 0.015 (0.016)	Data 0.002 (0.011)	Loss 1.8275 (1.9710)	Acc@1 76.562 (71.968)	Acc@5 96.094 (93.992)
Epoch: [68][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 1.8979 (1.9487)	Acc@1 73.047 (72.064)	Acc@5 95.312 (94.279)
Epoch: [68][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 1.9244 (1.9495)	Acc@1 75.000 (72.228)	Acc@5 93.359 (94.274)
Epoch: [68][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.8400 (1.9516)	Acc@1 74.219 (71.998)	Acc@5 95.312 (94.217)
Epoch: [68][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9648 (1.9491)	Acc@1 69.922 (71.997)	Acc@5 95.703 (94.384)
Epoch: [68][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.8782 (1.9404)	Acc@1 72.266 (72.112)	Acc@5 95.703 (94.531)
Epoch: [68][80/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 1.9423 (1.9427)	Acc@1 71.875 (72.015)	Acc@5 95.312 (94.444)
Epoch: [68][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0053 (1.9444)	Acc@1 71.484 (72.038)	Acc@5 91.797 (94.338)
Epoch: [68][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2347 (1.9509)	Acc@1 65.234 (71.867)	Acc@5 91.016 (94.291)
Epoch: [68][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2006 (1.9601)	Acc@1 66.016 (71.660)	Acc@5 93.359 (94.239)
Epoch: [68][120/196]	Time 0.011 (0.014)	Data 0.011 (0.004)	Loss 2.0529 (1.9673)	Acc@1 69.531 (71.526)	Acc@5 92.188 (94.102)
Epoch: [68][130/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0190 (1.9755)	Acc@1 71.094 (71.317)	Acc@5 92.969 (93.995)
Epoch: [68][140/196]	Time 0.011 (0.014)	Data 0.006 (0.004)	Loss 1.9209 (1.9804)	Acc@1 73.438 (71.141)	Acc@5 95.312 (93.944)
Epoch: [68][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1260 (1.9872)	Acc@1 67.188 (70.926)	Acc@5 91.797 (93.885)
Epoch: [68][160/196]	Time 0.012 (0.014)	Data 0.005 (0.004)	Loss 2.2222 (1.9970)	Acc@1 67.188 (70.635)	Acc@5 90.625 (93.760)
Epoch: [68][170/196]	Time 0.015 (0.014)	Data 0.002 (0.004)	Loss 2.1763 (2.0038)	Acc@1 66.797 (70.472)	Acc@5 92.969 (93.670)
Epoch: [68][180/196]	Time 0.017 (0.014)	Data 0.002 (0.004)	Loss 2.2459 (2.0109)	Acc@1 62.109 (70.330)	Acc@5 91.016 (93.573)
Epoch: [68][190/196]	Time 0.015 (0.014)	Data 0.002 (0.004)	Loss 2.1977 (2.0184)	Acc@1 62.500 (70.141)	Acc@5 92.188 (93.505)
[INFO] Storing checkpoint...

Epoch: [69 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [69][0/196]	Time 0.029 (0.029)	Data 0.179 (0.179)	Loss 2.0280 (2.0280)	Acc@1 69.922 (69.922)	Acc@5 92.188 (92.188)
Epoch: [69][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 2.2155 (2.0381)	Acc@1 66.406 (70.490)	Acc@5 90.625 (93.040)
Epoch: [69][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 1.7821 (1.9995)	Acc@1 75.391 (71.317)	Acc@5 97.656 (93.657)
Epoch: [69][30/196]	Time 0.014 (0.015)	Data 0.002 (0.008)	Loss 1.9645 (1.9782)	Acc@1 71.875 (71.585)	Acc@5 96.484 (94.015)
Epoch: [69][40/196]	Time 0.011 (0.015)	Data 0.010 (0.007)	Loss 1.8942 (1.9603)	Acc@1 75.391 (71.885)	Acc@5 94.922 (94.226)
Epoch: [69][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 1.8688 (1.9572)	Acc@1 75.391 (71.829)	Acc@5 93.750 (94.301)
Epoch: [69][60/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 1.9811 (1.9590)	Acc@1 66.797 (71.830)	Acc@5 96.875 (94.262)
Epoch: [69][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0372 (1.9664)	Acc@1 69.922 (71.616)	Acc@5 92.578 (94.102)
Epoch: [69][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 1.9455 (1.9754)	Acc@1 71.875 (71.335)	Acc@5 94.922 (94.025)
Epoch: [69][90/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.1495 (1.9837)	Acc@1 63.672 (71.128)	Acc@5 92.188 (93.892)
Epoch: [69][100/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.1483 (1.9905)	Acc@1 64.453 (70.908)	Acc@5 94.922 (93.901)
Epoch: [69][110/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.0891 (2.0026)	Acc@1 69.141 (70.566)	Acc@5 92.578 (93.729)
Epoch: [69][120/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1438 (2.0072)	Acc@1 67.969 (70.429)	Acc@5 92.188 (93.676)
Epoch: [69][130/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.0163 (2.0135)	Acc@1 73.047 (70.295)	Acc@5 92.969 (93.571)
Epoch: [69][140/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 2.0039 (2.0157)	Acc@1 73.438 (70.207)	Acc@5 93.359 (93.545)
Epoch: [69][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1041 (2.0188)	Acc@1 70.312 (70.178)	Acc@5 92.969 (93.517)
Epoch: [69][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.2871 (2.0197)	Acc@1 63.672 (70.152)	Acc@5 92.969 (93.517)
Epoch: [69][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1674 (2.0249)	Acc@1 65.625 (70.000)	Acc@5 89.844 (93.469)
Epoch: [69][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.9873 (2.0276)	Acc@1 68.359 (69.915)	Acc@5 94.922 (93.439)
Epoch: [69][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0590 (2.0303)	Acc@1 67.578 (69.871)	Acc@5 92.969 (93.388)
[INFO] Storing checkpoint...

Epoch: [70 | 70] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [127, 51, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [493, 512, 3, 3]
module.conv8.weight [359, 493, 3, 3]
Epoch: [70][0/196]	Time 0.032 (0.032)	Data 0.177 (0.177)	Loss 1.8674 (1.8674)	Acc@1 73.828 (73.828)	Acc@5 95.703 (95.703)
Epoch: [70][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 1.8741 (1.9563)	Acc@1 73.438 (70.916)	Acc@5 94.531 (94.780)
Epoch: [70][20/196]	Time 0.014 (0.016)	Data 0.008 (0.011)	Loss 2.0490 (1.9624)	Acc@1 69.922 (71.019)	Acc@5 93.359 (94.401)
Epoch: [70][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 1.7717 (1.9580)	Acc@1 75.781 (71.220)	Acc@5 97.266 (94.519)
Epoch: [70][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 2.0329 (1.9629)	Acc@1 70.703 (71.094)	Acc@5 93.750 (94.512)
Epoch: [70][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.0380 (1.9609)	Acc@1 70.703 (71.117)	Acc@5 92.969 (94.462)
Epoch: [70][60/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0374 (1.9640)	Acc@1 69.922 (71.055)	Acc@5 93.359 (94.390)
Epoch: [70][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9164 (1.9675)	Acc@1 73.438 (71.006)	Acc@5 92.969 (94.372)
Epoch: [70][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0235 (1.9730)	Acc@1 71.484 (70.920)	Acc@5 93.359 (94.324)
Epoch: [70][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1555 (1.9825)	Acc@1 68.750 (70.806)	Acc@5 90.625 (94.192)
Epoch: [70][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9568 (1.9859)	Acc@1 73.047 (70.657)	Acc@5 96.875 (94.172)
Epoch: [70][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0601 (1.9930)	Acc@1 68.359 (70.622)	Acc@5 92.188 (94.053)
Epoch: [70][120/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0795 (1.9984)	Acc@1 67.969 (70.484)	Acc@5 94.141 (93.937)
Epoch: [70][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9991 (1.9999)	Acc@1 71.094 (70.432)	Acc@5 92.969 (93.908)
Epoch: [70][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1678 (2.0062)	Acc@1 59.766 (70.221)	Acc@5 93.359 (93.805)
Epoch: [70][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1703 (2.0115)	Acc@1 69.141 (70.139)	Acc@5 90.625 (93.729)
Epoch: [70][160/196]	Time 0.018 (0.015)	Data 0.002 (0.004)	Loss 2.0233 (2.0173)	Acc@1 69.141 (70.048)	Acc@5 92.188 (93.619)
Epoch: [70][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2042 (2.0224)	Acc@1 64.844 (69.965)	Acc@5 91.406 (93.517)
Epoch: [70][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1606 (2.0257)	Acc@1 62.500 (69.874)	Acc@5 92.578 (93.510)
Epoch: [70][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1169 (2.0275)	Acc@1 67.969 (69.861)	Acc@5 92.578 (93.472)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [51, 3, 3, 3] >> [49, 3, 3, 3]
[module.conv2.weight]: [127, 51, 3, 3] >> [127, 49, 3, 3]
[module.conv3.weight]: [256, 127, 3, 3] >> [256, 127, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 256, 3, 3]
[module.conv5.weight]: [511, 256, 3, 3] >> [511, 256, 3, 3]
[module.conv6.weight]: [512, 511, 3, 3] >> [512, 511, 3, 3]
[module.conv7.weight]: [493, 512, 3, 3] >> [487, 512, 3, 3]
[module.conv8.weight]: [359, 493, 3, 3] >> [328, 487, 3, 3]
[module.fc.weight]: [100, 359] >> [100, 328]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
57.53
python src/cifar.py --workers 4 --dataset cifar100 --epochs 80 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_80.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 8.19M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [49, 3, 3, 3]
conv2 --> [127, 49, 3, 3]
conv3 --> [256, 127, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [511, 256, 3, 3]
conv6 --> [512, 511, 3, 3]
conv7 --> [487, 512, 3, 3]
conv8 --> [328, 487, 3, 3]
fc --> [328, 100]
1, 542578176, 1354752, 49
2, 5993197056, 14337792, 127
3, 8539471872, 18726912, 256
4, 17213423616, 37748736, 256
5, 10247602176, 18837504, 511
6, 20495204352, 37675008, 512
7, 6893862912, 8976384, 487
8, 4416380928, 5750496, 328
fc, 12595200, 32800, 0
===================
FLOP REPORT: 29044654800000.0 54033600000.0 143440384 135084 2526 15.619598388671875

Epoch: [71 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [71][0/196]	Time 0.710 (0.710)	Data 0.168 (0.168)	Loss 1.8108 (1.8108)	Acc@1 75.000 (75.000)	Acc@5 96.094 (96.094)
Epoch: [71][10/196]	Time 0.014 (0.078)	Data 0.003 (0.017)	Loss 2.0702 (1.9465)	Acc@1 69.922 (71.875)	Acc@5 93.359 (95.064)
Epoch: [71][20/196]	Time 0.014 (0.048)	Data 0.003 (0.010)	Loss 1.9456 (1.9300)	Acc@1 71.875 (72.098)	Acc@5 92.188 (95.033)
Epoch: [71][30/196]	Time 0.016 (0.037)	Data 0.002 (0.008)	Loss 1.9855 (1.9354)	Acc@1 69.922 (72.051)	Acc@5 93.359 (94.783)
Epoch: [71][40/196]	Time 0.015 (0.032)	Data 0.002 (0.006)	Loss 1.9936 (1.9460)	Acc@1 73.438 (72.037)	Acc@5 92.578 (94.588)
Epoch: [71][50/196]	Time 0.014 (0.028)	Data 0.003 (0.006)	Loss 1.9926 (1.9494)	Acc@1 69.141 (71.952)	Acc@5 94.141 (94.447)
Epoch: [71][60/196]	Time 0.015 (0.026)	Data 0.002 (0.005)	Loss 2.0718 (1.9525)	Acc@1 69.141 (71.907)	Acc@5 92.969 (94.371)
Epoch: [71][70/196]	Time 0.015 (0.025)	Data 0.002 (0.005)	Loss 2.0775 (1.9539)	Acc@1 70.312 (71.919)	Acc@5 91.406 (94.256)
Epoch: [71][80/196]	Time 0.015 (0.023)	Data 0.002 (0.004)	Loss 2.0204 (1.9522)	Acc@1 68.359 (72.015)	Acc@5 94.141 (94.362)
Epoch: [71][90/196]	Time 0.015 (0.023)	Data 0.002 (0.004)	Loss 1.9590 (1.9566)	Acc@1 71.484 (71.858)	Acc@5 94.922 (94.329)
Epoch: [71][100/196]	Time 0.015 (0.022)	Data 0.002 (0.004)	Loss 2.0616 (1.9648)	Acc@1 70.312 (71.539)	Acc@5 95.312 (94.334)
Epoch: [71][110/196]	Time 0.012 (0.021)	Data 0.005 (0.004)	Loss 1.9474 (1.9670)	Acc@1 72.656 (71.467)	Acc@5 95.703 (94.278)
Epoch: [71][120/196]	Time 0.014 (0.020)	Data 0.003 (0.004)	Loss 2.0454 (1.9674)	Acc@1 67.188 (71.426)	Acc@5 94.531 (94.221)
Epoch: [71][130/196]	Time 0.011 (0.020)	Data 0.011 (0.004)	Loss 2.2442 (1.9780)	Acc@1 62.500 (71.156)	Acc@5 91.016 (94.090)
Epoch: [71][140/196]	Time 0.016 (0.019)	Data 0.003 (0.004)	Loss 1.9506 (1.9876)	Acc@1 75.391 (70.928)	Acc@5 93.750 (93.963)
Epoch: [71][150/196]	Time 0.012 (0.019)	Data 0.009 (0.004)	Loss 2.1106 (1.9934)	Acc@1 67.188 (70.788)	Acc@5 92.969 (93.885)
Epoch: [71][160/196]	Time 0.014 (0.019)	Data 0.003 (0.004)	Loss 2.2440 (2.0022)	Acc@1 67.188 (70.596)	Acc@5 89.062 (93.760)
Epoch: [71][170/196]	Time 0.011 (0.018)	Data 0.009 (0.004)	Loss 2.1177 (2.0086)	Acc@1 65.234 (70.354)	Acc@5 94.141 (93.736)
Epoch: [71][180/196]	Time 0.016 (0.018)	Data 0.003 (0.004)	Loss 2.0211 (2.0149)	Acc@1 66.797 (70.198)	Acc@5 93.750 (93.649)
Epoch: [71][190/196]	Time 0.011 (0.018)	Data 0.009 (0.004)	Loss 2.0092 (2.0186)	Acc@1 73.438 (70.139)	Acc@5 94.922 (93.607)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [72 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [72][0/196]	Time 0.029 (0.029)	Data 0.172 (0.172)	Loss 2.0379 (2.0379)	Acc@1 68.359 (68.359)	Acc@5 92.188 (92.188)
Epoch: [72][10/196]	Time 0.017 (0.017)	Data 0.000 (0.017)	Loss 1.9877 (2.0760)	Acc@1 71.875 (68.395)	Acc@5 91.016 (92.365)
Epoch: [72][20/196]	Time 0.014 (0.016)	Data 0.003 (0.010)	Loss 1.7897 (2.0192)	Acc@1 76.172 (69.773)	Acc@5 96.094 (93.136)
Epoch: [72][30/196]	Time 0.017 (0.015)	Data 0.001 (0.008)	Loss 1.8694 (1.9834)	Acc@1 75.391 (71.069)	Acc@5 94.922 (93.712)
Epoch: [72][40/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 1.9797 (1.9795)	Acc@1 69.922 (71.341)	Acc@5 94.922 (93.845)
Epoch: [72][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 1.9706 (1.9675)	Acc@1 70.703 (71.829)	Acc@5 94.531 (94.033)
Epoch: [72][60/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 2.0758 (1.9656)	Acc@1 66.016 (72.112)	Acc@5 92.188 (94.019)
Epoch: [72][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9899 (1.9686)	Acc@1 74.609 (71.952)	Acc@5 92.188 (93.948)
Epoch: [72][80/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 1.8306 (1.9645)	Acc@1 73.047 (72.078)	Acc@5 94.922 (93.986)
Epoch: [72][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.2153 (1.9746)	Acc@1 64.844 (71.858)	Acc@5 89.062 (93.810)
Epoch: [72][100/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0094 (1.9790)	Acc@1 71.094 (71.689)	Acc@5 95.703 (93.831)
Epoch: [72][110/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.2086 (1.9882)	Acc@1 66.797 (71.418)	Acc@5 89.453 (93.694)
Epoch: [72][120/196]	Time 0.019 (0.015)	Data 0.000 (0.005)	Loss 2.1801 (1.9963)	Acc@1 64.062 (71.197)	Acc@5 92.969 (93.585)
Epoch: [72][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0653 (2.0000)	Acc@1 69.141 (71.097)	Acc@5 92.188 (93.598)
Epoch: [72][140/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1119 (2.0047)	Acc@1 67.969 (71.016)	Acc@5 91.016 (93.520)
Epoch: [72][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.9623 (2.0084)	Acc@1 71.094 (70.884)	Acc@5 94.141 (93.476)
Epoch: [72][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1680 (2.0149)	Acc@1 67.188 (70.681)	Acc@5 93.750 (93.420)
Epoch: [72][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1619 (2.0229)	Acc@1 67.578 (70.418)	Acc@5 90.625 (93.330)
Epoch: [72][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9354 (2.0270)	Acc@1 73.438 (70.328)	Acc@5 93.359 (93.267)
Epoch: [72][190/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0045 (2.0307)	Acc@1 73.047 (70.233)	Acc@5 92.969 (93.204)
[INFO] Storing checkpoint...

Epoch: [73 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [73][0/196]	Time 0.030 (0.030)	Data 0.173 (0.173)	Loss 1.9790 (1.9790)	Acc@1 69.531 (69.531)	Acc@5 94.141 (94.141)
Epoch: [73][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 1.9002 (2.0190)	Acc@1 76.172 (70.455)	Acc@5 94.531 (93.928)
Epoch: [73][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 1.9491 (2.0063)	Acc@1 70.312 (70.926)	Acc@5 94.141 (93.769)
Epoch: [73][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 1.8604 (1.9653)	Acc@1 72.656 (71.913)	Acc@5 94.922 (94.191)
Epoch: [73][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 2.0331 (1.9557)	Acc@1 73.047 (72.237)	Acc@5 93.750 (94.245)
Epoch: [73][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 1.9310 (1.9564)	Acc@1 71.875 (72.120)	Acc@5 94.141 (94.286)
Epoch: [73][60/196]	Time 0.011 (0.015)	Data 0.011 (0.006)	Loss 2.0062 (1.9550)	Acc@1 70.703 (72.041)	Acc@5 93.750 (94.352)
Epoch: [73][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0975 (1.9548)	Acc@1 67.969 (71.996)	Acc@5 91.797 (94.289)
Epoch: [73][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0786 (1.9591)	Acc@1 69.141 (71.914)	Acc@5 91.016 (94.136)
Epoch: [73][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9864 (1.9678)	Acc@1 71.094 (71.669)	Acc@5 95.703 (94.111)
Epoch: [73][100/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 1.9472 (1.9745)	Acc@1 73.047 (71.500)	Acc@5 93.359 (94.036)
Epoch: [73][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.8801 (1.9834)	Acc@1 73.438 (71.256)	Acc@5 94.922 (93.926)
Epoch: [73][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 1.8988 (1.9870)	Acc@1 71.875 (71.123)	Acc@5 96.484 (93.895)
Epoch: [73][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0811 (1.9922)	Acc@1 66.406 (71.007)	Acc@5 91.406 (93.848)
Epoch: [73][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0578 (1.9947)	Acc@1 71.094 (71.011)	Acc@5 95.703 (93.833)
Epoch: [73][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1199 (2.0025)	Acc@1 67.969 (70.807)	Acc@5 91.406 (93.719)
Epoch: [73][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0737 (2.0089)	Acc@1 70.703 (70.696)	Acc@5 92.188 (93.621)
Epoch: [73][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0332 (2.0115)	Acc@1 71.875 (70.635)	Acc@5 90.625 (93.538)
Epoch: [73][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9484 (2.0156)	Acc@1 70.703 (70.567)	Acc@5 92.578 (93.467)
Epoch: [73][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0240 (2.0202)	Acc@1 71.484 (70.439)	Acc@5 92.188 (93.396)
[INFO] Storing checkpoint...

Epoch: [74 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [74][0/196]	Time 0.028 (0.028)	Data 0.186 (0.186)	Loss 1.9241 (1.9241)	Acc@1 71.875 (71.875)	Acc@5 94.922 (94.922)
Epoch: [74][10/196]	Time 0.017 (0.017)	Data 0.000 (0.019)	Loss 1.9248 (1.9216)	Acc@1 72.656 (73.438)	Acc@5 96.484 (94.709)
Epoch: [74][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 2.0646 (1.9276)	Acc@1 64.062 (72.917)	Acc@5 92.578 (94.513)
Epoch: [74][30/196]	Time 0.019 (0.016)	Data 0.000 (0.008)	Loss 1.8529 (1.9280)	Acc@1 75.781 (72.807)	Acc@5 96.484 (94.594)
Epoch: [74][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 1.9395 (1.9216)	Acc@1 73.828 (72.971)	Acc@5 93.750 (94.598)
Epoch: [74][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.9582 (1.9293)	Acc@1 73.438 (72.633)	Acc@5 94.141 (94.432)
Epoch: [74][60/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 1.8935 (1.9311)	Acc@1 73.828 (72.631)	Acc@5 95.312 (94.499)
Epoch: [74][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9949 (1.9333)	Acc@1 72.266 (72.530)	Acc@5 94.141 (94.542)
Epoch: [74][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.9648 (1.9381)	Acc@1 71.875 (72.309)	Acc@5 95.703 (94.526)
Epoch: [74][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9011 (1.9448)	Acc@1 72.656 (72.188)	Acc@5 95.703 (94.471)
Epoch: [74][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.0865 (1.9539)	Acc@1 70.312 (71.906)	Acc@5 93.359 (94.384)
Epoch: [74][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1697 (1.9653)	Acc@1 65.234 (71.590)	Acc@5 92.578 (94.320)
Epoch: [74][120/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.1574 (1.9760)	Acc@1 65.625 (71.245)	Acc@5 92.578 (94.189)
Epoch: [74][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0703 (1.9824)	Acc@1 66.406 (71.073)	Acc@5 92.578 (94.105)
Epoch: [74][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0121 (1.9844)	Acc@1 70.703 (71.033)	Acc@5 92.578 (94.024)
Epoch: [74][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0482 (1.9892)	Acc@1 69.531 (70.913)	Acc@5 95.703 (93.998)
Epoch: [74][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1348 (1.9939)	Acc@1 69.141 (70.793)	Acc@5 91.406 (93.930)
Epoch: [74][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.3742 (2.0026)	Acc@1 62.500 (70.564)	Acc@5 89.844 (93.809)
Epoch: [74][180/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 2.0445 (2.0065)	Acc@1 69.531 (70.481)	Acc@5 92.188 (93.726)
Epoch: [74][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1824 (2.0112)	Acc@1 66.406 (70.366)	Acc@5 89.453 (93.664)
[INFO] Storing checkpoint...

Epoch: [75 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [75][0/196]	Time 0.028 (0.028)	Data 0.174 (0.174)	Loss 1.9000 (1.9000)	Acc@1 70.703 (70.703)	Acc@5 94.531 (94.531)
Epoch: [75][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 1.9511 (1.9663)	Acc@1 74.609 (70.632)	Acc@5 92.969 (94.354)
Epoch: [75][20/196]	Time 0.013 (0.015)	Data 0.004 (0.011)	Loss 2.0633 (1.9652)	Acc@1 71.484 (71.689)	Acc@5 93.359 (94.141)
Epoch: [75][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 1.8714 (1.9534)	Acc@1 72.656 (71.724)	Acc@5 94.141 (94.380)
Epoch: [75][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 1.9325 (1.9559)	Acc@1 73.438 (71.742)	Acc@5 94.922 (94.407)
Epoch: [75][50/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 1.9709 (1.9619)	Acc@1 70.703 (71.745)	Acc@5 93.750 (94.263)
Epoch: [75][60/196]	Time 0.011 (0.015)	Data 0.012 (0.006)	Loss 2.1142 (1.9646)	Acc@1 67.188 (71.676)	Acc@5 93.359 (94.230)
Epoch: [75][70/196]	Time 0.011 (0.014)	Data 0.011 (0.006)	Loss 1.8695 (1.9670)	Acc@1 75.000 (71.737)	Acc@5 95.312 (94.201)
Epoch: [75][80/196]	Time 0.011 (0.014)	Data 0.007 (0.006)	Loss 1.9478 (1.9730)	Acc@1 72.266 (71.484)	Acc@5 95.703 (94.184)
Epoch: [75][90/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 1.9649 (1.9778)	Acc@1 69.922 (71.197)	Acc@5 95.703 (94.136)
Epoch: [75][100/196]	Time 0.013 (0.014)	Data 0.005 (0.005)	Loss 2.0984 (1.9844)	Acc@1 66.406 (71.020)	Acc@5 93.750 (94.009)
Epoch: [75][110/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 1.9820 (1.9860)	Acc@1 73.438 (71.066)	Acc@5 95.703 (94.014)
Epoch: [75][120/196]	Time 0.012 (0.014)	Data 0.011 (0.005)	Loss 1.9495 (1.9879)	Acc@1 69.531 (70.987)	Acc@5 95.312 (93.979)
Epoch: [75][130/196]	Time 0.015 (0.014)	Data 0.003 (0.005)	Loss 1.9565 (1.9889)	Acc@1 71.484 (70.960)	Acc@5 94.531 (93.956)
Epoch: [75][140/196]	Time 0.011 (0.014)	Data 0.025 (0.005)	Loss 1.8978 (1.9937)	Acc@1 73.828 (70.878)	Acc@5 94.141 (93.858)
Epoch: [75][150/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 1.9984 (1.9977)	Acc@1 73.438 (70.752)	Acc@5 94.141 (93.817)
Epoch: [75][160/196]	Time 0.011 (0.014)	Data 0.011 (0.005)	Loss 2.0027 (1.9991)	Acc@1 72.656 (70.776)	Acc@5 93.359 (93.789)
Epoch: [75][170/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 2.0993 (2.0050)	Acc@1 68.750 (70.664)	Acc@5 90.625 (93.718)
Epoch: [75][180/196]	Time 0.012 (0.014)	Data 0.007 (0.005)	Loss 2.0717 (2.0103)	Acc@1 68.750 (70.589)	Acc@5 93.359 (93.601)
Epoch: [75][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1118 (2.0188)	Acc@1 67.578 (70.386)	Acc@5 94.531 (93.476)
[INFO] Storing checkpoint...

Epoch: [76 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [76][0/196]	Time 0.030 (0.030)	Data 0.179 (0.179)	Loss 1.8838 (1.8838)	Acc@1 75.781 (75.781)	Acc@5 93.750 (93.750)
Epoch: [76][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 1.9428 (1.9472)	Acc@1 72.656 (72.585)	Acc@5 96.484 (94.673)
Epoch: [76][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 2.0615 (1.9431)	Acc@1 68.359 (72.786)	Acc@5 91.406 (94.699)
Epoch: [76][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 1.9869 (1.9591)	Acc@1 69.922 (72.064)	Acc@5 92.578 (94.330)
Epoch: [76][40/196]	Time 0.012 (0.015)	Data 0.007 (0.007)	Loss 1.8159 (1.9509)	Acc@1 74.219 (72.418)	Acc@5 94.922 (94.188)
Epoch: [76][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 1.7768 (1.9503)	Acc@1 75.000 (72.373)	Acc@5 96.875 (94.278)
Epoch: [76][60/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0365 (1.9542)	Acc@1 69.531 (72.202)	Acc@5 93.750 (94.211)
Epoch: [76][70/196]	Time 0.017 (0.015)	Data 0.002 (0.005)	Loss 1.9154 (1.9530)	Acc@1 74.609 (72.266)	Acc@5 92.969 (94.218)
Epoch: [76][80/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.9726 (1.9592)	Acc@1 71.484 (72.126)	Acc@5 94.922 (94.203)
Epoch: [76][90/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0898 (1.9663)	Acc@1 66.406 (71.888)	Acc@5 92.969 (94.128)
Epoch: [76][100/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 2.0715 (1.9747)	Acc@1 69.531 (71.639)	Acc@5 92.969 (94.036)
Epoch: [76][110/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0114 (1.9796)	Acc@1 69.922 (71.435)	Acc@5 93.359 (93.961)
Epoch: [76][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.8852 (1.9847)	Acc@1 72.656 (71.168)	Acc@5 94.531 (93.905)
Epoch: [76][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1094 (1.9908)	Acc@1 68.750 (71.016)	Acc@5 90.234 (93.857)
Epoch: [76][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0896 (1.9945)	Acc@1 67.969 (70.936)	Acc@5 93.359 (93.789)
Epoch: [76][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2455 (1.9991)	Acc@1 65.234 (70.799)	Acc@5 89.062 (93.747)
Epoch: [76][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1336 (2.0039)	Acc@1 66.797 (70.684)	Acc@5 92.188 (93.672)
Epoch: [76][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9916 (2.0101)	Acc@1 68.750 (70.532)	Acc@5 95.312 (93.590)
Epoch: [76][180/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 2.2238 (2.0164)	Acc@1 66.016 (70.366)	Acc@5 89.062 (93.491)
Epoch: [76][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0865 (2.0209)	Acc@1 71.094 (70.251)	Acc@5 90.625 (93.429)
[INFO] Storing checkpoint...

Epoch: [77 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [77][0/196]	Time 0.030 (0.030)	Data 0.186 (0.186)	Loss 1.9158 (1.9158)	Acc@1 73.828 (73.828)	Acc@5 95.312 (95.312)
Epoch: [77][10/196]	Time 0.017 (0.017)	Data 0.000 (0.019)	Loss 2.0406 (1.9809)	Acc@1 70.703 (71.165)	Acc@5 92.188 (93.999)
Epoch: [77][20/196]	Time 0.014 (0.016)	Data 0.003 (0.011)	Loss 1.9120 (1.9812)	Acc@1 73.438 (71.038)	Acc@5 94.922 (93.843)
Epoch: [77][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9642 (1.9752)	Acc@1 72.656 (71.648)	Acc@5 92.969 (93.687)
Epoch: [77][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 2.0244 (1.9696)	Acc@1 67.188 (71.837)	Acc@5 94.141 (93.883)
Epoch: [77][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.9314 (1.9547)	Acc@1 74.219 (72.220)	Acc@5 93.359 (94.079)
Epoch: [77][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9059 (1.9476)	Acc@1 72.266 (72.394)	Acc@5 94.531 (94.166)
Epoch: [77][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.9971 (1.9466)	Acc@1 71.484 (72.497)	Acc@5 94.531 (94.207)
Epoch: [77][80/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.0389 (1.9482)	Acc@1 69.922 (72.386)	Acc@5 94.141 (94.247)
Epoch: [77][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9441 (1.9527)	Acc@1 73.438 (72.313)	Acc@5 95.703 (94.218)
Epoch: [77][100/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0504 (1.9568)	Acc@1 70.703 (72.204)	Acc@5 92.969 (94.179)
Epoch: [77][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1133 (1.9628)	Acc@1 66.797 (72.023)	Acc@5 89.062 (94.169)
Epoch: [77][120/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.1123 (1.9682)	Acc@1 67.969 (71.843)	Acc@5 90.625 (94.066)
Epoch: [77][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1882 (1.9729)	Acc@1 66.016 (71.660)	Acc@5 93.359 (94.042)
Epoch: [77][140/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.2239 (1.9794)	Acc@1 66.797 (71.437)	Acc@5 90.234 (93.936)
Epoch: [77][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0398 (1.9857)	Acc@1 71.094 (71.316)	Acc@5 93.359 (93.892)
Epoch: [77][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0485 (1.9912)	Acc@1 71.484 (71.140)	Acc@5 91.797 (93.818)
Epoch: [77][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2638 (1.9971)	Acc@1 65.234 (70.954)	Acc@5 90.625 (93.764)
Epoch: [77][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0761 (2.0026)	Acc@1 65.234 (70.805)	Acc@5 93.750 (93.687)
Epoch: [77][190/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 2.2226 (2.0075)	Acc@1 66.016 (70.703)	Acc@5 90.234 (93.597)
[INFO] Storing checkpoint...

Epoch: [78 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [78][0/196]	Time 0.030 (0.030)	Data 0.191 (0.191)	Loss 1.9723 (1.9723)	Acc@1 73.047 (73.047)	Acc@5 93.359 (93.359)
Epoch: [78][10/196]	Time 0.015 (0.017)	Data 0.002 (0.019)	Loss 1.9571 (2.0197)	Acc@1 72.656 (71.058)	Acc@5 95.312 (93.892)
Epoch: [78][20/196]	Time 0.014 (0.016)	Data 0.003 (0.011)	Loss 2.0257 (1.9697)	Acc@1 70.703 (72.098)	Acc@5 94.531 (94.420)
Epoch: [78][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 2.0105 (1.9573)	Acc@1 67.578 (72.089)	Acc@5 95.312 (94.745)
Epoch: [78][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 1.8959 (1.9492)	Acc@1 73.438 (72.399)	Acc@5 93.359 (94.598)
Epoch: [78][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.0087 (1.9452)	Acc@1 73.047 (72.495)	Acc@5 94.141 (94.531)
Epoch: [78][60/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 1.8975 (1.9422)	Acc@1 73.828 (72.592)	Acc@5 95.703 (94.570)
Epoch: [78][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9929 (1.9443)	Acc@1 74.219 (72.502)	Acc@5 91.797 (94.526)
Epoch: [78][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.1080 (1.9520)	Acc@1 68.359 (72.309)	Acc@5 93.359 (94.420)
Epoch: [78][90/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1027 (1.9621)	Acc@1 67.188 (72.042)	Acc@5 92.969 (94.360)
Epoch: [78][100/196]	Time 0.012 (0.014)	Data 0.006 (0.005)	Loss 2.1719 (1.9636)	Acc@1 66.016 (71.987)	Acc@5 93.750 (94.384)
Epoch: [78][110/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 2.1173 (1.9701)	Acc@1 67.578 (71.840)	Acc@5 94.141 (94.274)
Epoch: [78][120/196]	Time 0.012 (0.014)	Data 0.006 (0.005)	Loss 1.9667 (1.9757)	Acc@1 67.969 (71.649)	Acc@5 94.531 (94.199)
Epoch: [78][130/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 2.1089 (1.9851)	Acc@1 66.797 (71.362)	Acc@5 92.578 (94.084)
Epoch: [78][140/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 2.1282 (1.9926)	Acc@1 67.188 (71.185)	Acc@5 91.016 (93.963)
Epoch: [78][150/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 2.0111 (1.9969)	Acc@1 72.266 (71.014)	Acc@5 94.141 (93.926)
Epoch: [78][160/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 2.0459 (2.0001)	Acc@1 68.359 (70.897)	Acc@5 93.750 (93.888)
Epoch: [78][170/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 2.1082 (2.0037)	Acc@1 72.656 (70.783)	Acc@5 92.188 (93.853)
Epoch: [78][180/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 2.0037 (2.0069)	Acc@1 70.312 (70.694)	Acc@5 94.531 (93.813)
Epoch: [78][190/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 1.9943 (2.0090)	Acc@1 73.047 (70.636)	Acc@5 94.531 (93.783)
[INFO] Storing checkpoint...

Epoch: [79 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [79][0/196]	Time 0.030 (0.030)	Data 0.181 (0.181)	Loss 1.9462 (1.9462)	Acc@1 72.656 (72.656)	Acc@5 93.359 (93.359)
Epoch: [79][10/196]	Time 0.014 (0.016)	Data 0.004 (0.019)	Loss 1.9509 (1.9905)	Acc@1 72.266 (71.875)	Acc@5 94.531 (93.821)
Epoch: [79][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 1.8359 (1.9450)	Acc@1 75.391 (72.377)	Acc@5 96.484 (94.438)
Epoch: [79][30/196]	Time 0.012 (0.015)	Data 0.005 (0.008)	Loss 2.0044 (1.9444)	Acc@1 71.094 (72.404)	Acc@5 92.969 (94.468)
Epoch: [79][40/196]	Time 0.012 (0.015)	Data 0.008 (0.007)	Loss 1.9594 (1.9432)	Acc@1 71.094 (72.266)	Acc@5 94.922 (94.579)
Epoch: [79][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 1.9186 (1.9399)	Acc@1 76.172 (72.319)	Acc@5 93.750 (94.608)
Epoch: [79][60/196]	Time 0.016 (0.015)	Data 0.004 (0.006)	Loss 1.8821 (1.9468)	Acc@1 73.438 (72.061)	Acc@5 93.359 (94.525)
Epoch: [79][70/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.9982 (1.9539)	Acc@1 69.141 (71.732)	Acc@5 95.703 (94.476)
Epoch: [79][80/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9375 (1.9654)	Acc@1 73.438 (71.600)	Acc@5 93.750 (94.232)
Epoch: [79][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0920 (1.9712)	Acc@1 67.969 (71.557)	Acc@5 90.234 (94.123)
Epoch: [79][100/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.1029 (1.9776)	Acc@1 67.188 (71.349)	Acc@5 94.141 (94.090)
Epoch: [79][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1634 (1.9856)	Acc@1 66.797 (71.136)	Acc@5 91.797 (93.982)
Epoch: [79][120/196]	Time 0.012 (0.015)	Data 0.023 (0.005)	Loss 2.1206 (1.9925)	Acc@1 64.844 (71.032)	Acc@5 93.359 (93.931)
Epoch: [79][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0957 (1.9974)	Acc@1 66.406 (70.885)	Acc@5 92.969 (93.842)
Epoch: [79][140/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.0262 (2.0012)	Acc@1 72.656 (70.808)	Acc@5 93.359 (93.797)
Epoch: [79][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2176 (2.0070)	Acc@1 64.062 (70.672)	Acc@5 92.969 (93.742)
Epoch: [79][160/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 2.0791 (2.0102)	Acc@1 70.703 (70.592)	Acc@5 94.531 (93.692)
Epoch: [79][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0129 (2.0142)	Acc@1 70.703 (70.527)	Acc@5 94.922 (93.663)
Epoch: [79][180/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 2.0203 (2.0191)	Acc@1 71.094 (70.354)	Acc@5 93.359 (93.608)
Epoch: [79][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2777 (2.0229)	Acc@1 64.062 (70.268)	Acc@5 92.188 (93.605)
[INFO] Storing checkpoint...

Epoch: [80 | 80] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [127, 49, 3, 3]
module.conv3.weight [256, 127, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [487, 512, 3, 3]
module.conv8.weight [328, 487, 3, 3]
Epoch: [80][0/196]	Time 0.029 (0.029)	Data 0.193 (0.193)	Loss 1.8523 (1.8523)	Acc@1 74.219 (74.219)	Acc@5 96.484 (96.484)
Epoch: [80][10/196]	Time 0.017 (0.017)	Data 0.000 (0.020)	Loss 1.9411 (1.9083)	Acc@1 71.875 (73.757)	Acc@5 96.094 (95.490)
Epoch: [80][20/196]	Time 0.014 (0.016)	Data 0.003 (0.011)	Loss 1.8877 (1.8954)	Acc@1 75.000 (74.126)	Acc@5 95.703 (95.275)
Epoch: [80][30/196]	Time 0.017 (0.015)	Data 0.000 (0.009)	Loss 1.9082 (1.9033)	Acc@1 74.219 (73.702)	Acc@5 96.094 (95.123)
Epoch: [80][40/196]	Time 0.015 (0.015)	Data 0.003 (0.007)	Loss 1.8843 (1.9064)	Acc@1 73.047 (73.295)	Acc@5 94.531 (95.103)
Epoch: [80][50/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 1.8527 (1.9037)	Acc@1 77.344 (73.384)	Acc@5 95.312 (95.121)
Epoch: [80][60/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 1.8960 (1.9083)	Acc@1 72.266 (73.181)	Acc@5 94.141 (95.095)
Epoch: [80][70/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 2.0423 (1.9152)	Acc@1 69.531 (72.964)	Acc@5 93.359 (94.999)
Epoch: [80][80/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 1.9487 (1.9256)	Acc@1 72.266 (72.758)	Acc@5 94.531 (94.864)
Epoch: [80][90/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0792 (1.9379)	Acc@1 67.188 (72.377)	Acc@5 93.750 (94.660)
Epoch: [80][100/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 1.9146 (1.9444)	Acc@1 73.438 (72.273)	Acc@5 94.141 (94.516)
Epoch: [80][110/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 2.0351 (1.9594)	Acc@1 68.750 (71.900)	Acc@5 93.359 (94.362)
Epoch: [80][120/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.0357 (1.9715)	Acc@1 71.094 (71.568)	Acc@5 93.750 (94.257)
Epoch: [80][130/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 2.0737 (1.9776)	Acc@1 66.797 (71.344)	Acc@5 93.359 (94.209)
Epoch: [80][140/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.3150 (1.9862)	Acc@1 66.797 (71.152)	Acc@5 87.109 (94.080)
Epoch: [80][150/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 2.0612 (1.9906)	Acc@1 68.750 (71.034)	Acc@5 92.578 (94.022)
Epoch: [80][160/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 2.1344 (1.9953)	Acc@1 67.188 (71.002)	Acc@5 92.578 (93.978)
Epoch: [80][170/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 2.0207 (2.0020)	Acc@1 68.750 (70.744)	Acc@5 94.531 (93.926)
Epoch: [80][180/196]	Time 0.013 (0.014)	Data 0.004 (0.004)	Loss 1.8928 (2.0068)	Acc@1 72.656 (70.638)	Acc@5 94.141 (93.862)
Epoch: [80][190/196]	Time 0.012 (0.014)	Data 0.008 (0.004)	Loss 2.0680 (2.0128)	Acc@1 67.969 (70.503)	Acc@5 92.188 (93.775)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [49, 3, 3, 3] >> [46, 3, 3, 3]
[module.conv2.weight]: [127, 49, 3, 3] >> [127, 46, 3, 3]
[module.conv3.weight]: [256, 127, 3, 3] >> [255, 127, 3, 3]
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 255, 3, 3]
[module.conv5.weight]: [511, 256, 3, 3] >> [510, 256, 3, 3]
[module.conv6.weight]: [512, 511, 3, 3] >> [512, 510, 3, 3]
[module.conv7.weight]: [487, 512, 3, 3] >> [483, 512, 3, 3]
[module.conv8.weight]: [328, 487, 3, 3] >> [314, 483, 3, 3]
[module.fc.weight]: [100, 328] >> [100, 314]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
57.53
python src/cifar.py --workers 4 --dataset cifar100 --epochs 90 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_90.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 8.09M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [46, 3, 3, 3]
conv2 --> [127, 46, 3, 3]
conv3 --> [255, 127, 3, 3]
conv4 --> [256, 255, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [512, 510, 3, 3]
conv7 --> [483, 512, 3, 3]
conv8 --> [314, 483, 3, 3]
fc --> [314, 100]
1, 509359104, 1271808, 46
2, 5626266624, 13459968, 127
3, 8506114560, 18653760, 255
4, 17146183680, 37601280, 256
5, 10227548160, 18800640, 510
6, 20455096320, 37601280, 512
7, 6837239808, 8902656, 483
8, 4193150976, 5459832, 314
fc, 12057600, 31400, 0
===================
FLOP REPORT: 28716022200000.0 52744000000.0 141782624 131860 2503 15.416648864746094

Epoch: [81 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [81][0/196]	Time 0.696 (0.696)	Data 0.164 (0.164)	Loss 1.8806 (1.8806)	Acc@1 76.172 (76.172)	Acc@5 94.922 (94.922)
Epoch: [81][10/196]	Time 0.015 (0.077)	Data 0.003 (0.017)	Loss 2.0200 (1.9402)	Acc@1 71.484 (72.727)	Acc@5 93.750 (94.744)
Epoch: [81][20/196]	Time 0.015 (0.047)	Data 0.003 (0.010)	Loss 1.8372 (1.9139)	Acc@1 74.609 (72.935)	Acc@5 95.703 (95.219)
Epoch: [81][30/196]	Time 0.014 (0.037)	Data 0.003 (0.008)	Loss 1.8702 (1.9019)	Acc@1 73.828 (73.551)	Acc@5 94.531 (95.363)
Epoch: [81][40/196]	Time 0.013 (0.031)	Data 0.005 (0.006)	Loss 1.9667 (1.9200)	Acc@1 70.312 (73.161)	Acc@5 94.531 (94.970)
Epoch: [81][50/196]	Time 0.014 (0.028)	Data 0.002 (0.006)	Loss 1.9888 (1.9318)	Acc@1 71.484 (72.756)	Acc@5 95.312 (94.784)
Epoch: [81][60/196]	Time 0.012 (0.026)	Data 0.004 (0.005)	Loss 1.8953 (1.9374)	Acc@1 74.609 (72.624)	Acc@5 95.312 (94.627)
Epoch: [81][70/196]	Time 0.014 (0.024)	Data 0.003 (0.005)	Loss 2.0118 (1.9349)	Acc@1 66.797 (72.788)	Acc@5 94.922 (94.674)
Epoch: [81][80/196]	Time 0.012 (0.023)	Data 0.005 (0.004)	Loss 2.0210 (1.9336)	Acc@1 71.094 (72.859)	Acc@5 92.188 (94.686)
Epoch: [81][90/196]	Time 0.015 (0.022)	Data 0.002 (0.004)	Loss 1.9570 (1.9377)	Acc@1 71.094 (72.824)	Acc@5 93.750 (94.596)
Epoch: [81][100/196]	Time 0.016 (0.021)	Data 0.004 (0.004)	Loss 2.1823 (1.9481)	Acc@1 65.625 (72.548)	Acc@5 92.969 (94.450)
Epoch: [81][110/196]	Time 0.014 (0.021)	Data 0.002 (0.004)	Loss 2.0658 (1.9558)	Acc@1 69.141 (72.357)	Acc@5 92.969 (94.398)
Epoch: [81][120/196]	Time 0.011 (0.020)	Data 0.006 (0.004)	Loss 2.0853 (1.9633)	Acc@1 69.531 (72.104)	Acc@5 92.969 (94.347)
Epoch: [81][130/196]	Time 0.014 (0.020)	Data 0.002 (0.004)	Loss 2.0658 (1.9704)	Acc@1 67.188 (71.893)	Acc@5 93.750 (94.260)
Epoch: [81][140/196]	Time 0.011 (0.019)	Data 0.009 (0.004)	Loss 1.9545 (1.9777)	Acc@1 71.484 (71.692)	Acc@5 94.141 (94.174)
Epoch: [81][150/196]	Time 0.015 (0.019)	Data 0.003 (0.004)	Loss 2.0334 (1.9820)	Acc@1 72.266 (71.609)	Acc@5 91.016 (94.102)
Epoch: [81][160/196]	Time 0.012 (0.019)	Data 0.019 (0.004)	Loss 2.0454 (1.9870)	Acc@1 71.094 (71.518)	Acc@5 93.750 (94.014)
Epoch: [81][170/196]	Time 0.016 (0.018)	Data 0.001 (0.004)	Loss 2.0389 (1.9924)	Acc@1 69.922 (71.388)	Acc@5 93.750 (93.960)
Epoch: [81][180/196]	Time 0.012 (0.018)	Data 0.005 (0.004)	Loss 1.9809 (1.9934)	Acc@1 68.359 (71.323)	Acc@5 93.359 (93.912)
Epoch: [81][190/196]	Time 0.016 (0.018)	Data 0.000 (0.004)	Loss 2.1712 (1.9975)	Acc@1 67.578 (71.192)	Acc@5 91.406 (93.846)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [82 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [82][0/196]	Time 0.034 (0.034)	Data 0.176 (0.176)	Loss 1.8854 (1.8854)	Acc@1 72.656 (72.656)	Acc@5 96.094 (96.094)
Epoch: [82][10/196]	Time 0.018 (0.017)	Data 0.000 (0.018)	Loss 1.8715 (1.9777)	Acc@1 72.266 (70.881)	Acc@5 93.359 (94.531)
Epoch: [82][20/196]	Time 0.015 (0.016)	Data 0.002 (0.011)	Loss 1.8009 (1.9561)	Acc@1 76.562 (72.042)	Acc@5 95.312 (94.699)
Epoch: [82][30/196]	Time 0.014 (0.015)	Data 0.002 (0.008)	Loss 2.0114 (1.9461)	Acc@1 67.578 (72.240)	Acc@5 93.750 (94.871)
Epoch: [82][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 1.8950 (1.9451)	Acc@1 76.172 (72.228)	Acc@5 92.969 (94.912)
Epoch: [82][50/196]	Time 0.016 (0.015)	Data 0.002 (0.006)	Loss 1.9433 (1.9464)	Acc@1 74.609 (72.342)	Acc@5 94.922 (94.815)
Epoch: [82][60/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 1.9591 (1.9495)	Acc@1 73.828 (72.246)	Acc@5 94.141 (94.871)
Epoch: [82][70/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0132 (1.9521)	Acc@1 69.922 (72.233)	Acc@5 91.797 (94.740)
Epoch: [82][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.0794 (1.9632)	Acc@1 72.266 (72.102)	Acc@5 91.797 (94.584)
Epoch: [82][90/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 2.1603 (1.9693)	Acc@1 67.188 (71.914)	Acc@5 90.234 (94.415)
Epoch: [82][100/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 2.0563 (1.9716)	Acc@1 66.797 (71.863)	Acc@5 94.141 (94.392)
Epoch: [82][110/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 2.0275 (1.9769)	Acc@1 69.141 (71.643)	Acc@5 94.922 (94.366)
Epoch: [82][120/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 1.9949 (1.9801)	Acc@1 70.703 (71.546)	Acc@5 92.578 (94.237)
Epoch: [82][130/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 2.1670 (1.9845)	Acc@1 66.797 (71.341)	Acc@5 91.797 (94.179)
Epoch: [82][140/196]	Time 0.014 (0.014)	Data 0.002 (0.004)	Loss 2.1172 (1.9918)	Acc@1 64.844 (71.155)	Acc@5 93.359 (94.035)
Epoch: [82][150/196]	Time 0.015 (0.014)	Data 0.003 (0.004)	Loss 1.9995 (1.9923)	Acc@1 70.703 (71.179)	Acc@5 92.578 (94.011)
Epoch: [82][160/196]	Time 0.014 (0.014)	Data 0.003 (0.004)	Loss 1.9680 (1.9985)	Acc@1 73.438 (71.014)	Acc@5 96.094 (93.966)
Epoch: [82][170/196]	Time 0.014 (0.014)	Data 0.002 (0.004)	Loss 2.0510 (2.0054)	Acc@1 67.188 (70.838)	Acc@5 92.969 (93.887)
Epoch: [82][180/196]	Time 0.014 (0.014)	Data 0.002 (0.004)	Loss 2.0495 (2.0085)	Acc@1 69.922 (70.764)	Acc@5 92.188 (93.849)
Epoch: [82][190/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 2.0736 (2.0117)	Acc@1 68.750 (70.691)	Acc@5 92.969 (93.779)
[INFO] Storing checkpoint...

Epoch: [83 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [83][0/196]	Time 0.027 (0.027)	Data 0.161 (0.161)	Loss 1.8072 (1.8072)	Acc@1 76.172 (76.172)	Acc@5 96.875 (96.875)
Epoch: [83][10/196]	Time 0.014 (0.016)	Data 0.001 (0.017)	Loss 1.9671 (1.9833)	Acc@1 71.094 (71.911)	Acc@5 93.359 (94.105)
Epoch: [83][20/196]	Time 0.013 (0.015)	Data 0.004 (0.010)	Loss 2.0500 (1.9674)	Acc@1 71.875 (72.321)	Acc@5 92.969 (94.234)
Epoch: [83][30/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 1.9988 (1.9639)	Acc@1 72.266 (72.329)	Acc@5 94.531 (94.405)
Epoch: [83][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 1.7575 (1.9526)	Acc@1 76.172 (72.513)	Acc@5 98.828 (94.569)
Epoch: [83][50/196]	Time 0.011 (0.014)	Data 0.007 (0.006)	Loss 1.9278 (1.9436)	Acc@1 71.484 (72.541)	Acc@5 95.312 (94.623)
Epoch: [83][60/196]	Time 0.012 (0.014)	Data 0.009 (0.006)	Loss 1.9913 (1.9441)	Acc@1 73.438 (72.599)	Acc@5 92.969 (94.621)
Epoch: [83][70/196]	Time 0.014 (0.014)	Data 0.003 (0.006)	Loss 1.9098 (1.9407)	Acc@1 71.875 (72.689)	Acc@5 96.875 (94.652)
Epoch: [83][80/196]	Time 0.012 (0.014)	Data 0.008 (0.005)	Loss 1.9249 (1.9389)	Acc@1 71.484 (72.671)	Acc@5 96.484 (94.686)
Epoch: [83][90/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 1.9968 (1.9482)	Acc@1 72.656 (72.304)	Acc@5 95.312 (94.613)
Epoch: [83][100/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 2.1179 (1.9492)	Acc@1 67.969 (72.312)	Acc@5 93.359 (94.589)
Epoch: [83][110/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 2.1785 (1.9574)	Acc@1 68.750 (72.111)	Acc@5 91.016 (94.496)
Epoch: [83][120/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 2.1794 (1.9656)	Acc@1 66.797 (71.904)	Acc@5 91.406 (94.389)
Epoch: [83][130/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 2.0906 (1.9753)	Acc@1 68.359 (71.636)	Acc@5 92.969 (94.245)
Epoch: [83][140/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 1.9897 (1.9801)	Acc@1 74.219 (71.487)	Acc@5 90.234 (94.174)
Epoch: [83][150/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 2.1592 (1.9872)	Acc@1 68.359 (71.293)	Acc@5 94.141 (94.099)
Epoch: [83][160/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 2.0445 (1.9913)	Acc@1 72.656 (71.179)	Acc@5 90.625 (94.024)
Epoch: [83][170/196]	Time 0.012 (0.014)	Data 0.005 (0.004)	Loss 1.9477 (1.9984)	Acc@1 69.141 (70.959)	Acc@5 95.312 (93.930)
Epoch: [83][180/196]	Time 0.011 (0.014)	Data 0.009 (0.004)	Loss 2.0888 (2.0042)	Acc@1 70.703 (70.772)	Acc@5 92.578 (93.875)
Epoch: [83][190/196]	Time 0.012 (0.014)	Data 0.004 (0.004)	Loss 2.0754 (2.0084)	Acc@1 68.359 (70.658)	Acc@5 94.531 (93.826)
[INFO] Storing checkpoint...

Epoch: [84 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [84][0/196]	Time 0.028 (0.028)	Data 0.172 (0.172)	Loss 2.0320 (2.0320)	Acc@1 70.703 (70.703)	Acc@5 92.188 (92.188)
Epoch: [84][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 2.0221 (1.9459)	Acc@1 70.312 (72.940)	Acc@5 95.703 (94.496)
Epoch: [84][20/196]	Time 0.011 (0.015)	Data 0.006 (0.010)	Loss 1.9477 (1.9416)	Acc@1 68.359 (72.749)	Acc@5 94.531 (94.587)
Epoch: [84][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.9990 (1.9341)	Acc@1 69.922 (72.946)	Acc@5 95.703 (94.468)
Epoch: [84][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 1.8557 (1.9186)	Acc@1 75.781 (73.276)	Acc@5 95.312 (94.665)
Epoch: [84][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.9376 (1.9167)	Acc@1 70.312 (73.200)	Acc@5 94.922 (94.830)
Epoch: [84][60/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0339 (1.9184)	Acc@1 68.750 (73.085)	Acc@5 93.359 (94.858)
Epoch: [84][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.8747 (1.9235)	Acc@1 75.781 (72.970)	Acc@5 94.141 (94.735)
Epoch: [84][80/196]	Time 0.012 (0.015)	Data 0.017 (0.005)	Loss 2.1119 (1.9275)	Acc@1 66.797 (72.806)	Acc@5 93.359 (94.676)
Epoch: [84][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0254 (1.9351)	Acc@1 68.359 (72.626)	Acc@5 94.141 (94.626)
Epoch: [84][100/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 1.9949 (1.9400)	Acc@1 70.703 (72.502)	Acc@5 92.969 (94.493)
Epoch: [84][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.2629 (1.9491)	Acc@1 64.844 (72.245)	Acc@5 91.797 (94.369)
Epoch: [84][120/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.0224 (1.9600)	Acc@1 66.406 (71.936)	Acc@5 95.312 (94.286)
Epoch: [84][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0967 (1.9682)	Acc@1 69.531 (71.744)	Acc@5 93.359 (94.185)
Epoch: [84][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.0868 (1.9784)	Acc@1 67.578 (71.401)	Acc@5 91.406 (94.038)
Epoch: [84][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.8925 (1.9815)	Acc@1 73.438 (71.285)	Acc@5 96.484 (94.004)
Epoch: [84][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0799 (1.9872)	Acc@1 68.750 (71.145)	Acc@5 92.578 (93.937)
Epoch: [84][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1391 (1.9933)	Acc@1 69.141 (71.016)	Acc@5 92.578 (93.832)
Epoch: [84][180/196]	Time 0.013 (0.015)	Data 0.009 (0.004)	Loss 2.0709 (2.0005)	Acc@1 70.312 (70.854)	Acc@5 92.969 (93.759)
Epoch: [84][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9253 (2.0032)	Acc@1 74.609 (70.758)	Acc@5 94.922 (93.734)
[INFO] Storing checkpoint...

Epoch: [85 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [85][0/196]	Time 0.027 (0.027)	Data 0.173 (0.173)	Loss 2.0940 (2.0940)	Acc@1 68.750 (68.750)	Acc@5 94.141 (94.141)
Epoch: [85][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 2.0310 (2.0181)	Acc@1 67.969 (70.668)	Acc@5 94.531 (94.247)
Epoch: [85][20/196]	Time 0.011 (0.015)	Data 0.007 (0.011)	Loss 1.8584 (1.9838)	Acc@1 72.266 (71.187)	Acc@5 96.094 (94.847)
Epoch: [85][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 2.0980 (1.9718)	Acc@1 69.922 (71.535)	Acc@5 91.406 (94.708)
Epoch: [85][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 1.9626 (1.9537)	Acc@1 71.094 (72.085)	Acc@5 92.578 (94.655)
Epoch: [85][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 1.9004 (1.9452)	Acc@1 75.000 (72.250)	Acc@5 95.703 (94.815)
Epoch: [85][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 1.9366 (1.9436)	Acc@1 72.656 (72.387)	Acc@5 94.141 (94.736)
Epoch: [85][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0021 (1.9438)	Acc@1 72.266 (72.469)	Acc@5 92.578 (94.713)
Epoch: [85][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0600 (1.9585)	Acc@1 69.922 (72.131)	Acc@5 93.750 (94.507)
Epoch: [85][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9372 (1.9610)	Acc@1 73.438 (72.094)	Acc@5 93.359 (94.437)
Epoch: [85][100/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0884 (1.9572)	Acc@1 70.312 (72.173)	Acc@5 92.188 (94.496)
Epoch: [85][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0790 (1.9613)	Acc@1 66.016 (71.956)	Acc@5 94.531 (94.461)
Epoch: [85][120/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1101 (1.9685)	Acc@1 69.922 (71.807)	Acc@5 89.844 (94.318)
Epoch: [85][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9717 (1.9705)	Acc@1 73.047 (71.762)	Acc@5 94.141 (94.308)
Epoch: [85][140/196]	Time 0.012 (0.015)	Data 0.012 (0.004)	Loss 1.9994 (1.9740)	Acc@1 69.531 (71.651)	Acc@5 96.094 (94.301)
Epoch: [85][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1740 (1.9833)	Acc@1 65.234 (71.420)	Acc@5 90.625 (94.174)
Epoch: [85][160/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 1.8942 (1.9907)	Acc@1 75.391 (71.283)	Acc@5 93.750 (94.070)
Epoch: [85][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0897 (1.9995)	Acc@1 67.188 (71.034)	Acc@5 94.531 (93.960)
Epoch: [85][180/196]	Time 0.016 (0.015)	Data 0.008 (0.004)	Loss 2.0427 (2.0056)	Acc@1 71.484 (70.828)	Acc@5 94.141 (93.882)
Epoch: [85][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1035 (2.0114)	Acc@1 66.016 (70.609)	Acc@5 92.969 (93.822)
[INFO] Storing checkpoint...

Epoch: [86 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [86][0/196]	Time 0.027 (0.027)	Data 0.168 (0.168)	Loss 2.0179 (2.0179)	Acc@1 70.703 (70.703)	Acc@5 94.141 (94.141)
Epoch: [86][10/196]	Time 0.014 (0.016)	Data 0.002 (0.018)	Loss 2.0241 (1.9552)	Acc@1 68.359 (71.697)	Acc@5 94.141 (94.780)
Epoch: [86][20/196]	Time 0.011 (0.015)	Data 0.011 (0.011)	Loss 2.1414 (1.9451)	Acc@1 68.359 (72.359)	Acc@5 93.750 (94.736)
Epoch: [86][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.8325 (1.9258)	Acc@1 75.781 (72.807)	Acc@5 94.922 (94.897)
Epoch: [86][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 1.9649 (1.9356)	Acc@1 71.484 (72.675)	Acc@5 95.703 (94.788)
Epoch: [86][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.9414 (1.9349)	Acc@1 71.484 (72.672)	Acc@5 95.703 (94.761)
Epoch: [86][60/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 1.8832 (1.9337)	Acc@1 72.656 (72.669)	Acc@5 99.219 (94.819)
Epoch: [86][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.9764 (1.9495)	Acc@1 71.875 (72.260)	Acc@5 95.312 (94.762)
Epoch: [86][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 1.9683 (1.9572)	Acc@1 70.312 (72.068)	Acc@5 95.703 (94.623)
Epoch: [86][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1296 (1.9681)	Acc@1 71.875 (71.849)	Acc@5 92.188 (94.488)
Epoch: [86][100/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0747 (1.9772)	Acc@1 69.922 (71.585)	Acc@5 89.844 (94.419)
Epoch: [86][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1559 (1.9847)	Acc@1 69.141 (71.463)	Acc@5 92.188 (94.317)
Epoch: [86][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0056 (1.9877)	Acc@1 73.828 (71.436)	Acc@5 93.359 (94.244)
Epoch: [86][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2783 (1.9936)	Acc@1 62.500 (71.192)	Acc@5 92.188 (94.191)
Epoch: [86][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1799 (1.9968)	Acc@1 64.062 (71.049)	Acc@5 92.578 (94.168)
Epoch: [86][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1163 (2.0008)	Acc@1 68.750 (70.990)	Acc@5 95.312 (94.110)
Epoch: [86][160/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.0220 (2.0015)	Acc@1 72.656 (70.941)	Acc@5 93.750 (94.102)
Epoch: [86][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0688 (2.0058)	Acc@1 67.969 (70.810)	Acc@5 93.359 (94.038)
Epoch: [86][180/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 2.2171 (2.0110)	Acc@1 63.672 (70.679)	Acc@5 92.188 (93.968)
Epoch: [86][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0237 (2.0144)	Acc@1 68.750 (70.585)	Acc@5 94.141 (93.928)
[INFO] Storing checkpoint...

Epoch: [87 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [87][0/196]	Time 0.027 (0.027)	Data 0.170 (0.170)	Loss 1.8831 (1.8831)	Acc@1 75.000 (75.000)	Acc@5 95.703 (95.703)
Epoch: [87][10/196]	Time 0.016 (0.016)	Data 0.001 (0.018)	Loss 1.8387 (1.9749)	Acc@1 75.391 (72.266)	Acc@5 96.094 (94.425)
Epoch: [87][20/196]	Time 0.011 (0.015)	Data 0.006 (0.010)	Loss 1.9705 (1.9559)	Acc@1 68.359 (72.247)	Acc@5 97.266 (94.885)
Epoch: [87][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 1.9692 (1.9538)	Acc@1 70.312 (72.190)	Acc@5 94.922 (94.947)
Epoch: [87][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 1.9675 (1.9488)	Acc@1 74.219 (72.189)	Acc@5 96.094 (95.027)
Epoch: [87][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.8928 (1.9409)	Acc@1 71.875 (72.358)	Acc@5 96.484 (94.998)
Epoch: [87][60/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 2.0926 (1.9367)	Acc@1 67.578 (72.592)	Acc@5 92.969 (94.967)
Epoch: [87][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0342 (1.9377)	Acc@1 69.141 (72.541)	Acc@5 93.359 (94.922)
Epoch: [87][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 1.9532 (1.9392)	Acc@1 69.922 (72.352)	Acc@5 95.703 (94.985)
Epoch: [87][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.8412 (1.9410)	Acc@1 73.047 (72.330)	Acc@5 97.266 (94.918)
Epoch: [87][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0779 (1.9453)	Acc@1 66.406 (72.192)	Acc@5 95.312 (94.872)
Epoch: [87][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1060 (1.9491)	Acc@1 67.578 (72.051)	Acc@5 91.406 (94.781)
Epoch: [87][120/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 2.0684 (1.9514)	Acc@1 70.703 (72.046)	Acc@5 90.625 (94.722)
Epoch: [87][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0457 (1.9530)	Acc@1 70.703 (72.048)	Acc@5 94.141 (94.668)
Epoch: [87][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0832 (1.9578)	Acc@1 66.406 (71.955)	Acc@5 92.188 (94.578)
Epoch: [87][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1824 (1.9664)	Acc@1 68.750 (71.748)	Acc@5 93.359 (94.461)
Epoch: [87][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0014 (1.9735)	Acc@1 74.219 (71.652)	Acc@5 93.359 (94.332)
Epoch: [87][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1836 (1.9797)	Acc@1 64.453 (71.455)	Acc@5 94.141 (94.278)
Epoch: [87][180/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 2.0224 (1.9851)	Acc@1 69.141 (71.338)	Acc@5 92.969 (94.190)
Epoch: [87][190/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 2.1769 (1.9929)	Acc@1 67.969 (71.151)	Acc@5 91.406 (94.124)
[INFO] Storing checkpoint...

Epoch: [88 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [88][0/196]	Time 0.028 (0.028)	Data 0.182 (0.182)	Loss 1.9920 (1.9920)	Acc@1 69.531 (69.531)	Acc@5 94.141 (94.141)
Epoch: [88][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 1.9577 (2.0053)	Acc@1 74.219 (70.845)	Acc@5 94.922 (94.425)
Epoch: [88][20/196]	Time 0.011 (0.015)	Data 0.008 (0.011)	Loss 1.9924 (1.9989)	Acc@1 72.656 (71.819)	Acc@5 94.531 (94.382)
Epoch: [88][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0021 (1.9738)	Acc@1 69.531 (72.467)	Acc@5 93.359 (94.443)
Epoch: [88][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 1.9944 (1.9608)	Acc@1 71.484 (72.704)	Acc@5 95.312 (94.627)
Epoch: [88][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.8993 (1.9505)	Acc@1 71.875 (73.039)	Acc@5 95.312 (94.677)
Epoch: [88][60/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0547 (1.9445)	Acc@1 70.703 (73.060)	Acc@5 95.312 (94.743)
Epoch: [88][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0436 (1.9516)	Acc@1 69.922 (72.706)	Acc@5 92.578 (94.669)
Epoch: [88][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.9306 (1.9536)	Acc@1 67.578 (72.584)	Acc@5 96.484 (94.700)
Epoch: [88][90/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9334 (1.9586)	Acc@1 71.875 (72.497)	Acc@5 95.703 (94.660)
Epoch: [88][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1334 (1.9673)	Acc@1 69.141 (72.223)	Acc@5 94.531 (94.543)
Epoch: [88][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0704 (1.9737)	Acc@1 69.141 (72.002)	Acc@5 93.750 (94.415)
Epoch: [88][120/196]	Time 0.011 (0.015)	Data 0.005 (0.004)	Loss 1.9357 (1.9829)	Acc@1 71.875 (71.723)	Acc@5 94.922 (94.318)
Epoch: [88][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1124 (1.9872)	Acc@1 71.484 (71.589)	Acc@5 93.359 (94.269)
Epoch: [88][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 1.9100 (1.9912)	Acc@1 73.438 (71.562)	Acc@5 95.703 (94.188)
Epoch: [88][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0625 (1.9960)	Acc@1 67.969 (71.381)	Acc@5 94.531 (94.099)
Epoch: [88][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 1.9397 (2.0011)	Acc@1 72.266 (71.254)	Acc@5 92.969 (94.005)
Epoch: [88][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.8915 (2.0007)	Acc@1 76.172 (71.242)	Acc@5 94.141 (93.974)
Epoch: [88][180/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.0991 (2.0045)	Acc@1 69.141 (71.113)	Acc@5 91.797 (93.905)
Epoch: [88][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9659 (2.0082)	Acc@1 72.266 (71.018)	Acc@5 92.578 (93.828)
[INFO] Storing checkpoint...

Epoch: [89 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [89][0/196]	Time 0.027 (0.027)	Data 0.181 (0.181)	Loss 1.9212 (1.9212)	Acc@1 73.438 (73.438)	Acc@5 94.922 (94.922)
Epoch: [89][10/196]	Time 0.016 (0.016)	Data 0.001 (0.018)	Loss 1.8565 (1.9086)	Acc@1 75.000 (74.041)	Acc@5 95.703 (95.419)
Epoch: [89][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 1.8308 (1.8966)	Acc@1 77.344 (74.237)	Acc@5 96.484 (95.145)
Epoch: [89][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.6429 (1.8894)	Acc@1 82.422 (74.408)	Acc@5 96.094 (95.212)
Epoch: [89][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 1.8252 (1.8804)	Acc@1 75.391 (74.486)	Acc@5 95.312 (95.274)
Epoch: [89][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 1.9088 (1.8887)	Acc@1 71.484 (74.280)	Acc@5 94.922 (95.213)
Epoch: [89][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 1.9947 (1.8911)	Acc@1 68.750 (74.110)	Acc@5 94.531 (95.133)
Epoch: [89][70/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 1.9642 (1.9009)	Acc@1 69.922 (73.746)	Acc@5 94.922 (95.043)
Epoch: [89][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 1.8875 (1.9152)	Acc@1 77.344 (73.307)	Acc@5 94.922 (94.907)
Epoch: [89][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0780 (1.9252)	Acc@1 68.750 (73.068)	Acc@5 93.359 (94.840)
Epoch: [89][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1942 (1.9339)	Acc@1 65.625 (72.857)	Acc@5 91.406 (94.748)
Epoch: [89][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0175 (1.9416)	Acc@1 72.266 (72.656)	Acc@5 94.141 (94.672)
Epoch: [89][120/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 1.9285 (1.9486)	Acc@1 74.219 (72.505)	Acc@5 94.141 (94.586)
Epoch: [89][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1551 (1.9543)	Acc@1 70.703 (72.448)	Acc@5 88.672 (94.472)
Epoch: [89][140/196]	Time 0.011 (0.015)	Data 0.013 (0.004)	Loss 2.2197 (1.9612)	Acc@1 60.547 (72.191)	Acc@5 90.234 (94.398)
Epoch: [89][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1673 (1.9660)	Acc@1 66.016 (72.090)	Acc@5 94.141 (94.327)
Epoch: [89][160/196]	Time 0.011 (0.015)	Data 0.014 (0.004)	Loss 2.0991 (1.9701)	Acc@1 69.141 (71.960)	Acc@5 92.188 (94.262)
Epoch: [89][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2332 (1.9772)	Acc@1 61.328 (71.724)	Acc@5 90.234 (94.195)
Epoch: [89][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.0580 (1.9842)	Acc@1 69.141 (71.549)	Acc@5 94.922 (94.136)
Epoch: [89][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0498 (1.9897)	Acc@1 68.750 (71.425)	Acc@5 93.359 (94.053)
[INFO] Storing checkpoint...

Epoch: [90 | 90] LR: 0.100000
module.conv1.weight [46, 3, 3, 3]
module.conv2.weight [127, 46, 3, 3]
module.conv3.weight [255, 127, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [512, 510, 3, 3]
module.conv7.weight [483, 512, 3, 3]
module.conv8.weight [314, 483, 3, 3]
Epoch: [90][0/196]	Time 0.028 (0.028)	Data 0.179 (0.179)	Loss 1.8708 (1.8708)	Acc@1 78.125 (78.125)	Acc@5 96.094 (96.094)
Epoch: [90][10/196]	Time 0.016 (0.017)	Data 0.000 (0.019)	Loss 1.9405 (1.9237)	Acc@1 73.047 (73.224)	Acc@5 95.703 (95.028)
Epoch: [90][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 1.8458 (1.9145)	Acc@1 74.609 (73.549)	Acc@5 93.750 (95.201)
Epoch: [90][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 1.8973 (1.9091)	Acc@1 71.875 (73.627)	Acc@5 95.703 (95.350)
Epoch: [90][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 1.9681 (1.9170)	Acc@1 75.000 (73.323)	Acc@5 93.359 (95.046)
Epoch: [90][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 2.0915 (1.9232)	Acc@1 69.922 (73.430)	Acc@5 94.141 (94.884)
Epoch: [90][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 1.9235 (1.9255)	Acc@1 72.266 (73.169)	Acc@5 96.875 (94.871)
Epoch: [90][70/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0707 (1.9353)	Acc@1 70.703 (72.827)	Acc@5 93.750 (94.834)
Epoch: [90][80/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.0130 (1.9444)	Acc@1 70.703 (72.468)	Acc@5 92.578 (94.777)
Epoch: [90][90/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.9943 (1.9537)	Acc@1 67.188 (72.184)	Acc@5 94.141 (94.707)
Epoch: [90][100/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.1875 (1.9657)	Acc@1 66.016 (71.863)	Acc@5 92.188 (94.512)
Epoch: [90][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0385 (1.9684)	Acc@1 68.359 (71.717)	Acc@5 94.531 (94.559)
Epoch: [90][120/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 1.9408 (1.9758)	Acc@1 70.703 (71.523)	Acc@5 94.531 (94.451)
Epoch: [90][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1570 (1.9802)	Acc@1 66.797 (71.443)	Acc@5 91.797 (94.373)
Epoch: [90][140/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 2.0139 (1.9860)	Acc@1 70.703 (71.293)	Acc@5 94.531 (94.323)
Epoch: [90][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0796 (1.9864)	Acc@1 68.750 (71.298)	Acc@5 91.797 (94.298)
Epoch: [90][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1510 (1.9902)	Acc@1 67.188 (71.247)	Acc@5 93.359 (94.279)
Epoch: [90][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1737 (1.9948)	Acc@1 68.359 (71.110)	Acc@5 90.625 (94.259)
Epoch: [90][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1230 (1.9975)	Acc@1 66.797 (71.053)	Acc@5 92.969 (94.212)
Epoch: [90][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1217 (2.0034)	Acc@1 68.750 (70.912)	Acc@5 92.578 (94.126)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [46, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 46, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [255, 127, 3, 3] >> [254, 127, 3, 3]
[module.conv4.weight]: [256, 255, 3, 3] >> [256, 254, 3, 3]
[module.conv5.weight]: [510, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [512, 510, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [483, 512, 3, 3] >> [478, 511, 3, 3]
[module.conv8.weight]: [314, 483, 3, 3] >> [303, 478, 3, 3]
[module.fc.weight]: [100, 314] >> [100, 303]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
57.53
python src/cifar.py --workers 4 --dataset cifar100 --epochs 100 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_100.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.98M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [254, 127, 3, 3]
conv4 --> [256, 254, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [478, 511, 3, 3]
conv8 --> [303, 478, 3, 3]
fc --> [303, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8472757248, 18580608, 254
4, 17078943744, 37453824, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6753245184, 8793288, 478
8, 4004370432, 5214024, 303
fc, 11635200, 30300, 0
===================
FLOP REPORT: 28478829600000.0 52270400000.0 140701596 130676 2483 15.21440315246582

Epoch: [91 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [91][0/196]	Time 0.760 (0.760)	Data 0.164 (0.164)	Loss 1.9592 (1.9592)	Acc@1 71.875 (71.875)	Acc@5 94.531 (94.531)
Epoch: [91][10/196]	Time 0.015 (0.082)	Data 0.002 (0.017)	Loss 1.7623 (1.8991)	Acc@1 80.078 (74.254)	Acc@5 95.703 (94.957)
Epoch: [91][20/196]	Time 0.014 (0.050)	Data 0.003 (0.010)	Loss 1.6688 (1.8472)	Acc@1 79.297 (75.614)	Acc@5 98.047 (95.350)
Epoch: [91][30/196]	Time 0.015 (0.038)	Data 0.002 (0.008)	Loss 1.5036 (1.7791)	Acc@1 88.672 (77.747)	Acc@5 98.828 (96.081)
Epoch: [91][40/196]	Time 0.014 (0.033)	Data 0.002 (0.006)	Loss 1.5487 (1.7478)	Acc@1 84.766 (78.611)	Acc@5 96.484 (96.437)
Epoch: [91][50/196]	Time 0.012 (0.029)	Data 0.010 (0.006)	Loss 1.6144 (1.7167)	Acc@1 83.203 (79.488)	Acc@5 97.656 (96.714)
Epoch: [91][60/196]	Time 0.014 (0.027)	Data 0.003 (0.005)	Loss 1.5083 (1.6951)	Acc@1 87.109 (80.161)	Acc@5 99.219 (96.862)
Epoch: [91][70/196]	Time 0.011 (0.025)	Data 0.010 (0.005)	Loss 1.5762 (1.6763)	Acc@1 82.812 (80.876)	Acc@5 97.656 (96.985)
Epoch: [91][80/196]	Time 0.017 (0.024)	Data 0.000 (0.005)	Loss 1.4445 (1.6608)	Acc@1 85.938 (81.298)	Acc@5 99.219 (97.063)
Epoch: [91][90/196]	Time 0.011 (0.023)	Data 0.009 (0.005)	Loss 1.4677 (1.6458)	Acc@1 85.156 (81.718)	Acc@5 99.219 (97.197)
Epoch: [91][100/196]	Time 0.017 (0.022)	Data 0.001 (0.005)	Loss 1.6131 (1.6375)	Acc@1 83.594 (81.923)	Acc@5 97.266 (97.262)
Epoch: [91][110/196]	Time 0.011 (0.021)	Data 0.008 (0.005)	Loss 1.5091 (1.6262)	Acc@1 83.203 (82.299)	Acc@5 97.656 (97.368)
Epoch: [91][120/196]	Time 0.016 (0.021)	Data 0.000 (0.004)	Loss 1.4986 (1.6165)	Acc@1 85.156 (82.603)	Acc@5 98.438 (97.437)
Epoch: [91][130/196]	Time 0.011 (0.020)	Data 0.009 (0.004)	Loss 1.4974 (1.6065)	Acc@1 85.938 (82.893)	Acc@5 98.047 (97.516)
Epoch: [91][140/196]	Time 0.016 (0.020)	Data 0.001 (0.004)	Loss 1.5824 (1.5979)	Acc@1 80.078 (83.131)	Acc@5 98.438 (97.593)
Epoch: [91][150/196]	Time 0.011 (0.020)	Data 0.009 (0.004)	Loss 1.4353 (1.5889)	Acc@1 89.062 (83.392)	Acc@5 98.828 (97.659)
Epoch: [91][160/196]	Time 0.016 (0.019)	Data 0.001 (0.004)	Loss 1.5368 (1.5822)	Acc@1 83.594 (83.589)	Acc@5 98.438 (97.707)
Epoch: [91][170/196]	Time 0.011 (0.019)	Data 0.008 (0.004)	Loss 1.3804 (1.5765)	Acc@1 88.672 (83.699)	Acc@5 99.219 (97.761)
Epoch: [91][180/196]	Time 0.016 (0.019)	Data 0.001 (0.004)	Loss 1.4685 (1.5693)	Acc@1 88.281 (83.926)	Acc@5 99.219 (97.803)
Epoch: [91][190/196]	Time 0.011 (0.019)	Data 0.007 (0.004)	Loss 1.3887 (1.5640)	Acc@1 88.672 (84.046)	Acc@5 99.219 (97.820)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [92 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [92][0/196]	Time 0.027 (0.027)	Data 0.162 (0.162)	Loss 1.3731 (1.3731)	Acc@1 91.016 (91.016)	Acc@5 98.828 (98.828)
Epoch: [92][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 1.3521 (1.3826)	Acc@1 90.625 (89.915)	Acc@5 99.219 (99.006)
Epoch: [92][20/196]	Time 0.012 (0.015)	Data 0.006 (0.010)	Loss 1.4261 (1.3818)	Acc@1 89.062 (89.862)	Acc@5 99.609 (98.977)
Epoch: [92][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.4159 (1.3889)	Acc@1 87.500 (89.478)	Acc@5 98.047 (98.954)
Epoch: [92][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 1.4351 (1.3914)	Acc@1 86.328 (89.367)	Acc@5 99.219 (98.933)
Epoch: [92][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 1.4511 (1.3937)	Acc@1 85.938 (89.208)	Acc@5 98.047 (98.920)
Epoch: [92][60/196]	Time 0.011 (0.015)	Data 0.017 (0.006)	Loss 1.3207 (1.3884)	Acc@1 91.016 (89.370)	Acc@5 98.828 (98.918)
Epoch: [92][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.3925 (1.3877)	Acc@1 89.844 (89.338)	Acc@5 98.828 (98.960)
Epoch: [92][80/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 1.3990 (1.3861)	Acc@1 87.500 (89.366)	Acc@5 98.438 (98.978)
Epoch: [92][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.2968 (1.3834)	Acc@1 92.578 (89.384)	Acc@5 100.000 (99.030)
Epoch: [92][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.3341 (1.3818)	Acc@1 89.062 (89.333)	Acc@5 98.438 (98.998)
Epoch: [92][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.3676 (1.3802)	Acc@1 89.062 (89.351)	Acc@5 99.219 (98.983)
Epoch: [92][120/196]	Time 0.011 (0.015)	Data 0.016 (0.004)	Loss 1.3375 (1.3769)	Acc@1 91.797 (89.414)	Acc@5 98.828 (98.999)
Epoch: [92][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.3018 (1.3770)	Acc@1 89.844 (89.358)	Acc@5 100.000 (98.989)
Epoch: [92][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 1.3533 (1.3780)	Acc@1 91.016 (89.340)	Acc@5 100.000 (98.978)
Epoch: [92][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.3160 (1.3784)	Acc@1 91.406 (89.316)	Acc@5 99.219 (98.955)
Epoch: [92][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 1.3207 (1.3755)	Acc@1 90.625 (89.409)	Acc@5 99.609 (98.991)
Epoch: [92][170/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 1.3413 (1.3742)	Acc@1 87.891 (89.401)	Acc@5 100.000 (98.997)
Epoch: [92][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.3405 (1.3735)	Acc@1 87.891 (89.425)	Acc@5 99.219 (98.992)
Epoch: [92][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.3487 (1.3726)	Acc@1 90.625 (89.416)	Acc@5 98.828 (98.988)
[INFO] Storing checkpoint...

Epoch: [93 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [93][0/196]	Time 0.027 (0.027)	Data 0.180 (0.180)	Loss 1.3458 (1.3458)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [93][10/196]	Time 0.021 (0.017)	Data 0.001 (0.018)	Loss 1.2985 (1.3123)	Acc@1 91.406 (91.051)	Acc@5 99.219 (99.503)
Epoch: [93][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 1.2959 (1.2945)	Acc@1 90.625 (91.704)	Acc@5 99.609 (99.442)
Epoch: [93][30/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 1.2861 (1.2943)	Acc@1 93.359 (91.746)	Acc@5 99.609 (99.509)
Epoch: [93][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 1.2695 (1.2917)	Acc@1 92.578 (91.806)	Acc@5 98.438 (99.495)
Epoch: [93][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 1.3238 (1.2923)	Acc@1 91.797 (91.766)	Acc@5 98.047 (99.456)
Epoch: [93][60/196]	Time 0.011 (0.015)	Data 0.011 (0.006)	Loss 1.2688 (1.2962)	Acc@1 93.359 (91.656)	Acc@5 99.609 (99.417)
Epoch: [93][70/196]	Time 0.012 (0.015)	Data 0.004 (0.006)	Loss 1.3015 (1.2946)	Acc@1 89.844 (91.698)	Acc@5 99.219 (99.428)
Epoch: [93][80/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 1.3274 (1.2946)	Acc@1 90.234 (91.633)	Acc@5 99.609 (99.436)
Epoch: [93][90/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.3371 (1.2964)	Acc@1 90.625 (91.544)	Acc@5 99.219 (99.412)
Epoch: [93][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.3031 (1.2970)	Acc@1 91.016 (91.464)	Acc@5 100.000 (99.412)
Epoch: [93][110/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.2976 (1.2972)	Acc@1 91.406 (91.385)	Acc@5 99.609 (99.419)
Epoch: [93][120/196]	Time 0.011 (0.015)	Data 0.014 (0.005)	Loss 1.2936 (1.2962)	Acc@1 91.016 (91.419)	Acc@5 98.828 (99.406)
Epoch: [93][130/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.2675 (1.2937)	Acc@1 92.188 (91.475)	Acc@5 99.219 (99.413)
Epoch: [93][140/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 1.3271 (1.2946)	Acc@1 91.797 (91.401)	Acc@5 98.438 (99.382)
Epoch: [93][150/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.3507 (1.2937)	Acc@1 91.016 (91.432)	Acc@5 98.047 (99.366)
Epoch: [93][160/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.2895 (1.2924)	Acc@1 90.234 (91.440)	Acc@5 98.828 (99.372)
Epoch: [93][170/196]	Time 0.017 (0.015)	Data 0.003 (0.004)	Loss 1.2650 (1.2917)	Acc@1 90.234 (91.457)	Acc@5 99.609 (99.379)
Epoch: [93][180/196]	Time 0.011 (0.015)	Data 0.017 (0.005)	Loss 1.2501 (1.2924)	Acc@1 92.969 (91.395)	Acc@5 99.609 (99.363)
Epoch: [93][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.2367 (1.2917)	Acc@1 93.750 (91.365)	Acc@5 99.609 (99.374)
[INFO] Storing checkpoint...

Epoch: [94 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [94][0/196]	Time 0.028 (0.028)	Data 0.170 (0.170)	Loss 1.2367 (1.2367)	Acc@1 92.578 (92.578)	Acc@5 99.609 (99.609)
Epoch: [94][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 1.2132 (1.2445)	Acc@1 93.359 (92.507)	Acc@5 99.219 (99.467)
Epoch: [94][20/196]	Time 0.011 (0.015)	Data 0.008 (0.010)	Loss 1.1648 (1.2378)	Acc@1 96.484 (93.173)	Acc@5 100.000 (99.479)
Epoch: [94][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.2620 (1.2330)	Acc@1 91.797 (93.208)	Acc@5 99.609 (99.521)
Epoch: [94][40/196]	Time 0.011 (0.015)	Data 0.011 (0.007)	Loss 1.2315 (1.2310)	Acc@1 92.578 (93.264)	Acc@5 98.828 (99.486)
Epoch: [94][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.2338 (1.2329)	Acc@1 93.359 (93.168)	Acc@5 100.000 (99.464)
Epoch: [94][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 1.1844 (1.2320)	Acc@1 93.750 (93.116)	Acc@5 100.000 (99.462)
Epoch: [94][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.2159 (1.2298)	Acc@1 95.312 (93.183)	Acc@5 99.219 (99.488)
Epoch: [94][80/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 1.2765 (1.2298)	Acc@1 90.625 (93.118)	Acc@5 100.000 (99.508)
Epoch: [94][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.2512 (1.2323)	Acc@1 92.188 (93.003)	Acc@5 99.609 (99.506)
Epoch: [94][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.2674 (1.2316)	Acc@1 91.406 (92.984)	Acc@5 98.047 (99.493)
Epoch: [94][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.2203 (1.2297)	Acc@1 94.141 (93.053)	Acc@5 99.609 (99.521)
Epoch: [94][120/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 1.3170 (1.2325)	Acc@1 89.453 (92.943)	Acc@5 100.000 (99.496)
Epoch: [94][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.2472 (1.2343)	Acc@1 91.797 (92.870)	Acc@5 99.219 (99.484)
Epoch: [94][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 1.2110 (1.2338)	Acc@1 92.188 (92.863)	Acc@5 100.000 (99.482)
Epoch: [94][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.2560 (1.2331)	Acc@1 91.016 (92.845)	Acc@5 100.000 (99.493)
Epoch: [94][160/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 1.2729 (1.2332)	Acc@1 91.406 (92.796)	Acc@5 99.219 (99.493)
Epoch: [94][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.2171 (1.2334)	Acc@1 94.141 (92.747)	Acc@5 100.000 (99.491)
Epoch: [94][180/196]	Time 0.011 (0.015)	Data 0.013 (0.004)	Loss 1.2362 (1.2329)	Acc@1 91.797 (92.764)	Acc@5 100.000 (99.495)
Epoch: [94][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.2131 (1.2324)	Acc@1 90.625 (92.748)	Acc@5 100.000 (99.497)
[INFO] Storing checkpoint...

Epoch: [95 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [95][0/196]	Time 0.026 (0.026)	Data 0.170 (0.170)	Loss 1.1629 (1.1629)	Acc@1 95.312 (95.312)	Acc@5 99.219 (99.219)
Epoch: [95][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 1.2473 (1.1887)	Acc@1 91.406 (94.141)	Acc@5 100.000 (99.574)
Epoch: [95][20/196]	Time 0.012 (0.015)	Data 0.006 (0.010)	Loss 1.1602 (1.1912)	Acc@1 94.922 (93.992)	Acc@5 99.609 (99.554)
Epoch: [95][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.1945 (1.1958)	Acc@1 92.578 (93.876)	Acc@5 100.000 (99.584)
Epoch: [95][40/196]	Time 0.012 (0.015)	Data 0.010 (0.007)	Loss 1.1921 (1.1947)	Acc@1 94.141 (93.874)	Acc@5 100.000 (99.590)
Epoch: [95][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 1.1338 (1.1898)	Acc@1 95.312 (93.987)	Acc@5 100.000 (99.602)
Epoch: [95][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 1.1690 (1.1883)	Acc@1 93.750 (93.968)	Acc@5 99.609 (99.609)
Epoch: [95][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.1482 (1.1862)	Acc@1 94.531 (93.998)	Acc@5 99.609 (99.615)
Epoch: [95][80/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 1.1490 (1.1856)	Acc@1 95.312 (94.015)	Acc@5 100.000 (99.619)
Epoch: [95][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.1783 (1.1864)	Acc@1 94.922 (93.990)	Acc@5 99.609 (99.614)
Epoch: [95][100/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 1.2249 (1.1870)	Acc@1 92.188 (93.878)	Acc@5 99.609 (99.633)
Epoch: [95][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.2015 (1.1868)	Acc@1 93.359 (93.870)	Acc@5 100.000 (99.627)
Epoch: [95][120/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.1755 (1.1862)	Acc@1 93.750 (93.876)	Acc@5 99.219 (99.626)
Epoch: [95][130/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.1988 (1.1856)	Acc@1 93.750 (93.890)	Acc@5 100.000 (99.639)
Epoch: [95][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.1495 (1.1848)	Acc@1 94.141 (93.911)	Acc@5 100.000 (99.645)
Epoch: [95][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.1820 (1.1835)	Acc@1 92.969 (93.918)	Acc@5 99.609 (99.659)
Epoch: [95][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.2038 (1.1821)	Acc@1 92.969 (93.942)	Acc@5 99.609 (99.665)
Epoch: [95][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.1809 (1.1820)	Acc@1 94.922 (93.933)	Acc@5 99.609 (99.671)
Epoch: [95][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.1606 (1.1806)	Acc@1 94.531 (93.959)	Acc@5 100.000 (99.668)
Epoch: [95][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.1777 (1.1800)	Acc@1 94.531 (93.965)	Acc@5 98.828 (99.661)
[INFO] Storing checkpoint...

Epoch: [96 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [96][0/196]	Time 0.026 (0.026)	Data 0.171 (0.171)	Loss 1.1994 (1.1994)	Acc@1 93.750 (93.750)	Acc@5 98.438 (98.438)
Epoch: [96][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 1.1397 (1.1538)	Acc@1 94.922 (94.176)	Acc@5 99.609 (99.574)
Epoch: [96][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 1.1720 (1.1469)	Acc@1 92.578 (94.587)	Acc@5 100.000 (99.684)
Epoch: [96][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.1015 (1.1429)	Acc@1 95.312 (94.531)	Acc@5 100.000 (99.685)
Epoch: [96][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 1.1076 (1.1437)	Acc@1 96.094 (94.484)	Acc@5 99.609 (99.695)
Epoch: [96][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 1.1483 (1.1445)	Acc@1 94.531 (94.455)	Acc@5 99.609 (99.724)
Epoch: [96][60/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 1.1409 (1.1443)	Acc@1 96.484 (94.582)	Acc@5 99.609 (99.731)
Epoch: [96][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.1531 (1.1445)	Acc@1 92.969 (94.586)	Acc@5 100.000 (99.730)
Epoch: [96][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 1.2181 (1.1440)	Acc@1 92.578 (94.633)	Acc@5 98.828 (99.735)
Epoch: [96][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.1734 (1.1433)	Acc@1 93.750 (94.609)	Acc@5 99.609 (99.738)
Epoch: [96][100/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 1.1281 (1.1417)	Acc@1 93.750 (94.663)	Acc@5 100.000 (99.745)
Epoch: [96][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.1007 (1.1418)	Acc@1 95.703 (94.661)	Acc@5 100.000 (99.747)
Epoch: [96][120/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.1491 (1.1419)	Acc@1 95.312 (94.654)	Acc@5 99.609 (99.739)
Epoch: [96][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.1302 (1.1428)	Acc@1 94.922 (94.627)	Acc@5 99.609 (99.741)
Epoch: [96][140/196]	Time 0.019 (0.015)	Data 0.007 (0.004)	Loss 1.1748 (1.1427)	Acc@1 93.359 (94.617)	Acc@5 100.000 (99.756)
Epoch: [96][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.1297 (1.1415)	Acc@1 94.922 (94.645)	Acc@5 99.609 (99.757)
Epoch: [96][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.1769 (1.1415)	Acc@1 92.969 (94.631)	Acc@5 99.609 (99.745)
Epoch: [96][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.1409 (1.1405)	Acc@1 94.141 (94.664)	Acc@5 100.000 (99.751)
Epoch: [96][180/196]	Time 0.011 (0.015)	Data 0.020 (0.004)	Loss 1.1221 (1.1400)	Acc@1 94.922 (94.672)	Acc@5 99.609 (99.747)
Epoch: [96][190/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.0972 (1.1385)	Acc@1 96.484 (94.726)	Acc@5 100.000 (99.753)
[INFO] Storing checkpoint...

Epoch: [97 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [97][0/196]	Time 0.027 (0.027)	Data 0.185 (0.185)	Loss 1.0974 (1.0974)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [97][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 1.1082 (1.0955)	Acc@1 94.922 (96.413)	Acc@5 100.000 (99.787)
Epoch: [97][20/196]	Time 0.013 (0.015)	Data 0.004 (0.011)	Loss 1.1002 (1.0952)	Acc@1 96.484 (96.187)	Acc@5 100.000 (99.833)
Epoch: [97][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.1095 (1.0963)	Acc@1 94.531 (95.980)	Acc@5 100.000 (99.836)
Epoch: [97][40/196]	Time 0.011 (0.014)	Data 0.008 (0.007)	Loss 1.1662 (1.0964)	Acc@1 92.969 (96.037)	Acc@5 99.609 (99.857)
Epoch: [97][50/196]	Time 0.014 (0.015)	Data 0.001 (0.006)	Loss 1.1225 (1.0978)	Acc@1 96.484 (96.048)	Acc@5 99.609 (99.847)
Epoch: [97][60/196]	Time 0.011 (0.015)	Data 0.017 (0.006)	Loss 1.1177 (1.0998)	Acc@1 95.312 (95.940)	Acc@5 100.000 (99.846)
Epoch: [97][70/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.1079 (1.1016)	Acc@1 94.922 (95.841)	Acc@5 99.609 (99.829)
Epoch: [97][80/196]	Time 0.011 (0.015)	Data 0.011 (0.006)	Loss 1.1187 (1.1013)	Acc@1 94.922 (95.872)	Acc@5 99.609 (99.836)
Epoch: [97][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.1142 (1.1024)	Acc@1 95.703 (95.763)	Acc@5 99.609 (99.837)
Epoch: [97][100/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 1.1217 (1.1027)	Acc@1 94.922 (95.726)	Acc@5 100.000 (99.838)
Epoch: [97][110/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 1.1065 (1.1024)	Acc@1 93.359 (95.714)	Acc@5 99.219 (99.824)
Epoch: [97][120/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 1.1180 (1.1023)	Acc@1 96.094 (95.693)	Acc@5 99.609 (99.832)
Epoch: [97][130/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 1.0759 (1.1009)	Acc@1 96.875 (95.718)	Acc@5 100.000 (99.839)
Epoch: [97][140/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 1.0717 (1.0998)	Acc@1 96.094 (95.734)	Acc@5 100.000 (99.834)
Epoch: [97][150/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 1.0682 (1.1003)	Acc@1 97.266 (95.703)	Acc@5 99.609 (99.827)
Epoch: [97][160/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 1.0752 (1.1001)	Acc@1 95.703 (95.684)	Acc@5 100.000 (99.828)
Epoch: [97][170/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 1.1208 (1.0998)	Acc@1 94.922 (95.676)	Acc@5 99.609 (99.829)
Epoch: [97][180/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 1.1071 (1.0986)	Acc@1 94.141 (95.705)	Acc@5 100.000 (99.834)
Epoch: [97][190/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 1.0912 (1.0982)	Acc@1 96.094 (95.717)	Acc@5 100.000 (99.843)
[INFO] Storing checkpoint...

Epoch: [98 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [98][0/196]	Time 0.029 (0.029)	Data 0.179 (0.179)	Loss 1.1243 (1.1243)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [98][10/196]	Time 0.016 (0.017)	Data 0.001 (0.018)	Loss 1.0796 (1.0806)	Acc@1 96.094 (95.774)	Acc@5 100.000 (99.964)
Epoch: [98][20/196]	Time 0.020 (0.016)	Data 0.008 (0.011)	Loss 1.0444 (1.0712)	Acc@1 97.266 (96.391)	Acc@5 100.000 (99.926)
Epoch: [98][30/196]	Time 0.016 (0.016)	Data 0.001 (0.008)	Loss 1.0647 (1.0702)	Acc@1 96.484 (96.321)	Acc@5 100.000 (99.912)
Epoch: [98][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 0.9980 (1.0644)	Acc@1 99.219 (96.637)	Acc@5 100.000 (99.914)
Epoch: [98][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.0831 (1.0639)	Acc@1 94.922 (96.607)	Acc@5 100.000 (99.931)
Epoch: [98][60/196]	Time 0.012 (0.015)	Data 0.028 (0.006)	Loss 1.0679 (1.0642)	Acc@1 96.094 (96.561)	Acc@5 100.000 (99.942)
Epoch: [98][70/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.0876 (1.0655)	Acc@1 95.312 (96.523)	Acc@5 100.000 (99.928)
Epoch: [98][80/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.0581 (1.0647)	Acc@1 96.484 (96.562)	Acc@5 100.000 (99.932)
Epoch: [98][90/196]	Time 0.021 (0.016)	Data 0.001 (0.005)	Loss 1.0537 (1.0657)	Acc@1 96.875 (96.523)	Acc@5 100.000 (99.931)
Epoch: [98][100/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.0812 (1.0662)	Acc@1 96.094 (96.484)	Acc@5 100.000 (99.930)
Epoch: [98][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0739 (1.0651)	Acc@1 96.094 (96.502)	Acc@5 99.609 (99.923)
Epoch: [98][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.0508 (1.0661)	Acc@1 95.312 (96.413)	Acc@5 100.000 (99.919)
Epoch: [98][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.0413 (1.0661)	Acc@1 97.656 (96.398)	Acc@5 100.000 (99.911)
Epoch: [98][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 1.0640 (1.0665)	Acc@1 96.484 (96.351)	Acc@5 100.000 (99.909)
Epoch: [98][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.0342 (1.0659)	Acc@1 97.656 (96.337)	Acc@5 100.000 (99.912)
Epoch: [98][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.0492 (1.0663)	Acc@1 97.266 (96.307)	Acc@5 100.000 (99.901)
Epoch: [98][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.0721 (1.0666)	Acc@1 94.922 (96.265)	Acc@5 99.609 (99.893)
Epoch: [98][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 1.0848 (1.0668)	Acc@1 95.703 (96.271)	Acc@5 99.609 (99.890)
Epoch: [98][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.0681 (1.0665)	Acc@1 95.312 (96.266)	Acc@5 99.609 (99.890)
[INFO] Storing checkpoint...

Epoch: [99 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [99][0/196]	Time 0.027 (0.027)	Data 0.184 (0.184)	Loss 1.0348 (1.0348)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [99][10/196]	Time 0.016 (0.016)	Data 0.001 (0.019)	Loss 1.0713 (1.0433)	Acc@1 96.875 (96.946)	Acc@5 100.000 (99.964)
Epoch: [99][20/196]	Time 0.013 (0.015)	Data 0.003 (0.011)	Loss 1.0428 (1.0413)	Acc@1 96.875 (96.968)	Acc@5 100.000 (99.944)
Epoch: [99][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 1.0463 (1.0382)	Acc@1 97.656 (97.064)	Acc@5 99.609 (99.950)
Epoch: [99][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 1.0326 (1.0354)	Acc@1 96.875 (97.113)	Acc@5 100.000 (99.962)
Epoch: [99][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 1.0120 (1.0353)	Acc@1 99.219 (97.120)	Acc@5 100.000 (99.962)
Epoch: [99][60/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.0368 (1.0352)	Acc@1 97.266 (97.106)	Acc@5 100.000 (99.949)
Epoch: [99][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.0135 (1.0340)	Acc@1 98.047 (97.134)	Acc@5 99.609 (99.950)
Epoch: [99][80/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 1.0234 (1.0347)	Acc@1 95.703 (97.058)	Acc@5 100.000 (99.952)
Epoch: [99][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0943 (1.0366)	Acc@1 95.703 (96.991)	Acc@5 98.828 (99.940)
Epoch: [99][100/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.0285 (1.0365)	Acc@1 96.875 (96.976)	Acc@5 100.000 (99.938)
Epoch: [99][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.0469 (1.0372)	Acc@1 98.047 (96.945)	Acc@5 100.000 (99.937)
Epoch: [99][120/196]	Time 0.013 (0.015)	Data 0.003 (0.004)	Loss 0.9974 (1.0363)	Acc@1 97.266 (96.943)	Acc@5 100.000 (99.939)
Epoch: [99][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.0320 (1.0366)	Acc@1 96.875 (96.902)	Acc@5 100.000 (99.937)
Epoch: [99][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.0179 (1.0354)	Acc@1 98.438 (96.944)	Acc@5 100.000 (99.934)
Epoch: [99][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.0567 (1.0354)	Acc@1 95.312 (96.927)	Acc@5 100.000 (99.935)
Epoch: [99][160/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 1.0195 (1.0354)	Acc@1 97.656 (96.909)	Acc@5 100.000 (99.939)
Epoch: [99][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.0177 (1.0348)	Acc@1 96.484 (96.916)	Acc@5 100.000 (99.941)
Epoch: [99][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.0254 (1.0349)	Acc@1 97.656 (96.877)	Acc@5 100.000 (99.935)
Epoch: [99][190/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.0083 (1.0348)	Acc@1 97.656 (96.848)	Acc@5 100.000 (99.935)
[INFO] Storing checkpoint...

Epoch: [100 | 100] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [254, 127, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [100][0/196]	Time 0.028 (0.028)	Data 0.178 (0.178)	Loss 1.0291 (1.0291)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [100][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 1.0142 (1.0144)	Acc@1 97.266 (97.124)	Acc@5 100.000 (99.929)
Epoch: [100][20/196]	Time 0.014 (0.015)	Data 0.002 (0.011)	Loss 0.9976 (1.0103)	Acc@1 97.656 (97.359)	Acc@5 99.609 (99.907)
Epoch: [100][30/196]	Time 0.012 (0.015)	Data 0.005 (0.008)	Loss 1.0008 (1.0099)	Acc@1 97.266 (97.341)	Acc@5 100.000 (99.899)
Epoch: [100][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 1.0468 (1.0069)	Acc@1 96.094 (97.466)	Acc@5 99.609 (99.914)
Epoch: [100][50/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 1.0172 (1.0086)	Acc@1 96.875 (97.365)	Acc@5 100.000 (99.908)
Epoch: [100][60/196]	Time 0.011 (0.014)	Data 0.010 (0.006)	Loss 0.9885 (1.0061)	Acc@1 97.656 (97.464)	Acc@5 100.000 (99.917)
Epoch: [100][70/196]	Time 0.012 (0.014)	Data 0.005 (0.006)	Loss 0.9822 (1.0051)	Acc@1 98.047 (97.464)	Acc@5 100.000 (99.923)
Epoch: [100][80/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.9675 (1.0036)	Acc@1 98.828 (97.497)	Acc@5 100.000 (99.932)
Epoch: [100][90/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 1.0214 (1.0038)	Acc@1 96.875 (97.467)	Acc@5 99.609 (99.931)
Epoch: [100][100/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 1.0075 (1.0043)	Acc@1 96.875 (97.428)	Acc@5 100.000 (99.938)
Epoch: [100][110/196]	Time 0.012 (0.014)	Data 0.015 (0.005)	Loss 1.0118 (1.0056)	Acc@1 97.266 (97.354)	Acc@5 99.609 (99.937)
Epoch: [100][120/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 0.9998 (1.0040)	Acc@1 96.875 (97.385)	Acc@5 100.000 (99.942)
Epoch: [100][130/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 1.0472 (1.0045)	Acc@1 94.922 (97.352)	Acc@5 100.000 (99.943)
Epoch: [100][140/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 1.0079 (1.0043)	Acc@1 98.047 (97.352)	Acc@5 100.000 (99.947)
Epoch: [100][150/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 0.9908 (1.0040)	Acc@1 98.047 (97.348)	Acc@5 100.000 (99.946)
Epoch: [100][160/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 1.0255 (1.0054)	Acc@1 96.094 (97.285)	Acc@5 100.000 (99.939)
Epoch: [100][170/196]	Time 0.012 (0.014)	Data 0.011 (0.004)	Loss 1.0487 (1.0055)	Acc@1 95.703 (97.300)	Acc@5 100.000 (99.941)
Epoch: [100][180/196]	Time 0.015 (0.014)	Data 0.002 (0.004)	Loss 0.9811 (1.0054)	Acc@1 98.438 (97.281)	Acc@5 100.000 (99.942)
Epoch: [100][190/196]	Time 0.011 (0.014)	Data 0.006 (0.004)	Loss 1.0000 (1.0053)	Acc@1 96.484 (97.262)	Acc@5 100.000 (99.945)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [254, 127, 3, 3] >> [253, 127, 3, 3]
[module.conv4.weight]: [256, 254, 3, 3] >> [256, 253, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [478, 511, 3, 3] >> [478, 511, 3, 3]
[module.conv8.weight]: [303, 478, 3, 3] >> [303, 478, 3, 3]
[module.fc.weight]: [100, 303] >> [100, 303]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
69.62
python src/cifar.py --workers 4 --dataset cifar100 --epochs 110 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_110.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.98M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [253, 127, 3, 3]
conv4 --> [256, 253, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [478, 511, 3, 3]
conv8 --> [303, 478, 3, 3]
fc --> [303, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8439399936, 18507456, 253
4, 17011703808, 37306368, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6753245184, 8793288, 478
8, 4004370432, 5214024, 303
fc, 11635200, 30300, 0
===================
FLOP REPORT: 28439533800000.0 52244800000.0 140480988 130612 2482 15.207826614379883

Epoch: [101 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [101][0/196]	Time 0.773 (0.773)	Data 0.157 (0.157)	Loss 0.9951 (0.9951)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [101][10/196]	Time 0.016 (0.084)	Data 0.002 (0.016)	Loss 0.9983 (0.9876)	Acc@1 96.875 (97.763)	Acc@5 100.000 (99.964)
Epoch: [101][20/196]	Time 0.016 (0.051)	Data 0.002 (0.009)	Loss 0.9772 (0.9912)	Acc@1 98.828 (97.638)	Acc@5 100.000 (99.963)
Epoch: [101][30/196]	Time 0.015 (0.039)	Data 0.002 (0.007)	Loss 1.0203 (0.9878)	Acc@1 95.703 (97.618)	Acc@5 100.000 (99.975)
Epoch: [101][40/196]	Time 0.015 (0.034)	Data 0.002 (0.006)	Loss 0.9756 (0.9852)	Acc@1 97.656 (97.685)	Acc@5 100.000 (99.971)
Epoch: [101][50/196]	Time 0.016 (0.030)	Data 0.002 (0.005)	Loss 0.9792 (0.9831)	Acc@1 98.047 (97.725)	Acc@5 100.000 (99.954)
Epoch: [101][60/196]	Time 0.016 (0.028)	Data 0.002 (0.005)	Loss 0.9567 (0.9830)	Acc@1 98.438 (97.727)	Acc@5 100.000 (99.949)
Epoch: [101][70/196]	Time 0.015 (0.026)	Data 0.002 (0.004)	Loss 0.9687 (0.9837)	Acc@1 98.828 (97.651)	Acc@5 100.000 (99.956)
Epoch: [101][80/196]	Time 0.015 (0.024)	Data 0.002 (0.004)	Loss 0.9791 (0.9844)	Acc@1 96.875 (97.613)	Acc@5 100.000 (99.952)
Epoch: [101][90/196]	Time 0.017 (0.023)	Data 0.000 (0.004)	Loss 0.9999 (0.9840)	Acc@1 96.875 (97.626)	Acc@5 100.000 (99.953)
Epoch: [101][100/196]	Time 0.016 (0.023)	Data 0.002 (0.004)	Loss 0.9852 (0.9838)	Acc@1 97.266 (97.629)	Acc@5 100.000 (99.954)
Epoch: [101][110/196]	Time 0.017 (0.022)	Data 0.000 (0.004)	Loss 0.9669 (0.9826)	Acc@1 98.047 (97.667)	Acc@5 100.000 (99.958)
Epoch: [101][120/196]	Time 0.015 (0.021)	Data 0.002 (0.004)	Loss 0.9609 (0.9817)	Acc@1 98.047 (97.685)	Acc@5 100.000 (99.958)
Epoch: [101][130/196]	Time 0.016 (0.021)	Data 0.000 (0.004)	Loss 0.9725 (0.9820)	Acc@1 98.047 (97.680)	Acc@5 100.000 (99.958)
Epoch: [101][140/196]	Time 0.012 (0.020)	Data 0.005 (0.003)	Loss 0.9794 (0.9820)	Acc@1 98.047 (97.673)	Acc@5 100.000 (99.958)
Epoch: [101][150/196]	Time 0.016 (0.020)	Data 0.000 (0.003)	Loss 1.0063 (0.9826)	Acc@1 95.312 (97.612)	Acc@5 100.000 (99.961)
Epoch: [101][160/196]	Time 0.012 (0.020)	Data 0.007 (0.003)	Loss 0.9792 (0.9824)	Acc@1 97.656 (97.598)	Acc@5 100.000 (99.964)
Epoch: [101][170/196]	Time 0.015 (0.019)	Data 0.003 (0.003)	Loss 0.9416 (0.9821)	Acc@1 99.219 (97.601)	Acc@5 100.000 (99.963)
Epoch: [101][180/196]	Time 0.012 (0.019)	Data 0.006 (0.003)	Loss 0.9535 (0.9817)	Acc@1 98.828 (97.609)	Acc@5 100.000 (99.961)
Epoch: [101][190/196]	Time 0.015 (0.019)	Data 0.002 (0.003)	Loss 0.9644 (0.9820)	Acc@1 98.047 (97.576)	Acc@5 100.000 (99.957)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [102 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [102][0/196]	Time 0.033 (0.033)	Data 0.158 (0.158)	Loss 0.9581 (0.9581)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [102][10/196]	Time 0.016 (0.017)	Data 0.001 (0.017)	Loss 0.9674 (0.9690)	Acc@1 97.656 (97.408)	Acc@5 100.000 (99.964)
Epoch: [102][20/196]	Time 0.013 (0.016)	Data 0.004 (0.010)	Loss 0.9459 (0.9608)	Acc@1 98.828 (97.805)	Acc@5 100.000 (99.963)
Epoch: [102][30/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 0.9590 (0.9611)	Acc@1 97.266 (97.807)	Acc@5 100.000 (99.975)
Epoch: [102][40/196]	Time 0.014 (0.015)	Data 0.004 (0.006)	Loss 0.9674 (0.9592)	Acc@1 98.828 (97.913)	Acc@5 100.000 (99.981)
Epoch: [102][50/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.9666 (0.9593)	Acc@1 96.875 (97.924)	Acc@5 100.000 (99.985)
Epoch: [102][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.9276 (0.9590)	Acc@1 98.828 (97.983)	Acc@5 100.000 (99.981)
Epoch: [102][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.9645 (0.9581)	Acc@1 98.047 (98.008)	Acc@5 100.000 (99.983)
Epoch: [102][80/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.9645 (0.9582)	Acc@1 97.266 (97.994)	Acc@5 100.000 (99.986)
Epoch: [102][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9516 (0.9582)	Acc@1 98.438 (98.000)	Acc@5 100.000 (99.974)
Epoch: [102][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.9604 (0.9583)	Acc@1 98.438 (98.004)	Acc@5 100.000 (99.973)
Epoch: [102][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9778 (0.9588)	Acc@1 98.047 (97.984)	Acc@5 100.000 (99.968)
Epoch: [102][120/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 0.9945 (0.9588)	Acc@1 95.703 (97.973)	Acc@5 100.000 (99.964)
Epoch: [102][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9472 (0.9588)	Acc@1 97.266 (97.951)	Acc@5 100.000 (99.964)
Epoch: [102][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.9653 (0.9584)	Acc@1 97.656 (97.958)	Acc@5 100.000 (99.967)
Epoch: [102][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.9477 (0.9582)	Acc@1 98.047 (97.972)	Acc@5 100.000 (99.966)
Epoch: [102][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.9442 (0.9576)	Acc@1 98.828 (97.974)	Acc@5 100.000 (99.968)
Epoch: [102][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9621 (0.9575)	Acc@1 96.875 (97.969)	Acc@5 100.000 (99.968)
Epoch: [102][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.9540 (0.9572)	Acc@1 96.875 (97.969)	Acc@5 100.000 (99.970)
Epoch: [102][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9544 (0.9568)	Acc@1 98.047 (97.977)	Acc@5 100.000 (99.969)
[INFO] Storing checkpoint...

Epoch: [103 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [103][0/196]	Time 0.031 (0.031)	Data 0.165 (0.165)	Loss 0.9695 (0.9695)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [103][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.9373 (0.9444)	Acc@1 98.047 (98.011)	Acc@5 100.000 (99.964)
Epoch: [103][20/196]	Time 0.011 (0.015)	Data 0.008 (0.011)	Loss 0.9367 (0.9372)	Acc@1 98.828 (98.456)	Acc@5 100.000 (99.981)
Epoch: [103][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.9517 (0.9376)	Acc@1 98.828 (98.438)	Acc@5 100.000 (99.987)
Epoch: [103][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 0.9345 (0.9363)	Acc@1 98.438 (98.418)	Acc@5 100.000 (99.981)
Epoch: [103][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.9376 (0.9363)	Acc@1 99.219 (98.407)	Acc@5 100.000 (99.985)
Epoch: [103][60/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 0.9597 (0.9367)	Acc@1 97.656 (98.386)	Acc@5 99.609 (99.981)
Epoch: [103][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.9442 (0.9362)	Acc@1 98.828 (98.410)	Acc@5 100.000 (99.983)
Epoch: [103][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 0.9264 (0.9355)	Acc@1 98.828 (98.413)	Acc@5 100.000 (99.986)
Epoch: [103][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.9299 (0.9352)	Acc@1 98.438 (98.407)	Acc@5 100.000 (99.983)
Epoch: [103][100/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 0.9296 (0.9352)	Acc@1 98.828 (98.368)	Acc@5 100.000 (99.985)
Epoch: [103][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9296 (0.9347)	Acc@1 98.047 (98.374)	Acc@5 100.000 (99.986)
Epoch: [103][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.9471 (0.9345)	Acc@1 97.656 (98.363)	Acc@5 100.000 (99.984)
Epoch: [103][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9243 (0.9344)	Acc@1 98.438 (98.360)	Acc@5 100.000 (99.985)
Epoch: [103][140/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 0.9281 (0.9344)	Acc@1 98.438 (98.332)	Acc@5 99.609 (99.983)
Epoch: [103][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9226 (0.9346)	Acc@1 98.438 (98.306)	Acc@5 100.000 (99.984)
Epoch: [103][160/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.9166 (0.9342)	Acc@1 98.828 (98.299)	Acc@5 100.000 (99.983)
Epoch: [103][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.9511 (0.9343)	Acc@1 98.047 (98.284)	Acc@5 99.609 (99.977)
Epoch: [103][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.9316 (0.9337)	Acc@1 98.828 (98.302)	Acc@5 100.000 (99.978)
Epoch: [103][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9275 (0.9336)	Acc@1 99.219 (98.278)	Acc@5 100.000 (99.980)
[INFO] Storing checkpoint...

Epoch: [104 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [104][0/196]	Time 0.031 (0.031)	Data 0.164 (0.164)	Loss 0.9059 (0.9059)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [104][10/196]	Time 0.013 (0.016)	Data 0.002 (0.017)	Loss 0.9249 (0.9190)	Acc@1 99.219 (98.509)	Acc@5 100.000 (99.929)
Epoch: [104][20/196]	Time 0.016 (0.015)	Data 0.002 (0.010)	Loss 0.9242 (0.9184)	Acc@1 98.047 (98.512)	Acc@5 100.000 (99.963)
Epoch: [104][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.9320 (0.9198)	Acc@1 98.047 (98.501)	Acc@5 100.000 (99.975)
Epoch: [104][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 0.9067 (0.9194)	Acc@1 99.219 (98.485)	Acc@5 100.000 (99.981)
Epoch: [104][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.9127 (0.9184)	Acc@1 97.656 (98.491)	Acc@5 100.000 (99.985)
Epoch: [104][60/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 0.9214 (0.9179)	Acc@1 98.047 (98.470)	Acc@5 100.000 (99.987)
Epoch: [104][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.8950 (0.9177)	Acc@1 99.219 (98.471)	Acc@5 100.000 (99.989)
Epoch: [104][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.9184 (0.9177)	Acc@1 98.438 (98.442)	Acc@5 100.000 (99.990)
Epoch: [104][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.8979 (0.9170)	Acc@1 99.219 (98.459)	Acc@5 100.000 (99.991)
Epoch: [104][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.9095 (0.9169)	Acc@1 99.219 (98.453)	Acc@5 100.000 (99.992)
Epoch: [104][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9095 (0.9166)	Acc@1 97.266 (98.462)	Acc@5 100.000 (99.986)
Epoch: [104][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.8926 (0.9161)	Acc@1 99.219 (98.476)	Acc@5 100.000 (99.987)
Epoch: [104][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9074 (0.9160)	Acc@1 97.656 (98.458)	Acc@5 100.000 (99.988)
Epoch: [104][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.8890 (0.9155)	Acc@1 98.828 (98.446)	Acc@5 100.000 (99.986)
Epoch: [104][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9228 (0.9156)	Acc@1 96.875 (98.438)	Acc@5 100.000 (99.987)
Epoch: [104][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.8934 (0.9152)	Acc@1 99.219 (98.445)	Acc@5 100.000 (99.988)
Epoch: [104][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.9211 (0.9155)	Acc@1 97.266 (98.417)	Acc@5 100.000 (99.989)
Epoch: [104][180/196]	Time 0.013 (0.015)	Data 0.003 (0.004)	Loss 0.9011 (0.9154)	Acc@1 98.828 (98.392)	Acc@5 100.000 (99.989)
Epoch: [104][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.9071 (0.9149)	Acc@1 98.828 (98.405)	Acc@5 100.000 (99.990)
[INFO] Storing checkpoint...

Epoch: [105 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [105][0/196]	Time 0.031 (0.031)	Data 0.165 (0.165)	Loss 0.8945 (0.8945)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [105][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 0.9011 (0.8975)	Acc@1 98.438 (98.544)	Acc@5 100.000 (99.964)
Epoch: [105][20/196]	Time 0.011 (0.016)	Data 0.009 (0.011)	Loss 0.9116 (0.8964)	Acc@1 97.266 (98.717)	Acc@5 100.000 (99.981)
Epoch: [105][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 0.8894 (0.8989)	Acc@1 99.609 (98.614)	Acc@5 100.000 (99.987)
Epoch: [105][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 0.9144 (0.8975)	Acc@1 98.828 (98.657)	Acc@5 100.000 (99.981)
Epoch: [105][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.8638 (0.8960)	Acc@1 100.000 (98.752)	Acc@5 100.000 (99.977)
Epoch: [105][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.9460 (0.8968)	Acc@1 96.094 (98.681)	Acc@5 99.609 (99.974)
Epoch: [105][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.8868 (0.8971)	Acc@1 98.828 (98.641)	Acc@5 100.000 (99.978)
Epoch: [105][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 0.8925 (0.8962)	Acc@1 98.828 (98.674)	Acc@5 100.000 (99.981)
Epoch: [105][90/196]	Time 0.019 (0.015)	Data 0.000 (0.004)	Loss 0.8946 (0.8961)	Acc@1 98.438 (98.669)	Acc@5 100.000 (99.983)
Epoch: [105][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.8996 (0.8961)	Acc@1 99.609 (98.646)	Acc@5 100.000 (99.985)
Epoch: [105][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.9077 (0.8963)	Acc@1 98.047 (98.613)	Acc@5 100.000 (99.986)
Epoch: [105][120/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 0.8904 (0.8958)	Acc@1 98.828 (98.609)	Acc@5 100.000 (99.987)
Epoch: [105][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.9261 (0.8960)	Acc@1 96.875 (98.575)	Acc@5 100.000 (99.988)
Epoch: [105][140/196]	Time 0.013 (0.015)	Data 0.009 (0.004)	Loss 0.8675 (0.8956)	Acc@1 99.219 (98.576)	Acc@5 100.000 (99.989)
Epoch: [105][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8722 (0.8958)	Acc@1 100.000 (98.559)	Acc@5 100.000 (99.990)
Epoch: [105][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.8951 (0.8954)	Acc@1 98.047 (98.571)	Acc@5 100.000 (99.990)
Epoch: [105][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8978 (0.8958)	Acc@1 98.047 (98.554)	Acc@5 100.000 (99.991)
Epoch: [105][180/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 0.8680 (0.8953)	Acc@1 99.219 (98.545)	Acc@5 100.000 (99.991)
Epoch: [105][190/196]	Time 0.018 (0.015)	Data 0.000 (0.003)	Loss 0.8818 (0.8949)	Acc@1 98.047 (98.550)	Acc@5 100.000 (99.990)
[INFO] Storing checkpoint...

Epoch: [106 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [106][0/196]	Time 0.031 (0.031)	Data 0.183 (0.183)	Loss 0.8715 (0.8715)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [106][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 0.8726 (0.8825)	Acc@1 98.828 (99.006)	Acc@5 100.000 (100.000)
Epoch: [106][20/196]	Time 0.013 (0.016)	Data 0.005 (0.011)	Loss 0.8807 (0.8826)	Acc@1 98.828 (98.810)	Acc@5 100.000 (100.000)
Epoch: [106][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.8988 (0.8811)	Acc@1 98.828 (98.879)	Acc@5 100.000 (100.000)
Epoch: [106][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 0.9003 (0.8810)	Acc@1 98.438 (98.800)	Acc@5 100.000 (99.990)
Epoch: [106][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.8638 (0.8804)	Acc@1 99.609 (98.859)	Acc@5 100.000 (99.992)
Epoch: [106][60/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.9041 (0.8801)	Acc@1 98.438 (98.860)	Acc@5 99.609 (99.987)
Epoch: [106][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8747 (0.8807)	Acc@1 98.438 (98.779)	Acc@5 100.000 (99.989)
Epoch: [106][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.8745 (0.8810)	Acc@1 98.438 (98.717)	Acc@5 100.000 (99.990)
Epoch: [106][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.8990 (0.8805)	Acc@1 98.047 (98.712)	Acc@5 100.000 (99.991)
Epoch: [106][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.8798 (0.8797)	Acc@1 99.219 (98.743)	Acc@5 100.000 (99.992)
Epoch: [106][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8663 (0.8789)	Acc@1 98.828 (98.765)	Acc@5 100.000 (99.993)
Epoch: [106][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.8573 (0.8781)	Acc@1 100.000 (98.786)	Acc@5 100.000 (99.994)
Epoch: [106][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.8815 (0.8782)	Acc@1 98.438 (98.766)	Acc@5 100.000 (99.994)
Epoch: [106][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.8775 (0.8776)	Acc@1 99.219 (98.778)	Acc@5 100.000 (99.994)
Epoch: [106][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.8782 (0.8772)	Acc@1 99.219 (98.797)	Acc@5 100.000 (99.995)
Epoch: [106][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.8741 (0.8773)	Acc@1 98.828 (98.784)	Acc@5 100.000 (99.995)
Epoch: [106][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.8533 (0.8769)	Acc@1 99.609 (98.792)	Acc@5 100.000 (99.995)
Epoch: [106][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.8576 (0.8771)	Acc@1 98.438 (98.766)	Acc@5 100.000 (99.991)
Epoch: [106][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8862 (0.8769)	Acc@1 98.047 (98.752)	Acc@5 100.000 (99.992)
[INFO] Storing checkpoint...

Epoch: [107 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [107][0/196]	Time 0.032 (0.032)	Data 0.184 (0.184)	Loss 0.8670 (0.8670)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [107][10/196]	Time 0.016 (0.017)	Data 0.002 (0.019)	Loss 0.8591 (0.8608)	Acc@1 98.828 (99.148)	Acc@5 100.000 (100.000)
Epoch: [107][20/196]	Time 0.014 (0.016)	Data 0.003 (0.011)	Loss 0.8555 (0.8604)	Acc@1 98.828 (99.126)	Acc@5 100.000 (100.000)
Epoch: [107][30/196]	Time 0.012 (0.015)	Data 0.005 (0.008)	Loss 0.8652 (0.8629)	Acc@1 99.219 (99.005)	Acc@5 100.000 (99.987)
Epoch: [107][40/196]	Time 0.015 (0.015)	Data 0.003 (0.007)	Loss 0.8669 (0.8627)	Acc@1 98.828 (98.971)	Acc@5 100.000 (99.990)
Epoch: [107][50/196]	Time 0.013 (0.015)	Data 0.005 (0.006)	Loss 0.8445 (0.8612)	Acc@1 99.609 (99.043)	Acc@5 100.000 (99.992)
Epoch: [107][60/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 0.8639 (0.8609)	Acc@1 98.828 (99.033)	Acc@5 100.000 (99.994)
Epoch: [107][70/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 0.8531 (0.8607)	Acc@1 98.828 (99.043)	Acc@5 100.000 (99.994)
Epoch: [107][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 0.8733 (0.8599)	Acc@1 98.047 (99.064)	Acc@5 100.000 (99.995)
Epoch: [107][90/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 0.8645 (0.8597)	Acc@1 98.828 (99.056)	Acc@5 100.000 (99.996)
Epoch: [107][100/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 0.8485 (0.8594)	Acc@1 98.828 (99.060)	Acc@5 100.000 (99.992)
Epoch: [107][110/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.8532 (0.8593)	Acc@1 98.828 (99.029)	Acc@5 100.000 (99.993)
Epoch: [107][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.8685 (0.8593)	Acc@1 99.219 (99.006)	Acc@5 100.000 (99.994)
Epoch: [107][130/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.8531 (0.8593)	Acc@1 99.219 (98.986)	Acc@5 100.000 (99.994)
Epoch: [107][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.8439 (0.8593)	Acc@1 99.219 (98.953)	Acc@5 100.000 (99.994)
Epoch: [107][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.8580 (0.8596)	Acc@1 99.609 (98.939)	Acc@5 100.000 (99.990)
Epoch: [107][160/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 0.8536 (0.8596)	Acc@1 98.828 (98.928)	Acc@5 100.000 (99.990)
Epoch: [107][170/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 0.8613 (0.8594)	Acc@1 97.266 (98.915)	Acc@5 100.000 (99.991)
Epoch: [107][180/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 0.8744 (0.8594)	Acc@1 98.047 (98.902)	Acc@5 100.000 (99.991)
Epoch: [107][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.8478 (0.8591)	Acc@1 98.438 (98.896)	Acc@5 100.000 (99.992)
[INFO] Storing checkpoint...

Epoch: [108 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [108][0/196]	Time 0.030 (0.030)	Data 0.178 (0.178)	Loss 0.8474 (0.8474)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [108][10/196]	Time 0.016 (0.016)	Data 0.001 (0.018)	Loss 0.8517 (0.8454)	Acc@1 99.219 (99.254)	Acc@5 100.000 (100.000)
Epoch: [108][20/196]	Time 0.015 (0.016)	Data 0.003 (0.011)	Loss 0.8309 (0.8417)	Acc@1 99.609 (99.368)	Acc@5 100.000 (100.000)
Epoch: [108][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.8413 (0.8429)	Acc@1 99.609 (99.269)	Acc@5 100.000 (100.000)
Epoch: [108][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 0.8291 (0.8423)	Acc@1 99.219 (99.247)	Acc@5 100.000 (100.000)
Epoch: [108][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.8530 (0.8425)	Acc@1 98.828 (99.234)	Acc@5 100.000 (100.000)
Epoch: [108][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.8354 (0.8437)	Acc@1 98.828 (99.155)	Acc@5 100.000 (100.000)
Epoch: [108][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.8398 (0.8438)	Acc@1 99.219 (99.158)	Acc@5 100.000 (99.994)
Epoch: [108][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.8404 (0.8447)	Acc@1 99.609 (99.108)	Acc@5 100.000 (99.995)
Epoch: [108][90/196]	Time 0.019 (0.015)	Data 0.000 (0.004)	Loss 0.8320 (0.8445)	Acc@1 99.609 (99.111)	Acc@5 100.000 (99.991)
Epoch: [108][100/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 0.8217 (0.8445)	Acc@1 100.000 (99.110)	Acc@5 100.000 (99.988)
Epoch: [108][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8400 (0.8439)	Acc@1 98.828 (99.103)	Acc@5 100.000 (99.989)
Epoch: [108][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.8246 (0.8431)	Acc@1 99.219 (99.103)	Acc@5 100.000 (99.990)
Epoch: [108][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.8225 (0.8425)	Acc@1 100.000 (99.120)	Acc@5 100.000 (99.991)
Epoch: [108][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.8311 (0.8420)	Acc@1 99.219 (99.108)	Acc@5 100.000 (99.992)
Epoch: [108][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.8473 (0.8423)	Acc@1 98.047 (99.069)	Acc@5 100.000 (99.992)
Epoch: [108][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.8284 (0.8422)	Acc@1 99.609 (99.051)	Acc@5 100.000 (99.993)
Epoch: [108][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.8444 (0.8424)	Acc@1 98.047 (99.036)	Acc@5 100.000 (99.993)
Epoch: [108][180/196]	Time 0.011 (0.015)	Data 0.008 (0.003)	Loss 0.8659 (0.8423)	Acc@1 98.438 (99.029)	Acc@5 100.000 (99.994)
Epoch: [108][190/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 0.8383 (0.8419)	Acc@1 98.828 (99.014)	Acc@5 100.000 (99.994)
[INFO] Storing checkpoint...

Epoch: [109 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [109][0/196]	Time 0.029 (0.029)	Data 0.187 (0.187)	Loss 0.8184 (0.8184)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [109][10/196]	Time 0.013 (0.016)	Data 0.004 (0.019)	Loss 0.8345 (0.8330)	Acc@1 98.047 (98.722)	Acc@5 100.000 (99.964)
Epoch: [109][20/196]	Time 0.012 (0.016)	Data 0.005 (0.011)	Loss 0.8138 (0.8325)	Acc@1 99.219 (98.958)	Acc@5 100.000 (99.981)
Epoch: [109][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 0.8296 (0.8301)	Acc@1 98.828 (99.080)	Acc@5 100.000 (99.987)
Epoch: [109][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 0.8173 (0.8278)	Acc@1 99.219 (99.181)	Acc@5 100.000 (99.990)
Epoch: [109][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.8247 (0.8276)	Acc@1 98.828 (99.173)	Acc@5 100.000 (99.992)
Epoch: [109][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.8321 (0.8271)	Acc@1 98.047 (99.142)	Acc@5 100.000 (99.994)
Epoch: [109][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.8225 (0.8276)	Acc@1 99.219 (99.120)	Acc@5 100.000 (99.994)
Epoch: [109][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.8352 (0.8270)	Acc@1 98.828 (99.132)	Acc@5 100.000 (99.995)
Epoch: [109][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8221 (0.8267)	Acc@1 99.219 (99.150)	Acc@5 100.000 (99.996)
Epoch: [109][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.8199 (0.8270)	Acc@1 99.609 (99.153)	Acc@5 100.000 (99.996)
Epoch: [109][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8328 (0.8265)	Acc@1 98.438 (99.138)	Acc@5 100.000 (99.996)
Epoch: [109][120/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 0.8125 (0.8263)	Acc@1 99.219 (99.125)	Acc@5 100.000 (99.997)
Epoch: [109][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8105 (0.8265)	Acc@1 100.000 (99.123)	Acc@5 100.000 (99.994)
Epoch: [109][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.8222 (0.8261)	Acc@1 98.828 (99.116)	Acc@5 100.000 (99.994)
Epoch: [109][150/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 0.8459 (0.8258)	Acc@1 99.219 (99.118)	Acc@5 100.000 (99.995)
Epoch: [109][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.8098 (0.8254)	Acc@1 100.000 (99.131)	Acc@5 100.000 (99.995)
Epoch: [109][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8082 (0.8257)	Acc@1 99.609 (99.109)	Acc@5 100.000 (99.995)
Epoch: [109][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.8411 (0.8256)	Acc@1 98.047 (99.107)	Acc@5 100.000 (99.996)
Epoch: [109][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.8345 (0.8258)	Acc@1 98.438 (99.071)	Acc@5 100.000 (99.996)
[INFO] Storing checkpoint...

Epoch: [110 | 110] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [478, 511, 3, 3]
module.conv8.weight [303, 478, 3, 3]
Epoch: [110][0/196]	Time 0.030 (0.030)	Data 0.180 (0.180)	Loss 0.8066 (0.8066)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [110][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 0.8109 (0.8162)	Acc@1 98.828 (99.325)	Acc@5 100.000 (100.000)
Epoch: [110][20/196]	Time 0.012 (0.016)	Data 0.008 (0.011)	Loss 0.8203 (0.8160)	Acc@1 99.219 (99.293)	Acc@5 100.000 (100.000)
Epoch: [110][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 0.8292 (0.8181)	Acc@1 98.438 (99.156)	Acc@5 100.000 (100.000)
Epoch: [110][40/196]	Time 0.011 (0.016)	Data 0.009 (0.007)	Loss 0.8320 (0.8181)	Acc@1 97.656 (99.114)	Acc@5 100.000 (100.000)
Epoch: [110][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 0.8168 (0.8173)	Acc@1 99.219 (99.157)	Acc@5 100.000 (100.000)
Epoch: [110][60/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 0.8146 (0.8175)	Acc@1 98.828 (99.136)	Acc@5 100.000 (99.994)
Epoch: [110][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.8020 (0.8175)	Acc@1 99.219 (99.136)	Acc@5 100.000 (99.994)
Epoch: [110][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 0.8030 (0.8168)	Acc@1 99.609 (99.122)	Acc@5 100.000 (99.995)
Epoch: [110][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8112 (0.8157)	Acc@1 99.219 (99.154)	Acc@5 100.000 (99.996)
Epoch: [110][100/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 0.8020 (0.8147)	Acc@1 100.000 (99.188)	Acc@5 100.000 (99.996)
Epoch: [110][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.8150 (0.8142)	Acc@1 99.219 (99.191)	Acc@5 100.000 (99.996)
Epoch: [110][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.8106 (0.8141)	Acc@1 99.219 (99.186)	Acc@5 100.000 (99.997)
Epoch: [110][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.8120 (0.8139)	Acc@1 98.828 (99.174)	Acc@5 100.000 (99.997)
Epoch: [110][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.7957 (0.8133)	Acc@1 100.000 (99.180)	Acc@5 100.000 (99.997)
Epoch: [110][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8337 (0.8138)	Acc@1 97.656 (99.139)	Acc@5 100.000 (99.997)
Epoch: [110][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.8076 (0.8134)	Acc@1 98.828 (99.141)	Acc@5 100.000 (99.998)
Epoch: [110][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.8017 (0.8130)	Acc@1 98.828 (99.150)	Acc@5 100.000 (99.998)
Epoch: [110][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.7979 (0.8128)	Acc@1 99.609 (99.148)	Acc@5 100.000 (99.998)
Epoch: [110][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.8093 (0.8127)	Acc@1 98.438 (99.135)	Acc@5 100.000 (99.996)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [253, 127, 3, 3] >> [253, 127, 3, 3]
[module.conv4.weight]: [256, 253, 3, 3] >> [256, 253, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [478, 511, 3, 3] >> [477, 511, 3, 3]
[module.conv8.weight]: [303, 478, 3, 3] >> [300, 477, 3, 3]
[module.fc.weight]: [100, 303] >> [100, 300]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
69.62
python src/cifar.py --workers 4 --dataset cifar100 --epochs 120 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_120.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.96M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [253, 127, 3, 3]
conv4 --> [256, 253, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [477, 511, 3, 3]
conv8 --> [300, 477, 3, 3]
fc --> [300, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8439399936, 18507456, 253
4, 17011703808, 37306368, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6739117056, 8774892, 477
8, 3956428800, 5151600, 300
fc, 11520000, 30000, 0
===================
FLOP REPORT: 28415242800000.0 52238400000.0 140399868 130596 2478 15.168703079223633

Epoch: [111 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [111][0/196]	Time 0.767 (0.767)	Data 0.170 (0.170)	Loss 0.7972 (0.7972)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [111][10/196]	Time 0.016 (0.083)	Data 0.002 (0.017)	Loss 0.8042 (0.7961)	Acc@1 99.219 (99.361)	Acc@5 100.000 (100.000)
Epoch: [111][20/196]	Time 0.015 (0.051)	Data 0.002 (0.010)	Loss 0.7987 (0.7985)	Acc@1 99.219 (99.256)	Acc@5 100.000 (100.000)
Epoch: [111][30/196]	Time 0.015 (0.039)	Data 0.002 (0.008)	Loss 0.7943 (0.7984)	Acc@1 99.219 (99.257)	Acc@5 100.000 (100.000)
Epoch: [111][40/196]	Time 0.014 (0.033)	Data 0.002 (0.006)	Loss 0.8140 (0.7990)	Acc@1 98.438 (99.209)	Acc@5 100.000 (100.000)
Epoch: [111][50/196]	Time 0.016 (0.030)	Data 0.001 (0.006)	Loss 0.8046 (0.7992)	Acc@1 99.219 (99.211)	Acc@5 100.000 (100.000)
Epoch: [111][60/196]	Time 0.016 (0.027)	Data 0.002 (0.005)	Loss 0.7971 (0.7996)	Acc@1 99.219 (99.174)	Acc@5 100.000 (100.000)
Epoch: [111][70/196]	Time 0.017 (0.025)	Data 0.002 (0.005)	Loss 0.7975 (0.7993)	Acc@1 99.609 (99.197)	Acc@5 100.000 (100.000)
Epoch: [111][80/196]	Time 0.015 (0.024)	Data 0.002 (0.004)	Loss 0.7876 (0.7990)	Acc@1 100.000 (99.228)	Acc@5 100.000 (99.995)
Epoch: [111][90/196]	Time 0.014 (0.023)	Data 0.003 (0.005)	Loss 0.7873 (0.7983)	Acc@1 99.219 (99.227)	Acc@5 100.000 (99.996)
Epoch: [111][100/196]	Time 0.015 (0.022)	Data 0.002 (0.004)	Loss 0.7980 (0.7985)	Acc@1 99.219 (99.203)	Acc@5 100.000 (99.996)
Epoch: [111][110/196]	Time 0.016 (0.021)	Data 0.000 (0.005)	Loss 0.7771 (0.7985)	Acc@1 100.000 (99.191)	Acc@5 100.000 (99.993)
Epoch: [111][120/196]	Time 0.014 (0.021)	Data 0.002 (0.004)	Loss 0.8242 (0.7985)	Acc@1 98.047 (99.196)	Acc@5 100.000 (99.994)
Epoch: [111][130/196]	Time 0.019 (0.020)	Data 0.000 (0.004)	Loss 0.7830 (0.7981)	Acc@1 99.609 (99.189)	Acc@5 100.000 (99.994)
Epoch: [111][140/196]	Time 0.015 (0.020)	Data 0.002 (0.004)	Loss 0.7770 (0.7981)	Acc@1 99.609 (99.172)	Acc@5 100.000 (99.994)
Epoch: [111][150/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 0.7927 (0.7979)	Acc@1 99.219 (99.185)	Acc@5 100.000 (99.995)
Epoch: [111][160/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 0.7966 (0.7975)	Acc@1 98.828 (99.190)	Acc@5 100.000 (99.995)
Epoch: [111][170/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 0.7840 (0.7969)	Acc@1 99.609 (99.210)	Acc@5 100.000 (99.995)
Epoch: [111][180/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 0.7923 (0.7968)	Acc@1 99.609 (99.206)	Acc@5 100.000 (99.994)
Epoch: [111][190/196]	Time 0.014 (0.018)	Data 0.003 (0.004)	Loss 0.7899 (0.7964)	Acc@1 98.828 (99.206)	Acc@5 100.000 (99.994)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [112 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [112][0/196]	Time 0.030 (0.030)	Data 0.168 (0.168)	Loss 0.7696 (0.7696)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [112][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 0.8038 (0.7858)	Acc@1 98.438 (99.219)	Acc@5 100.000 (100.000)
Epoch: [112][20/196]	Time 0.011 (0.016)	Data 0.011 (0.011)	Loss 0.7802 (0.7836)	Acc@1 99.609 (99.293)	Acc@5 100.000 (100.000)
Epoch: [112][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 0.7960 (0.7827)	Acc@1 98.828 (99.408)	Acc@5 100.000 (100.000)
Epoch: [112][40/196]	Time 0.012 (0.015)	Data 0.008 (0.007)	Loss 0.7951 (0.7820)	Acc@1 98.047 (99.428)	Acc@5 100.000 (100.000)
Epoch: [112][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.7800 (0.7822)	Acc@1 99.609 (99.395)	Acc@5 100.000 (100.000)
Epoch: [112][60/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.7852 (0.7819)	Acc@1 99.219 (99.411)	Acc@5 100.000 (100.000)
Epoch: [112][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7758 (0.7824)	Acc@1 99.609 (99.411)	Acc@5 100.000 (99.994)
Epoch: [112][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.7759 (0.7821)	Acc@1 99.219 (99.407)	Acc@5 100.000 (99.995)
Epoch: [112][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.7833 (0.7813)	Acc@1 99.609 (99.429)	Acc@5 100.000 (99.996)
Epoch: [112][100/196]	Time 0.012 (0.015)	Data 0.019 (0.005)	Loss 0.7770 (0.7811)	Acc@1 99.219 (99.428)	Acc@5 100.000 (99.996)
Epoch: [112][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.7922 (0.7809)	Acc@1 99.219 (99.409)	Acc@5 100.000 (99.996)
Epoch: [112][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.7778 (0.7808)	Acc@1 100.000 (99.403)	Acc@5 100.000 (99.997)
Epoch: [112][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7781 (0.7810)	Acc@1 99.609 (99.383)	Acc@5 100.000 (99.997)
Epoch: [112][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.7892 (0.7811)	Acc@1 98.438 (99.354)	Acc@5 100.000 (99.994)
Epoch: [112][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7630 (0.7808)	Acc@1 100.000 (99.361)	Acc@5 100.000 (99.995)
Epoch: [112][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7813 (0.7807)	Acc@1 98.828 (99.359)	Acc@5 100.000 (99.995)
Epoch: [112][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.7793 (0.7807)	Acc@1 99.609 (99.356)	Acc@5 100.000 (99.995)
Epoch: [112][180/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.7772 (0.7805)	Acc@1 99.219 (99.348)	Acc@5 100.000 (99.996)
Epoch: [112][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7769 (0.7805)	Acc@1 99.219 (99.346)	Acc@5 100.000 (99.996)
[INFO] Storing checkpoint...

Epoch: [113 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [113][0/196]	Time 0.030 (0.030)	Data 0.170 (0.170)	Loss 0.7765 (0.7765)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [113][10/196]	Time 0.014 (0.016)	Data 0.003 (0.017)	Loss 0.7558 (0.7716)	Acc@1 100.000 (99.290)	Acc@5 100.000 (99.964)
Epoch: [113][20/196]	Time 0.013 (0.015)	Data 0.004 (0.010)	Loss 0.7619 (0.7740)	Acc@1 99.609 (99.256)	Acc@5 100.000 (99.963)
Epoch: [113][30/196]	Time 0.014 (0.015)	Data 0.002 (0.008)	Loss 0.7621 (0.7709)	Acc@1 99.609 (99.383)	Acc@5 100.000 (99.975)
Epoch: [113][40/196]	Time 0.013 (0.015)	Data 0.019 (0.007)	Loss 0.7581 (0.7700)	Acc@1 100.000 (99.381)	Acc@5 100.000 (99.981)
Epoch: [113][50/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 0.7667 (0.7692)	Acc@1 99.219 (99.410)	Acc@5 100.000 (99.985)
Epoch: [113][60/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 0.7613 (0.7690)	Acc@1 99.609 (99.385)	Acc@5 100.000 (99.987)
Epoch: [113][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.7923 (0.7692)	Acc@1 99.219 (99.362)	Acc@5 100.000 (99.989)
Epoch: [113][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.7770 (0.7690)	Acc@1 99.219 (99.363)	Acc@5 100.000 (99.990)
Epoch: [113][90/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7574 (0.7680)	Acc@1 99.609 (99.390)	Acc@5 100.000 (99.991)
Epoch: [113][100/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.7586 (0.7678)	Acc@1 99.609 (99.397)	Acc@5 100.000 (99.992)
Epoch: [113][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7677 (0.7678)	Acc@1 99.219 (99.384)	Acc@5 100.000 (99.993)
Epoch: [113][120/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.7721 (0.7683)	Acc@1 99.219 (99.374)	Acc@5 100.000 (99.994)
Epoch: [113][130/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.7516 (0.7679)	Acc@1 99.609 (99.377)	Acc@5 100.000 (99.994)
Epoch: [113][140/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 0.7546 (0.7675)	Acc@1 99.609 (99.377)	Acc@5 100.000 (99.994)
Epoch: [113][150/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.7526 (0.7670)	Acc@1 100.000 (99.392)	Acc@5 100.000 (99.995)
Epoch: [113][160/196]	Time 0.013 (0.014)	Data 0.005 (0.005)	Loss 0.7606 (0.7668)	Acc@1 99.219 (99.374)	Acc@5 100.000 (99.995)
Epoch: [113][170/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.7615 (0.7669)	Acc@1 100.000 (99.356)	Acc@5 100.000 (99.995)
Epoch: [113][180/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.7743 (0.7671)	Acc@1 98.438 (99.335)	Acc@5 100.000 (99.996)
Epoch: [113][190/196]	Time 0.015 (0.014)	Data 0.002 (0.004)	Loss 0.7745 (0.7670)	Acc@1 99.609 (99.331)	Acc@5 100.000 (99.996)
[INFO] Storing checkpoint...

Epoch: [114 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [114][0/196]	Time 0.030 (0.030)	Data 0.170 (0.170)	Loss 0.7497 (0.7497)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [114][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 0.7463 (0.7506)	Acc@1 100.000 (99.680)	Acc@5 100.000 (100.000)
Epoch: [114][20/196]	Time 0.012 (0.016)	Data 0.005 (0.010)	Loss 0.7618 (0.7533)	Acc@1 99.219 (99.554)	Acc@5 100.000 (100.000)
Epoch: [114][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 0.7586 (0.7542)	Acc@1 99.609 (99.559)	Acc@5 100.000 (100.000)
Epoch: [114][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 0.7436 (0.7531)	Acc@1 99.609 (99.581)	Acc@5 100.000 (100.000)
Epoch: [114][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.7480 (0.7537)	Acc@1 99.609 (99.533)	Acc@5 100.000 (100.000)
Epoch: [114][60/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 0.7545 (0.7540)	Acc@1 98.828 (99.488)	Acc@5 100.000 (100.000)
Epoch: [114][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7747 (0.7545)	Acc@1 98.438 (99.472)	Acc@5 100.000 (100.000)
Epoch: [114][80/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.7439 (0.7543)	Acc@1 100.000 (99.489)	Acc@5 100.000 (100.000)
Epoch: [114][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7575 (0.7537)	Acc@1 99.609 (99.489)	Acc@5 100.000 (100.000)
Epoch: [114][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.7629 (0.7532)	Acc@1 99.219 (99.489)	Acc@5 100.000 (100.000)
Epoch: [114][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7461 (0.7532)	Acc@1 100.000 (99.504)	Acc@5 100.000 (100.000)
Epoch: [114][120/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7433 (0.7532)	Acc@1 99.219 (99.483)	Acc@5 100.000 (100.000)
Epoch: [114][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7484 (0.7528)	Acc@1 99.609 (99.484)	Acc@5 100.000 (100.000)
Epoch: [114][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.7572 (0.7530)	Acc@1 98.828 (99.454)	Acc@5 100.000 (100.000)
Epoch: [114][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.7358 (0.7529)	Acc@1 100.000 (99.457)	Acc@5 100.000 (100.000)
Epoch: [114][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.7482 (0.7525)	Acc@1 98.828 (99.457)	Acc@5 100.000 (100.000)
Epoch: [114][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7409 (0.7525)	Acc@1 98.828 (99.436)	Acc@5 100.000 (99.998)
Epoch: [114][180/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 0.7351 (0.7524)	Acc@1 100.000 (99.428)	Acc@5 100.000 (99.998)
Epoch: [114][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7480 (0.7524)	Acc@1 99.219 (99.417)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [115 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [115][0/196]	Time 0.030 (0.030)	Data 0.175 (0.175)	Loss 0.7482 (0.7482)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [115][10/196]	Time 0.017 (0.016)	Data 0.001 (0.018)	Loss 0.7544 (0.7473)	Acc@1 98.828 (99.183)	Acc@5 100.000 (100.000)
Epoch: [115][20/196]	Time 0.011 (0.016)	Data 0.009 (0.011)	Loss 0.7421 (0.7475)	Acc@1 99.219 (99.182)	Acc@5 100.000 (100.000)
Epoch: [115][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 0.7406 (0.7488)	Acc@1 99.219 (99.194)	Acc@5 100.000 (99.987)
Epoch: [115][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 0.7478 (0.7481)	Acc@1 99.609 (99.257)	Acc@5 100.000 (99.990)
Epoch: [115][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.7447 (0.7468)	Acc@1 100.000 (99.265)	Acc@5 100.000 (99.992)
Epoch: [115][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.7301 (0.7460)	Acc@1 99.609 (99.270)	Acc@5 100.000 (99.994)
Epoch: [115][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7304 (0.7452)	Acc@1 100.000 (99.296)	Acc@5 100.000 (99.994)
Epoch: [115][80/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.7324 (0.7440)	Acc@1 100.000 (99.344)	Acc@5 100.000 (99.995)
Epoch: [115][90/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7294 (0.7431)	Acc@1 99.609 (99.369)	Acc@5 100.000 (99.996)
Epoch: [115][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7380 (0.7427)	Acc@1 99.609 (99.393)	Acc@5 100.000 (99.996)
Epoch: [115][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7440 (0.7421)	Acc@1 98.828 (99.409)	Acc@5 100.000 (99.996)
Epoch: [115][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.7342 (0.7422)	Acc@1 99.609 (99.403)	Acc@5 100.000 (99.997)
Epoch: [115][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.7557 (0.7419)	Acc@1 97.656 (99.398)	Acc@5 100.000 (99.997)
Epoch: [115][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.7362 (0.7414)	Acc@1 100.000 (99.415)	Acc@5 100.000 (99.997)
Epoch: [115][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.7218 (0.7409)	Acc@1 100.000 (99.418)	Acc@5 100.000 (99.997)
Epoch: [115][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.7316 (0.7406)	Acc@1 100.000 (99.415)	Acc@5 100.000 (99.998)
Epoch: [115][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7334 (0.7402)	Acc@1 100.000 (99.429)	Acc@5 100.000 (99.998)
Epoch: [115][180/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 0.7241 (0.7397)	Acc@1 100.000 (99.441)	Acc@5 100.000 (99.998)
Epoch: [115][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 0.7384 (0.7393)	Acc@1 99.219 (99.442)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [116 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [116][0/196]	Time 0.031 (0.031)	Data 0.189 (0.189)	Loss 0.7456 (0.7456)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [116][10/196]	Time 0.015 (0.016)	Data 0.003 (0.019)	Loss 0.7316 (0.7286)	Acc@1 99.609 (99.645)	Acc@5 100.000 (100.000)
Epoch: [116][20/196]	Time 0.014 (0.016)	Data 0.002 (0.011)	Loss 0.7220 (0.7296)	Acc@1 100.000 (99.591)	Acc@5 100.000 (100.000)
Epoch: [116][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 0.7207 (0.7294)	Acc@1 100.000 (99.509)	Acc@5 100.000 (100.000)
Epoch: [116][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 0.7213 (0.7293)	Acc@1 99.609 (99.543)	Acc@5 100.000 (100.000)
Epoch: [116][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.7253 (0.7286)	Acc@1 100.000 (99.556)	Acc@5 100.000 (100.000)
Epoch: [116][60/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 0.7222 (0.7282)	Acc@1 99.219 (99.552)	Acc@5 100.000 (100.000)
Epoch: [116][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.7250 (0.7275)	Acc@1 99.609 (99.543)	Acc@5 100.000 (100.000)
Epoch: [116][80/196]	Time 0.021 (0.015)	Data 0.003 (0.005)	Loss 0.7444 (0.7275)	Acc@1 98.828 (99.527)	Acc@5 100.000 (100.000)
Epoch: [116][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7343 (0.7275)	Acc@1 99.219 (99.511)	Acc@5 100.000 (100.000)
Epoch: [116][100/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 0.7271 (0.7270)	Acc@1 99.219 (99.524)	Acc@5 100.000 (100.000)
Epoch: [116][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7239 (0.7271)	Acc@1 99.609 (99.511)	Acc@5 100.000 (100.000)
Epoch: [116][120/196]	Time 0.012 (0.015)	Data 0.013 (0.005)	Loss 0.7288 (0.7267)	Acc@1 99.219 (99.516)	Acc@5 100.000 (100.000)
Epoch: [116][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.7176 (0.7262)	Acc@1 99.609 (99.511)	Acc@5 100.000 (100.000)
Epoch: [116][140/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.7226 (0.7260)	Acc@1 99.609 (99.512)	Acc@5 100.000 (100.000)
Epoch: [116][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.7124 (0.7257)	Acc@1 99.609 (99.506)	Acc@5 100.000 (100.000)
Epoch: [116][160/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 0.7183 (0.7256)	Acc@1 99.609 (99.505)	Acc@5 100.000 (99.998)
Epoch: [116][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7179 (0.7252)	Acc@1 100.000 (99.520)	Acc@5 100.000 (99.998)
Epoch: [116][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7098 (0.7250)	Acc@1 100.000 (99.527)	Acc@5 100.000 (99.998)
Epoch: [116][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7278 (0.7248)	Acc@1 99.219 (99.521)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [117 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [117][0/196]	Time 0.033 (0.033)	Data 0.195 (0.195)	Loss 0.7078 (0.7078)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [117][10/196]	Time 0.014 (0.017)	Data 0.003 (0.019)	Loss 0.7260 (0.7156)	Acc@1 99.219 (99.645)	Acc@5 100.000 (100.000)
Epoch: [117][20/196]	Time 0.012 (0.016)	Data 0.004 (0.012)	Loss 0.7209 (0.7172)	Acc@1 99.609 (99.554)	Acc@5 100.000 (100.000)
Epoch: [117][30/196]	Time 0.017 (0.015)	Data 0.001 (0.009)	Loss 0.7289 (0.7173)	Acc@1 99.219 (99.546)	Acc@5 100.000 (100.000)
Epoch: [117][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 0.7077 (0.7167)	Acc@1 99.609 (99.562)	Acc@5 100.000 (100.000)
Epoch: [117][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.7158 (0.7166)	Acc@1 99.219 (99.556)	Acc@5 100.000 (100.000)
Epoch: [117][60/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 0.7097 (0.7159)	Acc@1 99.609 (99.571)	Acc@5 100.000 (100.000)
Epoch: [117][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.7056 (0.7152)	Acc@1 100.000 (99.598)	Acc@5 100.000 (100.000)
Epoch: [117][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.7092 (0.7157)	Acc@1 99.609 (99.561)	Acc@5 100.000 (100.000)
Epoch: [117][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.7096 (0.7152)	Acc@1 99.609 (99.571)	Acc@5 100.000 (100.000)
Epoch: [117][100/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.7168 (0.7144)	Acc@1 99.219 (99.590)	Acc@5 100.000 (100.000)
Epoch: [117][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7126 (0.7148)	Acc@1 99.219 (99.560)	Acc@5 100.000 (100.000)
Epoch: [117][120/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.7116 (0.7147)	Acc@1 99.609 (99.551)	Acc@5 100.000 (100.000)
Epoch: [117][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.7284 (0.7148)	Acc@1 99.219 (99.538)	Acc@5 100.000 (100.000)
Epoch: [117][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7050 (0.7144)	Acc@1 99.609 (99.543)	Acc@5 100.000 (100.000)
Epoch: [117][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7089 (0.7143)	Acc@1 99.609 (99.540)	Acc@5 100.000 (100.000)
Epoch: [117][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7181 (0.7140)	Acc@1 98.828 (99.532)	Acc@5 100.000 (100.000)
Epoch: [117][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7111 (0.7137)	Acc@1 99.609 (99.525)	Acc@5 100.000 (100.000)
Epoch: [117][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7093 (0.7133)	Acc@1 99.219 (99.519)	Acc@5 100.000 (100.000)
Epoch: [117][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.7132 (0.7131)	Acc@1 99.609 (99.521)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [118 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [118][0/196]	Time 0.032 (0.032)	Data 0.181 (0.181)	Loss 0.7012 (0.7012)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [118][10/196]	Time 0.017 (0.017)	Data 0.000 (0.019)	Loss 0.6984 (0.7034)	Acc@1 99.609 (99.645)	Acc@5 100.000 (100.000)
Epoch: [118][20/196]	Time 0.011 (0.016)	Data 0.011 (0.012)	Loss 0.7149 (0.7053)	Acc@1 98.438 (99.461)	Acc@5 100.000 (100.000)
Epoch: [118][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 0.6923 (0.7028)	Acc@1 100.000 (99.559)	Acc@5 100.000 (100.000)
Epoch: [118][40/196]	Time 0.011 (0.016)	Data 0.011 (0.007)	Loss 0.7067 (0.7028)	Acc@1 99.219 (99.543)	Acc@5 100.000 (100.000)
Epoch: [118][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.6978 (0.7020)	Acc@1 100.000 (99.563)	Acc@5 100.000 (100.000)
Epoch: [118][60/196]	Time 0.012 (0.015)	Data 0.010 (0.006)	Loss 0.7209 (0.7020)	Acc@1 98.828 (99.558)	Acc@5 100.000 (100.000)
Epoch: [118][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.7038 (0.7019)	Acc@1 99.219 (99.549)	Acc@5 100.000 (100.000)
Epoch: [118][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.7007 (0.7016)	Acc@1 99.609 (99.537)	Acc@5 100.000 (100.000)
Epoch: [118][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7078 (0.7018)	Acc@1 99.609 (99.545)	Acc@5 100.000 (100.000)
Epoch: [118][100/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.7044 (0.7023)	Acc@1 99.609 (99.520)	Acc@5 100.000 (100.000)
Epoch: [118][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.6875 (0.7022)	Acc@1 100.000 (99.528)	Acc@5 100.000 (100.000)
Epoch: [118][120/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.7090 (0.7020)	Acc@1 99.219 (99.535)	Acc@5 100.000 (100.000)
Epoch: [118][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6904 (0.7018)	Acc@1 100.000 (99.538)	Acc@5 100.000 (100.000)
Epoch: [118][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.6999 (0.7014)	Acc@1 100.000 (99.548)	Acc@5 100.000 (100.000)
Epoch: [118][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.7004 (0.7012)	Acc@1 99.219 (99.545)	Acc@5 100.000 (100.000)
Epoch: [118][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.6852 (0.7010)	Acc@1 99.609 (99.541)	Acc@5 100.000 (100.000)
Epoch: [118][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6922 (0.7005)	Acc@1 99.609 (99.548)	Acc@5 100.000 (100.000)
Epoch: [118][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.7166 (0.7007)	Acc@1 98.828 (99.530)	Acc@5 100.000 (100.000)
Epoch: [118][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.7028 (0.7006)	Acc@1 99.219 (99.526)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [119 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [119][0/196]	Time 0.030 (0.030)	Data 0.184 (0.184)	Loss 0.6873 (0.6873)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [119][10/196]	Time 0.014 (0.016)	Data 0.003 (0.019)	Loss 0.6819 (0.6891)	Acc@1 100.000 (99.822)	Acc@5 100.000 (100.000)
Epoch: [119][20/196]	Time 0.012 (0.016)	Data 0.007 (0.011)	Loss 0.6854 (0.6892)	Acc@1 100.000 (99.721)	Acc@5 100.000 (100.000)
Epoch: [119][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 0.6793 (0.6899)	Acc@1 99.609 (99.685)	Acc@5 100.000 (100.000)
Epoch: [119][40/196]	Time 0.014 (0.015)	Data 0.004 (0.007)	Loss 0.6855 (0.6898)	Acc@1 99.609 (99.667)	Acc@5 100.000 (100.000)
Epoch: [119][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 0.6962 (0.6908)	Acc@1 99.219 (99.609)	Acc@5 100.000 (100.000)
Epoch: [119][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.6869 (0.6909)	Acc@1 100.000 (99.590)	Acc@5 100.000 (100.000)
Epoch: [119][70/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 0.6822 (0.6901)	Acc@1 100.000 (99.620)	Acc@5 100.000 (100.000)
Epoch: [119][80/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 0.6910 (0.6900)	Acc@1 99.219 (99.585)	Acc@5 100.000 (100.000)
Epoch: [119][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.7064 (0.6900)	Acc@1 99.219 (99.588)	Acc@5 100.000 (100.000)
Epoch: [119][100/196]	Time 0.012 (0.014)	Data 0.007 (0.005)	Loss 0.6764 (0.6895)	Acc@1 100.000 (99.582)	Acc@5 100.000 (100.000)
Epoch: [119][110/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 0.6834 (0.6887)	Acc@1 100.000 (99.613)	Acc@5 100.000 (100.000)
Epoch: [119][120/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.6936 (0.6882)	Acc@1 99.219 (99.619)	Acc@5 100.000 (100.000)
Epoch: [119][130/196]	Time 0.016 (0.014)	Data 0.002 (0.005)	Loss 0.6846 (0.6883)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [119][140/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 0.6973 (0.6880)	Acc@1 98.828 (99.607)	Acc@5 100.000 (100.000)
Epoch: [119][150/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.6983 (0.6880)	Acc@1 98.438 (99.594)	Acc@5 100.000 (100.000)
Epoch: [119][160/196]	Time 0.011 (0.014)	Data 0.016 (0.005)	Loss 0.6798 (0.6875)	Acc@1 99.609 (99.602)	Acc@5 100.000 (100.000)
Epoch: [119][170/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.6855 (0.6873)	Acc@1 99.219 (99.600)	Acc@5 100.000 (100.000)
Epoch: [119][180/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 0.6846 (0.6869)	Acc@1 99.609 (99.607)	Acc@5 100.000 (100.000)
Epoch: [119][190/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.6829 (0.6867)	Acc@1 100.000 (99.601)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [120 | 120] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [477, 511, 3, 3]
module.conv8.weight [300, 477, 3, 3]
Epoch: [120][0/196]	Time 0.031 (0.031)	Data 0.181 (0.181)	Loss 0.6719 (0.6719)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [120][10/196]	Time 0.015 (0.016)	Data 0.002 (0.019)	Loss 0.6817 (0.6780)	Acc@1 99.219 (99.538)	Acc@5 100.000 (100.000)
Epoch: [120][20/196]	Time 0.014 (0.015)	Data 0.004 (0.011)	Loss 0.6717 (0.6787)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [120][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.6755 (0.6787)	Acc@1 100.000 (99.647)	Acc@5 100.000 (100.000)
Epoch: [120][40/196]	Time 0.011 (0.015)	Data 0.010 (0.007)	Loss 0.6742 (0.6778)	Acc@1 99.609 (99.667)	Acc@5 100.000 (100.000)
Epoch: [120][50/196]	Time 0.017 (0.014)	Data 0.000 (0.007)	Loss 0.6753 (0.6781)	Acc@1 100.000 (99.632)	Acc@5 100.000 (100.000)
Epoch: [120][60/196]	Time 0.012 (0.014)	Data 0.008 (0.006)	Loss 0.6708 (0.6774)	Acc@1 100.000 (99.661)	Acc@5 100.000 (100.000)
Epoch: [120][70/196]	Time 0.017 (0.014)	Data 0.000 (0.006)	Loss 0.6688 (0.6777)	Acc@1 100.000 (99.648)	Acc@5 100.000 (100.000)
Epoch: [120][80/196]	Time 0.011 (0.014)	Data 0.006 (0.006)	Loss 0.6737 (0.6774)	Acc@1 100.000 (99.629)	Acc@5 100.000 (100.000)
Epoch: [120][90/196]	Time 0.017 (0.014)	Data 0.000 (0.006)	Loss 0.6779 (0.6775)	Acc@1 99.609 (99.614)	Acc@5 100.000 (100.000)
Epoch: [120][100/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.6863 (0.6776)	Acc@1 98.828 (99.575)	Acc@5 100.000 (100.000)
Epoch: [120][110/196]	Time 0.017 (0.014)	Data 0.001 (0.005)	Loss 0.6665 (0.6774)	Acc@1 100.000 (99.571)	Acc@5 100.000 (100.000)
Epoch: [120][120/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.6686 (0.6770)	Acc@1 100.000 (99.574)	Acc@5 100.000 (100.000)
Epoch: [120][130/196]	Time 0.017 (0.014)	Data 0.000 (0.005)	Loss 0.6711 (0.6768)	Acc@1 99.609 (99.574)	Acc@5 100.000 (100.000)
Epoch: [120][140/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 0.6846 (0.6767)	Acc@1 99.609 (99.562)	Acc@5 100.000 (100.000)
Epoch: [120][150/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.6776 (0.6765)	Acc@1 99.609 (99.568)	Acc@5 100.000 (100.000)
Epoch: [120][160/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 0.6664 (0.6763)	Acc@1 100.000 (99.558)	Acc@5 100.000 (100.000)
Epoch: [120][170/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.6807 (0.6762)	Acc@1 99.219 (99.548)	Acc@5 100.000 (100.000)
Epoch: [120][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6695 (0.6759)	Acc@1 99.609 (99.542)	Acc@5 100.000 (100.000)
Epoch: [120][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6644 (0.6758)	Acc@1 100.000 (99.538)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [253, 127, 3, 3] >> [253, 127, 3, 3]
[module.conv4.weight]: [256, 253, 3, 3] >> [256, 253, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [477, 511, 3, 3] >> [476, 511, 3, 3]
[module.conv8.weight]: [300, 477, 3, 3] >> [300, 476, 3, 3]
[module.fc.weight]: [100, 300] >> [100, 300]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
69.73
python src/cifar.py --workers 4 --dataset cifar100 --epochs 130 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_130.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.95M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [253, 127, 3, 3]
conv4 --> [256, 253, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [476, 511, 3, 3]
conv8 --> [300, 476, 3, 3]
fc --> [300, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8439399936, 18507456, 253
4, 17011703808, 37306368, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6724988928, 8756496, 476
8, 3948134400, 5140800, 300
fc, 11520000, 30000, 0
===================
FLOP REPORT: 28406484000000.0 52236800000.0 140370672 130592 2477 15.154779434204102

Epoch: [121 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [121][0/196]	Time 0.765 (0.765)	Data 0.177 (0.177)	Loss 0.6757 (0.6757)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [121][10/196]	Time 0.015 (0.083)	Data 0.002 (0.018)	Loss 0.6607 (0.6649)	Acc@1 100.000 (99.680)	Acc@5 100.000 (100.000)
Epoch: [121][20/196]	Time 0.014 (0.050)	Data 0.003 (0.010)	Loss 0.6671 (0.6656)	Acc@1 99.609 (99.684)	Acc@5 100.000 (100.000)
Epoch: [121][30/196]	Time 0.015 (0.039)	Data 0.002 (0.008)	Loss 0.6582 (0.6648)	Acc@1 100.000 (99.698)	Acc@5 100.000 (100.000)
Epoch: [121][40/196]	Time 0.012 (0.033)	Data 0.005 (0.007)	Loss 0.6534 (0.6646)	Acc@1 100.000 (99.733)	Acc@5 100.000 (100.000)
Epoch: [121][50/196]	Time 0.017 (0.029)	Data 0.001 (0.006)	Loss 0.6720 (0.6640)	Acc@1 99.609 (99.763)	Acc@5 100.000 (100.000)
Epoch: [121][60/196]	Time 0.011 (0.027)	Data 0.006 (0.005)	Loss 0.6595 (0.6641)	Acc@1 99.609 (99.744)	Acc@5 100.000 (100.000)
Epoch: [121][70/196]	Time 0.016 (0.025)	Data 0.000 (0.005)	Loss 0.6636 (0.6645)	Acc@1 100.000 (99.703)	Acc@5 100.000 (100.000)
Epoch: [121][80/196]	Time 0.015 (0.024)	Data 0.004 (0.004)	Loss 0.6781 (0.6654)	Acc@1 98.828 (99.662)	Acc@5 100.000 (100.000)
Epoch: [121][90/196]	Time 0.017 (0.023)	Data 0.000 (0.004)	Loss 0.6749 (0.6661)	Acc@1 99.609 (99.639)	Acc@5 100.000 (100.000)
Epoch: [121][100/196]	Time 0.012 (0.022)	Data 0.004 (0.004)	Loss 0.6519 (0.6662)	Acc@1 100.000 (99.621)	Acc@5 100.000 (100.000)
Epoch: [121][110/196]	Time 0.016 (0.021)	Data 0.000 (0.004)	Loss 0.6617 (0.6660)	Acc@1 99.609 (99.620)	Acc@5 100.000 (99.996)
Epoch: [121][120/196]	Time 0.013 (0.021)	Data 0.004 (0.004)	Loss 0.6500 (0.6660)	Acc@1 100.000 (99.600)	Acc@5 100.000 (99.997)
Epoch: [121][130/196]	Time 0.016 (0.020)	Data 0.000 (0.004)	Loss 0.6620 (0.6661)	Acc@1 100.000 (99.586)	Acc@5 100.000 (99.997)
Epoch: [121][140/196]	Time 0.012 (0.020)	Data 0.004 (0.004)	Loss 0.6684 (0.6659)	Acc@1 99.219 (99.582)	Acc@5 100.000 (99.997)
Epoch: [121][150/196]	Time 0.016 (0.019)	Data 0.001 (0.004)	Loss 0.6741 (0.6656)	Acc@1 99.219 (99.581)	Acc@5 100.000 (99.997)
Epoch: [121][160/196]	Time 0.013 (0.019)	Data 0.004 (0.004)	Loss 0.6716 (0.6654)	Acc@1 98.438 (99.566)	Acc@5 100.000 (99.998)
Epoch: [121][170/196]	Time 0.016 (0.019)	Data 0.000 (0.004)	Loss 0.6541 (0.6652)	Acc@1 99.609 (99.564)	Acc@5 100.000 (99.998)
Epoch: [121][180/196]	Time 0.012 (0.018)	Data 0.004 (0.004)	Loss 0.6718 (0.6652)	Acc@1 99.219 (99.562)	Acc@5 100.000 (99.998)
Epoch: [121][190/196]	Time 0.016 (0.018)	Data 0.000 (0.004)	Loss 0.6709 (0.6652)	Acc@1 99.219 (99.552)	Acc@5 100.000 (99.998)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [122 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [122][0/196]	Time 0.029 (0.029)	Data 0.170 (0.170)	Loss 0.6563 (0.6563)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [122][10/196]	Time 0.012 (0.016)	Data 0.005 (0.018)	Loss 0.6544 (0.6573)	Acc@1 100.000 (99.503)	Acc@5 100.000 (100.000)
Epoch: [122][20/196]	Time 0.013 (0.015)	Data 0.004 (0.010)	Loss 0.6480 (0.6576)	Acc@1 99.609 (99.535)	Acc@5 100.000 (100.000)
Epoch: [122][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 0.6654 (0.6568)	Acc@1 99.609 (99.584)	Acc@5 100.000 (100.000)
Epoch: [122][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.6563 (0.6572)	Acc@1 100.000 (99.590)	Acc@5 100.000 (100.000)
Epoch: [122][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.6535 (0.6572)	Acc@1 99.609 (99.571)	Acc@5 100.000 (100.000)
Epoch: [122][60/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.6463 (0.6565)	Acc@1 100.000 (99.590)	Acc@5 100.000 (100.000)
Epoch: [122][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.6424 (0.6558)	Acc@1 100.000 (99.631)	Acc@5 100.000 (100.000)
Epoch: [122][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.6486 (0.6554)	Acc@1 100.000 (99.629)	Acc@5 100.000 (100.000)
Epoch: [122][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.6514 (0.6548)	Acc@1 100.000 (99.652)	Acc@5 100.000 (100.000)
Epoch: [122][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.6480 (0.6545)	Acc@1 100.000 (99.656)	Acc@5 100.000 (100.000)
Epoch: [122][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.6678 (0.6541)	Acc@1 98.828 (99.659)	Acc@5 100.000 (100.000)
Epoch: [122][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.6477 (0.6539)	Acc@1 99.219 (99.645)	Acc@5 100.000 (100.000)
Epoch: [122][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6412 (0.6534)	Acc@1 100.000 (99.648)	Acc@5 100.000 (100.000)
Epoch: [122][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.6473 (0.6530)	Acc@1 100.000 (99.656)	Acc@5 100.000 (100.000)
Epoch: [122][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.6462 (0.6528)	Acc@1 99.609 (99.640)	Acc@5 100.000 (100.000)
Epoch: [122][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.6421 (0.6527)	Acc@1 99.609 (99.636)	Acc@5 100.000 (100.000)
Epoch: [122][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 0.6519 (0.6527)	Acc@1 99.219 (99.623)	Acc@5 100.000 (100.000)
Epoch: [122][180/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.6442 (0.6530)	Acc@1 100.000 (99.607)	Acc@5 100.000 (100.000)
Epoch: [122][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6608 (0.6528)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [123 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [123][0/196]	Time 0.031 (0.031)	Data 0.180 (0.180)	Loss 0.6408 (0.6408)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [123][10/196]	Time 0.015 (0.016)	Data 0.002 (0.019)	Loss 0.6447 (0.6477)	Acc@1 99.609 (99.680)	Acc@5 100.000 (100.000)
Epoch: [123][20/196]	Time 0.011 (0.015)	Data 0.004 (0.011)	Loss 0.6323 (0.6460)	Acc@1 100.000 (99.591)	Acc@5 100.000 (100.000)
Epoch: [123][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 0.6414 (0.6444)	Acc@1 100.000 (99.672)	Acc@5 100.000 (100.000)
Epoch: [123][40/196]	Time 0.012 (0.015)	Data 0.019 (0.007)	Loss 0.6368 (0.6431)	Acc@1 99.609 (99.695)	Acc@5 100.000 (100.000)
Epoch: [123][50/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 0.6356 (0.6427)	Acc@1 99.609 (99.686)	Acc@5 100.000 (100.000)
Epoch: [123][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 0.6477 (0.6431)	Acc@1 99.219 (99.622)	Acc@5 100.000 (100.000)
Epoch: [123][70/196]	Time 0.018 (0.015)	Data 0.000 (0.006)	Loss 0.6355 (0.6425)	Acc@1 100.000 (99.631)	Acc@5 100.000 (100.000)
Epoch: [123][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.6450 (0.6422)	Acc@1 99.219 (99.638)	Acc@5 100.000 (100.000)
Epoch: [123][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.6295 (0.6419)	Acc@1 100.000 (99.635)	Acc@5 100.000 (100.000)
Epoch: [123][100/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.6487 (0.6418)	Acc@1 99.219 (99.629)	Acc@5 100.000 (100.000)
Epoch: [123][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6291 (0.6419)	Acc@1 100.000 (99.623)	Acc@5 100.000 (100.000)
Epoch: [123][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.6421 (0.6422)	Acc@1 99.219 (99.603)	Acc@5 100.000 (100.000)
Epoch: [123][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.6492 (0.6423)	Acc@1 99.609 (99.600)	Acc@5 100.000 (100.000)
Epoch: [123][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.6299 (0.6421)	Acc@1 100.000 (99.607)	Acc@5 100.000 (100.000)
Epoch: [123][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6371 (0.6420)	Acc@1 100.000 (99.596)	Acc@5 100.000 (100.000)
Epoch: [123][160/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 0.6327 (0.6417)	Acc@1 99.609 (99.602)	Acc@5 100.000 (100.000)
Epoch: [123][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.6338 (0.6418)	Acc@1 99.609 (99.591)	Acc@5 100.000 (100.000)
Epoch: [123][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.6435 (0.6418)	Acc@1 99.609 (99.586)	Acc@5 100.000 (100.000)
Epoch: [123][190/196]	Time 0.020 (0.015)	Data 0.000 (0.004)	Loss 0.6341 (0.6417)	Acc@1 99.609 (99.577)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [124 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [124][0/196]	Time 0.029 (0.029)	Data 0.180 (0.180)	Loss 0.6310 (0.6310)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [124][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 0.6578 (0.6359)	Acc@1 98.438 (99.538)	Acc@5 100.000 (100.000)
Epoch: [124][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 0.6287 (0.6359)	Acc@1 100.000 (99.554)	Acc@5 100.000 (100.000)
Epoch: [124][30/196]	Time 0.011 (0.015)	Data 0.008 (0.008)	Loss 0.6557 (0.6349)	Acc@1 98.828 (99.584)	Acc@5 100.000 (100.000)
Epoch: [124][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 0.6222 (0.6360)	Acc@1 100.000 (99.552)	Acc@5 100.000 (100.000)
Epoch: [124][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.6325 (0.6350)	Acc@1 99.609 (99.563)	Acc@5 100.000 (100.000)
Epoch: [124][60/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.6286 (0.6348)	Acc@1 100.000 (99.565)	Acc@5 100.000 (100.000)
Epoch: [124][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.6329 (0.6347)	Acc@1 99.609 (99.571)	Acc@5 100.000 (100.000)
Epoch: [124][80/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 0.6383 (0.6345)	Acc@1 99.219 (99.556)	Acc@5 100.000 (100.000)
Epoch: [124][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6298 (0.6338)	Acc@1 99.609 (99.566)	Acc@5 100.000 (100.000)
Epoch: [124][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.6253 (0.6339)	Acc@1 100.000 (99.563)	Acc@5 100.000 (100.000)
Epoch: [124][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.6240 (0.6339)	Acc@1 100.000 (99.560)	Acc@5 100.000 (99.996)
Epoch: [124][120/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 0.6212 (0.6334)	Acc@1 99.609 (99.567)	Acc@5 100.000 (99.997)
Epoch: [124][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.6423 (0.6335)	Acc@1 98.828 (99.550)	Acc@5 100.000 (99.997)
Epoch: [124][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.6338 (0.6331)	Acc@1 99.609 (99.554)	Acc@5 100.000 (99.997)
Epoch: [124][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.6390 (0.6327)	Acc@1 99.219 (99.558)	Acc@5 100.000 (99.997)
Epoch: [124][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.6346 (0.6328)	Acc@1 99.219 (99.546)	Acc@5 100.000 (99.998)
Epoch: [124][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6216 (0.6329)	Acc@1 100.000 (99.543)	Acc@5 100.000 (99.998)
Epoch: [124][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.6375 (0.6326)	Acc@1 99.219 (99.542)	Acc@5 100.000 (99.998)
Epoch: [124][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6301 (0.6324)	Acc@1 99.609 (99.540)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [125 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [125][0/196]	Time 0.029 (0.029)	Data 0.169 (0.169)	Loss 0.6352 (0.6352)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [125][10/196]	Time 0.012 (0.015)	Data 0.005 (0.018)	Loss 0.6189 (0.6278)	Acc@1 99.609 (99.432)	Acc@5 100.000 (100.000)
Epoch: [125][20/196]	Time 0.013 (0.015)	Data 0.004 (0.011)	Loss 0.6101 (0.6264)	Acc@1 100.000 (99.498)	Acc@5 100.000 (100.000)
Epoch: [125][30/196]	Time 0.014 (0.015)	Data 0.002 (0.008)	Loss 0.6159 (0.6263)	Acc@1 99.609 (99.471)	Acc@5 100.000 (100.000)
Epoch: [125][40/196]	Time 0.011 (0.015)	Data 0.005 (0.007)	Loss 0.6352 (0.6265)	Acc@1 99.219 (99.476)	Acc@5 100.000 (100.000)
Epoch: [125][50/196]	Time 0.013 (0.015)	Data 0.003 (0.006)	Loss 0.6207 (0.6252)	Acc@1 99.609 (99.517)	Acc@5 100.000 (100.000)
Epoch: [125][60/196]	Time 0.010 (0.014)	Data 0.007 (0.006)	Loss 0.6491 (0.6250)	Acc@1 99.219 (99.520)	Acc@5 100.000 (100.000)
Epoch: [125][70/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.6173 (0.6242)	Acc@1 100.000 (99.543)	Acc@5 100.000 (100.000)
Epoch: [125][80/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.6289 (0.6234)	Acc@1 99.219 (99.566)	Acc@5 100.000 (100.000)
Epoch: [125][90/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.6159 (0.6226)	Acc@1 100.000 (99.592)	Acc@5 100.000 (100.000)
Epoch: [125][100/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.6243 (0.6223)	Acc@1 98.828 (99.594)	Acc@5 100.000 (100.000)
Epoch: [125][110/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 0.6340 (0.6230)	Acc@1 98.438 (99.560)	Acc@5 100.000 (100.000)
Epoch: [125][120/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.6195 (0.6229)	Acc@1 99.609 (99.558)	Acc@5 100.000 (100.000)
Epoch: [125][130/196]	Time 0.011 (0.014)	Data 0.017 (0.005)	Loss 0.6141 (0.6227)	Acc@1 99.609 (99.562)	Acc@5 100.000 (100.000)
Epoch: [125][140/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.6251 (0.6228)	Acc@1 99.219 (99.554)	Acc@5 100.000 (100.000)
Epoch: [125][150/196]	Time 0.013 (0.014)	Data 0.016 (0.005)	Loss 0.6130 (0.6225)	Acc@1 99.609 (99.565)	Acc@5 100.000 (100.000)
Epoch: [125][160/196]	Time 0.017 (0.014)	Data 0.001 (0.005)	Loss 0.6232 (0.6224)	Acc@1 99.219 (99.556)	Acc@5 100.000 (100.000)
Epoch: [125][170/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 0.6092 (0.6228)	Acc@1 100.000 (99.534)	Acc@5 100.000 (100.000)
Epoch: [125][180/196]	Time 0.016 (0.014)	Data 0.001 (0.004)	Loss 0.6248 (0.6225)	Acc@1 99.219 (99.536)	Acc@5 100.000 (100.000)
Epoch: [125][190/196]	Time 0.011 (0.014)	Data 0.005 (0.004)	Loss 0.6079 (0.6227)	Acc@1 100.000 (99.523)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [126 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [126][0/196]	Time 0.030 (0.030)	Data 0.176 (0.176)	Loss 0.6094 (0.6094)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [126][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.6198 (0.6161)	Acc@1 98.828 (99.645)	Acc@5 100.000 (99.964)
Epoch: [126][20/196]	Time 0.015 (0.015)	Data 0.003 (0.011)	Loss 0.6178 (0.6141)	Acc@1 100.000 (99.572)	Acc@5 100.000 (99.981)
Epoch: [126][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.6255 (0.6149)	Acc@1 99.219 (99.534)	Acc@5 100.000 (99.987)
Epoch: [126][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 0.6140 (0.6139)	Acc@1 99.609 (99.562)	Acc@5 100.000 (99.990)
Epoch: [126][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.6009 (0.6134)	Acc@1 100.000 (99.579)	Acc@5 100.000 (99.992)
Epoch: [126][60/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.5978 (0.6129)	Acc@1 100.000 (99.597)	Acc@5 100.000 (99.994)
Epoch: [126][70/196]	Time 0.014 (0.015)	Data 0.000 (0.005)	Loss 0.6201 (0.6127)	Acc@1 99.609 (99.587)	Acc@5 100.000 (99.994)
Epoch: [126][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.6035 (0.6124)	Acc@1 100.000 (99.590)	Acc@5 100.000 (99.995)
Epoch: [126][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6149 (0.6121)	Acc@1 99.219 (99.588)	Acc@5 100.000 (99.996)
Epoch: [126][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.5985 (0.6115)	Acc@1 100.000 (99.602)	Acc@5 100.000 (99.996)
Epoch: [126][110/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 0.6207 (0.6112)	Acc@1 99.219 (99.616)	Acc@5 100.000 (99.996)
Epoch: [126][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.6215 (0.6110)	Acc@1 99.609 (99.619)	Acc@5 100.000 (99.997)
Epoch: [126][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.6005 (0.6108)	Acc@1 100.000 (99.615)	Acc@5 100.000 (99.997)
Epoch: [126][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.6176 (0.6107)	Acc@1 99.219 (99.615)	Acc@5 100.000 (99.997)
Epoch: [126][150/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 0.6143 (0.6104)	Acc@1 99.609 (99.622)	Acc@5 100.000 (99.997)
Epoch: [126][160/196]	Time 0.011 (0.014)	Data 0.008 (0.004)	Loss 0.6019 (0.6101)	Acc@1 99.609 (99.624)	Acc@5 100.000 (99.998)
Epoch: [126][170/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 0.6156 (0.6103)	Acc@1 98.828 (99.614)	Acc@5 100.000 (99.998)
Epoch: [126][180/196]	Time 0.013 (0.014)	Data 0.008 (0.004)	Loss 0.6001 (0.6100)	Acc@1 99.609 (99.607)	Acc@5 100.000 (99.998)
Epoch: [126][190/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 0.6145 (0.6099)	Acc@1 98.828 (99.595)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [127 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [127][0/196]	Time 0.031 (0.031)	Data 0.180 (0.180)	Loss 0.5944 (0.5944)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [127][10/196]	Time 0.014 (0.016)	Data 0.002 (0.019)	Loss 0.6196 (0.6044)	Acc@1 99.219 (99.574)	Acc@5 100.000 (100.000)
Epoch: [127][20/196]	Time 0.012 (0.015)	Data 0.005 (0.011)	Loss 0.5984 (0.6027)	Acc@1 99.609 (99.665)	Acc@5 100.000 (100.000)
Epoch: [127][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 0.6206 (0.6020)	Acc@1 99.609 (99.660)	Acc@5 100.000 (100.000)
Epoch: [127][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 0.6041 (0.6018)	Acc@1 99.609 (99.628)	Acc@5 100.000 (100.000)
Epoch: [127][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.6052 (0.6013)	Acc@1 99.219 (99.640)	Acc@5 100.000 (100.000)
Epoch: [127][60/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.5933 (0.6014)	Acc@1 100.000 (99.641)	Acc@5 100.000 (100.000)
Epoch: [127][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.5974 (0.6017)	Acc@1 99.609 (99.604)	Acc@5 100.000 (100.000)
Epoch: [127][80/196]	Time 0.011 (0.015)	Data 0.014 (0.005)	Loss 0.6006 (0.6014)	Acc@1 99.609 (99.590)	Acc@5 100.000 (100.000)
Epoch: [127][90/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 0.6192 (0.6021)	Acc@1 99.219 (99.562)	Acc@5 100.000 (100.000)
Epoch: [127][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.6307 (0.6023)	Acc@1 98.438 (99.547)	Acc@5 100.000 (100.000)
Epoch: [127][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.5955 (0.6024)	Acc@1 100.000 (99.543)	Acc@5 100.000 (100.000)
Epoch: [127][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.6053 (0.6022)	Acc@1 98.828 (99.548)	Acc@5 100.000 (100.000)
Epoch: [127][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5937 (0.6022)	Acc@1 99.609 (99.538)	Acc@5 100.000 (100.000)
Epoch: [127][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.6047 (0.6020)	Acc@1 99.609 (99.543)	Acc@5 100.000 (100.000)
Epoch: [127][150/196]	Time 0.024 (0.015)	Data 0.001 (0.004)	Loss 0.5961 (0.6015)	Acc@1 99.609 (99.560)	Acc@5 100.000 (100.000)
Epoch: [127][160/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 0.5892 (0.6013)	Acc@1 100.000 (99.549)	Acc@5 100.000 (100.000)
Epoch: [127][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5955 (0.6011)	Acc@1 100.000 (99.555)	Acc@5 100.000 (100.000)
Epoch: [127][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.5939 (0.6010)	Acc@1 99.219 (99.545)	Acc@5 100.000 (100.000)
Epoch: [127][190/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 0.5990 (0.6007)	Acc@1 99.609 (99.544)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [128 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [128][0/196]	Time 0.030 (0.030)	Data 0.183 (0.183)	Loss 0.5896 (0.5896)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [128][10/196]	Time 0.016 (0.016)	Data 0.000 (0.019)	Loss 0.5925 (0.5892)	Acc@1 99.609 (99.751)	Acc@5 100.000 (100.000)
Epoch: [128][20/196]	Time 0.013 (0.015)	Data 0.003 (0.011)	Loss 0.6053 (0.5907)	Acc@1 99.219 (99.702)	Acc@5 100.000 (100.000)
Epoch: [128][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.5959 (0.5926)	Acc@1 99.609 (99.559)	Acc@5 100.000 (100.000)
Epoch: [128][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.5987 (0.5921)	Acc@1 99.219 (99.562)	Acc@5 100.000 (100.000)
Epoch: [128][50/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.5908 (0.5922)	Acc@1 99.609 (99.556)	Acc@5 100.000 (100.000)
Epoch: [128][60/196]	Time 0.011 (0.014)	Data 0.008 (0.006)	Loss 0.5843 (0.5918)	Acc@1 100.000 (99.571)	Acc@5 100.000 (100.000)
Epoch: [128][70/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.5838 (0.5915)	Acc@1 99.609 (99.593)	Acc@5 100.000 (100.000)
Epoch: [128][80/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.6025 (0.5910)	Acc@1 99.219 (99.614)	Acc@5 100.000 (100.000)
Epoch: [128][90/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.5971 (0.5912)	Acc@1 98.828 (99.588)	Acc@5 100.000 (100.000)
Epoch: [128][100/196]	Time 0.012 (0.014)	Data 0.002 (0.005)	Loss 0.5962 (0.5908)	Acc@1 98.828 (99.594)	Acc@5 100.000 (100.000)
Epoch: [128][110/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 0.5830 (0.5905)	Acc@1 99.219 (99.592)	Acc@5 100.000 (100.000)
Epoch: [128][120/196]	Time 0.011 (0.014)	Data 0.005 (0.005)	Loss 0.5859 (0.5903)	Acc@1 99.609 (99.596)	Acc@5 100.000 (100.000)
Epoch: [128][130/196]	Time 0.011 (0.014)	Data 0.016 (0.005)	Loss 0.5887 (0.5898)	Acc@1 98.828 (99.606)	Acc@5 100.000 (100.000)
Epoch: [128][140/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.5894 (0.5899)	Acc@1 99.609 (99.598)	Acc@5 100.000 (100.000)
Epoch: [128][150/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 0.5848 (0.5896)	Acc@1 99.609 (99.596)	Acc@5 100.000 (100.000)
Epoch: [128][160/196]	Time 0.013 (0.014)	Data 0.002 (0.005)	Loss 0.5851 (0.5900)	Acc@1 100.000 (99.578)	Acc@5 100.000 (100.000)
Epoch: [128][170/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.5872 (0.5901)	Acc@1 99.609 (99.564)	Acc@5 100.000 (100.000)
Epoch: [128][180/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.5987 (0.5900)	Acc@1 99.219 (99.566)	Acc@5 100.000 (100.000)
Epoch: [128][190/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.5924 (0.5898)	Acc@1 99.219 (99.564)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [129 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [129][0/196]	Time 0.030 (0.030)	Data 0.187 (0.187)	Loss 0.5893 (0.5893)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [129][10/196]	Time 0.017 (0.016)	Data 0.001 (0.019)	Loss 0.5797 (0.5814)	Acc@1 99.609 (99.751)	Acc@5 100.000 (100.000)
Epoch: [129][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 0.6015 (0.5808)	Acc@1 99.219 (99.721)	Acc@5 100.000 (100.000)
Epoch: [129][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 0.5714 (0.5799)	Acc@1 100.000 (99.710)	Acc@5 100.000 (100.000)
Epoch: [129][40/196]	Time 0.011 (0.015)	Data 0.010 (0.007)	Loss 0.5858 (0.5811)	Acc@1 99.609 (99.638)	Acc@5 100.000 (100.000)
Epoch: [129][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.5763 (0.5814)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [129][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 0.5830 (0.5815)	Acc@1 100.000 (99.597)	Acc@5 100.000 (100.000)
Epoch: [129][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.5924 (0.5816)	Acc@1 99.219 (99.587)	Acc@5 100.000 (100.000)
Epoch: [129][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 0.5818 (0.5816)	Acc@1 98.828 (99.580)	Acc@5 100.000 (100.000)
Epoch: [129][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5753 (0.5817)	Acc@1 100.000 (99.575)	Acc@5 100.000 (100.000)
Epoch: [129][100/196]	Time 0.012 (0.015)	Data 0.012 (0.004)	Loss 0.5755 (0.5815)	Acc@1 99.609 (99.559)	Acc@5 100.000 (100.000)
Epoch: [129][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.5873 (0.5817)	Acc@1 99.219 (99.550)	Acc@5 100.000 (100.000)
Epoch: [129][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.5770 (0.5818)	Acc@1 100.000 (99.542)	Acc@5 100.000 (100.000)
Epoch: [129][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5786 (0.5818)	Acc@1 99.609 (99.550)	Acc@5 100.000 (100.000)
Epoch: [129][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.5814 (0.5817)	Acc@1 99.609 (99.540)	Acc@5 100.000 (100.000)
Epoch: [129][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5783 (0.5818)	Acc@1 99.609 (99.537)	Acc@5 100.000 (100.000)
Epoch: [129][160/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 0.5920 (0.5820)	Acc@1 98.828 (99.520)	Acc@5 100.000 (100.000)
Epoch: [129][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5957 (0.5821)	Acc@1 98.828 (99.516)	Acc@5 100.000 (100.000)
Epoch: [129][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.5829 (0.5820)	Acc@1 99.219 (99.514)	Acc@5 100.000 (100.000)
Epoch: [129][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5859 (0.5824)	Acc@1 99.609 (99.497)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [130 | 130] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [253, 127, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [300, 476, 3, 3]
Epoch: [130][0/196]	Time 0.032 (0.032)	Data 0.179 (0.179)	Loss 0.5947 (0.5947)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [130][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.5675 (0.5797)	Acc@1 99.609 (99.254)	Acc@5 100.000 (100.000)
Epoch: [130][20/196]	Time 0.011 (0.015)	Data 0.012 (0.011)	Loss 0.5793 (0.5791)	Acc@1 99.609 (99.312)	Acc@5 100.000 (100.000)
Epoch: [130][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.5662 (0.5760)	Acc@1 99.609 (99.458)	Acc@5 100.000 (100.000)
Epoch: [130][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.5668 (0.5756)	Acc@1 100.000 (99.514)	Acc@5 100.000 (100.000)
Epoch: [130][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.5774 (0.5762)	Acc@1 99.609 (99.464)	Acc@5 100.000 (100.000)
Epoch: [130][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.5712 (0.5759)	Acc@1 99.219 (99.468)	Acc@5 100.000 (100.000)
Epoch: [130][70/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 0.5844 (0.5763)	Acc@1 98.047 (99.439)	Acc@5 100.000 (100.000)
Epoch: [130][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.5660 (0.5763)	Acc@1 99.609 (99.431)	Acc@5 100.000 (100.000)
Epoch: [130][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.5721 (0.5767)	Acc@1 99.609 (99.412)	Acc@5 100.000 (100.000)
Epoch: [130][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.5809 (0.5764)	Acc@1 99.219 (99.408)	Acc@5 100.000 (100.000)
Epoch: [130][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5838 (0.5764)	Acc@1 98.438 (99.398)	Acc@5 100.000 (100.000)
Epoch: [130][120/196]	Time 0.013 (0.015)	Data 0.006 (0.004)	Loss 0.5726 (0.5763)	Acc@1 99.609 (99.400)	Acc@5 100.000 (100.000)
Epoch: [130][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5843 (0.5764)	Acc@1 99.219 (99.398)	Acc@5 100.000 (100.000)
Epoch: [130][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.5705 (0.5765)	Acc@1 99.609 (99.388)	Acc@5 100.000 (100.000)
Epoch: [130][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5695 (0.5767)	Acc@1 99.609 (99.382)	Acc@5 100.000 (100.000)
Epoch: [130][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.5602 (0.5763)	Acc@1 100.000 (99.393)	Acc@5 100.000 (100.000)
Epoch: [130][170/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 0.5835 (0.5764)	Acc@1 99.219 (99.386)	Acc@5 100.000 (100.000)
Epoch: [130][180/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 0.5721 (0.5762)	Acc@1 99.609 (99.394)	Acc@5 100.000 (100.000)
Epoch: [130][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5865 (0.5758)	Acc@1 98.047 (99.393)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [253, 127, 3, 3] >> [252, 127, 3, 3]
[module.conv4.weight]: [256, 253, 3, 3] >> [256, 252, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [476, 511, 3, 3] >> [476, 511, 3, 3]
[module.conv8.weight]: [300, 476, 3, 3] >> [299, 476, 3, 3]
[module.fc.weight]: [100, 300] >> [100, 299]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
69.73
python src/cifar.py --workers 4 --dataset cifar100 --epochs 140 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_140.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.94M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [252, 127, 3, 3]
conv4 --> [256, 252, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [476, 511, 3, 3]
conv8 --> [299, 476, 3, 3]
fc --> [299, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8406042624, 18434304, 252
4, 16944463872, 37158912, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6724988928, 8756496, 476
8, 3934973952, 5123664, 299
fc, 11481600, 29900, 0
===================
FLOP REPORT: 28362032400000.0 52209600000.0 140132828 130524 2475 15.139837265014648

Epoch: [131 | 140] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [131][0/196]	Time 0.741 (0.741)	Data 0.168 (0.168)	Loss 0.5826 (0.5826)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [131][10/196]	Time 0.014 (0.081)	Data 0.002 (0.017)	Loss 0.5713 (0.5704)	Acc@1 99.219 (99.254)	Acc@5 100.000 (100.000)
Epoch: [131][20/196]	Time 0.014 (0.049)	Data 0.003 (0.010)	Loss 0.5804 (0.5706)	Acc@1 99.219 (99.349)	Acc@5 100.000 (100.000)
Epoch: [131][30/196]	Time 0.014 (0.038)	Data 0.002 (0.008)	Loss 0.5647 (0.5713)	Acc@1 99.609 (99.269)	Acc@5 100.000 (100.000)
Epoch: [131][40/196]	Time 0.014 (0.032)	Data 0.003 (0.006)	Loss 0.5652 (0.5703)	Acc@1 99.609 (99.333)	Acc@5 100.000 (100.000)
Epoch: [131][50/196]	Time 0.016 (0.029)	Data 0.000 (0.006)	Loss 0.5651 (0.5702)	Acc@1 99.609 (99.341)	Acc@5 100.000 (100.000)
Epoch: [131][60/196]	Time 0.011 (0.026)	Data 0.011 (0.006)	Loss 0.5686 (0.5703)	Acc@1 99.609 (99.360)	Acc@5 100.000 (100.000)
Epoch: [131][70/196]	Time 0.017 (0.025)	Data 0.000 (0.005)	Loss 0.5910 (0.5706)	Acc@1 98.828 (99.362)	Acc@5 100.000 (100.000)
Epoch: [131][80/196]	Time 0.011 (0.024)	Data 0.009 (0.005)	Loss 0.5710 (0.5705)	Acc@1 99.219 (99.359)	Acc@5 100.000 (100.000)
Epoch: [131][90/196]	Time 0.016 (0.023)	Data 0.001 (0.005)	Loss 0.5551 (0.5704)	Acc@1 100.000 (99.356)	Acc@5 100.000 (100.000)
Epoch: [131][100/196]	Time 0.011 (0.022)	Data 0.020 (0.005)	Loss 0.5894 (0.5708)	Acc@1 98.828 (99.339)	Acc@5 100.000 (99.996)
Epoch: [131][110/196]	Time 0.016 (0.021)	Data 0.001 (0.005)	Loss 0.5784 (0.5706)	Acc@1 99.219 (99.338)	Acc@5 100.000 (99.996)
Epoch: [131][120/196]	Time 0.011 (0.021)	Data 0.011 (0.005)	Loss 0.5646 (0.5704)	Acc@1 99.609 (99.345)	Acc@5 100.000 (99.997)
Epoch: [131][130/196]	Time 0.017 (0.020)	Data 0.001 (0.005)	Loss 0.5774 (0.5704)	Acc@1 98.828 (99.329)	Acc@5 100.000 (99.997)
Epoch: [131][140/196]	Time 0.011 (0.020)	Data 0.012 (0.005)	Loss 0.5916 (0.5706)	Acc@1 97.656 (99.316)	Acc@5 100.000 (99.997)
Epoch: [131][150/196]	Time 0.016 (0.019)	Data 0.001 (0.005)	Loss 0.5521 (0.5702)	Acc@1 99.609 (99.322)	Acc@5 100.000 (99.997)
Epoch: [131][160/196]	Time 0.011 (0.019)	Data 0.009 (0.005)	Loss 0.5624 (0.5702)	Acc@1 98.828 (99.313)	Acc@5 100.000 (99.998)
Epoch: [131][170/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 0.5514 (0.5700)	Acc@1 100.000 (99.319)	Acc@5 100.000 (99.998)
Epoch: [131][180/196]	Time 0.012 (0.019)	Data 0.018 (0.004)	Loss 0.6055 (0.5701)	Acc@1 98.438 (99.307)	Acc@5 100.000 (99.998)
Epoch: [131][190/196]	Time 0.017 (0.018)	Data 0.000 (0.004)	Loss 0.5650 (0.5697)	Acc@1 99.609 (99.315)	Acc@5 100.000 (99.998)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [132 | 140] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [132][0/196]	Time 0.028 (0.028)	Data 0.176 (0.176)	Loss 0.5763 (0.5763)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [132][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.5551 (0.5631)	Acc@1 99.609 (99.290)	Acc@5 100.000 (100.000)
Epoch: [132][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 0.5702 (0.5665)	Acc@1 98.828 (99.182)	Acc@5 100.000 (99.981)
Epoch: [132][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.5582 (0.5656)	Acc@1 100.000 (99.269)	Acc@5 100.000 (99.987)
Epoch: [132][40/196]	Time 0.012 (0.015)	Data 0.010 (0.007)	Loss 0.5620 (0.5647)	Acc@1 99.219 (99.266)	Acc@5 100.000 (99.990)
Epoch: [132][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.5645 (0.5639)	Acc@1 99.219 (99.311)	Acc@5 100.000 (99.992)
Epoch: [132][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.5658 (0.5639)	Acc@1 99.219 (99.315)	Acc@5 100.000 (99.994)
Epoch: [132][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5617 (0.5634)	Acc@1 99.609 (99.345)	Acc@5 100.000 (99.994)
Epoch: [132][80/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 0.5575 (0.5630)	Acc@1 99.609 (99.368)	Acc@5 100.000 (99.995)
Epoch: [132][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5742 (0.5635)	Acc@1 98.828 (99.348)	Acc@5 100.000 (99.996)
Epoch: [132][100/196]	Time 0.011 (0.015)	Data 0.014 (0.005)	Loss 0.5492 (0.5632)	Acc@1 99.219 (99.343)	Acc@5 100.000 (99.996)
Epoch: [132][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5531 (0.5630)	Acc@1 100.000 (99.345)	Acc@5 100.000 (99.996)
Epoch: [132][120/196]	Time 0.011 (0.015)	Data 0.015 (0.005)	Loss 0.5531 (0.5625)	Acc@1 99.609 (99.358)	Acc@5 100.000 (99.997)
Epoch: [132][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5662 (0.5627)	Acc@1 99.219 (99.344)	Acc@5 100.000 (99.997)
Epoch: [132][140/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.6068 (0.5631)	Acc@1 98.438 (99.341)	Acc@5 100.000 (99.997)
Epoch: [132][150/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 0.5549 (0.5630)	Acc@1 99.609 (99.343)	Acc@5 100.000 (99.997)
Epoch: [132][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.5647 (0.5628)	Acc@1 98.828 (99.338)	Acc@5 100.000 (99.998)
Epoch: [132][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5587 (0.5626)	Acc@1 100.000 (99.347)	Acc@5 100.000 (99.998)
Epoch: [132][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.5605 (0.5625)	Acc@1 98.828 (99.324)	Acc@5 100.000 (99.998)
Epoch: [132][190/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5593 (0.5624)	Acc@1 100.000 (99.333)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [133 | 140] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [133][0/196]	Time 0.029 (0.029)	Data 0.178 (0.178)	Loss 0.5451 (0.5451)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [133][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.5451 (0.5506)	Acc@1 99.609 (99.503)	Acc@5 100.000 (100.000)
Epoch: [133][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 0.5378 (0.5518)	Acc@1 100.000 (99.516)	Acc@5 100.000 (100.000)
Epoch: [133][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 0.5465 (0.5516)	Acc@1 100.000 (99.496)	Acc@5 100.000 (100.000)
Epoch: [133][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 0.5469 (0.5523)	Acc@1 99.609 (99.476)	Acc@5 100.000 (100.000)
Epoch: [133][50/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 0.5419 (0.5521)	Acc@1 100.000 (99.479)	Acc@5 100.000 (100.000)
Epoch: [133][60/196]	Time 0.011 (0.015)	Data 0.011 (0.006)	Loss 0.5466 (0.5532)	Acc@1 100.000 (99.411)	Acc@5 100.000 (100.000)
Epoch: [133][70/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.5631 (0.5539)	Acc@1 98.828 (99.389)	Acc@5 100.000 (100.000)
Epoch: [133][80/196]	Time 0.011 (0.015)	Data 0.022 (0.006)	Loss 0.5533 (0.5546)	Acc@1 99.219 (99.334)	Acc@5 100.000 (100.000)
Epoch: [133][90/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 0.5643 (0.5545)	Acc@1 98.438 (99.335)	Acc@5 100.000 (100.000)
Epoch: [133][100/196]	Time 0.011 (0.015)	Data 0.018 (0.006)	Loss 0.5518 (0.5552)	Acc@1 99.219 (99.284)	Acc@5 100.000 (100.000)
Epoch: [133][110/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.5428 (0.5547)	Acc@1 99.609 (99.286)	Acc@5 100.000 (100.000)
Epoch: [133][120/196]	Time 0.012 (0.015)	Data 0.012 (0.006)	Loss 0.5407 (0.5541)	Acc@1 99.609 (99.306)	Acc@5 100.000 (100.000)
Epoch: [133][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5447 (0.5541)	Acc@1 99.219 (99.305)	Acc@5 100.000 (100.000)
Epoch: [133][140/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 0.5397 (0.5539)	Acc@1 99.609 (99.316)	Acc@5 100.000 (100.000)
Epoch: [133][150/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5476 (0.5539)	Acc@1 99.219 (99.312)	Acc@5 100.000 (100.000)
Epoch: [133][160/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 0.5479 (0.5541)	Acc@1 99.219 (99.292)	Acc@5 100.000 (100.000)
Epoch: [133][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5482 (0.5537)	Acc@1 99.609 (99.306)	Acc@5 100.000 (100.000)
Epoch: [133][180/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.5465 (0.5538)	Acc@1 99.219 (99.312)	Acc@5 100.000 (100.000)
Epoch: [133][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5379 (0.5535)	Acc@1 99.609 (99.313)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [134 | 140] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [134][0/196]	Time 0.027 (0.027)	Data 0.171 (0.171)	Loss 0.5356 (0.5356)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [134][10/196]	Time 0.016 (0.016)	Data 0.001 (0.017)	Loss 0.5452 (0.5463)	Acc@1 98.438 (99.396)	Acc@5 100.000 (100.000)
Epoch: [134][20/196]	Time 0.011 (0.015)	Data 0.013 (0.011)	Loss 0.5625 (0.5493)	Acc@1 98.438 (99.275)	Acc@5 100.000 (100.000)
Epoch: [134][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.5420 (0.5500)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [134][40/196]	Time 0.011 (0.014)	Data 0.010 (0.007)	Loss 0.5615 (0.5498)	Acc@1 98.828 (99.219)	Acc@5 100.000 (100.000)
Epoch: [134][50/196]	Time 0.016 (0.014)	Data 0.000 (0.007)	Loss 0.5379 (0.5509)	Acc@1 100.000 (99.196)	Acc@5 100.000 (100.000)
Epoch: [134][60/196]	Time 0.011 (0.014)	Data 0.006 (0.006)	Loss 0.5626 (0.5499)	Acc@1 98.828 (99.212)	Acc@5 100.000 (100.000)
Epoch: [134][70/196]	Time 0.012 (0.014)	Data 0.005 (0.006)	Loss 0.5652 (0.5496)	Acc@1 98.438 (99.224)	Acc@5 100.000 (100.000)
Epoch: [134][80/196]	Time 0.011 (0.014)	Data 0.010 (0.006)	Loss 0.5607 (0.5493)	Acc@1 99.609 (99.243)	Acc@5 100.000 (100.000)
Epoch: [134][90/196]	Time 0.013 (0.014)	Data 0.004 (0.006)	Loss 0.5540 (0.5493)	Acc@1 98.828 (99.245)	Acc@5 100.000 (100.000)
Epoch: [134][100/196]	Time 0.015 (0.014)	Data 0.002 (0.006)	Loss 0.5469 (0.5490)	Acc@1 99.609 (99.246)	Acc@5 100.000 (100.000)
Epoch: [134][110/196]	Time 0.011 (0.014)	Data 0.015 (0.006)	Loss 0.5646 (0.5489)	Acc@1 98.438 (99.250)	Acc@5 100.000 (100.000)
Epoch: [134][120/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.5851 (0.5494)	Acc@1 98.438 (99.225)	Acc@5 100.000 (100.000)
Epoch: [134][130/196]	Time 0.011 (0.014)	Data 0.012 (0.005)	Loss 0.5338 (0.5490)	Acc@1 100.000 (99.237)	Acc@5 100.000 (100.000)
Epoch: [134][140/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.5605 (0.5489)	Acc@1 99.219 (99.235)	Acc@5 100.000 (100.000)
Epoch: [134][150/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.5483 (0.5494)	Acc@1 99.219 (99.201)	Acc@5 100.000 (100.000)
Epoch: [134][160/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.5550 (0.5501)	Acc@1 99.219 (99.178)	Acc@5 100.000 (100.000)
Epoch: [134][170/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.5392 (0.5504)	Acc@1 99.219 (99.168)	Acc@5 100.000 (100.000)
Epoch: [134][180/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.5430 (0.5504)	Acc@1 99.609 (99.160)	Acc@5 100.000 (100.000)
Epoch: [134][190/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 0.5453 (0.5502)	Acc@1 99.609 (99.168)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [135 | 140] LR: 0.010000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [135][0/196]	Time 0.027 (0.027)	Data 0.172 (0.172)	Loss 0.5344 (0.5344)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [135][10/196]	Time 0.018 (0.016)	Data 0.000 (0.018)	Loss 0.5388 (0.5407)	Acc@1 99.219 (99.183)	Acc@5 100.000 (100.000)
Epoch: [135][20/196]	Time 0.011 (0.015)	Data 0.011 (0.011)	Loss 0.5348 (0.5409)	Acc@1 99.219 (99.275)	Acc@5 100.000 (100.000)
Epoch: [135][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 0.5545 (0.5424)	Acc@1 98.438 (99.244)	Acc@5 100.000 (100.000)
Epoch: [135][40/196]	Time 0.012 (0.015)	Data 0.010 (0.007)	Loss 0.5331 (0.5409)	Acc@1 100.000 (99.247)	Acc@5 100.000 (99.990)
Epoch: [135][50/196]	Time 0.012 (0.015)	Data 0.004 (0.007)	Loss 0.5381 (0.5419)	Acc@1 99.219 (99.203)	Acc@5 100.000 (99.992)
Epoch: [135][60/196]	Time 0.011 (0.014)	Data 0.011 (0.006)	Loss 0.5792 (0.5428)	Acc@1 98.828 (99.180)	Acc@5 100.000 (99.994)
Epoch: [135][70/196]	Time 0.013 (0.014)	Data 0.004 (0.006)	Loss 0.5566 (0.5435)	Acc@1 98.438 (99.142)	Acc@5 100.000 (99.994)
Epoch: [135][80/196]	Time 0.012 (0.014)	Data 0.011 (0.006)	Loss 0.5483 (0.5435)	Acc@1 98.828 (99.151)	Acc@5 100.000 (99.995)
Epoch: [135][90/196]	Time 0.011 (0.014)	Data 0.006 (0.006)	Loss 0.5519 (0.5432)	Acc@1 99.609 (99.159)	Acc@5 100.000 (99.996)
Epoch: [135][100/196]	Time 0.011 (0.014)	Data 0.013 (0.005)	Loss 0.5416 (0.5439)	Acc@1 99.219 (99.138)	Acc@5 100.000 (99.996)
Epoch: [135][110/196]	Time 0.016 (0.014)	Data 0.004 (0.005)	Loss 0.5210 (0.5436)	Acc@1 100.000 (99.148)	Acc@5 100.000 (99.996)
Epoch: [135][120/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.5520 (0.5436)	Acc@1 98.828 (99.148)	Acc@5 100.000 (99.997)
Epoch: [135][130/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 0.5352 (0.5435)	Acc@1 99.219 (99.141)	Acc@5 100.000 (99.997)
Epoch: [135][140/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.5755 (0.5434)	Acc@1 98.438 (99.138)	Acc@5 100.000 (99.997)
Epoch: [135][150/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.5345 (0.5437)	Acc@1 99.609 (99.133)	Acc@5 100.000 (99.997)
Epoch: [135][160/196]	Time 0.011 (0.014)	Data 0.011 (0.005)	Loss 0.5374 (0.5436)	Acc@1 99.219 (99.134)	Acc@5 100.000 (99.998)
Epoch: [135][170/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 0.5542 (0.5438)	Acc@1 98.047 (99.118)	Acc@5 100.000 (99.998)
Epoch: [135][180/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.5418 (0.5438)	Acc@1 98.828 (99.113)	Acc@5 100.000 (99.998)
Epoch: [135][190/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 0.5372 (0.5438)	Acc@1 99.609 (99.108)	Acc@5 100.000 (99.996)
[INFO] Storing checkpoint...

Epoch: [136 | 140] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [136][0/196]	Time 0.028 (0.028)	Data 0.181 (0.181)	Loss 0.5547 (0.5547)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [136][10/196]	Time 0.018 (0.016)	Data 0.000 (0.018)	Loss 0.5223 (0.5346)	Acc@1 100.000 (99.325)	Acc@5 100.000 (100.000)
Epoch: [136][20/196]	Time 0.011 (0.016)	Data 0.010 (0.011)	Loss 0.5267 (0.5323)	Acc@1 99.609 (99.405)	Acc@5 100.000 (100.000)
Epoch: [136][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 0.5357 (0.5313)	Acc@1 98.828 (99.383)	Acc@5 100.000 (100.000)
Epoch: [136][40/196]	Time 0.011 (0.015)	Data 0.010 (0.007)	Loss 0.5200 (0.5307)	Acc@1 99.609 (99.409)	Acc@5 100.000 (100.000)
Epoch: [136][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.5181 (0.5316)	Acc@1 100.000 (99.395)	Acc@5 100.000 (99.992)
Epoch: [136][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.5336 (0.5307)	Acc@1 99.219 (99.443)	Acc@5 100.000 (99.994)
Epoch: [136][70/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.5198 (0.5297)	Acc@1 99.219 (99.461)	Acc@5 100.000 (99.994)
Epoch: [136][80/196]	Time 0.011 (0.015)	Data 0.015 (0.005)	Loss 0.5304 (0.5292)	Acc@1 99.219 (99.465)	Acc@5 100.000 (99.995)
Epoch: [136][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.5337 (0.5286)	Acc@1 99.219 (99.463)	Acc@5 100.000 (99.996)
Epoch: [136][100/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.5313 (0.5281)	Acc@1 98.828 (99.470)	Acc@5 100.000 (99.996)
Epoch: [136][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5356 (0.5278)	Acc@1 99.219 (99.476)	Acc@5 100.000 (99.996)
Epoch: [136][120/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.5277 (0.5277)	Acc@1 99.219 (99.467)	Acc@5 100.000 (99.997)
Epoch: [136][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.5191 (0.5271)	Acc@1 99.609 (99.484)	Acc@5 100.000 (99.997)
Epoch: [136][140/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 0.5089 (0.5265)	Acc@1 100.000 (99.504)	Acc@5 100.000 (99.997)
Epoch: [136][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5252 (0.5263)	Acc@1 99.609 (99.508)	Acc@5 100.000 (99.997)
Epoch: [136][160/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.5161 (0.5259)	Acc@1 100.000 (99.515)	Acc@5 100.000 (99.998)
Epoch: [136][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5411 (0.5260)	Acc@1 99.609 (99.509)	Acc@5 100.000 (99.998)
Epoch: [136][180/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.5265 (0.5257)	Acc@1 99.219 (99.510)	Acc@5 100.000 (99.998)
Epoch: [136][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5184 (0.5255)	Acc@1 100.000 (99.526)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [137 | 140] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [137][0/196]	Time 0.027 (0.027)	Data 0.178 (0.178)	Loss 0.5131 (0.5131)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [137][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.5068 (0.5163)	Acc@1 100.000 (99.680)	Acc@5 100.000 (100.000)
Epoch: [137][20/196]	Time 0.011 (0.015)	Data 0.007 (0.011)	Loss 0.5087 (0.5165)	Acc@1 100.000 (99.758)	Acc@5 100.000 (100.000)
Epoch: [137][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.5250 (0.5167)	Acc@1 99.609 (99.773)	Acc@5 100.000 (100.000)
Epoch: [137][40/196]	Time 0.011 (0.015)	Data 0.010 (0.007)	Loss 0.5198 (0.5173)	Acc@1 99.609 (99.771)	Acc@5 100.000 (100.000)
Epoch: [137][50/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 0.5173 (0.5177)	Acc@1 99.219 (99.724)	Acc@5 100.000 (100.000)
Epoch: [137][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.5334 (0.5175)	Acc@1 99.609 (99.731)	Acc@5 100.000 (100.000)
Epoch: [137][70/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.5185 (0.5175)	Acc@1 99.609 (99.741)	Acc@5 100.000 (99.994)
Epoch: [137][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.5048 (0.5171)	Acc@1 100.000 (99.740)	Acc@5 100.000 (99.995)
Epoch: [137][90/196]	Time 0.020 (0.015)	Data 0.001 (0.005)	Loss 0.5100 (0.5168)	Acc@1 100.000 (99.751)	Acc@5 100.000 (99.996)
Epoch: [137][100/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 0.5128 (0.5168)	Acc@1 100.000 (99.760)	Acc@5 100.000 (99.996)
Epoch: [137][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.5186 (0.5166)	Acc@1 99.609 (99.764)	Acc@5 100.000 (99.996)
Epoch: [137][120/196]	Time 0.011 (0.015)	Data 0.015 (0.005)	Loss 0.5218 (0.5165)	Acc@1 99.609 (99.774)	Acc@5 100.000 (99.997)
Epoch: [137][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5018 (0.5163)	Acc@1 100.000 (99.767)	Acc@5 100.000 (99.997)
Epoch: [137][140/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.5162 (0.5160)	Acc@1 99.609 (99.776)	Acc@5 100.000 (99.997)
Epoch: [137][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.5174 (0.5162)	Acc@1 99.609 (99.762)	Acc@5 100.000 (99.997)
Epoch: [137][160/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.5074 (0.5159)	Acc@1 100.000 (99.772)	Acc@5 100.000 (99.998)
Epoch: [137][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5171 (0.5158)	Acc@1 100.000 (99.774)	Acc@5 100.000 (99.998)
Epoch: [137][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.5127 (0.5159)	Acc@1 100.000 (99.769)	Acc@5 100.000 (99.998)
Epoch: [137][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5213 (0.5159)	Acc@1 100.000 (99.769)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [138 | 140] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [138][0/196]	Time 0.026 (0.026)	Data 0.185 (0.185)	Loss 0.5135 (0.5135)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [138][10/196]	Time 0.016 (0.016)	Data 0.001 (0.019)	Loss 0.5202 (0.5147)	Acc@1 99.609 (99.751)	Acc@5 100.000 (100.000)
Epoch: [138][20/196]	Time 0.011 (0.015)	Data 0.009 (0.012)	Loss 0.5071 (0.5133)	Acc@1 100.000 (99.795)	Acc@5 100.000 (100.000)
Epoch: [138][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 0.5156 (0.5144)	Acc@1 99.609 (99.735)	Acc@5 100.000 (100.000)
Epoch: [138][40/196]	Time 0.011 (0.015)	Data 0.007 (0.008)	Loss 0.5138 (0.5135)	Acc@1 100.000 (99.781)	Acc@5 100.000 (100.000)
Epoch: [138][50/196]	Time 0.016 (0.014)	Data 0.001 (0.008)	Loss 0.5128 (0.5133)	Acc@1 100.000 (99.770)	Acc@5 100.000 (100.000)
Epoch: [138][60/196]	Time 0.011 (0.014)	Data 0.006 (0.007)	Loss 0.5133 (0.5130)	Acc@1 99.609 (99.782)	Acc@5 100.000 (100.000)
Epoch: [138][70/196]	Time 0.016 (0.014)	Data 0.001 (0.007)	Loss 0.5191 (0.5129)	Acc@1 99.219 (99.780)	Acc@5 100.000 (100.000)
Epoch: [138][80/196]	Time 0.013 (0.014)	Data 0.004 (0.007)	Loss 0.5129 (0.5130)	Acc@1 100.000 (99.783)	Acc@5 100.000 (100.000)
Epoch: [138][90/196]	Time 0.016 (0.014)	Data 0.001 (0.006)	Loss 0.5026 (0.5130)	Acc@1 100.000 (99.785)	Acc@5 100.000 (100.000)
Epoch: [138][100/196]	Time 0.014 (0.014)	Data 0.003 (0.006)	Loss 0.5300 (0.5130)	Acc@1 99.219 (99.791)	Acc@5 100.000 (100.000)
Epoch: [138][110/196]	Time 0.016 (0.014)	Data 0.001 (0.006)	Loss 0.5225 (0.5131)	Acc@1 99.609 (99.785)	Acc@5 100.000 (100.000)
Epoch: [138][120/196]	Time 0.014 (0.014)	Data 0.003 (0.006)	Loss 0.5192 (0.5129)	Acc@1 99.609 (99.784)	Acc@5 100.000 (100.000)
Epoch: [138][130/196]	Time 0.017 (0.014)	Data 0.001 (0.006)	Loss 0.5102 (0.5129)	Acc@1 100.000 (99.782)	Acc@5 100.000 (100.000)
Epoch: [138][140/196]	Time 0.013 (0.014)	Data 0.002 (0.005)	Loss 0.5269 (0.5131)	Acc@1 98.828 (99.776)	Acc@5 100.000 (100.000)
Epoch: [138][150/196]	Time 0.019 (0.014)	Data 0.000 (0.005)	Loss 0.5141 (0.5128)	Acc@1 100.000 (99.788)	Acc@5 100.000 (100.000)
Epoch: [138][160/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.5165 (0.5126)	Acc@1 99.219 (99.791)	Acc@5 100.000 (100.000)
Epoch: [138][170/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 0.5121 (0.5126)	Acc@1 99.609 (99.783)	Acc@5 100.000 (100.000)
Epoch: [138][180/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.5091 (0.5126)	Acc@1 100.000 (99.786)	Acc@5 100.000 (100.000)
Epoch: [138][190/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.5060 (0.5124)	Acc@1 100.000 (99.785)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [139 | 140] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [139][0/196]	Time 0.027 (0.027)	Data 0.187 (0.187)	Loss 0.5113 (0.5113)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [139][10/196]	Time 0.016 (0.016)	Data 0.000 (0.019)	Loss 0.5098 (0.5106)	Acc@1 100.000 (99.858)	Acc@5 100.000 (100.000)
Epoch: [139][20/196]	Time 0.011 (0.015)	Data 0.012 (0.011)	Loss 0.5078 (0.5099)	Acc@1 100.000 (99.833)	Acc@5 100.000 (100.000)
Epoch: [139][30/196]	Time 0.016 (0.015)	Data 0.001 (0.009)	Loss 0.5088 (0.5101)	Acc@1 100.000 (99.836)	Acc@5 100.000 (100.000)
Epoch: [139][40/196]	Time 0.011 (0.015)	Data 0.009 (0.008)	Loss 0.5068 (0.5107)	Acc@1 100.000 (99.819)	Acc@5 100.000 (100.000)
Epoch: [139][50/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 0.5241 (0.5104)	Acc@1 99.609 (99.831)	Acc@5 100.000 (100.000)
Epoch: [139][60/196]	Time 0.011 (0.015)	Data 0.011 (0.006)	Loss 0.5088 (0.5100)	Acc@1 100.000 (99.846)	Acc@5 100.000 (100.000)
Epoch: [139][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.5182 (0.5099)	Acc@1 99.609 (99.851)	Acc@5 100.000 (100.000)
Epoch: [139][80/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.5039 (0.5094)	Acc@1 100.000 (99.865)	Acc@5 100.000 (100.000)
Epoch: [139][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.5062 (0.5093)	Acc@1 100.000 (99.867)	Acc@5 100.000 (100.000)
Epoch: [139][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.5110 (0.5091)	Acc@1 100.000 (99.872)	Acc@5 100.000 (100.000)
Epoch: [139][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.5085 (0.5093)	Acc@1 100.000 (99.859)	Acc@5 100.000 (100.000)
Epoch: [139][120/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.5071 (0.5093)	Acc@1 100.000 (99.855)	Acc@5 100.000 (100.000)
Epoch: [139][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5026 (0.5094)	Acc@1 100.000 (99.854)	Acc@5 100.000 (100.000)
Epoch: [139][140/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.5158 (0.5096)	Acc@1 99.609 (99.839)	Acc@5 100.000 (100.000)
Epoch: [139][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.5034 (0.5095)	Acc@1 100.000 (99.840)	Acc@5 100.000 (100.000)
Epoch: [139][160/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.5079 (0.5095)	Acc@1 99.609 (99.840)	Acc@5 100.000 (100.000)
Epoch: [139][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5074 (0.5094)	Acc@1 100.000 (99.840)	Acc@5 100.000 (100.000)
Epoch: [139][180/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.5180 (0.5094)	Acc@1 99.219 (99.834)	Acc@5 100.000 (100.000)
Epoch: [139][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5122 (0.5094)	Acc@1 100.000 (99.836)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [140 | 140] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [140][0/196]	Time 0.027 (0.027)	Data 0.180 (0.180)	Loss 0.5107 (0.5107)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [140][10/196]	Time 0.016 (0.016)	Data 0.000 (0.019)	Loss 0.5051 (0.5074)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [140][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 0.5056 (0.5066)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [140][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 0.5141 (0.5070)	Acc@1 99.609 (99.899)	Acc@5 100.000 (100.000)
Epoch: [140][40/196]	Time 0.011 (0.015)	Data 0.011 (0.007)	Loss 0.5098 (0.5075)	Acc@1 99.609 (99.914)	Acc@5 100.000 (100.000)
Epoch: [140][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.5043 (0.5071)	Acc@1 100.000 (99.916)	Acc@5 100.000 (100.000)
Epoch: [140][60/196]	Time 0.011 (0.015)	Data 0.011 (0.006)	Loss 0.5037 (0.5071)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [140][70/196]	Time 0.016 (0.015)	Data 0.002 (0.006)	Loss 0.5070 (0.5070)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [140][80/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.5076 (0.5071)	Acc@1 99.609 (99.923)	Acc@5 100.000 (100.000)
Epoch: [140][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.5120 (0.5073)	Acc@1 99.609 (99.910)	Acc@5 100.000 (100.000)
Epoch: [140][100/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.5054 (0.5074)	Acc@1 100.000 (99.903)	Acc@5 100.000 (100.000)
Epoch: [140][110/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.5064 (0.5072)	Acc@1 100.000 (99.905)	Acc@5 100.000 (100.000)
Epoch: [140][120/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.5065 (0.5075)	Acc@1 99.609 (99.881)	Acc@5 100.000 (100.000)
Epoch: [140][130/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.5086 (0.5074)	Acc@1 99.609 (99.875)	Acc@5 100.000 (100.000)
Epoch: [140][140/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.5091 (0.5073)	Acc@1 100.000 (99.878)	Acc@5 100.000 (100.000)
Epoch: [140][150/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5089 (0.5073)	Acc@1 100.000 (99.876)	Acc@5 100.000 (100.000)
Epoch: [140][160/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.5006 (0.5073)	Acc@1 100.000 (99.869)	Acc@5 100.000 (100.000)
Epoch: [140][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4992 (0.5074)	Acc@1 100.000 (99.863)	Acc@5 100.000 (100.000)
Epoch: [140][180/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.5082 (0.5073)	Acc@1 99.609 (99.866)	Acc@5 100.000 (100.000)
Epoch: [140][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5045 (0.5074)	Acc@1 100.000 (99.863)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [252, 127, 3, 3] >> [252, 127, 3, 3]
[module.conv4.weight]: [256, 252, 3, 3] >> [256, 252, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [476, 511, 3, 3] >> [476, 511, 3, 3]
[module.conv8.weight]: [299, 476, 3, 3] >> [299, 476, 3, 3]
[module.fc.weight]: [100, 299] >> [100, 299]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
70.01
python src/cifar.py --workers 4 --dataset cifar100 --epochs 150 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_150.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.94M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [252, 127, 3, 3]
conv4 --> [256, 252, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [476, 511, 3, 3]
conv8 --> [299, 476, 3, 3]
fc --> [299, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8406042624, 18434304, 252
4, 16944463872, 37158912, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6724988928, 8756496, 476
8, 3934973952, 5123664, 299
fc, 11481600, 29900, 0
===================
FLOP REPORT: 28362032400000.0 52209600000.0 140132828 130524 2475 15.139837265014648

Epoch: [141 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [141][0/196]	Time 0.735 (0.735)	Data 0.169 (0.169)	Loss 0.5026 (0.5026)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [141][10/196]	Time 0.015 (0.080)	Data 0.002 (0.017)	Loss 0.5040 (0.5061)	Acc@1 100.000 (99.822)	Acc@5 100.000 (100.000)
Epoch: [141][20/196]	Time 0.015 (0.049)	Data 0.002 (0.010)	Loss 0.5158 (0.5065)	Acc@1 99.219 (99.777)	Acc@5 100.000 (100.000)
Epoch: [141][30/196]	Time 0.015 (0.038)	Data 0.002 (0.007)	Loss 0.5140 (0.5061)	Acc@1 99.219 (99.798)	Acc@5 100.000 (100.000)
Epoch: [141][40/196]	Time 0.016 (0.033)	Data 0.002 (0.006)	Loss 0.5024 (0.5061)	Acc@1 100.000 (99.819)	Acc@5 100.000 (100.000)
Epoch: [141][50/196]	Time 0.016 (0.029)	Data 0.000 (0.005)	Loss 0.5039 (0.5063)	Acc@1 100.000 (99.793)	Acc@5 100.000 (100.000)
Epoch: [141][60/196]	Time 0.015 (0.027)	Data 0.002 (0.005)	Loss 0.5046 (0.5063)	Acc@1 100.000 (99.801)	Acc@5 100.000 (100.000)
Epoch: [141][70/196]	Time 0.017 (0.025)	Data 0.000 (0.005)	Loss 0.5057 (0.5060)	Acc@1 100.000 (99.824)	Acc@5 100.000 (100.000)
Epoch: [141][80/196]	Time 0.015 (0.024)	Data 0.002 (0.004)	Loss 0.5016 (0.5060)	Acc@1 100.000 (99.831)	Acc@5 100.000 (100.000)
Epoch: [141][90/196]	Time 0.016 (0.023)	Data 0.000 (0.004)	Loss 0.5000 (0.5060)	Acc@1 100.000 (99.828)	Acc@5 100.000 (100.000)
Epoch: [141][100/196]	Time 0.014 (0.022)	Data 0.003 (0.004)	Loss 0.5065 (0.5058)	Acc@1 99.609 (99.841)	Acc@5 100.000 (100.000)
Epoch: [141][110/196]	Time 0.016 (0.021)	Data 0.000 (0.004)	Loss 0.5179 (0.5058)	Acc@1 99.609 (99.852)	Acc@5 100.000 (100.000)
Epoch: [141][120/196]	Time 0.012 (0.021)	Data 0.002 (0.004)	Loss 0.5016 (0.5057)	Acc@1 100.000 (99.855)	Acc@5 100.000 (100.000)
Epoch: [141][130/196]	Time 0.015 (0.020)	Data 0.002 (0.004)	Loss 0.5134 (0.5058)	Acc@1 99.219 (99.851)	Acc@5 100.000 (100.000)
Epoch: [141][140/196]	Time 0.018 (0.020)	Data 0.002 (0.004)	Loss 0.5046 (0.5059)	Acc@1 100.000 (99.850)	Acc@5 100.000 (99.997)
Epoch: [141][150/196]	Time 0.015 (0.020)	Data 0.002 (0.004)	Loss 0.5059 (0.5058)	Acc@1 100.000 (99.853)	Acc@5 100.000 (99.997)
Epoch: [141][160/196]	Time 0.017 (0.019)	Data 0.001 (0.003)	Loss 0.5074 (0.5058)	Acc@1 100.000 (99.850)	Acc@5 100.000 (99.998)
Epoch: [141][170/196]	Time 0.013 (0.019)	Data 0.004 (0.003)	Loss 0.5012 (0.5058)	Acc@1 100.000 (99.847)	Acc@5 100.000 (99.998)
Epoch: [141][180/196]	Time 0.016 (0.019)	Data 0.000 (0.003)	Loss 0.4998 (0.5058)	Acc@1 100.000 (99.847)	Acc@5 100.000 (99.998)
Epoch: [141][190/196]	Time 0.011 (0.019)	Data 0.006 (0.003)	Loss 0.5036 (0.5058)	Acc@1 100.000 (99.849)	Acc@5 100.000 (99.998)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [142 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [142][0/196]	Time 0.028 (0.028)	Data 0.163 (0.163)	Loss 0.5007 (0.5007)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [142][10/196]	Time 0.015 (0.017)	Data 0.002 (0.016)	Loss 0.4971 (0.5037)	Acc@1 100.000 (99.858)	Acc@5 100.000 (100.000)
Epoch: [142][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 0.5053 (0.5045)	Acc@1 99.609 (99.833)	Acc@5 100.000 (100.000)
Epoch: [142][30/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 0.5005 (0.5043)	Acc@1 100.000 (99.836)	Acc@5 100.000 (100.000)
Epoch: [142][40/196]	Time 0.014 (0.015)	Data 0.010 (0.006)	Loss 0.5027 (0.5046)	Acc@1 100.000 (99.857)	Acc@5 100.000 (100.000)
Epoch: [142][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 0.5017 (0.5046)	Acc@1 100.000 (99.877)	Acc@5 100.000 (100.000)
Epoch: [142][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.5058 (0.5049)	Acc@1 100.000 (99.846)	Acc@5 100.000 (100.000)
Epoch: [142][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.5086 (0.5048)	Acc@1 99.609 (99.840)	Acc@5 100.000 (100.000)
Epoch: [142][80/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 0.5011 (0.5047)	Acc@1 100.000 (99.846)	Acc@5 100.000 (100.000)
Epoch: [142][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.5027 (0.5045)	Acc@1 99.609 (99.854)	Acc@5 100.000 (100.000)
Epoch: [142][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.5140 (0.5046)	Acc@1 99.609 (99.845)	Acc@5 100.000 (100.000)
Epoch: [142][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5075 (0.5045)	Acc@1 99.609 (99.842)	Acc@5 100.000 (100.000)
Epoch: [142][120/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 0.4994 (0.5045)	Acc@1 100.000 (99.845)	Acc@5 100.000 (100.000)
Epoch: [142][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5072 (0.5044)	Acc@1 100.000 (99.854)	Acc@5 100.000 (100.000)
Epoch: [142][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.5007 (0.5043)	Acc@1 100.000 (99.859)	Acc@5 100.000 (100.000)
Epoch: [142][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4990 (0.5042)	Acc@1 100.000 (99.865)	Acc@5 100.000 (100.000)
Epoch: [142][160/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.5018 (0.5041)	Acc@1 100.000 (99.869)	Acc@5 100.000 (100.000)
Epoch: [142][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.5065 (0.5041)	Acc@1 99.609 (99.865)	Acc@5 100.000 (100.000)
Epoch: [142][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.5008 (0.5041)	Acc@1 100.000 (99.866)	Acc@5 100.000 (100.000)
Epoch: [142][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5183 (0.5041)	Acc@1 99.609 (99.869)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [143 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [143][0/196]	Time 0.027 (0.027)	Data 0.174 (0.174)	Loss 0.5058 (0.5058)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [143][10/196]	Time 0.016 (0.016)	Data 0.001 (0.018)	Loss 0.5042 (0.5033)	Acc@1 100.000 (99.858)	Acc@5 100.000 (100.000)
Epoch: [143][20/196]	Time 0.014 (0.015)	Data 0.002 (0.011)	Loss 0.5009 (0.5025)	Acc@1 100.000 (99.907)	Acc@5 100.000 (100.000)
Epoch: [143][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.5029 (0.5020)	Acc@1 99.609 (99.912)	Acc@5 100.000 (100.000)
Epoch: [143][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 0.5003 (0.5018)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [143][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.5001 (0.5017)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [143][60/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 0.5008 (0.5017)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [143][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.5053 (0.5020)	Acc@1 99.609 (99.917)	Acc@5 100.000 (100.000)
Epoch: [143][80/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.4964 (0.5019)	Acc@1 100.000 (99.913)	Acc@5 100.000 (100.000)
Epoch: [143][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.5026 (0.5020)	Acc@1 100.000 (99.910)	Acc@5 100.000 (100.000)
Epoch: [143][100/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.5000 (0.5020)	Acc@1 100.000 (99.903)	Acc@5 100.000 (100.000)
Epoch: [143][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4967 (0.5023)	Acc@1 100.000 (99.887)	Acc@5 100.000 (100.000)
Epoch: [143][120/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.5066 (0.5022)	Acc@1 99.219 (99.881)	Acc@5 100.000 (100.000)
Epoch: [143][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5030 (0.5021)	Acc@1 100.000 (99.890)	Acc@5 100.000 (100.000)
Epoch: [143][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.4989 (0.5020)	Acc@1 100.000 (99.895)	Acc@5 100.000 (100.000)
Epoch: [143][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5085 (0.5020)	Acc@1 99.609 (99.889)	Acc@5 100.000 (100.000)
Epoch: [143][160/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.5022 (0.5019)	Acc@1 100.000 (99.893)	Acc@5 100.000 (100.000)
Epoch: [143][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5120 (0.5020)	Acc@1 99.219 (99.886)	Acc@5 100.000 (100.000)
Epoch: [143][180/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.5020 (0.5020)	Acc@1 100.000 (99.881)	Acc@5 100.000 (100.000)
Epoch: [143][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 0.5188 (0.5021)	Acc@1 99.219 (99.881)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [144 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [144][0/196]	Time 0.028 (0.028)	Data 0.170 (0.170)	Loss 0.4986 (0.4986)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [144][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 0.5021 (0.5002)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [144][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 0.4948 (0.5002)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [144][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.5068 (0.5004)	Acc@1 99.609 (99.924)	Acc@5 100.000 (100.000)
Epoch: [144][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 0.4994 (0.5007)	Acc@1 100.000 (99.905)	Acc@5 100.000 (100.000)
Epoch: [144][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.5079 (0.5009)	Acc@1 99.609 (99.908)	Acc@5 100.000 (100.000)
Epoch: [144][60/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.5007 (0.5007)	Acc@1 99.609 (99.910)	Acc@5 100.000 (100.000)
Epoch: [144][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4976 (0.5009)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [144][80/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 0.4999 (0.5009)	Acc@1 100.000 (99.904)	Acc@5 100.000 (100.000)
Epoch: [144][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4969 (0.5008)	Acc@1 100.000 (99.910)	Acc@5 100.000 (100.000)
Epoch: [144][100/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.4958 (0.5010)	Acc@1 100.000 (99.899)	Acc@5 100.000 (100.000)
Epoch: [144][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4989 (0.5007)	Acc@1 100.000 (99.905)	Acc@5 100.000 (100.000)
Epoch: [144][120/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.4984 (0.5008)	Acc@1 100.000 (99.903)	Acc@5 100.000 (100.000)
Epoch: [144][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4966 (0.5007)	Acc@1 100.000 (99.908)	Acc@5 100.000 (100.000)
Epoch: [144][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.5010 (0.5007)	Acc@1 100.000 (99.906)	Acc@5 100.000 (100.000)
Epoch: [144][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4993 (0.5008)	Acc@1 100.000 (99.907)	Acc@5 100.000 (100.000)
Epoch: [144][160/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 0.4957 (0.5007)	Acc@1 100.000 (99.905)	Acc@5 100.000 (100.000)
Epoch: [144][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 0.5047 (0.5007)	Acc@1 99.609 (99.899)	Acc@5 100.000 (100.000)
Epoch: [144][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.5141 (0.5007)	Acc@1 99.609 (99.901)	Acc@5 100.000 (100.000)
Epoch: [144][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4957 (0.5005)	Acc@1 100.000 (99.906)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [145 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [145][0/196]	Time 0.029 (0.029)	Data 0.170 (0.170)	Loss 0.5011 (0.5011)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [145][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.4977 (0.4998)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [145][20/196]	Time 0.011 (0.015)	Data 0.005 (0.011)	Loss 0.5084 (0.4994)	Acc@1 99.219 (99.888)	Acc@5 100.000 (100.000)
Epoch: [145][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.4975 (0.4992)	Acc@1 100.000 (99.912)	Acc@5 100.000 (99.987)
Epoch: [145][40/196]	Time 0.015 (0.015)	Data 0.003 (0.007)	Loss 0.4960 (0.4989)	Acc@1 100.000 (99.924)	Acc@5 100.000 (99.990)
Epoch: [145][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.4973 (0.4986)	Acc@1 100.000 (99.931)	Acc@5 100.000 (99.992)
Epoch: [145][60/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 0.4993 (0.4986)	Acc@1 100.000 (99.917)	Acc@5 100.000 (99.994)
Epoch: [145][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4956 (0.4986)	Acc@1 100.000 (99.923)	Acc@5 100.000 (99.994)
Epoch: [145][80/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.4966 (0.4988)	Acc@1 100.000 (99.918)	Acc@5 100.000 (99.995)
Epoch: [145][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4934 (0.4988)	Acc@1 100.000 (99.914)	Acc@5 100.000 (99.996)
Epoch: [145][100/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.4956 (0.4988)	Acc@1 100.000 (99.915)	Acc@5 100.000 (99.996)
Epoch: [145][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.4954 (0.4987)	Acc@1 100.000 (99.919)	Acc@5 100.000 (99.996)
Epoch: [145][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4972 (0.4989)	Acc@1 100.000 (99.903)	Acc@5 100.000 (99.997)
Epoch: [145][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4997 (0.4989)	Acc@1 100.000 (99.902)	Acc@5 100.000 (99.997)
Epoch: [145][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4948 (0.4989)	Acc@1 100.000 (99.889)	Acc@5 100.000 (99.997)
Epoch: [145][150/196]	Time 0.019 (0.015)	Data 0.000 (0.004)	Loss 0.4954 (0.4989)	Acc@1 100.000 (99.891)	Acc@5 100.000 (99.997)
Epoch: [145][160/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 0.4990 (0.4988)	Acc@1 99.609 (99.888)	Acc@5 100.000 (99.998)
Epoch: [145][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.4961 (0.4989)	Acc@1 100.000 (99.888)	Acc@5 100.000 (99.998)
Epoch: [145][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.4957 (0.4989)	Acc@1 100.000 (99.886)	Acc@5 100.000 (99.998)
Epoch: [145][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4947 (0.4988)	Acc@1 100.000 (99.890)	Acc@5 100.000 (99.998)
[INFO] Storing checkpoint...

Epoch: [146 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [146][0/196]	Time 0.026 (0.026)	Data 0.172 (0.172)	Loss 0.4960 (0.4960)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [146][10/196]	Time 0.013 (0.015)	Data 0.004 (0.018)	Loss 0.4948 (0.4961)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [146][20/196]	Time 0.012 (0.015)	Data 0.005 (0.011)	Loss 0.4962 (0.4959)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [146][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 0.4970 (0.4965)	Acc@1 99.609 (99.937)	Acc@5 100.000 (100.000)
Epoch: [146][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 0.5009 (0.4971)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [146][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 0.4960 (0.4971)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [146][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4993 (0.4974)	Acc@1 99.609 (99.930)	Acc@5 100.000 (100.000)
Epoch: [146][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.5016 (0.4974)	Acc@1 99.609 (99.928)	Acc@5 100.000 (100.000)
Epoch: [146][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4969 (0.4974)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [146][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4985 (0.4973)	Acc@1 99.609 (99.927)	Acc@5 100.000 (100.000)
Epoch: [146][100/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.4919 (0.4974)	Acc@1 100.000 (99.915)	Acc@5 100.000 (100.000)
Epoch: [146][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4980 (0.4973)	Acc@1 100.000 (99.916)	Acc@5 100.000 (100.000)
Epoch: [146][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.4984 (0.4974)	Acc@1 100.000 (99.910)	Acc@5 100.000 (100.000)
Epoch: [146][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4963 (0.4975)	Acc@1 100.000 (99.911)	Acc@5 100.000 (100.000)
Epoch: [146][140/196]	Time 0.013 (0.014)	Data 0.005 (0.004)	Loss 0.4954 (0.4975)	Acc@1 100.000 (99.909)	Acc@5 100.000 (100.000)
Epoch: [146][150/196]	Time 0.011 (0.014)	Data 0.017 (0.004)	Loss 0.4995 (0.4974)	Acc@1 99.609 (99.907)	Acc@5 100.000 (100.000)
Epoch: [146][160/196]	Time 0.015 (0.014)	Data 0.005 (0.004)	Loss 0.4974 (0.4974)	Acc@1 100.000 (99.908)	Acc@5 100.000 (100.000)
Epoch: [146][170/196]	Time 0.011 (0.014)	Data 0.008 (0.004)	Loss 0.4946 (0.4973)	Acc@1 100.000 (99.909)	Acc@5 100.000 (100.000)
Epoch: [146][180/196]	Time 0.012 (0.014)	Data 0.004 (0.004)	Loss 0.4970 (0.4972)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [146][190/196]	Time 0.011 (0.014)	Data 0.008 (0.004)	Loss 0.5085 (0.4974)	Acc@1 99.219 (99.904)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [147 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [147][0/196]	Time 0.026 (0.026)	Data 0.180 (0.180)	Loss 0.4969 (0.4969)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [147][10/196]	Time 0.016 (0.016)	Data 0.000 (0.019)	Loss 0.5018 (0.4963)	Acc@1 99.609 (99.929)	Acc@5 100.000 (100.000)
Epoch: [147][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 0.4949 (0.4965)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [147][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 0.4950 (0.4971)	Acc@1 100.000 (99.899)	Acc@5 100.000 (100.000)
Epoch: [147][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.4938 (0.4972)	Acc@1 100.000 (99.895)	Acc@5 100.000 (100.000)
Epoch: [147][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.5041 (0.4971)	Acc@1 99.609 (99.893)	Acc@5 100.000 (100.000)
Epoch: [147][60/196]	Time 0.011 (0.015)	Data 0.005 (0.006)	Loss 0.4985 (0.4973)	Acc@1 99.609 (99.872)	Acc@5 100.000 (100.000)
Epoch: [147][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4951 (0.4969)	Acc@1 100.000 (99.890)	Acc@5 100.000 (100.000)
Epoch: [147][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4945 (0.4967)	Acc@1 100.000 (99.889)	Acc@5 100.000 (100.000)
Epoch: [147][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.5039 (0.4968)	Acc@1 99.219 (99.884)	Acc@5 100.000 (100.000)
Epoch: [147][100/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.4960 (0.4967)	Acc@1 100.000 (99.892)	Acc@5 100.000 (100.000)
Epoch: [147][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4969 (0.4967)	Acc@1 100.000 (99.894)	Acc@5 100.000 (100.000)
Epoch: [147][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4965 (0.4965)	Acc@1 100.000 (99.897)	Acc@5 100.000 (100.000)
Epoch: [147][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4934 (0.4964)	Acc@1 100.000 (99.899)	Acc@5 100.000 (100.000)
Epoch: [147][140/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 0.4911 (0.4964)	Acc@1 100.000 (99.900)	Acc@5 100.000 (100.000)
Epoch: [147][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4939 (0.4963)	Acc@1 100.000 (99.904)	Acc@5 100.000 (100.000)
Epoch: [147][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.4961 (0.4962)	Acc@1 100.000 (99.903)	Acc@5 100.000 (100.000)
Epoch: [147][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.4958 (0.4962)	Acc@1 100.000 (99.899)	Acc@5 100.000 (100.000)
Epoch: [147][180/196]	Time 0.011 (0.015)	Data 0.014 (0.004)	Loss 0.4968 (0.4962)	Acc@1 100.000 (99.903)	Acc@5 100.000 (100.000)
Epoch: [147][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4962 (0.4963)	Acc@1 100.000 (99.902)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [148 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [148][0/196]	Time 0.028 (0.028)	Data 0.182 (0.182)	Loss 0.4947 (0.4947)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [148][10/196]	Time 0.016 (0.016)	Data 0.000 (0.019)	Loss 0.4934 (0.4940)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [148][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 0.5077 (0.4943)	Acc@1 99.609 (99.944)	Acc@5 100.000 (100.000)
Epoch: [148][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 0.4915 (0.4946)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [148][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 0.5082 (0.4951)	Acc@1 99.609 (99.914)	Acc@5 100.000 (100.000)
Epoch: [148][50/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 0.5003 (0.4952)	Acc@1 99.609 (99.916)	Acc@5 100.000 (100.000)
Epoch: [148][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.4917 (0.4951)	Acc@1 100.000 (99.904)	Acc@5 100.000 (100.000)
Epoch: [148][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.5006 (0.4952)	Acc@1 100.000 (99.917)	Acc@5 100.000 (100.000)
Epoch: [148][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.5095 (0.4953)	Acc@1 99.609 (99.923)	Acc@5 100.000 (100.000)
Epoch: [148][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4925 (0.4952)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [148][100/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 0.4922 (0.4952)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [148][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4953 (0.4953)	Acc@1 100.000 (99.916)	Acc@5 100.000 (100.000)
Epoch: [148][120/196]	Time 0.015 (0.015)	Data 0.023 (0.004)	Loss 0.4914 (0.4954)	Acc@1 100.000 (99.910)	Acc@5 100.000 (100.000)
Epoch: [148][130/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 0.4959 (0.4953)	Acc@1 100.000 (99.914)	Acc@5 100.000 (100.000)
Epoch: [148][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.4954 (0.4952)	Acc@1 100.000 (99.917)	Acc@5 100.000 (100.000)
Epoch: [148][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4994 (0.4952)	Acc@1 99.609 (99.912)	Acc@5 100.000 (100.000)
Epoch: [148][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.5079 (0.4953)	Acc@1 99.609 (99.908)	Acc@5 100.000 (100.000)
Epoch: [148][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4976 (0.4954)	Acc@1 99.609 (99.902)	Acc@5 100.000 (100.000)
Epoch: [148][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4935 (0.4952)	Acc@1 100.000 (99.907)	Acc@5 100.000 (100.000)
Epoch: [148][190/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4913 (0.4951)	Acc@1 100.000 (99.910)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [149 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [149][0/196]	Time 0.026 (0.026)	Data 0.183 (0.183)	Loss 0.4928 (0.4928)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [149][10/196]	Time 0.016 (0.015)	Data 0.000 (0.019)	Loss 0.4906 (0.4932)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [149][20/196]	Time 0.011 (0.015)	Data 0.011 (0.011)	Loss 0.4887 (0.4929)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [149][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.4891 (0.4928)	Acc@1 100.000 (99.975)	Acc@5 100.000 (100.000)
Epoch: [149][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 0.4914 (0.4931)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [149][50/196]	Time 0.018 (0.015)	Data 0.000 (0.006)	Loss 0.4954 (0.4933)	Acc@1 99.609 (99.962)	Acc@5 100.000 (100.000)
Epoch: [149][60/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 0.4925 (0.4935)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [149][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4911 (0.4933)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Epoch: [149][80/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.4952 (0.4936)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [149][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.4963 (0.4938)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [149][100/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.4947 (0.4938)	Acc@1 99.609 (99.927)	Acc@5 100.000 (100.000)
Epoch: [149][110/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.4988 (0.4937)	Acc@1 99.609 (99.926)	Acc@5 100.000 (100.000)
Epoch: [149][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4898 (0.4935)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [149][130/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 0.4897 (0.4935)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [149][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.4927 (0.4934)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [149][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.4905 (0.4933)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [149][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.4946 (0.4933)	Acc@1 99.609 (99.934)	Acc@5 100.000 (100.000)
Epoch: [149][170/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.5003 (0.4933)	Acc@1 99.609 (99.934)	Acc@5 100.000 (100.000)
Epoch: [149][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.4956 (0.4934)	Acc@1 99.609 (99.924)	Acc@5 100.000 (100.000)
Epoch: [149][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4913 (0.4933)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [150 | 150] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [150][0/196]	Time 0.030 (0.030)	Data 0.183 (0.183)	Loss 0.4916 (0.4916)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [150][10/196]	Time 0.017 (0.016)	Data 0.001 (0.019)	Loss 0.4939 (0.4920)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [150][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 0.4969 (0.4940)	Acc@1 99.609 (99.907)	Acc@5 100.000 (100.000)
Epoch: [150][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.4930 (0.4938)	Acc@1 100.000 (99.887)	Acc@5 100.000 (100.000)
Epoch: [150][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 0.4908 (0.4935)	Acc@1 100.000 (99.886)	Acc@5 100.000 (100.000)
Epoch: [150][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.4913 (0.4930)	Acc@1 100.000 (99.900)	Acc@5 100.000 (100.000)
Epoch: [150][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 0.4920 (0.4927)	Acc@1 100.000 (99.910)	Acc@5 100.000 (100.000)
Epoch: [150][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4900 (0.4925)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [150][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4939 (0.4925)	Acc@1 100.000 (99.913)	Acc@5 100.000 (100.000)
Epoch: [150][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4898 (0.4924)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [150][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4946 (0.4923)	Acc@1 99.609 (99.923)	Acc@5 100.000 (100.000)
Epoch: [150][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.4922 (0.4923)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [150][120/196]	Time 0.027 (0.015)	Data 0.008 (0.004)	Loss 0.4932 (0.4922)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [150][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4909 (0.4922)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [150][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.4879 (0.4922)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [150][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.4910 (0.4922)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [150][160/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 0.4894 (0.4921)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [150][170/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 0.4901 (0.4920)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [150][180/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.4910 (0.4921)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [150][190/196]	Time 0.013 (0.015)	Data 0.003 (0.003)	Loss 0.4895 (0.4921)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [252, 127, 3, 3] >> [252, 127, 3, 3]
[module.conv4.weight]: [256, 252, 3, 3] >> [256, 252, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [476, 511, 3, 3] >> [476, 511, 3, 3]
[module.conv8.weight]: [299, 476, 3, 3] >> [299, 476, 3, 3]
[module.fc.weight]: [100, 299] >> [100, 299]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
70.16
python src/cifar.py --workers 4 --dataset cifar100 --epochs 160 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_160.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.94M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [252, 127, 3, 3]
conv4 --> [256, 252, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [476, 511, 3, 3]
conv8 --> [299, 476, 3, 3]
fc --> [299, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8406042624, 18434304, 252
4, 16944463872, 37158912, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6724988928, 8756496, 476
8, 3934973952, 5123664, 299
fc, 11481600, 29900, 0
===================
FLOP REPORT: 28362032400000.0 52209600000.0 140132828 130524 2475 15.139837265014648

Epoch: [151 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [151][0/196]	Time 0.733 (0.733)	Data 0.161 (0.161)	Loss 0.4864 (0.4864)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [151][10/196]	Time 0.016 (0.080)	Data 0.002 (0.016)	Loss 0.4879 (0.4898)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [151][20/196]	Time 0.015 (0.049)	Data 0.003 (0.010)	Loss 0.4887 (0.4895)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [151][30/196]	Time 0.018 (0.038)	Data 0.000 (0.007)	Loss 0.4914 (0.4898)	Acc@1 100.000 (99.975)	Acc@5 100.000 (100.000)
Epoch: [151][40/196]	Time 0.015 (0.033)	Data 0.002 (0.006)	Loss 0.4895 (0.4903)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [151][50/196]	Time 0.016 (0.029)	Data 0.001 (0.005)	Loss 0.4881 (0.4903)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [151][60/196]	Time 0.015 (0.027)	Data 0.003 (0.005)	Loss 0.4930 (0.4908)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [151][70/196]	Time 0.017 (0.025)	Data 0.000 (0.005)	Loss 0.4961 (0.4908)	Acc@1 99.609 (99.945)	Acc@5 100.000 (100.000)
Epoch: [151][80/196]	Time 0.013 (0.024)	Data 0.004 (0.005)	Loss 0.4888 (0.4909)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [151][90/196]	Time 0.016 (0.023)	Data 0.000 (0.004)	Loss 0.4897 (0.4908)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [151][100/196]	Time 0.011 (0.022)	Data 0.007 (0.004)	Loss 0.4905 (0.4906)	Acc@1 99.609 (99.946)	Acc@5 100.000 (100.000)
Epoch: [151][110/196]	Time 0.016 (0.021)	Data 0.000 (0.004)	Loss 0.4932 (0.4907)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [151][120/196]	Time 0.013 (0.021)	Data 0.004 (0.004)	Loss 0.4884 (0.4907)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [151][130/196]	Time 0.016 (0.020)	Data 0.000 (0.004)	Loss 0.4889 (0.4906)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [151][140/196]	Time 0.011 (0.020)	Data 0.009 (0.004)	Loss 0.4880 (0.4906)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [151][150/196]	Time 0.016 (0.020)	Data 0.000 (0.004)	Loss 0.4923 (0.4908)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [151][160/196]	Time 0.011 (0.019)	Data 0.012 (0.004)	Loss 0.4878 (0.4908)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [151][170/196]	Time 0.019 (0.019)	Data 0.001 (0.004)	Loss 0.4877 (0.4908)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [151][180/196]	Time 0.011 (0.019)	Data 0.007 (0.004)	Loss 0.4895 (0.4907)	Acc@1 99.609 (99.944)	Acc@5 100.000 (100.000)
Epoch: [151][190/196]	Time 0.017 (0.018)	Data 0.000 (0.004)	Loss 0.4915 (0.4907)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [152 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [152][0/196]	Time 0.023 (0.023)	Data 0.159 (0.159)	Loss 0.4893 (0.4893)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [152][10/196]	Time 0.013 (0.015)	Data 0.006 (0.017)	Loss 0.4877 (0.4906)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [152][20/196]	Time 0.013 (0.015)	Data 0.004 (0.010)	Loss 0.4871 (0.4902)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [152][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 0.4921 (0.4906)	Acc@1 99.609 (99.924)	Acc@5 100.000 (100.000)
Epoch: [152][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.4872 (0.4908)	Acc@1 100.000 (99.905)	Acc@5 100.000 (100.000)
Epoch: [152][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.4886 (0.4907)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [152][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.4904 (0.4906)	Acc@1 100.000 (99.917)	Acc@5 100.000 (100.000)
Epoch: [152][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4846 (0.4903)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [152][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 0.4886 (0.4900)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [152][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4860 (0.4900)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Epoch: [152][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4865 (0.4899)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Epoch: [152][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4868 (0.4900)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [152][120/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4897 (0.4899)	Acc@1 99.609 (99.913)	Acc@5 100.000 (100.000)
Epoch: [152][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4857 (0.4898)	Acc@1 100.000 (99.917)	Acc@5 100.000 (100.000)
Epoch: [152][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.4874 (0.4897)	Acc@1 100.000 (99.917)	Acc@5 100.000 (100.000)
Epoch: [152][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4892 (0.4898)	Acc@1 100.000 (99.915)	Acc@5 100.000 (100.000)
Epoch: [152][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.4919 (0.4899)	Acc@1 99.609 (99.910)	Acc@5 100.000 (100.000)
Epoch: [152][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4860 (0.4900)	Acc@1 100.000 (99.909)	Acc@5 100.000 (100.000)
Epoch: [152][180/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.4904 (0.4899)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [152][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4851 (0.4898)	Acc@1 100.000 (99.916)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [153 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [153][0/196]	Time 0.025 (0.025)	Data 0.168 (0.168)	Loss 0.4861 (0.4861)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [153][10/196]	Time 0.015 (0.015)	Data 0.003 (0.018)	Loss 0.4855 (0.4866)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [153][20/196]	Time 0.011 (0.015)	Data 0.006 (0.010)	Loss 0.4900 (0.4883)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [153][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.4887 (0.4881)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [153][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.4867 (0.4878)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [153][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.4869 (0.4876)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [153][60/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 0.4861 (0.4879)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [153][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4850 (0.4881)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [153][80/196]	Time 0.011 (0.015)	Data 0.019 (0.005)	Loss 0.4853 (0.4880)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [153][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4887 (0.4879)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [153][100/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 0.4891 (0.4880)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [153][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4840 (0.4882)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [153][120/196]	Time 0.011 (0.015)	Data 0.015 (0.004)	Loss 0.4843 (0.4882)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [153][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4889 (0.4882)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [153][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4954 (0.4882)	Acc@1 99.609 (99.936)	Acc@5 100.000 (100.000)
Epoch: [153][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4894 (0.4883)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [153][160/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 0.4923 (0.4883)	Acc@1 99.609 (99.932)	Acc@5 100.000 (100.000)
Epoch: [153][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4875 (0.4882)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [153][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4885 (0.4883)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Epoch: [153][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4852 (0.4882)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [154 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [154][0/196]	Time 0.023 (0.023)	Data 0.171 (0.171)	Loss 0.4897 (0.4897)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [154][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 0.4887 (0.4866)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [154][20/196]	Time 0.011 (0.016)	Data 0.009 (0.011)	Loss 0.4878 (0.4874)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [154][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 0.4896 (0.4877)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [154][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 0.4895 (0.4877)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [154][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.4831 (0.4875)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [154][60/196]	Time 0.016 (0.015)	Data 0.008 (0.005)	Loss 0.4872 (0.4872)	Acc@1 100.000 (99.955)	Acc@5 100.000 (100.000)
Epoch: [154][70/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 0.4852 (0.4875)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [154][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4921 (0.4875)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [154][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4822 (0.4874)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [154][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4861 (0.4874)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [154][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4911 (0.4873)	Acc@1 99.609 (99.937)	Acc@5 100.000 (100.000)
Epoch: [154][120/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4860 (0.4873)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [154][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4843 (0.4873)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [154][140/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 0.4833 (0.4873)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [154][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4871 (0.4875)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [154][160/196]	Time 0.011 (0.015)	Data 0.006 (0.003)	Loss 0.4871 (0.4875)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [154][170/196]	Time 0.016 (0.015)	Data 0.001 (0.003)	Loss 0.4847 (0.4874)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [154][180/196]	Time 0.011 (0.015)	Data 0.008 (0.003)	Loss 0.4883 (0.4873)	Acc@1 99.609 (99.931)	Acc@5 100.000 (100.000)
Epoch: [154][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 0.4869 (0.4873)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [155 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [155][0/196]	Time 0.028 (0.028)	Data 0.172 (0.172)	Loss 0.4871 (0.4871)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [155][10/196]	Time 0.017 (0.016)	Data 0.002 (0.018)	Loss 0.4847 (0.4863)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [155][20/196]	Time 0.015 (0.015)	Data 0.002 (0.010)	Loss 0.4868 (0.4858)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [155][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.4887 (0.4856)	Acc@1 99.609 (99.950)	Acc@5 100.000 (100.000)
Epoch: [155][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 0.4849 (0.4856)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [155][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.4885 (0.4858)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [155][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4951 (0.4859)	Acc@1 99.219 (99.936)	Acc@5 100.000 (100.000)
Epoch: [155][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4862 (0.4857)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [155][80/196]	Time 0.023 (0.015)	Data 0.002 (0.004)	Loss 0.4866 (0.4859)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [155][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4852 (0.4859)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [155][100/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.4831 (0.4859)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [155][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4903 (0.4859)	Acc@1 99.609 (99.937)	Acc@5 100.000 (100.000)
Epoch: [155][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.4922 (0.4859)	Acc@1 99.609 (99.932)	Acc@5 100.000 (100.000)
Epoch: [155][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4840 (0.4858)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [155][140/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.4839 (0.4858)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [155][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4863 (0.4858)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [155][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.4827 (0.4858)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [155][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4896 (0.4859)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [155][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4876 (0.4860)	Acc@1 99.609 (99.931)	Acc@5 100.000 (100.000)
Epoch: [155][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4813 (0.4860)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [156 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [156][0/196]	Time 0.024 (0.024)	Data 0.176 (0.176)	Loss 0.4875 (0.4875)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [156][10/196]	Time 0.016 (0.015)	Data 0.000 (0.019)	Loss 0.4872 (0.4846)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [156][20/196]	Time 0.016 (0.015)	Data 0.006 (0.011)	Loss 0.4817 (0.4847)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [156][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 0.4852 (0.4850)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [156][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.4838 (0.4849)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [156][50/196]	Time 0.015 (0.014)	Data 0.002 (0.007)	Loss 0.4899 (0.4850)	Acc@1 99.609 (99.939)	Acc@5 100.000 (100.000)
Epoch: [156][60/196]	Time 0.011 (0.014)	Data 0.015 (0.006)	Loss 0.4844 (0.4851)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [156][70/196]	Time 0.015 (0.014)	Data 0.003 (0.006)	Loss 0.4835 (0.4848)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [156][80/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.4867 (0.4848)	Acc@1 99.609 (99.932)	Acc@5 100.000 (100.000)
Epoch: [156][90/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4835 (0.4848)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Epoch: [156][100/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.4825 (0.4848)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [156][110/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4831 (0.4848)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [156][120/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 0.4822 (0.4848)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [156][130/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.4841 (0.4849)	Acc@1 100.000 (99.919)	Acc@5 100.000 (100.000)
Epoch: [156][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.4832 (0.4848)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [156][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4809 (0.4848)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [156][160/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 0.4825 (0.4848)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [156][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4870 (0.4847)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [156][180/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 0.4894 (0.4848)	Acc@1 99.609 (99.924)	Acc@5 100.000 (100.000)
Epoch: [156][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4822 (0.4847)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [157 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [157][0/196]	Time 0.026 (0.026)	Data 0.177 (0.177)	Loss 0.4826 (0.4826)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [157][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 0.4822 (0.4841)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [157][20/196]	Time 0.012 (0.015)	Data 0.005 (0.011)	Loss 0.4886 (0.4848)	Acc@1 99.609 (99.888)	Acc@5 100.000 (100.000)
Epoch: [157][30/196]	Time 0.017 (0.015)	Data 0.001 (0.008)	Loss 0.4829 (0.4845)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [157][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 0.4843 (0.4842)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [157][50/196]	Time 0.016 (0.015)	Data 0.001 (0.007)	Loss 0.4827 (0.4842)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [157][60/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 0.4844 (0.4840)	Acc@1 100.000 (99.955)	Acc@5 100.000 (100.000)
Epoch: [157][70/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.4876 (0.4838)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [157][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4915 (0.4841)	Acc@1 99.219 (99.937)	Acc@5 100.000 (100.000)
Epoch: [157][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4844 (0.4842)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [157][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4823 (0.4840)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [157][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.4795 (0.4840)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [157][120/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4818 (0.4841)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [157][130/196]	Time 0.014 (0.015)	Data 0.000 (0.005)	Loss 0.4819 (0.4842)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [157][140/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4847 (0.4841)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [157][150/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 0.4818 (0.4842)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Epoch: [157][160/196]	Time 0.013 (0.014)	Data 0.004 (0.004)	Loss 0.4898 (0.4843)	Acc@1 99.609 (99.920)	Acc@5 100.000 (100.000)
Epoch: [157][170/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 0.4803 (0.4842)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [157][180/196]	Time 0.013 (0.014)	Data 0.006 (0.004)	Loss 0.4857 (0.4841)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Epoch: [157][190/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 0.4845 (0.4840)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [158 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [158][0/196]	Time 0.023 (0.023)	Data 0.188 (0.188)	Loss 0.4811 (0.4811)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [158][10/196]	Time 0.017 (0.015)	Data 0.000 (0.020)	Loss 0.4822 (0.4838)	Acc@1 100.000 (99.858)	Acc@5 100.000 (100.000)
Epoch: [158][20/196]	Time 0.014 (0.015)	Data 0.003 (0.011)	Loss 0.4784 (0.4831)	Acc@1 100.000 (99.888)	Acc@5 100.000 (100.000)
Epoch: [158][30/196]	Time 0.017 (0.015)	Data 0.001 (0.009)	Loss 0.4824 (0.4830)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [158][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 0.4863 (0.4827)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [158][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 0.4861 (0.4829)	Acc@1 99.609 (99.931)	Acc@5 100.000 (100.000)
Epoch: [158][60/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 0.4839 (0.4829)	Acc@1 99.609 (99.923)	Acc@5 100.000 (100.000)
Epoch: [158][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4834 (0.4829)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [158][80/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.4826 (0.4829)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [158][90/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4808 (0.4829)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [158][100/196]	Time 0.012 (0.015)	Data 0.003 (0.005)	Loss 0.4777 (0.4829)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [158][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4789 (0.4827)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [158][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4795 (0.4827)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [158][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.4795 (0.4827)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [158][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4874 (0.4828)	Acc@1 99.609 (99.922)	Acc@5 100.000 (100.000)
Epoch: [158][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4824 (0.4827)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Epoch: [158][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4802 (0.4827)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Epoch: [158][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.4902 (0.4827)	Acc@1 99.609 (99.922)	Acc@5 100.000 (100.000)
Epoch: [158][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.4868 (0.4827)	Acc@1 99.609 (99.924)	Acc@5 100.000 (100.000)
Epoch: [158][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4802 (0.4826)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [159 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [159][0/196]	Time 0.026 (0.026)	Data 0.184 (0.184)	Loss 0.4833 (0.4833)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [159][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 0.4806 (0.4812)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [159][20/196]	Time 0.013 (0.015)	Data 0.004 (0.011)	Loss 0.4833 (0.4817)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [159][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.4832 (0.4812)	Acc@1 99.609 (99.962)	Acc@5 100.000 (100.000)
Epoch: [159][40/196]	Time 0.013 (0.015)	Data 0.008 (0.007)	Loss 0.4841 (0.4813)	Acc@1 99.609 (99.952)	Acc@5 100.000 (100.000)
Epoch: [159][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.4811 (0.4816)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [159][60/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 0.4841 (0.4818)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [159][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4789 (0.4817)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [159][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.4780 (0.4815)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [159][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4807 (0.4815)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [159][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.4804 (0.4816)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [159][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4776 (0.4816)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [159][120/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.4824 (0.4817)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [159][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4804 (0.4816)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [159][140/196]	Time 0.020 (0.015)	Data 0.007 (0.004)	Loss 0.4785 (0.4815)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [159][150/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.4883 (0.4817)	Acc@1 99.609 (99.938)	Acc@5 100.000 (100.000)
Epoch: [159][160/196]	Time 0.011 (0.015)	Data 0.014 (0.004)	Loss 0.4801 (0.4816)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [159][170/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 0.4797 (0.4815)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [159][180/196]	Time 0.011 (0.014)	Data 0.010 (0.004)	Loss 0.4845 (0.4815)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [159][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4769 (0.4816)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [160 | 160] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [160][0/196]	Time 0.024 (0.024)	Data 0.201 (0.201)	Loss 0.4814 (0.4814)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [160][10/196]	Time 0.017 (0.016)	Data 0.000 (0.020)	Loss 0.4860 (0.4814)	Acc@1 99.609 (99.929)	Acc@5 100.000 (100.000)
Epoch: [160][20/196]	Time 0.011 (0.016)	Data 0.006 (0.012)	Loss 0.4854 (0.4820)	Acc@1 100.000 (99.888)	Acc@5 100.000 (100.000)
Epoch: [160][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 0.4885 (0.4816)	Acc@1 99.609 (99.912)	Acc@5 100.000 (100.000)
Epoch: [160][40/196]	Time 0.012 (0.015)	Data 0.007 (0.007)	Loss 0.4856 (0.4813)	Acc@1 99.609 (99.914)	Acc@5 100.000 (100.000)
Epoch: [160][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.4951 (0.4816)	Acc@1 99.609 (99.893)	Acc@5 100.000 (100.000)
Epoch: [160][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.4787 (0.4812)	Acc@1 100.000 (99.910)	Acc@5 100.000 (100.000)
Epoch: [160][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4779 (0.4810)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [160][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.4819 (0.4810)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [160][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4794 (0.4809)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Epoch: [160][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4833 (0.4808)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [160][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4783 (0.4807)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [160][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.4910 (0.4810)	Acc@1 99.219 (99.929)	Acc@5 100.000 (100.000)
Epoch: [160][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4782 (0.4809)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [160][140/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.4766 (0.4808)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [160][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.4778 (0.4807)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [160][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4773 (0.4807)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [160][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.4792 (0.4805)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [160][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.4788 (0.4805)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [160][190/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 0.4825 (0.4805)	Acc@1 99.609 (99.933)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [252, 127, 3, 3] >> [252, 127, 3, 3]
[module.conv4.weight]: [256, 252, 3, 3] >> [256, 252, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [476, 511, 3, 3] >> [476, 511, 3, 3]
[module.conv8.weight]: [299, 476, 3, 3] >> [299, 476, 3, 3]
[module.fc.weight]: [100, 299] >> [100, 299]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
70.47
python src/cifar.py --workers 4 --dataset cifar100 --epochs 170 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_170.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.94M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [252, 127, 3, 3]
conv4 --> [256, 252, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [476, 511, 3, 3]
conv8 --> [299, 476, 3, 3]
fc --> [299, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8406042624, 18434304, 252
4, 16944463872, 37158912, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6724988928, 8756496, 476
8, 3934973952, 5123664, 299
fc, 11481600, 29900, 0
===================
FLOP REPORT: 28362032400000.0 52209600000.0 140132828 130524 2475 15.139837265014648

Epoch: [161 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [161][0/196]	Time 0.736 (0.736)	Data 0.173 (0.173)	Loss 0.4781 (0.4781)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [161][10/196]	Time 0.014 (0.080)	Data 0.003 (0.017)	Loss 0.4749 (0.4781)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [161][20/196]	Time 0.015 (0.049)	Data 0.002 (0.010)	Loss 0.4769 (0.4786)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [161][30/196]	Time 0.015 (0.038)	Data 0.003 (0.008)	Loss 0.4794 (0.4788)	Acc@1 100.000 (99.975)	Acc@5 100.000 (100.000)
Epoch: [161][40/196]	Time 0.015 (0.032)	Data 0.002 (0.006)	Loss 0.4776 (0.4786)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [161][50/196]	Time 0.012 (0.028)	Data 0.004 (0.006)	Loss 0.4838 (0.4788)	Acc@1 99.609 (99.946)	Acc@5 100.000 (100.000)
Epoch: [161][60/196]	Time 0.014 (0.026)	Data 0.002 (0.006)	Loss 0.4848 (0.4789)	Acc@1 99.609 (99.936)	Acc@5 100.000 (100.000)
Epoch: [161][70/196]	Time 0.016 (0.024)	Data 0.000 (0.006)	Loss 0.4806 (0.4789)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [161][80/196]	Time 0.014 (0.023)	Data 0.002 (0.005)	Loss 0.4754 (0.4790)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [161][90/196]	Time 0.016 (0.022)	Data 0.000 (0.005)	Loss 0.4752 (0.4790)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [161][100/196]	Time 0.015 (0.022)	Data 0.002 (0.005)	Loss 0.4842 (0.4791)	Acc@1 99.609 (99.930)	Acc@5 100.000 (100.000)
Epoch: [161][110/196]	Time 0.017 (0.021)	Data 0.000 (0.005)	Loss 0.4776 (0.4791)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [161][120/196]	Time 0.014 (0.020)	Data 0.003 (0.005)	Loss 0.4782 (0.4791)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [161][130/196]	Time 0.016 (0.020)	Data 0.000 (0.005)	Loss 0.4789 (0.4792)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [161][140/196]	Time 0.014 (0.020)	Data 0.003 (0.005)	Loss 0.4833 (0.4791)	Acc@1 99.609 (99.934)	Acc@5 100.000 (100.000)
Epoch: [161][150/196]	Time 0.016 (0.019)	Data 0.000 (0.005)	Loss 0.4773 (0.4790)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [161][160/196]	Time 0.014 (0.019)	Data 0.003 (0.004)	Loss 0.4757 (0.4788)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [161][170/196]	Time 0.016 (0.019)	Data 0.000 (0.004)	Loss 0.4764 (0.4789)	Acc@1 100.000 (99.941)	Acc@5 100.000 (100.000)
Epoch: [161][180/196]	Time 0.014 (0.018)	Data 0.002 (0.004)	Loss 0.4761 (0.4787)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [161][190/196]	Time 0.014 (0.018)	Data 0.000 (0.004)	Loss 0.4815 (0.4788)	Acc@1 99.609 (99.943)	Acc@5 100.000 (100.000)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [162 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [162][0/196]	Time 0.029 (0.029)	Data 0.166 (0.166)	Loss 0.4776 (0.4776)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [162][10/196]	Time 0.014 (0.016)	Data 0.002 (0.017)	Loss 0.4796 (0.4783)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [162][20/196]	Time 0.011 (0.015)	Data 0.011 (0.011)	Loss 0.4800 (0.4785)	Acc@1 99.609 (99.926)	Acc@5 100.000 (100.000)
Epoch: [162][30/196]	Time 0.014 (0.015)	Data 0.003 (0.008)	Loss 0.4798 (0.4786)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [162][40/196]	Time 0.012 (0.015)	Data 0.011 (0.007)	Loss 0.4773 (0.4787)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [162][50/196]	Time 0.016 (0.014)	Data 0.000 (0.007)	Loss 0.4819 (0.4785)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [162][60/196]	Time 0.012 (0.014)	Data 0.009 (0.006)	Loss 0.4757 (0.4786)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [162][70/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.4764 (0.4784)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [162][80/196]	Time 0.012 (0.014)	Data 0.006 (0.006)	Loss 0.4906 (0.4784)	Acc@1 99.219 (99.947)	Acc@5 100.000 (100.000)
Epoch: [162][90/196]	Time 0.014 (0.014)	Data 0.003 (0.006)	Loss 0.4800 (0.4784)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [162][100/196]	Time 0.013 (0.014)	Data 0.010 (0.005)	Loss 0.4808 (0.4785)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [162][110/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.4776 (0.4785)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [162][120/196]	Time 0.012 (0.014)	Data 0.010 (0.005)	Loss 0.4768 (0.4785)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [162][130/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.4768 (0.4784)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [162][140/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.4779 (0.4785)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [162][150/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4770 (0.4785)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [162][160/196]	Time 0.011 (0.014)	Data 0.012 (0.005)	Loss 0.4789 (0.4784)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [162][170/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4802 (0.4784)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [162][180/196]	Time 0.012 (0.014)	Data 0.011 (0.005)	Loss 0.4786 (0.4784)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [162][190/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4763 (0.4783)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [163 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [163][0/196]	Time 0.031 (0.031)	Data 0.173 (0.173)	Loss 0.4759 (0.4759)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [163][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.4771 (0.4791)	Acc@1 100.000 (99.822)	Acc@5 100.000 (100.000)
Epoch: [163][20/196]	Time 0.014 (0.015)	Data 0.005 (0.010)	Loss 0.4786 (0.4785)	Acc@1 100.000 (99.888)	Acc@5 100.000 (100.000)
Epoch: [163][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 0.4751 (0.4780)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [163][40/196]	Time 0.011 (0.015)	Data 0.011 (0.007)	Loss 0.4741 (0.4779)	Acc@1 100.000 (99.905)	Acc@5 100.000 (100.000)
Epoch: [163][50/196]	Time 0.016 (0.015)	Data 0.001 (0.007)	Loss 0.4782 (0.4777)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [163][60/196]	Time 0.011 (0.014)	Data 0.009 (0.006)	Loss 0.4792 (0.4778)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [163][70/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.4744 (0.4777)	Acc@1 100.000 (99.906)	Acc@5 100.000 (100.000)
Epoch: [163][80/196]	Time 0.011 (0.014)	Data 0.010 (0.006)	Loss 0.4751 (0.4775)	Acc@1 100.000 (99.918)	Acc@5 100.000 (100.000)
Epoch: [163][90/196]	Time 0.014 (0.014)	Data 0.003 (0.006)	Loss 0.4789 (0.4774)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [163][100/196]	Time 0.011 (0.014)	Data 0.011 (0.006)	Loss 0.4783 (0.4773)	Acc@1 99.609 (99.923)	Acc@5 100.000 (100.000)
Epoch: [163][110/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4783 (0.4773)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [163][120/196]	Time 0.012 (0.014)	Data 0.009 (0.005)	Loss 0.4752 (0.4771)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [163][130/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4790 (0.4771)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [163][140/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.4746 (0.4771)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [163][150/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4762 (0.4770)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [163][160/196]	Time 0.011 (0.014)	Data 0.011 (0.005)	Loss 0.4757 (0.4771)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [163][170/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 0.4772 (0.4770)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [163][180/196]	Time 0.011 (0.014)	Data 0.012 (0.005)	Loss 0.4773 (0.4770)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [163][190/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.4737 (0.4769)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [164 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [164][0/196]	Time 0.028 (0.028)	Data 0.178 (0.178)	Loss 0.4749 (0.4749)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [164][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.4785 (0.4766)	Acc@1 100.000 (99.893)	Acc@5 100.000 (100.000)
Epoch: [164][20/196]	Time 0.012 (0.015)	Data 0.009 (0.012)	Loss 0.4778 (0.4772)	Acc@1 100.000 (99.870)	Acc@5 100.000 (100.000)
Epoch: [164][30/196]	Time 0.017 (0.015)	Data 0.000 (0.009)	Loss 0.4733 (0.4766)	Acc@1 100.000 (99.912)	Acc@5 100.000 (100.000)
Epoch: [164][40/196]	Time 0.012 (0.015)	Data 0.009 (0.007)	Loss 0.4753 (0.4767)	Acc@1 100.000 (99.886)	Acc@5 100.000 (100.000)
Epoch: [164][50/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 0.4732 (0.4763)	Acc@1 100.000 (99.900)	Acc@5 100.000 (100.000)
Epoch: [164][60/196]	Time 0.011 (0.015)	Data 0.022 (0.006)	Loss 0.4786 (0.4763)	Acc@1 100.000 (99.917)	Acc@5 100.000 (100.000)
Epoch: [164][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.4749 (0.4762)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [164][80/196]	Time 0.011 (0.015)	Data 0.015 (0.006)	Loss 0.4732 (0.4761)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [164][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4759 (0.4762)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Epoch: [164][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.4734 (0.4761)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [164][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4738 (0.4760)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [164][120/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 0.4733 (0.4759)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [164][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4732 (0.4759)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [164][140/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.4768 (0.4759)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [164][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4741 (0.4758)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [164][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4764 (0.4759)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Epoch: [164][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4776 (0.4758)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [164][180/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 0.4747 (0.4757)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Epoch: [164][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 0.4869 (0.4758)	Acc@1 99.219 (99.930)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [165 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [165][0/196]	Time 0.030 (0.030)	Data 0.176 (0.176)	Loss 0.4730 (0.4730)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [165][10/196]	Time 0.014 (0.016)	Data 0.002 (0.018)	Loss 0.4725 (0.4739)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [165][20/196]	Time 0.011 (0.015)	Data 0.008 (0.011)	Loss 0.4719 (0.4744)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [165][30/196]	Time 0.014 (0.015)	Data 0.003 (0.009)	Loss 0.4756 (0.4747)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [165][40/196]	Time 0.011 (0.014)	Data 0.009 (0.008)	Loss 0.4734 (0.4746)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [165][50/196]	Time 0.016 (0.014)	Data 0.000 (0.007)	Loss 0.4727 (0.4744)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [165][60/196]	Time 0.011 (0.014)	Data 0.010 (0.007)	Loss 0.4756 (0.4743)	Acc@1 100.000 (99.968)	Acc@5 100.000 (100.000)
Epoch: [165][70/196]	Time 0.016 (0.014)	Data 0.001 (0.006)	Loss 0.4768 (0.4743)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [165][80/196]	Time 0.013 (0.014)	Data 0.018 (0.006)	Loss 0.4715 (0.4743)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [165][90/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.4742 (0.4745)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [165][100/196]	Time 0.012 (0.014)	Data 0.022 (0.006)	Loss 0.4745 (0.4744)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [165][110/196]	Time 0.016 (0.014)	Data 0.001 (0.006)	Loss 0.4735 (0.4744)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [165][120/196]	Time 0.011 (0.014)	Data 0.010 (0.006)	Loss 0.4753 (0.4744)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [165][130/196]	Time 0.017 (0.014)	Data 0.001 (0.005)	Loss 0.4744 (0.4745)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [165][140/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.4710 (0.4746)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [165][150/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4719 (0.4745)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [165][160/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 0.4756 (0.4744)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [165][170/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4738 (0.4745)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [165][180/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.4739 (0.4745)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [165][190/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4750 (0.4745)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [166 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [166][0/196]	Time 0.027 (0.027)	Data 0.189 (0.189)	Loss 0.4754 (0.4754)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [166][10/196]	Time 0.015 (0.017)	Data 0.004 (0.019)	Loss 0.4726 (0.4743)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [166][20/196]	Time 0.011 (0.016)	Data 0.005 (0.011)	Loss 0.4722 (0.4735)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [166][30/196]	Time 0.015 (0.015)	Data 0.000 (0.008)	Loss 0.4703 (0.4732)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [166][40/196]	Time 0.011 (0.015)	Data 0.010 (0.007)	Loss 0.4742 (0.4736)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [166][50/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 0.4740 (0.4737)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [166][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 0.4715 (0.4735)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [166][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.4744 (0.4735)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [166][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.4746 (0.4737)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [166][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4728 (0.4739)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [166][100/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 0.4714 (0.4739)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [166][110/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4759 (0.4739)	Acc@1 99.609 (99.923)	Acc@5 100.000 (100.000)
Epoch: [166][120/196]	Time 0.011 (0.014)	Data 0.009 (0.005)	Loss 0.4724 (0.4738)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [166][130/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 0.4710 (0.4738)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [166][140/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4743 (0.4739)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Epoch: [166][150/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4705 (0.4738)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Epoch: [166][160/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.4768 (0.4739)	Acc@1 100.000 (99.920)	Acc@5 100.000 (100.000)
Epoch: [166][170/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4742 (0.4739)	Acc@1 100.000 (99.925)	Acc@5 100.000 (100.000)
Epoch: [166][180/196]	Time 0.015 (0.014)	Data 0.003 (0.005)	Loss 0.4713 (0.4738)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [166][190/196]	Time 0.019 (0.014)	Data 0.000 (0.005)	Loss 0.4724 (0.4739)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [167 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [167][0/196]	Time 0.029 (0.029)	Data 0.197 (0.197)	Loss 0.4716 (0.4716)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [167][10/196]	Time 0.012 (0.016)	Data 0.005 (0.020)	Loss 0.4735 (0.4726)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [167][20/196]	Time 0.012 (0.015)	Data 0.005 (0.012)	Loss 0.4728 (0.4727)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [167][30/196]	Time 0.014 (0.015)	Data 0.003 (0.009)	Loss 0.4719 (0.4726)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [167][40/196]	Time 0.011 (0.015)	Data 0.008 (0.008)	Loss 0.4729 (0.4728)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [167][50/196]	Time 0.014 (0.015)	Data 0.002 (0.007)	Loss 0.4712 (0.4726)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [167][60/196]	Time 0.011 (0.015)	Data 0.017 (0.007)	Loss 0.4735 (0.4726)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [167][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.4728 (0.4725)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [167][80/196]	Time 0.011 (0.015)	Data 0.012 (0.006)	Loss 0.4705 (0.4724)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [167][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4695 (0.4725)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [167][100/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 0.4751 (0.4725)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [167][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4711 (0.4726)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [167][120/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 0.4749 (0.4726)	Acc@1 99.609 (99.948)	Acc@5 100.000 (100.000)
Epoch: [167][130/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4702 (0.4725)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [167][140/196]	Time 0.011 (0.015)	Data 0.019 (0.005)	Loss 0.4691 (0.4724)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [167][150/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4696 (0.4724)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [167][160/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 0.4718 (0.4724)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [167][170/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.4723 (0.4723)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [167][180/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.4720 (0.4723)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [167][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4722 (0.4723)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [168 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [168][0/196]	Time 0.029 (0.029)	Data 0.179 (0.179)	Loss 0.4725 (0.4725)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [168][10/196]	Time 0.014 (0.016)	Data 0.002 (0.019)	Loss 0.4723 (0.4727)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [168][20/196]	Time 0.013 (0.015)	Data 0.004 (0.011)	Loss 0.4713 (0.4721)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [168][30/196]	Time 0.014 (0.015)	Data 0.002 (0.009)	Loss 0.4722 (0.4721)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [168][40/196]	Time 0.012 (0.014)	Data 0.009 (0.008)	Loss 0.4705 (0.4720)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [168][50/196]	Time 0.016 (0.014)	Data 0.001 (0.007)	Loss 0.4722 (0.4720)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [168][60/196]	Time 0.011 (0.014)	Data 0.006 (0.007)	Loss 0.4694 (0.4717)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [168][70/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.4712 (0.4718)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [168][80/196]	Time 0.011 (0.014)	Data 0.006 (0.006)	Loss 0.4722 (0.4718)	Acc@1 99.609 (99.937)	Acc@5 100.000 (100.000)
Epoch: [168][90/196]	Time 0.016 (0.014)	Data 0.001 (0.006)	Loss 0.4701 (0.4719)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [168][100/196]	Time 0.011 (0.014)	Data 0.006 (0.006)	Loss 0.4711 (0.4717)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [168][110/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.4714 (0.4718)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [168][120/196]	Time 0.013 (0.014)	Data 0.004 (0.006)	Loss 0.4690 (0.4719)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [168][130/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.4763 (0.4719)	Acc@1 99.609 (99.925)	Acc@5 100.000 (100.000)
Epoch: [168][140/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 0.4690 (0.4718)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [168][150/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 0.4717 (0.4718)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Epoch: [168][160/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 0.4704 (0.4718)	Acc@1 100.000 (99.918)	Acc@5 100.000 (100.000)
Epoch: [168][170/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4697 (0.4717)	Acc@1 100.000 (99.918)	Acc@5 100.000 (100.000)
Epoch: [168][180/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 0.4718 (0.4717)	Acc@1 100.000 (99.918)	Acc@5 100.000 (100.000)
Epoch: [168][190/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4705 (0.4717)	Acc@1 100.000 (99.918)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [169 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [169][0/196]	Time 0.027 (0.027)	Data 0.190 (0.190)	Loss 0.4762 (0.4762)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [169][10/196]	Time 0.016 (0.016)	Data 0.000 (0.020)	Loss 0.4704 (0.4717)	Acc@1 100.000 (99.893)	Acc@5 100.000 (100.000)
Epoch: [169][20/196]	Time 0.011 (0.015)	Data 0.009 (0.012)	Loss 0.4698 (0.4708)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [169][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 0.4700 (0.4703)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [169][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 0.4709 (0.4702)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [169][50/196]	Time 0.014 (0.015)	Data 0.002 (0.007)	Loss 0.4697 (0.4700)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [169][60/196]	Time 0.011 (0.015)	Data 0.014 (0.006)	Loss 0.4707 (0.4702)	Acc@1 100.000 (99.968)	Acc@5 100.000 (100.000)
Epoch: [169][70/196]	Time 0.014 (0.015)	Data 0.000 (0.006)	Loss 0.4678 (0.4700)	Acc@1 100.000 (99.972)	Acc@5 100.000 (100.000)
Epoch: [169][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.4709 (0.4701)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Epoch: [169][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.4702 (0.4702)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [169][100/196]	Time 0.013 (0.015)	Data 0.008 (0.005)	Loss 0.4680 (0.4701)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [169][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.4718 (0.4700)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [169][120/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 0.4710 (0.4701)	Acc@1 99.609 (99.952)	Acc@5 100.000 (100.000)
Epoch: [169][130/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.4684 (0.4701)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [169][140/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.4687 (0.4701)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [169][150/196]	Time 0.011 (0.015)	Data 0.005 (0.004)	Loss 0.4765 (0.4701)	Acc@1 99.609 (99.948)	Acc@5 100.000 (100.000)
Epoch: [169][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.4700 (0.4701)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [169][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4711 (0.4701)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [169][180/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 0.4744 (0.4701)	Acc@1 99.609 (99.942)	Acc@5 100.000 (100.000)
Epoch: [169][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4692 (0.4701)	Acc@1 100.000 (99.941)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [170 | 170] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [170][0/196]	Time 0.029 (0.029)	Data 0.186 (0.186)	Loss 0.4692 (0.4692)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [170][10/196]	Time 0.013 (0.016)	Data 0.004 (0.019)	Loss 0.4675 (0.4685)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [170][20/196]	Time 0.015 (0.015)	Data 0.002 (0.011)	Loss 0.4684 (0.4690)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [170][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 0.4728 (0.4692)	Acc@1 99.609 (99.924)	Acc@5 100.000 (100.000)
Epoch: [170][40/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 0.4669 (0.4690)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [170][50/196]	Time 0.016 (0.014)	Data 0.000 (0.007)	Loss 0.4662 (0.4689)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [170][60/196]	Time 0.014 (0.014)	Data 0.002 (0.006)	Loss 0.4694 (0.4689)	Acc@1 100.000 (99.955)	Acc@5 100.000 (100.000)
Epoch: [170][70/196]	Time 0.017 (0.014)	Data 0.000 (0.006)	Loss 0.4699 (0.4690)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [170][80/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 0.4707 (0.4692)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [170][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4724 (0.4692)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [170][100/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4664 (0.4691)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [170][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4687 (0.4691)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [170][120/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.4684 (0.4693)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [170][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4685 (0.4693)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [170][140/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4672 (0.4693)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [170][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.4674 (0.4693)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [170][160/196]	Time 0.016 (0.014)	Data 0.002 (0.005)	Loss 0.4723 (0.4693)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [170][170/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4689 (0.4694)	Acc@1 100.000 (99.941)	Acc@5 100.000 (100.000)
Epoch: [170][180/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4672 (0.4694)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [170][190/196]	Time 0.017 (0.014)	Data 0.000 (0.005)	Loss 0.4692 (0.4694)	Acc@1 100.000 (99.941)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [252, 127, 3, 3] >> [252, 127, 3, 3]
[module.conv4.weight]: [256, 252, 3, 3] >> [256, 252, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [476, 511, 3, 3] >> [476, 511, 3, 3]
[module.conv8.weight]: [299, 476, 3, 3] >> [299, 476, 3, 3]
[module.fc.weight]: [100, 299] >> [100, 299]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
70.52
python src/cifar.py --workers 4 --dataset cifar100 --epochs 180 --learning-rate 0.1 --schedule 91 136 --checkpoint ./output/cifar/vgg11/0.2 --arch vgg11_bn_flat --gpu-id 0 --train_batch 128 --test_batch 100 --save_checkpoin 30 --resume ./output/cifar/vgg11/0.2/checkpoint.pth.tar --sparse_interval 10 --threshold 0.0001 --var_group_lasso_coeff 0.2 --arch_name vgg11_bn_flat_0.2_180.py --en_group_lasso  --arch_out_dir1 src/models/cifar --arch_out_dir2 ./output/cifar/vgg11/arch/0.2
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Files already downloaded and verified
==> creating model 'vgg11_bn_flat'
    Total params: 7.94M
==> Resuming from checkpoint..
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [127, 45, 3, 3]
conv3 --> [252, 127, 3, 3]
conv4 --> [256, 252, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [511, 509, 3, 3]
conv7 --> [476, 511, 3, 3]
conv8 --> [299, 476, 3, 3]
fc --> [299, 100]
1, 498286080, 1244160, 45
2, 5503956480, 13167360, 127
3, 8406042624, 18434304, 252
4, 16944463872, 37158912, 256
5, 10207494144, 18763776, 509
6, 20375115264, 37454256, 511
7, 6724988928, 8756496, 476
8, 3934973952, 5123664, 299
fc, 11481600, 29900, 0
===================
FLOP REPORT: 28362032400000.0 52209600000.0 140132828 130524 2475 15.139837265014648

Epoch: [171 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
src/cifar.py:306: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [171][0/196]	Time 0.742 (0.742)	Data 0.167 (0.167)	Loss 0.4669 (0.4669)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [171][10/196]	Time 0.014 (0.081)	Data 0.002 (0.017)	Loss 0.4686 (0.4688)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [171][20/196]	Time 0.015 (0.049)	Data 0.003 (0.010)	Loss 0.4660 (0.4685)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [171][30/196]	Time 0.015 (0.038)	Data 0.002 (0.008)	Loss 0.4689 (0.4689)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [171][40/196]	Time 0.014 (0.032)	Data 0.002 (0.006)	Loss 0.4699 (0.4688)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [171][50/196]	Time 0.014 (0.029)	Data 0.003 (0.006)	Loss 0.4678 (0.4687)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [171][60/196]	Time 0.015 (0.027)	Data 0.003 (0.005)	Loss 0.4681 (0.4685)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [171][70/196]	Time 0.015 (0.025)	Data 0.002 (0.005)	Loss 0.4676 (0.4685)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [171][80/196]	Time 0.014 (0.024)	Data 0.002 (0.004)	Loss 0.4671 (0.4685)	Acc@1 100.000 (99.928)	Acc@5 100.000 (100.000)
Epoch: [171][90/196]	Time 0.011 (0.023)	Data 0.007 (0.004)	Loss 0.4648 (0.4685)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [171][100/196]	Time 0.016 (0.022)	Data 0.000 (0.004)	Loss 0.4676 (0.4684)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [171][110/196]	Time 0.011 (0.021)	Data 0.008 (0.004)	Loss 0.4649 (0.4684)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [171][120/196]	Time 0.016 (0.021)	Data 0.000 (0.004)	Loss 0.4688 (0.4684)	Acc@1 100.000 (99.932)	Acc@5 100.000 (100.000)
Epoch: [171][130/196]	Time 0.012 (0.020)	Data 0.008 (0.004)	Loss 0.4714 (0.4684)	Acc@1 100.000 (99.931)	Acc@5 100.000 (100.000)
Epoch: [171][140/196]	Time 0.016 (0.020)	Data 0.001 (0.004)	Loss 0.4715 (0.4683)	Acc@1 99.609 (99.931)	Acc@5 100.000 (100.000)
Epoch: [171][150/196]	Time 0.011 (0.019)	Data 0.009 (0.004)	Loss 0.4728 (0.4683)	Acc@1 99.609 (99.930)	Acc@5 100.000 (100.000)
Epoch: [171][160/196]	Time 0.016 (0.019)	Data 0.001 (0.004)	Loss 0.4656 (0.4683)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [171][170/196]	Time 0.012 (0.019)	Data 0.005 (0.004)	Loss 0.4735 (0.4683)	Acc@1 99.609 (99.929)	Acc@5 100.000 (100.000)
Epoch: [171][180/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 0.4677 (0.4682)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [171][190/196]	Time 0.011 (0.018)	Data 0.006 (0.004)	Loss 0.4700 (0.4681)	Acc@1 99.609 (99.935)	Acc@5 100.000 (100.000)
src/cifar.py:394: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
[INFO] Storing checkpoint...

Epoch: [172 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [172][0/196]	Time 0.028 (0.028)	Data 0.165 (0.165)	Loss 0.4659 (0.4659)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [172][10/196]	Time 0.017 (0.016)	Data 0.000 (0.017)	Loss 0.4670 (0.4662)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [172][20/196]	Time 0.012 (0.015)	Data 0.005 (0.010)	Loss 0.4677 (0.4667)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [172][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.4663 (0.4667)	Acc@1 100.000 (99.975)	Acc@5 100.000 (100.000)
Epoch: [172][40/196]	Time 0.011 (0.015)	Data 0.014 (0.007)	Loss 0.4662 (0.4667)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [172][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.4656 (0.4669)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [172][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 0.4656 (0.4670)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [172][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4662 (0.4669)	Acc@1 100.000 (99.967)	Acc@5 100.000 (100.000)
Epoch: [172][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.4706 (0.4669)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Epoch: [172][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4649 (0.4669)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [172][100/196]	Time 0.010 (0.015)	Data 0.008 (0.005)	Loss 0.4632 (0.4669)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [172][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4673 (0.4669)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [172][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4645 (0.4669)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [172][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4665 (0.4669)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [172][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4678 (0.4670)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [172][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4709 (0.4670)	Acc@1 99.609 (99.956)	Acc@5 100.000 (100.000)
Epoch: [172][160/196]	Time 0.011 (0.015)	Data 0.016 (0.004)	Loss 0.4646 (0.4670)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Epoch: [172][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4640 (0.4670)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [172][180/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.4641 (0.4671)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [172][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4662 (0.4670)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [173 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [173][0/196]	Time 0.027 (0.027)	Data 0.164 (0.164)	Loss 0.4678 (0.4678)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [173][10/196]	Time 0.014 (0.015)	Data 0.002 (0.017)	Loss 0.4636 (0.4656)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [173][20/196]	Time 0.014 (0.015)	Data 0.003 (0.010)	Loss 0.4675 (0.4661)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [173][30/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 0.4661 (0.4662)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [173][40/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 0.4649 (0.4661)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [173][50/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 0.4652 (0.4662)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [173][60/196]	Time 0.011 (0.014)	Data 0.008 (0.006)	Loss 0.4671 (0.4662)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [173][70/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 0.4643 (0.4661)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [173][80/196]	Time 0.015 (0.014)	Data 0.003 (0.005)	Loss 0.4656 (0.4661)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [173][90/196]	Time 0.011 (0.014)	Data 0.012 (0.005)	Loss 0.4651 (0.4660)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [173][100/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4662 (0.4660)	Acc@1 99.609 (99.946)	Acc@5 100.000 (100.000)
Epoch: [173][110/196]	Time 0.011 (0.014)	Data 0.012 (0.005)	Loss 0.4647 (0.4660)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [173][120/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4784 (0.4661)	Acc@1 99.219 (99.945)	Acc@5 100.000 (100.000)
Epoch: [173][130/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.4650 (0.4660)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [173][140/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4646 (0.4660)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [173][150/196]	Time 0.010 (0.014)	Data 0.007 (0.004)	Loss 0.4684 (0.4660)	Acc@1 99.609 (99.948)	Acc@5 100.000 (100.000)
Epoch: [173][160/196]	Time 0.013 (0.014)	Data 0.004 (0.004)	Loss 0.4657 (0.4660)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [173][170/196]	Time 0.011 (0.014)	Data 0.007 (0.004)	Loss 0.4666 (0.4660)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [173][180/196]	Time 0.013 (0.014)	Data 0.004 (0.004)	Loss 0.4634 (0.4660)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [173][190/196]	Time 0.010 (0.014)	Data 0.009 (0.004)	Loss 0.4681 (0.4660)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [174 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [174][0/196]	Time 0.027 (0.027)	Data 0.174 (0.174)	Loss 0.4641 (0.4641)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [174][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 0.4649 (0.4649)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [174][20/196]	Time 0.011 (0.015)	Data 0.005 (0.011)	Loss 0.4650 (0.4651)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [174][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 0.4681 (0.4657)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [174][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 0.4625 (0.4654)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [174][50/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 0.4677 (0.4653)	Acc@1 99.609 (99.954)	Acc@5 100.000 (100.000)
Epoch: [174][60/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 0.4742 (0.4653)	Acc@1 99.609 (99.949)	Acc@5 100.000 (100.000)
Epoch: [174][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.4667 (0.4652)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Epoch: [174][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.4644 (0.4651)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [174][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4638 (0.4650)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [174][100/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 0.4712 (0.4650)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [174][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4650 (0.4650)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [174][120/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 0.4668 (0.4649)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [174][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4634 (0.4650)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [174][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4623 (0.4649)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [174][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4620 (0.4649)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [174][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.4734 (0.4649)	Acc@1 99.609 (99.959)	Acc@5 100.000 (100.000)
Epoch: [174][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4633 (0.4650)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [174][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 0.4673 (0.4650)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [174][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4623 (0.4649)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [175 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [175][0/196]	Time 0.027 (0.027)	Data 0.178 (0.178)	Loss 0.4649 (0.4649)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [175][10/196]	Time 0.014 (0.016)	Data 0.002 (0.018)	Loss 0.4637 (0.4639)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [175][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 0.4629 (0.4642)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [175][30/196]	Time 0.014 (0.015)	Data 0.002 (0.008)	Loss 0.4616 (0.4640)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [175][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 0.4631 (0.4643)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [175][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 0.4620 (0.4642)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [175][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 0.4692 (0.4643)	Acc@1 99.609 (99.955)	Acc@5 100.000 (100.000)
Epoch: [175][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.4647 (0.4641)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [175][80/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.4631 (0.4639)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Epoch: [175][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.4644 (0.4638)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Epoch: [175][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4629 (0.4640)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [175][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4640 (0.4640)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Epoch: [175][120/196]	Time 0.010 (0.015)	Data 0.008 (0.004)	Loss 0.4646 (0.4640)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [175][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4633 (0.4641)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [175][140/196]	Time 0.010 (0.015)	Data 0.008 (0.004)	Loss 0.4651 (0.4641)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [175][150/196]	Time 0.023 (0.015)	Data 0.000 (0.004)	Loss 0.4610 (0.4641)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [175][160/196]	Time 0.010 (0.015)	Data 0.008 (0.004)	Loss 0.4630 (0.4642)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [175][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4630 (0.4641)	Acc@1 100.000 (99.941)	Acc@5 100.000 (100.000)
Epoch: [175][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4623 (0.4640)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [175][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 0.4624 (0.4639)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [176 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [176][0/196]	Time 0.028 (0.028)	Data 0.187 (0.187)	Loss 0.4628 (0.4628)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [176][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 0.4621 (0.4622)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [176][20/196]	Time 0.014 (0.015)	Data 0.003 (0.011)	Loss 0.4616 (0.4624)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [176][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.4608 (0.4630)	Acc@1 100.000 (99.975)	Acc@5 100.000 (100.000)
Epoch: [176][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 0.4623 (0.4628)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [176][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.4624 (0.4626)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [176][60/196]	Time 0.011 (0.015)	Data 0.016 (0.006)	Loss 0.4631 (0.4627)	Acc@1 100.000 (99.987)	Acc@5 100.000 (100.000)
Epoch: [176][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.4609 (0.4625)	Acc@1 100.000 (99.989)	Acc@5 100.000 (100.000)
Epoch: [176][80/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 0.4627 (0.4625)	Acc@1 100.000 (99.986)	Acc@5 100.000 (100.000)
Epoch: [176][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.4678 (0.4626)	Acc@1 99.609 (99.979)	Acc@5 100.000 (100.000)
Epoch: [176][100/196]	Time 0.011 (0.015)	Data 0.015 (0.005)	Loss 0.4650 (0.4626)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [176][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.4613 (0.4629)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Epoch: [176][120/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.4627 (0.4629)	Acc@1 100.000 (99.955)	Acc@5 100.000 (100.000)
Epoch: [176][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.4617 (0.4629)	Acc@1 100.000 (99.955)	Acc@5 100.000 (100.000)
Epoch: [176][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4621 (0.4629)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [176][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.4652 (0.4629)	Acc@1 99.609 (99.951)	Acc@5 100.000 (100.000)
Epoch: [176][160/196]	Time 0.010 (0.015)	Data 0.010 (0.004)	Loss 0.4614 (0.4630)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [176][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.4610 (0.4629)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [176][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.4611 (0.4629)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [176][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.4631 (0.4629)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [177 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [177][0/196]	Time 0.027 (0.027)	Data 0.181 (0.181)	Loss 0.4652 (0.4652)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [177][10/196]	Time 0.017 (0.016)	Data 0.000 (0.019)	Loss 0.4638 (0.4620)	Acc@1 99.609 (99.893)	Acc@5 100.000 (100.000)
Epoch: [177][20/196]	Time 0.015 (0.015)	Data 0.002 (0.011)	Loss 0.4616 (0.4625)	Acc@1 100.000 (99.907)	Acc@5 100.000 (100.000)
Epoch: [177][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.4673 (0.4626)	Acc@1 99.609 (99.912)	Acc@5 100.000 (100.000)
Epoch: [177][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 0.4594 (0.4624)	Acc@1 100.000 (99.933)	Acc@5 100.000 (100.000)
Epoch: [177][50/196]	Time 0.016 (0.014)	Data 0.000 (0.007)	Loss 0.4618 (0.4624)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [177][60/196]	Time 0.013 (0.014)	Data 0.008 (0.006)	Loss 0.4617 (0.4622)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [177][70/196]	Time 0.016 (0.014)	Data 0.001 (0.006)	Loss 0.4603 (0.4623)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [177][80/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 0.4613 (0.4622)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [177][90/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4608 (0.4623)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [177][100/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 0.4612 (0.4622)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [177][110/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4609 (0.4621)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [177][120/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 0.4639 (0.4621)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Epoch: [177][130/196]	Time 0.017 (0.014)	Data 0.001 (0.005)	Loss 0.4600 (0.4621)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [177][140/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 0.4634 (0.4621)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [177][150/196]	Time 0.017 (0.014)	Data 0.001 (0.005)	Loss 0.4611 (0.4620)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Epoch: [177][160/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4636 (0.4620)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Epoch: [177][170/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 0.4681 (0.4620)	Acc@1 99.609 (99.952)	Acc@5 100.000 (100.000)
Epoch: [177][180/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.4584 (0.4619)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [177][190/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 0.4627 (0.4618)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [178 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [178][0/196]	Time 0.028 (0.028)	Data 0.180 (0.180)	Loss 0.4597 (0.4597)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [178][10/196]	Time 0.017 (0.016)	Data 0.001 (0.019)	Loss 0.4604 (0.4603)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [178][20/196]	Time 0.011 (0.015)	Data 0.007 (0.011)	Loss 0.4600 (0.4604)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [178][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 0.4594 (0.4605)	Acc@1 100.000 (99.975)	Acc@5 100.000 (100.000)
Epoch: [178][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 0.4615 (0.4609)	Acc@1 99.609 (99.952)	Acc@5 100.000 (100.000)
Epoch: [178][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.4605 (0.4609)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [178][60/196]	Time 0.015 (0.015)	Data 0.004 (0.006)	Loss 0.4591 (0.4607)	Acc@1 100.000 (99.955)	Acc@5 100.000 (100.000)
Epoch: [178][70/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4590 (0.4608)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [178][80/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 0.4601 (0.4607)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [178][90/196]	Time 0.016 (0.014)	Data 0.001 (0.005)	Loss 0.4599 (0.4609)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [178][100/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.4689 (0.4610)	Acc@1 99.609 (99.950)	Acc@5 100.000 (100.000)
Epoch: [178][110/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4602 (0.4610)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Epoch: [178][120/196]	Time 0.012 (0.014)	Data 0.005 (0.005)	Loss 0.4606 (0.4609)	Acc@1 100.000 (99.955)	Acc@5 100.000 (100.000)
Epoch: [178][130/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.4695 (0.4608)	Acc@1 99.609 (99.955)	Acc@5 100.000 (100.000)
Epoch: [178][140/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 0.4595 (0.4609)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [178][150/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.4598 (0.4609)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [178][160/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.4645 (0.4608)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [178][170/196]	Time 0.014 (0.014)	Data 0.004 (0.005)	Loss 0.4596 (0.4608)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [178][180/196]	Time 0.011 (0.014)	Data 0.007 (0.005)	Loss 0.4584 (0.4607)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [178][190/196]	Time 0.011 (0.014)	Data 0.020 (0.005)	Loss 0.4600 (0.4607)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [179 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [179][0/196]	Time 0.030 (0.030)	Data 0.183 (0.183)	Loss 0.4683 (0.4683)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [179][10/196]	Time 0.016 (0.016)	Data 0.001 (0.019)	Loss 0.4584 (0.4600)	Acc@1 100.000 (99.929)	Acc@5 100.000 (100.000)
Epoch: [179][20/196]	Time 0.013 (0.015)	Data 0.004 (0.011)	Loss 0.4590 (0.4596)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [179][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 0.4601 (0.4597)	Acc@1 100.000 (99.975)	Acc@5 100.000 (100.000)
Epoch: [179][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 0.4663 (0.4598)	Acc@1 99.219 (99.952)	Acc@5 100.000 (100.000)
Epoch: [179][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 0.4583 (0.4599)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [179][60/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 0.4630 (0.4598)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [179][70/196]	Time 0.016 (0.014)	Data 0.000 (0.006)	Loss 0.4572 (0.4597)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Epoch: [179][80/196]	Time 0.011 (0.014)	Data 0.006 (0.006)	Loss 0.4659 (0.4601)	Acc@1 99.609 (99.937)	Acc@5 100.000 (100.000)
Epoch: [179][90/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4580 (0.4600)	Acc@1 100.000 (99.936)	Acc@5 100.000 (100.000)
Epoch: [179][100/196]	Time 0.013 (0.014)	Data 0.004 (0.005)	Loss 0.4608 (0.4600)	Acc@1 99.609 (99.934)	Acc@5 100.000 (100.000)
Epoch: [179][110/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4598 (0.4600)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [179][120/196]	Time 0.015 (0.014)	Data 0.003 (0.005)	Loss 0.4598 (0.4599)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [179][130/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.4572 (0.4598)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [179][140/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.4583 (0.4598)	Acc@1 100.000 (99.947)	Acc@5 100.000 (100.000)
Epoch: [179][150/196]	Time 0.017 (0.014)	Data 0.000 (0.005)	Loss 0.4585 (0.4597)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [179][160/196]	Time 0.017 (0.014)	Data 0.002 (0.005)	Loss 0.4609 (0.4597)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [179][170/196]	Time 0.016 (0.014)	Data 0.000 (0.004)	Loss 0.4573 (0.4597)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [179][180/196]	Time 0.014 (0.014)	Data 0.002 (0.004)	Loss 0.4572 (0.4596)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [179][190/196]	Time 0.017 (0.014)	Data 0.000 (0.004)	Loss 0.4611 (0.4596)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
[INFO] Storing checkpoint...

Epoch: [180 | 180] LR: 0.001000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [127, 45, 3, 3]
module.conv3.weight [252, 127, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [511, 509, 3, 3]
module.conv7.weight [476, 511, 3, 3]
module.conv8.weight [299, 476, 3, 3]
Epoch: [180][0/196]	Time 0.030 (0.030)	Data 0.181 (0.181)	Loss 0.4572 (0.4572)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [180][10/196]	Time 0.016 (0.016)	Data 0.000 (0.019)	Loss 0.4611 (0.4600)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Epoch: [180][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 0.4587 (0.4596)	Acc@1 100.000 (99.963)	Acc@5 100.000 (100.000)
Epoch: [180][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 0.4585 (0.4590)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [180][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 0.4569 (0.4589)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [180][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 0.4574 (0.4587)	Acc@1 100.000 (99.962)	Acc@5 100.000 (100.000)
Epoch: [180][60/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 0.4719 (0.4593)	Acc@1 99.609 (99.955)	Acc@5 100.000 (100.000)
Epoch: [180][70/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 0.4577 (0.4592)	Acc@1 100.000 (99.950)	Acc@5 100.000 (100.000)
Epoch: [180][80/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 0.4568 (0.4591)	Acc@1 100.000 (99.952)	Acc@5 100.000 (100.000)
Epoch: [180][90/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.4577 (0.4590)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Epoch: [180][100/196]	Time 0.010 (0.015)	Data 0.010 (0.005)	Loss 0.4644 (0.4590)	Acc@1 99.219 (99.942)	Acc@5 100.000 (100.000)
Epoch: [180][110/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 0.4589 (0.4590)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [180][120/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 0.4562 (0.4590)	Acc@1 100.000 (99.939)	Acc@5 100.000 (100.000)
Epoch: [180][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4562 (0.4590)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Epoch: [180][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 0.4577 (0.4589)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [180][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.4604 (0.4591)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Epoch: [180][160/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 0.4580 (0.4590)	Acc@1 100.000 (99.944)	Acc@5 100.000 (100.000)
Epoch: [180][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4629 (0.4591)	Acc@1 99.609 (99.936)	Acc@5 100.000 (100.000)
Epoch: [180][180/196]	Time 0.010 (0.015)	Data 0.006 (0.004)	Loss 0.4599 (0.4590)	Acc@1 100.000 (99.937)	Acc@5 100.000 (100.000)
Epoch: [180][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.4580 (0.4590)	Acc@1 100.000 (99.941)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [45, 3, 3, 3]
[module.conv2.weight]: [127, 45, 3, 3] >> [127, 45, 3, 3]
[module.conv3.weight]: [252, 127, 3, 3] >> [252, 127, 3, 3]
[module.conv4.weight]: [256, 252, 3, 3] >> [256, 252, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [509, 256, 3, 3]
[module.conv6.weight]: [511, 509, 3, 3] >> [511, 509, 3, 3]
[module.conv7.weight]: [476, 511, 3, 3] >> [476, 511, 3, 3]
[module.conv8.weight]: [299, 476, 3, 3] >> [299, 476, 3, 3]
[module.fc.weight]: [100, 299] >> [100, 299]
[INFO] Generating a new dense architecture...
[INFO] Storing checkpoint...
Best acc:
70.61

Total time:
1773.486697
[2021-06-20T02:45:34.261368] Command finished with return code 0


[2021-06-20T02:45:34.261771] The experiment completed successfully. Finalizing run...
Cleaning up all outstanding Run operations, waiting 900.0 seconds
1 items cleaning up...
Cleanup took 0.061673641204833984 seconds
[2021-06-20T02:45:34.517219] Finished context manager injector.
2021/06/20 02:45:35 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status
2021/06/20 02:45:35 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 2
FilteredData: 0.
2021/06/20 02:45:35 Process Exiting with Code:  0
2021/06/20 02:45:36 All App Insights Logs was send successfully
