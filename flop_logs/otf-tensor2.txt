bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/21 01:33:03 Starting App Insight Logger for task:  runTaskLet
2021/06/21 01:33:03 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612
2021/06/21 01:33:03 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info
bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/21 01:33:03 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status
[2021-06-21T01:33:03.115745] Entering context manager injector.
[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15'])
Script type = COMMAND
[2021-06-21T01:33:03.777482] Command=python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
[2021-06-21T01:33:03.777778] Entering Run History Context Manager.
[2021-06-21T01:33:05.326580] Command Working Directory=/mnt/batch/tasks/shared/LS_root/jobs/prunetrain/azureml/onthefly-tensor6/wd/azureml/OnTheFly-Tensor6
[2021-06-21T01:33:05.326836] Starting Linux command : python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
2021/06/21 01:33:08 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 1
FilteredData: 0.
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/data/torch/cifar-100-python.tar.gz
0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%Extracting ./dataset/data/torch/cifar-100-python.tar.gz to ./dataset/data/torch
==> creating model 'vgg11_bn_flat'
    Total params: 9.27M
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375

Epoch: [1 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
cifar.py:337: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [1][0/196]	Time 1.073 (1.073)	Data 0.213 (0.213)	Loss 6.0767 (6.0767)	Acc@1 0.781 (0.781)	Acc@5 2.734 (2.734)
Epoch: [1][10/196]	Time 0.015 (0.111)	Data 0.002 (0.021)	Loss 6.4391 (6.3919)	Acc@1 3.906 (2.344)	Acc@5 14.844 (10.121)
Epoch: [1][20/196]	Time 0.015 (0.065)	Data 0.002 (0.012)	Loss 6.6107 (6.5527)	Acc@1 2.344 (2.400)	Acc@5 12.109 (10.100)
Epoch: [1][30/196]	Time 0.015 (0.049)	Data 0.002 (0.009)	Loss 6.0210 (6.4617)	Acc@1 4.688 (2.646)	Acc@5 10.938 (10.307)
Epoch: [1][40/196]	Time 0.015 (0.041)	Data 0.002 (0.007)	Loss 5.6894 (6.2943)	Acc@1 4.297 (2.963)	Acc@5 11.719 (10.776)
Epoch: [1][50/196]	Time 0.015 (0.036)	Data 0.002 (0.006)	Loss 5.6249 (6.1818)	Acc@1 2.734 (2.987)	Acc@5 12.891 (11.336)
Epoch: [1][60/196]	Time 0.013 (0.032)	Data 0.003 (0.006)	Loss 5.5732 (6.0792)	Acc@1 3.906 (3.227)	Acc@5 16.016 (12.186)
Epoch: [1][70/196]	Time 0.016 (0.030)	Data 0.000 (0.005)	Loss 5.3888 (5.9983)	Acc@1 7.422 (3.422)	Acc@5 18.750 (12.836)
Epoch: [1][80/196]	Time 0.015 (0.028)	Data 0.002 (0.005)	Loss 5.4105 (5.9305)	Acc@1 5.469 (3.588)	Acc@5 21.484 (13.571)
Epoch: [1][90/196]	Time 0.015 (0.026)	Data 0.002 (0.005)	Loss 5.3989 (5.8708)	Acc@1 2.734 (3.760)	Acc@5 19.141 (14.440)
Epoch: [1][100/196]	Time 0.015 (0.025)	Data 0.003 (0.004)	Loss 5.2039 (5.8170)	Acc@1 5.859 (3.941)	Acc@5 21.875 (15.114)
Epoch: [1][110/196]	Time 0.017 (0.024)	Data 0.000 (0.004)	Loss 5.3531 (5.7677)	Acc@1 5.859 (4.124)	Acc@5 18.750 (15.692)
Epoch: [1][120/196]	Time 0.012 (0.023)	Data 0.009 (0.004)	Loss 5.1691 (5.7169)	Acc@1 8.203 (4.439)	Acc@5 28.125 (16.522)
Epoch: [1][130/196]	Time 0.019 (0.023)	Data 0.001 (0.004)	Loss 5.1434 (5.6733)	Acc@1 6.641 (4.682)	Acc@5 25.781 (17.313)
Epoch: [1][140/196]	Time 0.011 (0.022)	Data 0.006 (0.004)	Loss 5.0666 (5.6329)	Acc@1 9.375 (4.995)	Acc@5 28.125 (17.985)
Epoch: [1][150/196]	Time 0.015 (0.022)	Data 0.002 (0.004)	Loss 5.0566 (5.5970)	Acc@1 6.641 (5.202)	Acc@5 27.734 (18.613)
Epoch: [1][160/196]	Time 0.011 (0.021)	Data 0.008 (0.004)	Loss 4.8967 (5.5570)	Acc@1 10.156 (5.461)	Acc@5 32.031 (19.437)
Epoch: [1][170/196]	Time 0.015 (0.021)	Data 0.002 (0.004)	Loss 4.9406 (5.5223)	Acc@1 7.812 (5.702)	Acc@5 31.641 (20.084)
Epoch: [1][180/196]	Time 0.012 (0.020)	Data 0.007 (0.004)	Loss 4.9508 (5.4941)	Acc@1 10.547 (5.885)	Acc@5 32.812 (20.563)
Epoch: [1][190/196]	Time 0.017 (0.020)	Data 0.000 (0.004)	Loss 4.9225 (5.4649)	Acc@1 7.422 (6.078)	Acc@5 29.688 (21.049)
cifar.py:424: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
num momentum params: 26
[0.1, 5.451542841491699, 3.8005691027641295, 6.196, 9.9, tensor(0.2163, device='cuda:0', grad_fn=<DivBackward0>), 4.148910045623779, 0.48664569854736334]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [2 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [2][0/196]	Time 0.023 (0.023)	Data 0.152 (0.152)	Loss 4.8749 (4.8749)	Acc@1 9.766 (9.766)	Acc@5 33.594 (33.594)
Epoch: [2][10/196]	Time 0.017 (0.016)	Data 0.000 (0.016)	Loss 4.8565 (4.8413)	Acc@1 10.938 (10.582)	Acc@5 32.812 (34.020)
Epoch: [2][20/196]	Time 0.013 (0.016)	Data 0.004 (0.009)	Loss 4.8079 (4.8328)	Acc@1 9.375 (10.993)	Acc@5 33.594 (34.040)
Epoch: [2][30/196]	Time 0.018 (0.015)	Data 0.001 (0.008)	Loss 4.8907 (4.8273)	Acc@1 8.984 (10.912)	Acc@5 29.297 (33.959)
Epoch: [2][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 4.6831 (4.8152)	Acc@1 9.766 (10.918)	Acc@5 37.891 (34.308)
Epoch: [2][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 4.7346 (4.8010)	Acc@1 11.719 (11.045)	Acc@5 33.203 (34.452)
Epoch: [2][60/196]	Time 0.016 (0.015)	Data 0.002 (0.006)	Loss 4.8748 (4.7814)	Acc@1 9.766 (11.238)	Acc@5 28.906 (34.849)
Epoch: [2][70/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 4.6192 (4.7654)	Acc@1 15.625 (11.548)	Acc@5 41.016 (35.409)
Epoch: [2][80/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 4.6814 (4.7504)	Acc@1 11.328 (11.714)	Acc@5 37.891 (35.639)
Epoch: [2][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 4.7422 (4.7430)	Acc@1 15.234 (11.796)	Acc@5 41.406 (35.873)
Epoch: [2][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 4.4845 (4.7311)	Acc@1 13.281 (12.013)	Acc@5 39.453 (36.166)
Epoch: [2][110/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 4.6405 (4.7172)	Acc@1 12.891 (12.120)	Acc@5 38.672 (36.469)
Epoch: [2][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 4.4274 (4.7000)	Acc@1 16.016 (12.284)	Acc@5 42.578 (36.787)
Epoch: [2][130/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 4.4718 (4.6839)	Acc@1 15.234 (12.449)	Acc@5 44.531 (37.208)
Epoch: [2][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 4.4196 (4.6677)	Acc@1 18.750 (12.694)	Acc@5 45.703 (37.578)
Epoch: [2][150/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 4.3993 (4.6519)	Acc@1 21.094 (12.953)	Acc@5 44.531 (37.909)
Epoch: [2][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 4.3392 (4.6369)	Acc@1 18.359 (13.189)	Acc@5 45.703 (38.337)
Epoch: [2][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 4.3626 (4.6242)	Acc@1 17.578 (13.345)	Acc@5 41.406 (38.619)
Epoch: [2][180/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 4.3290 (4.6063)	Acc@1 18.359 (13.596)	Acc@5 46.875 (39.071)
Epoch: [2][190/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 4.3660 (4.5905)	Acc@1 17.188 (13.823)	Acc@5 39.844 (39.443)
num momentum params: 26
[0.1, 4.58261954574585, 3.330878276824951, 13.878, 18.49, tensor(0.2244, device='cuda:0', grad_fn=<DivBackward0>), 3.01883864402771, 0.367572546005249]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [3 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [3][0/196]	Time 0.022 (0.022)	Data 0.160 (0.160)	Loss 4.2411 (4.2411)	Acc@1 15.625 (15.625)	Acc@5 48.047 (48.047)
Epoch: [3][10/196]	Time 0.018 (0.017)	Data 0.001 (0.018)	Loss 4.0515 (4.2856)	Acc@1 20.703 (17.862)	Acc@5 52.734 (46.129)
Epoch: [3][20/196]	Time 0.012 (0.017)	Data 0.006 (0.010)	Loss 4.0145 (4.2058)	Acc@1 24.609 (19.085)	Acc@5 55.078 (48.493)
Epoch: [3][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 4.1551 (4.1983)	Acc@1 18.359 (19.443)	Acc@5 52.344 (48.702)
Epoch: [3][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 4.1838 (4.1923)	Acc@1 15.234 (19.426)	Acc@5 46.484 (48.714)
Epoch: [3][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 4.2278 (4.1789)	Acc@1 21.875 (19.516)	Acc@5 49.609 (48.928)
Epoch: [3][60/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 4.1407 (4.1694)	Acc@1 17.188 (19.698)	Acc@5 47.656 (49.059)
Epoch: [3][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 4.0371 (4.1568)	Acc@1 20.703 (19.867)	Acc@5 51.953 (49.477)
Epoch: [3][80/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 3.9273 (4.1330)	Acc@1 25.391 (20.337)	Acc@5 54.297 (50.121)
Epoch: [3][90/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 3.9879 (4.1101)	Acc@1 21.094 (20.656)	Acc@5 51.562 (50.610)
Epoch: [3][100/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 3.8280 (4.1052)	Acc@1 21.484 (20.661)	Acc@5 58.203 (50.685)
Epoch: [3][110/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 4.2122 (4.0921)	Acc@1 19.141 (20.876)	Acc@5 46.875 (50.887)
Epoch: [3][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 3.9424 (4.0841)	Acc@1 23.828 (21.003)	Acc@5 54.297 (51.004)
Epoch: [3][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 3.9706 (4.0744)	Acc@1 21.875 (21.100)	Acc@5 54.297 (51.211)
Epoch: [3][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 3.7969 (4.0610)	Acc@1 26.172 (21.304)	Acc@5 59.375 (51.543)
Epoch: [3][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 3.7367 (4.0448)	Acc@1 21.875 (21.492)	Acc@5 59.375 (51.922)
Epoch: [3][160/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 3.7199 (4.0289)	Acc@1 29.688 (21.771)	Acc@5 55.859 (52.196)
Epoch: [3][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 3.7407 (4.0155)	Acc@1 28.906 (22.026)	Acc@5 57.422 (52.492)
Epoch: [3][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.7439 (3.9998)	Acc@1 26.172 (22.233)	Acc@5 57.031 (52.829)
Epoch: [3][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 3.6095 (3.9861)	Acc@1 28.125 (22.474)	Acc@5 62.109 (53.076)
num momentum params: 26
[0.1, 3.9809811921691893, 3.085015676021576, 22.53, 23.5, tensor(0.2293, device='cuda:0', grad_fn=<DivBackward0>), 3.052283763885498, 0.37801218032836914]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [4 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [4][0/196]	Time 0.020 (0.020)	Data 0.158 (0.158)	Loss 3.6290 (3.6290)	Acc@1 26.562 (26.562)	Acc@5 60.547 (60.547)
Epoch: [4][10/196]	Time 0.014 (0.016)	Data 0.004 (0.016)	Loss 3.7792 (3.7024)	Acc@1 22.266 (26.491)	Acc@5 58.203 (58.736)
Epoch: [4][20/196]	Time 0.012 (0.015)	Data 0.005 (0.010)	Loss 3.6495 (3.6972)	Acc@1 26.172 (26.953)	Acc@5 56.250 (58.798)
Epoch: [4][30/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 3.6883 (3.6996)	Acc@1 27.734 (26.802)	Acc@5 58.594 (58.783)
Epoch: [4][40/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 3.6678 (3.6739)	Acc@1 29.688 (27.420)	Acc@5 57.031 (59.442)
Epoch: [4][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 3.8484 (3.6677)	Acc@1 25.391 (27.436)	Acc@5 56.250 (59.544)
Epoch: [4][60/196]	Time 0.013 (0.015)	Data 0.003 (0.006)	Loss 3.5876 (3.6561)	Acc@1 28.906 (27.369)	Acc@5 59.766 (59.907)
Epoch: [4][70/196]	Time 0.018 (0.015)	Data 0.000 (0.006)	Loss 3.6291 (3.6445)	Acc@1 26.562 (27.641)	Acc@5 60.938 (60.068)
Epoch: [4][80/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 3.7052 (3.6386)	Acc@1 25.000 (27.643)	Acc@5 58.203 (60.233)
Epoch: [4][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 3.5694 (3.6295)	Acc@1 26.562 (27.816)	Acc@5 62.500 (60.457)
Epoch: [4][100/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 3.3905 (3.6144)	Acc@1 33.203 (28.102)	Acc@5 68.359 (60.802)
Epoch: [4][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 3.6371 (3.6041)	Acc@1 26.953 (28.220)	Acc@5 62.109 (61.103)
Epoch: [4][120/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 3.6600 (3.5950)	Acc@1 25.000 (28.380)	Acc@5 59.375 (61.299)
Epoch: [4][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.5037 (3.5868)	Acc@1 27.344 (28.504)	Acc@5 63.672 (61.474)
Epoch: [4][140/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 3.4144 (3.5785)	Acc@1 32.812 (28.712)	Acc@5 62.891 (61.663)
Epoch: [4][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 3.3753 (3.5695)	Acc@1 36.719 (28.968)	Acc@5 69.141 (61.874)
Epoch: [4][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.1903 (3.5585)	Acc@1 36.719 (29.173)	Acc@5 70.312 (62.058)
Epoch: [4][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 3.1840 (3.5495)	Acc@1 40.625 (29.340)	Acc@5 69.922 (62.230)
Epoch: [4][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 3.2966 (3.5372)	Acc@1 30.078 (29.515)	Acc@5 69.531 (62.511)
Epoch: [4][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 3.4570 (3.5273)	Acc@1 30.859 (29.645)	Acc@5 63.281 (62.709)
num momentum params: 26
[0.1, 3.5255663372039794, 3.062322402000427, 29.7, 24.56, tensor(0.2341, device='cuda:0', grad_fn=<DivBackward0>), 2.9602227210998535, 0.36643433570861816]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [5 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [5][0/196]	Time 0.021 (0.021)	Data 0.162 (0.162)	Loss 3.4069 (3.4069)	Acc@1 30.078 (30.078)	Acc@5 61.719 (61.719)
Epoch: [5][10/196]	Time 0.016 (0.017)	Data 0.001 (0.017)	Loss 3.2332 (3.3716)	Acc@1 35.547 (31.747)	Acc@5 66.797 (65.483)
Epoch: [5][20/196]	Time 0.011 (0.016)	Data 0.017 (0.011)	Loss 3.1361 (3.3221)	Acc@1 35.156 (32.664)	Acc@5 72.656 (66.685)
Epoch: [5][30/196]	Time 0.017 (0.016)	Data 0.000 (0.009)	Loss 3.2208 (3.3025)	Acc@1 32.031 (33.216)	Acc@5 67.969 (67.112)
Epoch: [5][40/196]	Time 0.011 (0.016)	Data 0.011 (0.007)	Loss 3.4068 (3.3046)	Acc@1 30.469 (33.155)	Acc@5 65.625 (67.254)
Epoch: [5][50/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 3.2846 (3.2957)	Acc@1 34.766 (33.203)	Acc@5 64.844 (67.295)
Epoch: [5][60/196]	Time 0.013 (0.016)	Data 0.014 (0.006)	Loss 3.2574 (3.2927)	Acc@1 32.422 (33.113)	Acc@5 66.797 (67.431)
Epoch: [5][70/196]	Time 0.015 (0.016)	Data 0.001 (0.006)	Loss 3.1412 (3.2833)	Acc@1 37.109 (33.269)	Acc@5 67.578 (67.523)
Epoch: [5][80/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 3.2343 (3.2753)	Acc@1 38.281 (33.574)	Acc@5 69.531 (67.597)
Epoch: [5][90/196]	Time 0.012 (0.016)	Data 0.001 (0.005)	Loss 3.1332 (3.2658)	Acc@1 35.547 (33.731)	Acc@5 67.188 (67.707)
Epoch: [5][100/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 3.1113 (3.2536)	Acc@1 36.328 (33.969)	Acc@5 69.922 (67.949)
Epoch: [5][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 3.2727 (3.2506)	Acc@1 33.594 (34.136)	Acc@5 67.969 (67.934)
Epoch: [5][120/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 3.3079 (3.2412)	Acc@1 30.078 (34.239)	Acc@5 67.188 (68.162)
Epoch: [5][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 3.1079 (3.2347)	Acc@1 39.844 (34.429)	Acc@5 73.828 (68.270)
Epoch: [5][140/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.9805 (3.2280)	Acc@1 40.625 (34.588)	Acc@5 70.703 (68.323)
Epoch: [5][150/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 3.0926 (3.2190)	Acc@1 35.156 (34.791)	Acc@5 73.047 (68.489)
Epoch: [5][160/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 3.2571 (3.2104)	Acc@1 34.766 (34.962)	Acc@5 67.969 (68.631)
Epoch: [5][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.1180 (3.2012)	Acc@1 37.109 (35.165)	Acc@5 70.312 (68.860)
Epoch: [5][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 3.1188 (3.1943)	Acc@1 38.281 (35.307)	Acc@5 69.531 (68.959)
Epoch: [5][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.8497 (3.1872)	Acc@1 42.188 (35.438)	Acc@5 77.344 (69.075)
num momentum params: 26
[0.1, 3.185016854476929, 2.439368436336517, 35.456, 35.29, tensor(0.2391, device='cuda:0', grad_fn=<DivBackward0>), 3.022977352142334, 0.3692464828491211]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [6 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [6][0/196]	Time 0.020 (0.020)	Data 0.170 (0.170)	Loss 3.0677 (3.0677)	Acc@1 37.109 (37.109)	Acc@5 66.016 (66.016)
Epoch: [6][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 2.8267 (3.0196)	Acc@1 34.375 (38.459)	Acc@5 78.516 (70.810)
Epoch: [6][20/196]	Time 0.011 (0.016)	Data 0.017 (0.012)	Loss 3.0545 (2.9921)	Acc@1 37.891 (39.156)	Acc@5 73.828 (72.359)
Epoch: [6][30/196]	Time 0.021 (0.016)	Data 0.001 (0.010)	Loss 2.7253 (2.9775)	Acc@1 48.438 (39.642)	Acc@5 77.344 (72.555)
Epoch: [6][40/196]	Time 0.012 (0.016)	Data 0.008 (0.008)	Loss 2.9164 (2.9678)	Acc@1 39.844 (40.120)	Acc@5 75.391 (72.818)
Epoch: [6][50/196]	Time 0.015 (0.016)	Data 0.001 (0.007)	Loss 2.9117 (2.9658)	Acc@1 37.109 (39.790)	Acc@5 76.172 (72.886)
Epoch: [6][60/196]	Time 0.012 (0.016)	Data 0.006 (0.006)	Loss 2.9950 (2.9706)	Acc@1 38.672 (39.530)	Acc@5 72.656 (72.861)
Epoch: [6][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.9477 (2.9710)	Acc@1 43.750 (39.552)	Acc@5 70.703 (72.761)
Epoch: [6][80/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 2.7753 (2.9645)	Acc@1 44.922 (39.646)	Acc@5 78.516 (72.859)
Epoch: [6][90/196]	Time 0.019 (0.015)	Data 0.001 (0.006)	Loss 2.8996 (2.9595)	Acc@1 37.891 (39.749)	Acc@5 75.391 (73.043)
Epoch: [6][100/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.8094 (2.9532)	Acc@1 44.922 (39.937)	Acc@5 74.219 (73.155)
Epoch: [6][110/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 3.0359 (2.9501)	Acc@1 37.500 (40.020)	Acc@5 72.266 (73.195)
Epoch: [6][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.8006 (2.9441)	Acc@1 42.969 (40.092)	Acc@5 74.609 (73.350)
Epoch: [6][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.8818 (2.9426)	Acc@1 42.188 (40.196)	Acc@5 71.875 (73.309)
Epoch: [6][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 3.0759 (2.9379)	Acc@1 36.719 (40.182)	Acc@5 69.922 (73.404)
Epoch: [6][150/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.9624 (2.9387)	Acc@1 39.062 (40.180)	Acc@5 73.438 (73.375)
Epoch: [6][160/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.9032 (2.9358)	Acc@1 40.234 (40.171)	Acc@5 75.000 (73.474)
Epoch: [6][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.6679 (2.9287)	Acc@1 47.266 (40.275)	Acc@5 78.516 (73.602)
Epoch: [6][180/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.8131 (2.9248)	Acc@1 42.578 (40.293)	Acc@5 79.297 (73.645)
Epoch: [6][190/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.7621 (2.9187)	Acc@1 41.016 (40.425)	Acc@5 76.172 (73.726)
num momentum params: 26
[0.1, 2.9176369456481934, 2.4039607214927674, 40.422, 38.0, tensor(0.2453, device='cuda:0', grad_fn=<DivBackward0>), 2.9889326095581055, 0.37099480628967285]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [7 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [7][0/196]	Time 0.020 (0.020)	Data 0.169 (0.169)	Loss 2.6906 (2.6906)	Acc@1 39.062 (39.062)	Acc@5 78.125 (78.125)
Epoch: [7][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 2.8532 (2.8638)	Acc@1 38.672 (40.661)	Acc@5 76.953 (75.071)
Epoch: [7][20/196]	Time 0.011 (0.016)	Data 0.006 (0.010)	Loss 2.9414 (2.8122)	Acc@1 37.109 (41.369)	Acc@5 73.828 (75.911)
Epoch: [7][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.7791 (2.7942)	Acc@1 44.922 (42.351)	Acc@5 76.562 (76.336)
Epoch: [7][40/196]	Time 0.012 (0.015)	Data 0.010 (0.007)	Loss 2.9247 (2.7877)	Acc@1 40.625 (42.750)	Acc@5 73.047 (76.410)
Epoch: [7][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.6472 (2.7713)	Acc@1 43.359 (43.061)	Acc@5 81.641 (76.815)
Epoch: [7][60/196]	Time 0.015 (0.016)	Data 0.009 (0.005)	Loss 2.7348 (2.7765)	Acc@1 43.359 (42.911)	Acc@5 77.734 (76.550)
Epoch: [7][70/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.6844 (2.7780)	Acc@1 41.797 (43.007)	Acc@5 79.688 (76.480)
Epoch: [7][80/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.7765 (2.7766)	Acc@1 47.656 (43.152)	Acc@5 74.609 (76.437)
Epoch: [7][90/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.8075 (2.7735)	Acc@1 44.141 (43.286)	Acc@5 73.438 (76.399)
Epoch: [7][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.7734 (2.7713)	Acc@1 43.359 (43.340)	Acc@5 73.828 (76.330)
Epoch: [7][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.7186 (2.7702)	Acc@1 47.266 (43.468)	Acc@5 76.562 (76.334)
Epoch: [7][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.6774 (2.7699)	Acc@1 46.094 (43.501)	Acc@5 79.297 (76.346)
Epoch: [7][130/196]	Time 0.011 (0.015)	Data 0.018 (0.004)	Loss 2.7296 (2.7636)	Acc@1 46.094 (43.667)	Acc@5 78.906 (76.470)
Epoch: [7][140/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.6641 (2.7580)	Acc@1 44.922 (43.792)	Acc@5 78.125 (76.607)
Epoch: [7][150/196]	Time 0.013 (0.015)	Data 0.016 (0.004)	Loss 2.5113 (2.7525)	Acc@1 50.000 (43.885)	Acc@5 80.859 (76.643)
Epoch: [7][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.6735 (2.7502)	Acc@1 44.531 (43.927)	Acc@5 80.078 (76.662)
Epoch: [7][170/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 2.6385 (2.7489)	Acc@1 47.266 (43.956)	Acc@5 77.344 (76.697)
Epoch: [7][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.7270 (2.7474)	Acc@1 43.750 (44.007)	Acc@5 77.734 (76.677)
Epoch: [7][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.8673 (2.7454)	Acc@1 40.234 (44.042)	Acc@5 74.609 (76.747)
num momentum params: 26
[0.1, 2.7457998484802246, 2.2247783839702606, 44.004, 41.03, tensor(0.2498, device='cuda:0', grad_fn=<DivBackward0>), 3.0158352851867676, 0.3757486343383789]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [8 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [8][0/196]	Time 0.021 (0.021)	Data 0.173 (0.173)	Loss 2.6002 (2.6002)	Acc@1 44.531 (44.531)	Acc@5 79.297 (79.297)
Epoch: [8][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 2.6990 (2.6335)	Acc@1 41.406 (45.952)	Acc@5 77.344 (79.048)
Epoch: [8][20/196]	Time 0.015 (0.015)	Data 0.002 (0.012)	Loss 2.6701 (2.6322)	Acc@1 46.094 (45.703)	Acc@5 78.516 (78.757)
Epoch: [8][30/196]	Time 0.016 (0.015)	Data 0.000 (0.009)	Loss 2.5267 (2.6263)	Acc@1 49.609 (45.892)	Acc@5 80.469 (79.057)
Epoch: [8][40/196]	Time 0.011 (0.015)	Data 0.006 (0.008)	Loss 2.5273 (2.6277)	Acc@1 47.266 (45.941)	Acc@5 81.641 (78.878)
Epoch: [8][50/196]	Time 0.017 (0.015)	Data 0.001 (0.007)	Loss 2.4598 (2.6159)	Acc@1 51.562 (46.492)	Acc@5 82.812 (78.799)
Epoch: [8][60/196]	Time 0.014 (0.015)	Data 0.010 (0.006)	Loss 2.4236 (2.6105)	Acc@1 50.000 (46.657)	Acc@5 80.078 (78.663)
Epoch: [8][70/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.5391 (2.6063)	Acc@1 45.703 (46.622)	Acc@5 80.078 (78.868)
Epoch: [8][80/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 2.5126 (2.6135)	Acc@1 50.781 (46.417)	Acc@5 80.859 (78.646)
Epoch: [8][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.5517 (2.6169)	Acc@1 48.438 (46.317)	Acc@5 81.250 (78.674)
Epoch: [8][100/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.4862 (2.6133)	Acc@1 52.734 (46.422)	Acc@5 80.078 (78.728)
Epoch: [8][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.5424 (2.6098)	Acc@1 51.172 (46.622)	Acc@5 78.906 (78.765)
Epoch: [8][120/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.8693 (2.6093)	Acc@1 35.547 (46.565)	Acc@5 73.438 (78.790)
Epoch: [8][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.6633 (2.6133)	Acc@1 44.141 (46.496)	Acc@5 77.344 (78.674)
Epoch: [8][140/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.6281 (2.6159)	Acc@1 45.312 (46.362)	Acc@5 77.344 (78.635)
Epoch: [8][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.7001 (2.6153)	Acc@1 42.969 (46.378)	Acc@5 76.172 (78.648)
Epoch: [8][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4072 (2.6162)	Acc@1 51.562 (46.395)	Acc@5 82.422 (78.651)
Epoch: [8][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.5378 (2.6143)	Acc@1 51.953 (46.480)	Acc@5 80.469 (78.719)
Epoch: [8][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.6816 (2.6113)	Acc@1 46.484 (46.543)	Acc@5 77.734 (78.759)
Epoch: [8][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.7964 (2.6110)	Acc@1 41.406 (46.546)	Acc@5 75.781 (78.806)
num momentum params: 26
[0.1, 2.610018901672363, 2.516627998352051, 46.592, 36.57, tensor(0.2563, device='cuda:0', grad_fn=<DivBackward0>), 2.9368131160736084, 0.3686692714691162]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [9 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [9][0/196]	Time 0.021 (0.021)	Data 0.170 (0.170)	Loss 2.4528 (2.4528)	Acc@1 44.922 (44.922)	Acc@5 80.859 (80.859)
Epoch: [9][10/196]	Time 0.017 (0.016)	Data 0.003 (0.018)	Loss 2.4683 (2.5481)	Acc@1 50.391 (48.153)	Acc@5 80.078 (79.759)
Epoch: [9][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 2.3559 (2.5233)	Acc@1 51.953 (48.624)	Acc@5 83.203 (80.190)
Epoch: [9][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.5760 (2.5169)	Acc@1 47.656 (48.614)	Acc@5 80.469 (80.305)
Epoch: [9][40/196]	Time 0.012 (0.015)	Data 0.009 (0.007)	Loss 2.5687 (2.5096)	Acc@1 48.828 (48.952)	Acc@5 79.688 (80.497)
Epoch: [9][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.4325 (2.4971)	Acc@1 52.344 (49.341)	Acc@5 82.422 (80.790)
Epoch: [9][60/196]	Time 0.018 (0.015)	Data 0.002 (0.006)	Loss 2.5200 (2.5049)	Acc@1 51.172 (49.071)	Acc@5 79.297 (80.763)
Epoch: [9][70/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.4169 (2.5035)	Acc@1 48.047 (49.065)	Acc@5 82.812 (80.931)
Epoch: [9][80/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.6042 (2.5076)	Acc@1 47.266 (49.035)	Acc@5 76.172 (80.720)
Epoch: [9][90/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.3672 (2.5047)	Acc@1 52.734 (49.111)	Acc@5 81.250 (80.696)
Epoch: [9][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.3001 (2.4979)	Acc@1 56.250 (49.284)	Acc@5 85.156 (80.836)
Epoch: [9][110/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.2400 (2.4993)	Acc@1 55.859 (49.159)	Acc@5 85.547 (80.768)
Epoch: [9][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.7035 (2.4994)	Acc@1 42.578 (49.061)	Acc@5 77.344 (80.717)
Epoch: [9][130/196]	Time 0.011 (0.015)	Data 0.015 (0.004)	Loss 2.5545 (2.5004)	Acc@1 47.656 (49.073)	Acc@5 79.297 (80.686)
Epoch: [9][140/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.4840 (2.5030)	Acc@1 49.219 (49.044)	Acc@5 81.641 (80.632)
Epoch: [9][150/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.5397 (2.5049)	Acc@1 49.609 (48.989)	Acc@5 76.953 (80.585)
Epoch: [9][160/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.5596 (2.5057)	Acc@1 48.438 (48.979)	Acc@5 79.688 (80.617)
Epoch: [9][170/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.5063 (2.5033)	Acc@1 48.047 (49.070)	Acc@5 78.516 (80.658)
Epoch: [9][180/196]	Time 0.022 (0.016)	Data 0.001 (0.004)	Loss 2.3838 (2.5058)	Acc@1 52.734 (49.083)	Acc@5 83.203 (80.613)
Epoch: [9][190/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 2.7451 (2.5106)	Acc@1 42.969 (48.986)	Acc@5 75.391 (80.508)
num momentum params: 26
[0.1, 2.5110535527801514, 2.1313619923591616, 48.984, 43.57, tensor(0.2643, device='cuda:0', grad_fn=<DivBackward0>), 3.0716798305511475, 0.3745589256286621]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [10 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [10][0/196]	Time 0.021 (0.021)	Data 0.175 (0.175)	Loss 2.3977 (2.3977)	Acc@1 53.125 (53.125)	Acc@5 80.469 (80.469)
Epoch: [10][10/196]	Time 0.023 (0.016)	Data 0.000 (0.018)	Loss 2.3926 (2.4279)	Acc@1 47.656 (50.284)	Acc@5 84.766 (81.747)
Epoch: [10][20/196]	Time 0.013 (0.016)	Data 0.006 (0.011)	Loss 2.2373 (2.4259)	Acc@1 55.859 (50.614)	Acc@5 87.109 (81.938)
Epoch: [10][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.3382 (2.4146)	Acc@1 50.781 (50.769)	Acc@5 83.984 (82.145)
Epoch: [10][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.3434 (2.4114)	Acc@1 50.000 (50.896)	Acc@5 84.375 (82.260)
Epoch: [10][50/196]	Time 0.013 (0.016)	Data 0.001 (0.006)	Loss 2.3638 (2.4029)	Acc@1 50.781 (51.080)	Acc@5 83.984 (82.506)
Epoch: [10][60/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.4835 (2.4073)	Acc@1 50.781 (51.172)	Acc@5 83.594 (82.435)
Epoch: [10][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1945 (2.4062)	Acc@1 54.297 (51.227)	Acc@5 87.500 (82.405)
Epoch: [10][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.4804 (2.4025)	Acc@1 50.000 (51.230)	Acc@5 79.688 (82.436)
Epoch: [10][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4794 (2.4049)	Acc@1 50.391 (51.253)	Acc@5 78.906 (82.422)
Epoch: [10][100/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.6041 (2.4088)	Acc@1 44.922 (51.149)	Acc@5 76.953 (82.360)
Epoch: [10][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.4088 (2.4131)	Acc@1 55.859 (51.207)	Acc@5 83.594 (82.235)
Epoch: [10][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.5390 (2.4108)	Acc@1 45.703 (51.369)	Acc@5 81.641 (82.270)
Epoch: [10][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.3978 (2.4101)	Acc@1 50.391 (51.449)	Acc@5 82.031 (82.243)
Epoch: [10][140/196]	Time 0.019 (0.015)	Data 0.001 (0.004)	Loss 2.3841 (2.4152)	Acc@1 55.078 (51.399)	Acc@5 82.031 (82.145)
Epoch: [10][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.4952 (2.4196)	Acc@1 53.125 (51.376)	Acc@5 80.859 (82.122)
Epoch: [10][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.5986 (2.4197)	Acc@1 46.875 (51.371)	Acc@5 78.516 (82.126)
Epoch: [10][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4494 (2.4223)	Acc@1 52.344 (51.295)	Acc@5 82.422 (82.082)
Epoch: [10][180/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.5663 (2.4249)	Acc@1 48.828 (51.258)	Acc@5 79.297 (82.027)
Epoch: [10][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.4343 (2.4274)	Acc@1 47.266 (51.201)	Acc@5 83.203 (82.035)
num momentum params: 26
[0.1, 2.4286351315307617, 2.2013924181461335, 51.2, 42.41, tensor(0.2745, device='cuda:0', grad_fn=<DivBackward0>), 3.0343990325927734, 0.36931800842285156]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [512, 512, 3, 3]
Before - module.bn8.weight: [512]
Before - module.bn8.bias: [512]
Before - module.fc.weight: [100, 512]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [64, 3, 3, 3]
After - module.bn1.weight: [64]
After - module.bn1.bias: [64]
After - module.conv2.weight: [128, 64, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [512, 512, 3, 3]
After - module.bn7.weight: [512]
After - module.bn7.bias: [512]
After - module.conv8.weight: [512, 512, 3, 3]
After - module.bn8.weight: [512]
After - module.bn8.bias: [512]
After - module.fc.weight: [100, 512]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375
[INFO] Storing checkpoint...

Epoch: [11 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [11][0/196]	Time 0.020 (0.020)	Data 0.167 (0.167)	Loss 2.2797 (2.2797)	Acc@1 56.641 (56.641)	Acc@5 80.859 (80.859)
Epoch: [11][10/196]	Time 0.016 (0.015)	Data 0.002 (0.018)	Loss 2.3916 (2.3939)	Acc@1 51.562 (52.557)	Acc@5 81.250 (81.889)
Epoch: [11][20/196]	Time 0.013 (0.015)	Data 0.004 (0.010)	Loss 2.2556 (2.3659)	Acc@1 56.250 (52.753)	Acc@5 81.641 (82.571)
Epoch: [11][30/196]	Time 0.014 (0.015)	Data 0.002 (0.008)	Loss 2.2238 (2.3524)	Acc@1 54.688 (52.999)	Acc@5 85.156 (83.115)
Epoch: [11][40/196]	Time 0.013 (0.015)	Data 0.005 (0.007)	Loss 2.5060 (2.3534)	Acc@1 46.875 (53.011)	Acc@5 81.250 (83.279)
Epoch: [11][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.2078 (2.3391)	Acc@1 62.500 (53.531)	Acc@5 82.422 (83.578)
Epoch: [11][60/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.3185 (2.3411)	Acc@1 51.953 (53.567)	Acc@5 84.766 (83.402)
Epoch: [11][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.3465 (2.3469)	Acc@1 53.125 (53.406)	Acc@5 85.156 (83.335)
Epoch: [11][80/196]	Time 0.015 (0.015)	Data 0.000 (0.005)	Loss 2.5331 (2.3482)	Acc@1 52.344 (53.419)	Acc@5 79.688 (83.271)
Epoch: [11][90/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.4274 (2.3485)	Acc@1 54.688 (53.546)	Acc@5 80.078 (83.237)
Epoch: [11][100/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.3949 (2.3485)	Acc@1 53.125 (53.512)	Acc@5 82.812 (83.230)
Epoch: [11][110/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.2902 (2.3549)	Acc@1 56.250 (53.357)	Acc@5 87.500 (83.270)
Epoch: [11][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3339 (2.3577)	Acc@1 48.828 (53.261)	Acc@5 80.859 (83.177)
Epoch: [11][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.4092 (2.3626)	Acc@1 51.562 (53.086)	Acc@5 84.375 (83.167)
Epoch: [11][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3721 (2.3667)	Acc@1 50.781 (52.959)	Acc@5 82.812 (83.114)
Epoch: [11][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3394 (2.3685)	Acc@1 53.516 (52.910)	Acc@5 85.156 (83.102)
Epoch: [11][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3210 (2.3722)	Acc@1 54.688 (52.822)	Acc@5 85.938 (83.092)
Epoch: [11][170/196]	Time 0.011 (0.015)	Data 0.016 (0.004)	Loss 2.3088 (2.3731)	Acc@1 54.688 (52.812)	Acc@5 85.938 (83.023)
Epoch: [11][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3611 (2.3726)	Acc@1 52.734 (52.812)	Acc@5 83.984 (83.039)
Epoch: [11][190/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 2.4639 (2.3779)	Acc@1 50.781 (52.693)	Acc@5 79.688 (82.935)
num momentum params: 26
[0.1, 2.379678485412598, 2.1696648275852204, 52.604, 42.54, tensor(0.2844, device='cuda:0', grad_fn=<DivBackward0>), 2.8851287364959717, 0.37249207496643066]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [12 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [12][0/196]	Time 0.024 (0.024)	Data 0.170 (0.170)	Loss 2.3754 (2.3754)	Acc@1 53.516 (53.516)	Acc@5 82.031 (82.031)
Epoch: [12][10/196]	Time 0.013 (0.015)	Data 0.004 (0.018)	Loss 2.2215 (2.3556)	Acc@1 54.688 (52.628)	Acc@5 87.891 (83.736)
Epoch: [12][20/196]	Time 0.017 (0.015)	Data 0.002 (0.011)	Loss 2.3128 (2.3155)	Acc@1 54.688 (54.111)	Acc@5 85.547 (84.263)
Epoch: [12][30/196]	Time 0.011 (0.015)	Data 0.007 (0.009)	Loss 2.1543 (2.3051)	Acc@1 56.250 (54.473)	Acc@5 85.547 (84.274)
Epoch: [12][40/196]	Time 0.016 (0.015)	Data 0.002 (0.008)	Loss 2.3538 (2.3099)	Acc@1 53.906 (54.449)	Acc@5 84.375 (84.070)
Epoch: [12][50/196]	Time 0.013 (0.015)	Data 0.008 (0.007)	Loss 2.3784 (2.3310)	Acc@1 53.516 (54.006)	Acc@5 83.984 (83.655)
Epoch: [12][60/196]	Time 0.018 (0.015)	Data 0.000 (0.006)	Loss 2.3713 (2.3254)	Acc@1 52.344 (54.015)	Acc@5 83.984 (83.882)
Epoch: [12][70/196]	Time 0.012 (0.015)	Data 0.009 (0.006)	Loss 2.3621 (2.3284)	Acc@1 54.688 (53.906)	Acc@5 82.031 (83.913)
Epoch: [12][80/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.2771 (2.3304)	Acc@1 50.391 (53.848)	Acc@5 86.328 (83.854)
Epoch: [12][90/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.2852 (2.3285)	Acc@1 53.906 (53.915)	Acc@5 83.594 (83.838)
Epoch: [12][100/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.3073 (2.3350)	Acc@1 51.953 (53.779)	Acc@5 86.328 (83.806)
Epoch: [12][110/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.4037 (2.3360)	Acc@1 53.906 (53.783)	Acc@5 83.203 (83.833)
Epoch: [12][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.2657 (2.3347)	Acc@1 54.688 (53.890)	Acc@5 85.938 (83.942)
Epoch: [12][130/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.2404 (2.3393)	Acc@1 57.031 (53.796)	Acc@5 85.547 (83.874)
Epoch: [12][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.3487 (2.3397)	Acc@1 54.297 (53.915)	Acc@5 86.719 (83.854)
Epoch: [12][150/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 2.3677 (2.3409)	Acc@1 55.859 (53.893)	Acc@5 82.812 (83.798)
Epoch: [12][160/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2536 (2.3417)	Acc@1 54.297 (53.819)	Acc@5 86.719 (83.795)
Epoch: [12][170/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.3717 (2.3428)	Acc@1 54.688 (53.822)	Acc@5 81.250 (83.751)
Epoch: [12][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2933 (2.3436)	Acc@1 56.250 (53.723)	Acc@5 84.766 (83.738)
Epoch: [12][190/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.3556 (2.3452)	Acc@1 57.031 (53.665)	Acc@5 84.375 (83.745)
num momentum params: 26
[0.1, 2.345661754150391, 2.2147258841991424, 53.672, 43.71, tensor(0.2950, device='cuda:0', grad_fn=<DivBackward0>), 2.922147512435913, 0.37822675704956055]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [13 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [13][0/196]	Time 0.022 (0.022)	Data 0.171 (0.171)	Loss 2.3811 (2.3811)	Acc@1 53.125 (53.125)	Acc@5 83.594 (83.594)
Epoch: [13][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 2.3242 (2.2997)	Acc@1 56.250 (55.256)	Acc@5 84.375 (85.014)
Epoch: [13][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 2.2760 (2.2774)	Acc@1 56.250 (55.859)	Acc@5 83.984 (85.249)
Epoch: [13][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.2450 (2.2751)	Acc@1 58.984 (55.746)	Acc@5 85.547 (85.333)
Epoch: [13][40/196]	Time 0.012 (0.015)	Data 0.006 (0.007)	Loss 2.5325 (2.3045)	Acc@1 49.219 (54.754)	Acc@5 83.203 (84.899)
Epoch: [13][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.4602 (2.2965)	Acc@1 50.391 (54.848)	Acc@5 80.859 (85.034)
Epoch: [13][60/196]	Time 0.012 (0.015)	Data 0.007 (0.006)	Loss 2.2669 (2.2924)	Acc@1 57.422 (54.950)	Acc@5 82.422 (85.137)
Epoch: [13][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.3263 (2.2849)	Acc@1 51.953 (55.100)	Acc@5 84.375 (85.277)
Epoch: [13][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.3649 (2.2884)	Acc@1 53.906 (54.953)	Acc@5 81.250 (85.243)
Epoch: [13][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2715 (2.2953)	Acc@1 57.422 (54.868)	Acc@5 85.938 (84.985)
Epoch: [13][100/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1473 (2.2987)	Acc@1 58.203 (54.761)	Acc@5 85.938 (84.920)
Epoch: [13][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.2504 (2.3012)	Acc@1 54.688 (54.768)	Acc@5 85.156 (84.910)
Epoch: [13][120/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2188 (2.3030)	Acc@1 58.594 (54.849)	Acc@5 86.328 (84.898)
Epoch: [13][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.5035 (2.3056)	Acc@1 48.438 (54.777)	Acc@5 81.250 (84.858)
Epoch: [13][140/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.4612 (2.3093)	Acc@1 48.828 (54.712)	Acc@5 82.812 (84.804)
Epoch: [13][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2768 (2.3156)	Acc@1 53.906 (54.669)	Acc@5 87.891 (84.714)
Epoch: [13][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4737 (2.3189)	Acc@1 49.219 (54.610)	Acc@5 83.203 (84.698)
Epoch: [13][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.5939 (2.3217)	Acc@1 47.266 (54.560)	Acc@5 78.125 (84.645)
Epoch: [13][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1694 (2.3191)	Acc@1 57.422 (54.608)	Acc@5 86.719 (84.673)
Epoch: [13][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.5747 (2.3179)	Acc@1 48.047 (54.624)	Acc@5 80.469 (84.704)
num momentum params: 26
[0.1, 2.3205748107147217, 1.9808932662010192, 54.574, 47.81, tensor(0.3058, device='cuda:0', grad_fn=<DivBackward0>), 2.9762990474700928, 0.37482190132141113]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [14 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [14][0/196]	Time 0.022 (0.022)	Data 0.175 (0.175)	Loss 2.1845 (2.1845)	Acc@1 62.109 (62.109)	Acc@5 85.547 (85.547)
Epoch: [14][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 2.2582 (2.2858)	Acc@1 55.859 (54.972)	Acc@5 85.938 (85.973)
Epoch: [14][20/196]	Time 0.012 (0.016)	Data 0.016 (0.012)	Loss 2.2698 (2.2650)	Acc@1 58.594 (55.804)	Acc@5 86.719 (86.272)
Epoch: [14][30/196]	Time 0.013 (0.016)	Data 0.008 (0.009)	Loss 2.2610 (2.2507)	Acc@1 58.984 (56.250)	Acc@5 85.938 (86.366)
Epoch: [14][40/196]	Time 0.011 (0.016)	Data 0.010 (0.010)	Loss 2.4002 (2.2744)	Acc@1 52.344 (55.716)	Acc@5 79.688 (85.909)
Epoch: [14][50/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.2799 (2.2838)	Acc@1 56.250 (55.706)	Acc@5 87.109 (85.784)
Epoch: [14][60/196]	Time 0.011 (0.016)	Data 0.009 (0.007)	Loss 2.1510 (2.2790)	Acc@1 61.719 (56.116)	Acc@5 87.500 (85.829)
Epoch: [14][70/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 2.3177 (2.2869)	Acc@1 58.594 (56.085)	Acc@5 80.469 (85.629)
Epoch: [14][80/196]	Time 0.011 (0.016)	Data 0.010 (0.006)	Loss 2.2035 (2.2846)	Acc@1 57.031 (55.999)	Acc@5 86.719 (85.692)
Epoch: [14][90/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.2131 (2.2852)	Acc@1 57.812 (56.040)	Acc@5 85.156 (85.624)
Epoch: [14][100/196]	Time 0.011 (0.016)	Data 0.012 (0.006)	Loss 2.3020 (2.2871)	Acc@1 55.859 (55.944)	Acc@5 83.594 (85.551)
Epoch: [14][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1672 (2.2853)	Acc@1 59.766 (55.940)	Acc@5 85.547 (85.645)
Epoch: [14][120/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 2.3114 (2.2889)	Acc@1 55.469 (55.834)	Acc@5 83.594 (85.573)
Epoch: [14][130/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2759 (2.2899)	Acc@1 55.469 (55.865)	Acc@5 84.766 (85.532)
Epoch: [14][140/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.4163 (2.2941)	Acc@1 50.000 (55.807)	Acc@5 86.328 (85.519)
Epoch: [14][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2662 (2.2959)	Acc@1 54.297 (55.771)	Acc@5 87.891 (85.454)
Epoch: [14][160/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1798 (2.2958)	Acc@1 61.328 (55.779)	Acc@5 86.719 (85.477)
Epoch: [14][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2086 (2.2986)	Acc@1 59.375 (55.725)	Acc@5 88.672 (85.456)
Epoch: [14][180/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.3121 (2.2999)	Acc@1 57.031 (55.732)	Acc@5 84.375 (85.437)
Epoch: [14][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3985 (2.3002)	Acc@1 54.297 (55.780)	Acc@5 84.766 (85.402)
num momentum params: 26
[0.1, 2.3008111822509765, 2.2788089680671693, 55.796, 42.85, tensor(0.3170, device='cuda:0', grad_fn=<DivBackward0>), 3.074518203735352, 0.37369871139526367]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [15 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [15][0/196]	Time 0.022 (0.022)	Data 0.174 (0.174)	Loss 2.1109 (2.1109)	Acc@1 56.250 (56.250)	Acc@5 89.844 (89.844)
Epoch: [15][10/196]	Time 0.017 (0.016)	Data 0.001 (0.018)	Loss 2.1917 (2.2559)	Acc@1 57.812 (57.031)	Acc@5 86.328 (85.973)
Epoch: [15][20/196]	Time 0.011 (0.015)	Data 0.007 (0.011)	Loss 2.2079 (2.2401)	Acc@1 55.078 (57.515)	Acc@5 89.453 (86.682)
Epoch: [15][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 2.1284 (2.2394)	Acc@1 59.375 (57.598)	Acc@5 91.016 (86.643)
Epoch: [15][40/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 2.3385 (2.2373)	Acc@1 53.906 (57.593)	Acc@5 85.547 (86.652)
Epoch: [15][50/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 2.2950 (2.2459)	Acc@1 57.031 (57.475)	Acc@5 86.328 (86.527)
Epoch: [15][60/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.3005 (2.2405)	Acc@1 57.422 (57.684)	Acc@5 85.938 (86.495)
Epoch: [15][70/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 2.4016 (2.2409)	Acc@1 53.906 (57.702)	Acc@5 80.078 (86.444)
Epoch: [15][80/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.3715 (2.2493)	Acc@1 51.562 (57.499)	Acc@5 84.766 (86.294)
Epoch: [15][90/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 2.4016 (2.2498)	Acc@1 51.562 (57.413)	Acc@5 86.328 (86.354)
Epoch: [15][100/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2134 (2.2487)	Acc@1 58.984 (57.538)	Acc@5 84.375 (86.328)
Epoch: [15][110/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.3539 (2.2510)	Acc@1 57.031 (57.552)	Acc@5 82.031 (86.275)
Epoch: [15][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2673 (2.2520)	Acc@1 55.469 (57.474)	Acc@5 84.766 (86.251)
Epoch: [15][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.3648 (2.2541)	Acc@1 53.906 (57.374)	Acc@5 84.766 (86.236)
Epoch: [15][140/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1893 (2.2577)	Acc@1 60.156 (57.311)	Acc@5 86.719 (86.192)
Epoch: [15][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.3728 (2.2613)	Acc@1 56.250 (57.199)	Acc@5 83.984 (86.163)
Epoch: [15][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.3528 (2.2693)	Acc@1 53.516 (57.014)	Acc@5 85.156 (86.081)
Epoch: [15][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2065 (2.2697)	Acc@1 59.766 (57.070)	Acc@5 87.109 (86.056)
Epoch: [15][180/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2154 (2.2712)	Acc@1 56.641 (57.042)	Acc@5 84.375 (86.037)
Epoch: [15][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2679 (2.2752)	Acc@1 57.422 (56.984)	Acc@5 87.109 (86.001)
num momentum params: 26
[0.1, 2.2760653210449218, 2.0301952528953553, 56.982, 47.44, tensor(0.3283, device='cuda:0', grad_fn=<DivBackward0>), 2.968905448913574, 0.3725254535675049]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [16 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [16][0/196]	Time 0.024 (0.024)	Data 0.174 (0.174)	Loss 2.2819 (2.2819)	Acc@1 53.906 (53.906)	Acc@5 86.719 (86.719)
Epoch: [16][10/196]	Time 0.016 (0.016)	Data 0.001 (0.017)	Loss 2.1624 (2.2434)	Acc@1 62.109 (57.884)	Acc@5 89.062 (86.186)
Epoch: [16][20/196]	Time 0.011 (0.015)	Data 0.008 (0.011)	Loss 2.2816 (2.2059)	Acc@1 56.250 (58.650)	Acc@5 87.109 (87.035)
Epoch: [16][30/196]	Time 0.017 (0.015)	Data 0.000 (0.009)	Loss 2.2092 (2.1944)	Acc@1 57.031 (58.795)	Acc@5 87.109 (87.223)
Epoch: [16][40/196]	Time 0.018 (0.015)	Data 0.000 (0.008)	Loss 1.9695 (2.1865)	Acc@1 65.625 (59.061)	Acc@5 89.844 (87.271)
Epoch: [16][50/196]	Time 0.017 (0.015)	Data 0.001 (0.007)	Loss 2.1990 (2.1923)	Acc@1 58.203 (59.184)	Acc@5 87.109 (87.263)
Epoch: [16][60/196]	Time 0.018 (0.015)	Data 0.001 (0.007)	Loss 2.1993 (2.2028)	Acc@1 60.547 (58.965)	Acc@5 86.719 (87.116)
Epoch: [16][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.3242 (2.2072)	Acc@1 54.688 (58.858)	Acc@5 85.547 (87.043)
Epoch: [16][80/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.2343 (2.2096)	Acc@1 58.203 (58.883)	Acc@5 86.328 (87.008)
Epoch: [16][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2036 (2.2131)	Acc@1 60.156 (58.864)	Acc@5 87.891 (86.998)
Epoch: [16][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2782 (2.2208)	Acc@1 57.031 (58.644)	Acc@5 85.547 (86.920)
Epoch: [16][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0503 (2.2244)	Acc@1 62.109 (58.601)	Acc@5 90.234 (86.856)
Epoch: [16][120/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.2803 (2.2287)	Acc@1 61.719 (58.568)	Acc@5 85.156 (86.803)
Epoch: [16][130/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.2748 (2.2305)	Acc@1 56.641 (58.519)	Acc@5 82.812 (86.760)
Epoch: [16][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2893 (2.2316)	Acc@1 54.688 (58.466)	Acc@5 85.938 (86.749)
Epoch: [16][150/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 2.1806 (2.2360)	Acc@1 58.984 (58.252)	Acc@5 85.938 (86.716)
Epoch: [16][160/196]	Time 0.019 (0.016)	Data 0.000 (0.004)	Loss 2.1838 (2.2406)	Acc@1 60.156 (58.220)	Acc@5 86.328 (86.631)
Epoch: [16][170/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.2281 (2.2433)	Acc@1 56.641 (58.093)	Acc@5 87.109 (86.605)
Epoch: [16][180/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1855 (2.2436)	Acc@1 60.938 (58.100)	Acc@5 86.719 (86.609)
Epoch: [16][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0507 (2.2468)	Acc@1 63.672 (58.013)	Acc@5 91.797 (86.569)
num momentum params: 26
[0.1, 2.2452159617614744, 1.895856430530548, 58.082, 50.77, tensor(0.3406, device='cuda:0', grad_fn=<DivBackward0>), 3.037810802459717, 0.372255802154541]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [17 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [17][0/196]	Time 0.026 (0.026)	Data 0.169 (0.169)	Loss 2.1593 (2.1593)	Acc@1 62.891 (62.891)	Acc@5 85.938 (85.938)
Epoch: [17][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 2.1721 (2.1731)	Acc@1 60.156 (60.582)	Acc@5 87.500 (88.281)
Epoch: [17][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 2.1720 (2.1829)	Acc@1 58.594 (59.840)	Acc@5 86.719 (87.742)
Epoch: [17][30/196]	Time 0.016 (0.016)	Data 0.001 (0.008)	Loss 2.2534 (2.1962)	Acc@1 57.031 (59.614)	Acc@5 88.672 (87.538)
Epoch: [17][40/196]	Time 0.015 (0.016)	Data 0.008 (0.007)	Loss 2.1371 (2.1798)	Acc@1 65.234 (60.242)	Acc@5 88.281 (87.605)
Epoch: [17][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.1187 (2.1774)	Acc@1 58.984 (60.156)	Acc@5 89.453 (87.646)
Epoch: [17][60/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.2394 (2.1813)	Acc@1 55.469 (59.964)	Acc@5 86.328 (87.724)
Epoch: [17][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.3506 (2.1838)	Acc@1 57.422 (59.909)	Acc@5 82.422 (87.638)
Epoch: [17][80/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.1790 (2.1931)	Acc@1 59.766 (59.674)	Acc@5 85.156 (87.505)
Epoch: [17][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0382 (2.1950)	Acc@1 59.375 (59.650)	Acc@5 90.625 (87.539)
Epoch: [17][100/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.5180 (2.2010)	Acc@1 51.562 (59.518)	Acc@5 83.203 (87.438)
Epoch: [17][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2648 (2.2077)	Acc@1 57.422 (59.326)	Acc@5 87.500 (87.321)
Epoch: [17][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3856 (2.2088)	Acc@1 55.078 (59.230)	Acc@5 85.547 (87.351)
Epoch: [17][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2892 (2.2128)	Acc@1 57.422 (59.148)	Acc@5 88.281 (87.342)
Epoch: [17][140/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 2.2750 (2.2179)	Acc@1 58.203 (59.056)	Acc@5 85.547 (87.251)
Epoch: [17][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1722 (2.2195)	Acc@1 60.156 (59.070)	Acc@5 85.547 (87.272)
Epoch: [17][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1586 (2.2221)	Acc@1 59.766 (58.967)	Acc@5 87.109 (87.221)
Epoch: [17][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1967 (2.2265)	Acc@1 61.719 (58.866)	Acc@5 84.766 (87.178)
Epoch: [17][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.2759 (2.2311)	Acc@1 55.469 (58.725)	Acc@5 82.812 (87.114)
Epoch: [17][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3141 (2.2373)	Acc@1 57.422 (58.551)	Acc@5 87.891 (87.046)
num momentum params: 26
[0.1, 2.238585965576172, 2.064412860870361, 58.544, 47.35, tensor(0.3483, device='cuda:0', grad_fn=<DivBackward0>), 2.9743900299072266, 0.38156867027282715]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [18 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [18][0/196]	Time 0.024 (0.024)	Data 0.162 (0.162)	Loss 2.2715 (2.2715)	Acc@1 57.812 (57.812)	Acc@5 85.156 (85.156)
Epoch: [18][10/196]	Time 0.016 (0.015)	Data 0.000 (0.017)	Loss 2.2552 (2.2176)	Acc@1 59.375 (60.263)	Acc@5 87.500 (87.429)
Epoch: [18][20/196]	Time 0.011 (0.015)	Data 0.008 (0.010)	Loss 2.1135 (2.1881)	Acc@1 63.281 (60.882)	Acc@5 90.625 (88.151)
Epoch: [18][30/196]	Time 0.017 (0.015)	Data 0.001 (0.008)	Loss 2.2958 (2.1780)	Acc@1 58.594 (60.887)	Acc@5 86.719 (88.382)
Epoch: [18][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 2.1024 (2.1732)	Acc@1 60.156 (60.795)	Acc@5 89.062 (88.319)
Epoch: [18][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.3152 (2.1826)	Acc@1 55.859 (60.478)	Acc@5 87.500 (88.251)
Epoch: [18][60/196]	Time 0.012 (0.015)	Data 0.007 (0.006)	Loss 2.1522 (2.1808)	Acc@1 62.500 (60.592)	Acc@5 87.500 (88.211)
Epoch: [18][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3067 (2.1880)	Acc@1 58.203 (60.382)	Acc@5 86.719 (88.105)
Epoch: [18][80/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.2436 (2.1934)	Acc@1 59.375 (60.311)	Acc@5 87.109 (87.987)
Epoch: [18][90/196]	Time 0.013 (0.015)	Data 0.020 (0.005)	Loss 2.2074 (2.1963)	Acc@1 59.766 (60.255)	Acc@5 88.281 (87.938)
Epoch: [18][100/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.1173 (2.1935)	Acc@1 60.547 (60.183)	Acc@5 87.891 (87.976)
Epoch: [18][110/196]	Time 0.013 (0.015)	Data 0.006 (0.005)	Loss 2.3155 (2.1955)	Acc@1 56.641 (60.177)	Acc@5 87.891 (87.972)
Epoch: [18][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.4129 (2.2012)	Acc@1 53.125 (60.011)	Acc@5 84.375 (87.874)
Epoch: [18][130/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.1215 (2.2064)	Acc@1 64.844 (59.882)	Acc@5 87.891 (87.798)
Epoch: [18][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0974 (2.2030)	Acc@1 65.234 (59.968)	Acc@5 88.672 (87.849)
Epoch: [18][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1857 (2.2114)	Acc@1 63.281 (59.742)	Acc@5 88.281 (87.738)
Epoch: [18][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1928 (2.2172)	Acc@1 62.109 (59.632)	Acc@5 87.891 (87.694)
Epoch: [18][170/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.2942 (2.2162)	Acc@1 57.812 (59.677)	Acc@5 90.234 (87.717)
Epoch: [18][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2373 (2.2177)	Acc@1 59.766 (59.615)	Acc@5 84.766 (87.701)
Epoch: [18][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.4159 (2.2219)	Acc@1 53.906 (59.504)	Acc@5 86.328 (87.674)
num momentum params: 26
[0.1, 2.2247761740112306, 1.9841653442382812, 59.424, 48.03, tensor(0.3577, device='cuda:0', grad_fn=<DivBackward0>), 2.9207077026367188, 0.3765120506286621]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [19 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [19][0/196]	Time 0.027 (0.027)	Data 0.164 (0.164)	Loss 2.1753 (2.1753)	Acc@1 62.500 (62.500)	Acc@5 87.500 (87.500)
Epoch: [19][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 2.1780 (2.2192)	Acc@1 60.156 (59.801)	Acc@5 88.281 (88.033)
Epoch: [19][20/196]	Time 0.014 (0.015)	Data 0.004 (0.012)	Loss 2.0549 (2.2244)	Acc@1 66.797 (59.635)	Acc@5 91.016 (87.928)
Epoch: [19][30/196]	Time 0.017 (0.015)	Data 0.001 (0.010)	Loss 2.2306 (2.2039)	Acc@1 60.547 (60.030)	Acc@5 87.891 (88.206)
Epoch: [19][40/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 2.1428 (2.1771)	Acc@1 58.984 (60.547)	Acc@5 90.234 (88.529)
Epoch: [19][50/196]	Time 0.016 (0.015)	Data 0.001 (0.007)	Loss 2.1378 (2.1808)	Acc@1 63.672 (60.371)	Acc@5 91.016 (88.534)
Epoch: [19][60/196]	Time 0.015 (0.015)	Data 0.007 (0.007)	Loss 2.1692 (2.1694)	Acc@1 62.891 (60.752)	Acc@5 87.500 (88.697)
Epoch: [19][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1747 (2.1716)	Acc@1 62.500 (60.723)	Acc@5 89.062 (88.622)
Epoch: [19][80/196]	Time 0.011 (0.015)	Data 0.015 (0.006)	Loss 2.3223 (2.1800)	Acc@1 56.641 (60.470)	Acc@5 84.375 (88.508)
Epoch: [19][90/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.3503 (2.1920)	Acc@1 59.766 (60.268)	Acc@5 85.156 (88.320)
Epoch: [19][100/196]	Time 0.012 (0.015)	Data 0.017 (0.006)	Loss 2.0535 (2.1944)	Acc@1 64.062 (60.261)	Acc@5 89.844 (88.274)
Epoch: [19][110/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.1124 (2.1989)	Acc@1 61.328 (60.227)	Acc@5 90.625 (88.218)
Epoch: [19][120/196]	Time 0.011 (0.015)	Data 0.016 (0.006)	Loss 2.3301 (2.2068)	Acc@1 54.297 (60.085)	Acc@5 87.500 (88.171)
Epoch: [19][130/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.2708 (2.2073)	Acc@1 55.859 (60.109)	Acc@5 88.281 (88.195)
Epoch: [19][140/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.3644 (2.2113)	Acc@1 53.906 (60.018)	Acc@5 85.156 (88.068)
Epoch: [19][150/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.1803 (2.2151)	Acc@1 61.719 (59.978)	Acc@5 89.453 (87.989)
Epoch: [19][160/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.2617 (2.2175)	Acc@1 58.203 (59.904)	Acc@5 86.719 (87.927)
Epoch: [19][170/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1934 (2.2202)	Acc@1 62.500 (59.916)	Acc@5 87.109 (87.829)
Epoch: [19][180/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.2561 (2.2235)	Acc@1 57.031 (59.839)	Acc@5 86.719 (87.815)
Epoch: [19][190/196]	Time 0.012 (0.015)	Data 0.016 (0.005)	Loss 2.1026 (2.2268)	Acc@1 64.453 (59.753)	Acc@5 89.453 (87.790)
num momentum params: 26
[0.1, 2.2289249450683593, 2.1524549651145937, 59.724, 45.65, tensor(0.3631, device='cuda:0', grad_fn=<DivBackward0>), 2.955007791519165, 0.3767375946044922]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [20 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [20][0/196]	Time 0.027 (0.027)	Data 0.164 (0.164)	Loss 2.0060 (2.0060)	Acc@1 65.234 (65.234)	Acc@5 92.188 (92.188)
Epoch: [20][10/196]	Time 0.017 (0.016)	Data 0.000 (0.017)	Loss 2.2558 (2.1384)	Acc@1 57.031 (61.470)	Acc@5 88.281 (89.453)
Epoch: [20][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 2.1413 (2.1541)	Acc@1 62.500 (61.235)	Acc@5 87.891 (89.081)
Epoch: [20][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 2.1706 (2.1460)	Acc@1 57.812 (61.202)	Acc@5 89.844 (89.113)
Epoch: [20][40/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 2.0095 (2.1492)	Acc@1 65.625 (61.452)	Acc@5 93.359 (89.062)
Epoch: [20][50/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.2101 (2.1444)	Acc@1 60.938 (61.673)	Acc@5 87.891 (89.078)
Epoch: [20][60/196]	Time 0.018 (0.015)	Data 0.001 (0.006)	Loss 2.3644 (2.1539)	Acc@1 59.766 (61.578)	Acc@5 86.719 (88.966)
Epoch: [20][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1254 (2.1554)	Acc@1 62.891 (61.488)	Acc@5 88.672 (88.980)
Epoch: [20][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.4057 (2.1629)	Acc@1 53.125 (61.362)	Acc@5 84.766 (88.927)
Epoch: [20][90/196]	Time 0.020 (0.016)	Data 0.001 (0.005)	Loss 2.0239 (2.1646)	Acc@1 68.750 (61.259)	Acc@5 90.234 (88.917)
Epoch: [20][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.5127 (2.1711)	Acc@1 53.906 (61.189)	Acc@5 83.203 (88.772)
Epoch: [20][110/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1273 (2.1757)	Acc@1 66.406 (61.047)	Acc@5 89.844 (88.686)
Epoch: [20][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.3300 (2.1804)	Acc@1 58.203 (60.938)	Acc@5 86.719 (88.707)
Epoch: [20][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1993 (2.1836)	Acc@1 59.375 (60.842)	Acc@5 89.062 (88.740)
Epoch: [20][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1595 (2.1859)	Acc@1 60.156 (60.755)	Acc@5 87.891 (88.672)
Epoch: [20][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3090 (2.1876)	Acc@1 60.156 (60.775)	Acc@5 88.281 (88.633)
Epoch: [20][160/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2311 (2.1912)	Acc@1 62.109 (60.726)	Acc@5 87.500 (88.570)
Epoch: [20][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1944 (2.1954)	Acc@1 58.984 (60.702)	Acc@5 87.891 (88.519)
Epoch: [20][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.4277 (2.2001)	Acc@1 53.906 (60.607)	Acc@5 84.766 (88.456)
Epoch: [20][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1510 (2.2028)	Acc@1 59.766 (60.592)	Acc@5 89.062 (88.420)
num momentum params: 26
[0.1, 2.20544296585083, 1.9631859076023102, 60.55, 47.88, tensor(0.3727, device='cuda:0', grad_fn=<DivBackward0>), 3.0401129722595215, 0.3771636486053467]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [512, 512, 3, 3]
Before - module.bn8.weight: [512]
Before - module.bn8.bias: [512]
Before - module.fc.weight: [100, 512]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv8.weight]: [512, 512, 3, 3] >> [510, 512, 3, 3]
[module.bn8.weight]: 512 >> 510
running_mean [510]
running_var [510]
num_batches_tracked []
[module.fc.weight]: [100, 512] >> [100, 510]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [64, 3, 3, 3]
After - module.bn1.weight: [64]
After - module.bn1.bias: [64]
After - module.conv2.weight: [128, 64, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [512, 512, 3, 3]
After - module.bn7.weight: [512]
After - module.bn7.bias: [512]
After - module.conv8.weight: [510, 512, 3, 3]
After - module.bn8.weight: [510]
After - module.bn8.bias: [510]
After - module.fc.weight: [100, 510]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [510, 512, 3, 3]
fc --> [510, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7219445760, 9400320, 510
fc, 19584000, 51000, 0
===================
FLOP REPORT: 31135957200000.0 60617600000.0 152778552 151544 2750 17.667335510253906
[INFO] Storing checkpoint...

Epoch: [21 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [21][0/196]	Time 0.179 (0.179)	Data 0.167 (0.167)	Loss 2.1611 (2.1611)	Acc@1 61.719 (61.719)	Acc@5 87.500 (87.500)
Epoch: [21][10/196]	Time 0.015 (0.030)	Data 0.003 (0.017)	Loss 2.0217 (2.1672)	Acc@1 66.797 (61.932)	Acc@5 90.234 (88.601)
Epoch: [21][20/196]	Time 0.014 (0.023)	Data 0.002 (0.010)	Loss 2.1079 (2.1340)	Acc@1 62.109 (62.779)	Acc@5 91.797 (89.453)
Epoch: [21][30/196]	Time 0.016 (0.020)	Data 0.000 (0.008)	Loss 2.1111 (2.1327)	Acc@1 58.984 (62.361)	Acc@5 90.625 (89.567)
Epoch: [21][40/196]	Time 0.016 (0.020)	Data 0.001 (0.007)	Loss 2.2487 (2.1417)	Acc@1 62.891 (62.090)	Acc@5 87.500 (89.539)
Epoch: [21][50/196]	Time 0.018 (0.019)	Data 0.000 (0.006)	Loss 2.1812 (2.1316)	Acc@1 58.203 (62.331)	Acc@5 87.500 (89.599)
Epoch: [21][60/196]	Time 0.018 (0.018)	Data 0.002 (0.005)	Loss 2.2639 (2.1287)	Acc@1 59.766 (62.462)	Acc@5 87.500 (89.594)
Epoch: [21][70/196]	Time 0.016 (0.018)	Data 0.000 (0.005)	Loss 2.1947 (2.1305)	Acc@1 59.766 (62.417)	Acc@5 87.891 (89.624)
Epoch: [21][80/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 1.9148 (2.1269)	Acc@1 67.969 (62.500)	Acc@5 92.969 (89.598)
Epoch: [21][90/196]	Time 0.017 (0.017)	Data 0.001 (0.004)	Loss 2.1084 (2.1275)	Acc@1 60.938 (62.479)	Acc@5 87.891 (89.569)
Epoch: [21][100/196]	Time 0.016 (0.017)	Data 0.003 (0.004)	Loss 2.2638 (2.1291)	Acc@1 57.422 (62.523)	Acc@5 85.938 (89.476)
Epoch: [21][110/196]	Time 0.017 (0.017)	Data 0.001 (0.004)	Loss 2.1629 (2.1373)	Acc@1 61.328 (62.335)	Acc@5 90.234 (89.393)
Epoch: [21][120/196]	Time 0.014 (0.017)	Data 0.003 (0.004)	Loss 2.3059 (2.1494)	Acc@1 56.250 (62.058)	Acc@5 88.281 (89.201)
Epoch: [21][130/196]	Time 0.017 (0.017)	Data 0.001 (0.004)	Loss 2.2404 (2.1555)	Acc@1 56.641 (61.930)	Acc@5 89.062 (89.113)
Epoch: [21][140/196]	Time 0.014 (0.017)	Data 0.003 (0.004)	Loss 2.2657 (2.1641)	Acc@1 59.766 (61.744)	Acc@5 87.500 (89.049)
Epoch: [21][150/196]	Time 0.018 (0.017)	Data 0.000 (0.004)	Loss 2.2094 (2.1654)	Acc@1 62.891 (61.729)	Acc@5 89.062 (89.031)
Epoch: [21][160/196]	Time 0.015 (0.017)	Data 0.003 (0.003)	Loss 2.2587 (2.1698)	Acc@1 61.328 (61.532)	Acc@5 88.672 (88.992)
Epoch: [21][170/196]	Time 0.017 (0.017)	Data 0.000 (0.003)	Loss 2.2149 (2.1728)	Acc@1 60.547 (61.490)	Acc@5 87.891 (88.971)
Epoch: [21][180/196]	Time 0.011 (0.016)	Data 0.007 (0.003)	Loss 2.1766 (2.1799)	Acc@1 60.938 (61.356)	Acc@5 88.672 (88.849)
Epoch: [21][190/196]	Time 0.016 (0.016)	Data 0.000 (0.003)	Loss 2.2074 (2.1811)	Acc@1 60.547 (61.369)	Acc@5 87.891 (88.827)
num momentum params: 26
[0.1, 2.184066012878418, 1.983449214696884, 61.318, 49.35, tensor(0.3817, device='cuda:0', grad_fn=<DivBackward0>), 3.2559845447540283, 0.40109300613403326]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [22 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [22][0/196]	Time 0.037 (0.037)	Data 0.165 (0.165)	Loss 1.9998 (1.9998)	Acc@1 67.578 (67.578)	Acc@5 89.844 (89.844)
Epoch: [22][10/196]	Time 0.016 (0.017)	Data 0.000 (0.017)	Loss 2.0766 (2.1636)	Acc@1 63.672 (62.109)	Acc@5 90.625 (88.956)
Epoch: [22][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 2.1158 (2.1226)	Acc@1 62.500 (63.430)	Acc@5 88.281 (89.676)
Epoch: [22][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 1.9691 (2.0983)	Acc@1 66.016 (63.621)	Acc@5 92.578 (89.970)
Epoch: [22][40/196]	Time 0.021 (0.016)	Data 0.003 (0.007)	Loss 2.0239 (2.0986)	Acc@1 64.453 (63.662)	Acc@5 92.969 (90.006)
Epoch: [22][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9571 (2.1010)	Acc@1 67.188 (63.657)	Acc@5 91.406 (89.882)
Epoch: [22][60/196]	Time 0.013 (0.016)	Data 0.008 (0.006)	Loss 2.2585 (2.1076)	Acc@1 58.203 (63.429)	Acc@5 88.672 (89.933)
Epoch: [22][70/196]	Time 0.018 (0.015)	Data 0.001 (0.006)	Loss 2.1633 (2.1226)	Acc@1 59.766 (63.006)	Acc@5 90.625 (89.761)
Epoch: [22][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.2288 (2.1276)	Acc@1 61.328 (62.871)	Acc@5 87.500 (89.742)
Epoch: [22][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1975 (2.1329)	Acc@1 62.500 (62.749)	Acc@5 87.109 (89.621)
Epoch: [22][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.2739 (2.1435)	Acc@1 58.984 (62.481)	Acc@5 88.672 (89.461)
Epoch: [22][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2789 (2.1556)	Acc@1 58.594 (62.243)	Acc@5 86.328 (89.277)
Epoch: [22][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2115 (2.1503)	Acc@1 62.891 (62.329)	Acc@5 89.062 (89.427)
Epoch: [22][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0369 (2.1471)	Acc@1 65.234 (62.414)	Acc@5 94.531 (89.507)
Epoch: [22][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3021 (2.1524)	Acc@1 60.547 (62.306)	Acc@5 84.766 (89.400)
Epoch: [22][150/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.1524 (2.1539)	Acc@1 63.281 (62.384)	Acc@5 89.062 (89.404)
Epoch: [22][160/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.4756 (2.1589)	Acc@1 58.203 (62.342)	Acc@5 82.812 (89.325)
Epoch: [22][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2801 (2.1668)	Acc@1 62.500 (62.230)	Acc@5 87.109 (89.165)
Epoch: [22][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2855 (2.1711)	Acc@1 57.422 (62.122)	Acc@5 86.719 (89.095)
Epoch: [22][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1273 (2.1751)	Acc@1 64.844 (62.058)	Acc@5 90.625 (89.026)
num momentum params: 26
[0.1, 2.1756955625915526, 1.7733009099960326, 62.038, 52.59, tensor(0.3881, device='cuda:0', grad_fn=<DivBackward0>), 3.027296543121338, 0.37984585762023926]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [23 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [23][0/196]	Time 0.032 (0.032)	Data 0.172 (0.172)	Loss 2.0781 (2.0781)	Acc@1 59.375 (59.375)	Acc@5 90.625 (90.625)
Epoch: [23][10/196]	Time 0.016 (0.017)	Data 0.000 (0.018)	Loss 1.9972 (2.0891)	Acc@1 66.797 (64.524)	Acc@5 91.016 (90.554)
Epoch: [23][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 2.1221 (2.1027)	Acc@1 64.844 (64.007)	Acc@5 88.672 (90.123)
Epoch: [23][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.9858 (2.0844)	Acc@1 69.922 (64.378)	Acc@5 92.969 (90.348)
Epoch: [23][40/196]	Time 0.017 (0.015)	Data 0.000 (0.007)	Loss 2.2029 (2.0868)	Acc@1 63.672 (64.129)	Acc@5 90.234 (90.320)
Epoch: [23][50/196]	Time 0.016 (0.015)	Data 0.001 (0.007)	Loss 1.9868 (2.0969)	Acc@1 66.406 (63.978)	Acc@5 92.188 (90.142)
Epoch: [23][60/196]	Time 0.018 (0.015)	Data 0.000 (0.006)	Loss 2.2524 (2.1035)	Acc@1 62.109 (63.966)	Acc@5 86.328 (90.074)
Epoch: [23][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.3745 (2.1157)	Acc@1 58.594 (63.578)	Acc@5 86.719 (89.877)
Epoch: [23][80/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1881 (2.1231)	Acc@1 63.672 (63.383)	Acc@5 89.844 (89.781)
Epoch: [23][90/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1144 (2.1231)	Acc@1 63.281 (63.389)	Acc@5 90.234 (89.736)
Epoch: [23][100/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.2687 (2.1272)	Acc@1 58.594 (63.243)	Acc@5 87.500 (89.677)
Epoch: [23][110/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.1543 (2.1278)	Acc@1 63.281 (63.239)	Acc@5 91.016 (89.597)
Epoch: [23][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2660 (2.1379)	Acc@1 65.234 (62.987)	Acc@5 87.500 (89.534)
Epoch: [23][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.2353 (2.1446)	Acc@1 64.062 (62.798)	Acc@5 87.500 (89.435)
Epoch: [23][140/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.3840 (2.1483)	Acc@1 53.125 (62.683)	Acc@5 87.891 (89.400)
Epoch: [23][150/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.2354 (2.1572)	Acc@1 60.938 (62.438)	Acc@5 91.406 (89.319)
Epoch: [23][160/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1729 (2.1606)	Acc@1 62.891 (62.401)	Acc@5 89.062 (89.274)
Epoch: [23][170/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.1089 (2.1653)	Acc@1 64.062 (62.292)	Acc@5 88.672 (89.197)
Epoch: [23][180/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1628 (2.1675)	Acc@1 65.625 (62.269)	Acc@5 88.281 (89.179)
Epoch: [23][190/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2815 (2.1685)	Acc@1 61.719 (62.273)	Acc@5 87.891 (89.144)
num momentum params: 26
[0.1, 2.1700699714660643, 1.9018907427787781, 62.21, 50.83, tensor(0.3937, device='cuda:0', grad_fn=<DivBackward0>), 3.0019102096557617, 0.3811368942260742]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [24 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [24][0/196]	Time 0.035 (0.035)	Data 0.169 (0.169)	Loss 1.9898 (1.9898)	Acc@1 66.406 (66.406)	Acc@5 93.750 (93.750)
Epoch: [24][10/196]	Time 0.017 (0.017)	Data 0.000 (0.017)	Loss 2.1836 (2.0639)	Acc@1 62.109 (65.874)	Acc@5 89.062 (90.803)
Epoch: [24][20/196]	Time 0.012 (0.016)	Data 0.023 (0.011)	Loss 1.9503 (2.0776)	Acc@1 67.969 (65.141)	Acc@5 93.359 (90.848)
Epoch: [24][30/196]	Time 0.018 (0.016)	Data 0.000 (0.009)	Loss 2.1534 (2.0760)	Acc@1 62.891 (65.071)	Acc@5 89.844 (90.713)
Epoch: [24][40/196]	Time 0.012 (0.016)	Data 0.011 (0.008)	Loss 2.2638 (2.0881)	Acc@1 60.156 (64.768)	Acc@5 85.938 (90.549)
Epoch: [24][50/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 2.0717 (2.0894)	Acc@1 66.016 (64.530)	Acc@5 90.234 (90.602)
Epoch: [24][60/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 1.9177 (2.0962)	Acc@1 68.750 (64.280)	Acc@5 94.141 (90.587)
Epoch: [24][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.2542 (2.1070)	Acc@1 60.938 (64.079)	Acc@5 87.500 (90.432)
Epoch: [24][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1464 (2.1178)	Acc@1 64.062 (63.817)	Acc@5 89.062 (90.234)
Epoch: [24][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0694 (2.1267)	Acc@1 66.016 (63.517)	Acc@5 90.234 (90.097)
Epoch: [24][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.3476 (2.1381)	Acc@1 58.984 (63.289)	Acc@5 89.062 (89.917)
Epoch: [24][110/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.1711 (2.1400)	Acc@1 60.938 (63.200)	Acc@5 89.453 (89.918)
Epoch: [24][120/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 2.2066 (2.1450)	Acc@1 63.672 (63.146)	Acc@5 87.500 (89.811)
Epoch: [24][130/196]	Time 0.012 (0.015)	Data 0.016 (0.005)	Loss 2.3176 (2.1516)	Acc@1 55.469 (62.947)	Acc@5 85.547 (89.686)
Epoch: [24][140/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.3450 (2.1535)	Acc@1 60.547 (62.965)	Acc@5 87.891 (89.628)
Epoch: [24][150/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.2216 (2.1570)	Acc@1 59.766 (62.885)	Acc@5 89.453 (89.603)
Epoch: [24][160/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1329 (2.1603)	Acc@1 63.672 (62.794)	Acc@5 92.969 (89.601)
Epoch: [24][170/196]	Time 0.011 (0.015)	Data 0.016 (0.004)	Loss 2.2042 (2.1629)	Acc@1 60.938 (62.758)	Acc@5 87.109 (89.567)
Epoch: [24][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2528 (2.1671)	Acc@1 59.375 (62.608)	Acc@5 86.328 (89.505)
Epoch: [24][190/196]	Time 0.011 (0.015)	Data 0.013 (0.004)	Loss 2.2479 (2.1687)	Acc@1 60.547 (62.598)	Acc@5 91.016 (89.504)
num momentum params: 26
[0.1, 2.172147180786133, 1.8810041654109955, 62.524, 50.88, tensor(0.3986, device='cuda:0', grad_fn=<DivBackward0>), 3.012887954711914, 0.3782956600189209]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [25 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [25][0/196]	Time 0.030 (0.030)	Data 0.175 (0.175)	Loss 2.1321 (2.1321)	Acc@1 66.016 (66.016)	Acc@5 89.453 (89.453)
Epoch: [25][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 2.1872 (2.1659)	Acc@1 59.375 (62.678)	Acc@5 91.406 (89.631)
Epoch: [25][20/196]	Time 0.013 (0.016)	Data 0.004 (0.010)	Loss 1.9404 (2.1043)	Acc@1 66.406 (64.342)	Acc@5 94.922 (90.606)
Epoch: [25][30/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 1.8675 (2.0816)	Acc@1 69.531 (64.970)	Acc@5 96.484 (91.028)
Epoch: [25][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 2.1258 (2.0893)	Acc@1 63.281 (64.815)	Acc@5 88.672 (90.930)
Epoch: [25][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.1118 (2.0880)	Acc@1 64.844 (64.737)	Acc@5 87.109 (90.786)
Epoch: [25][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.1086 (2.0906)	Acc@1 66.016 (64.741)	Acc@5 90.234 (90.721)
Epoch: [25][70/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.0848 (2.0866)	Acc@1 62.891 (64.888)	Acc@5 90.625 (90.719)
Epoch: [25][80/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0889 (2.0933)	Acc@1 65.234 (64.810)	Acc@5 89.844 (90.639)
Epoch: [25][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0708 (2.0942)	Acc@1 63.281 (64.801)	Acc@5 92.188 (90.586)
Epoch: [25][100/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 2.2196 (2.1010)	Acc@1 62.891 (64.635)	Acc@5 88.672 (90.466)
Epoch: [25][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.3019 (2.1106)	Acc@1 57.812 (64.348)	Acc@5 87.500 (90.329)
Epoch: [25][120/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0815 (2.1137)	Acc@1 65.625 (64.298)	Acc@5 91.016 (90.289)
Epoch: [25][130/196]	Time 0.015 (0.015)	Data 0.001 (0.005)	Loss 2.4000 (2.1180)	Acc@1 57.031 (64.200)	Acc@5 84.375 (90.208)
Epoch: [25][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2089 (2.1225)	Acc@1 60.547 (64.002)	Acc@5 86.328 (90.137)
Epoch: [25][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1430 (2.1318)	Acc@1 66.797 (63.742)	Acc@5 87.891 (89.996)
Epoch: [25][160/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1912 (2.1383)	Acc@1 66.406 (63.594)	Acc@5 88.672 (89.936)
Epoch: [25][170/196]	Time 0.013 (0.015)	Data 0.006 (0.004)	Loss 2.2201 (2.1455)	Acc@1 59.375 (63.407)	Acc@5 91.016 (89.857)
Epoch: [25][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1364 (2.1517)	Acc@1 61.719 (63.238)	Acc@5 91.406 (89.829)
Epoch: [25][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2355 (2.1564)	Acc@1 61.719 (63.144)	Acc@5 86.328 (89.750)
num momentum params: 26
[0.1, 2.1581790768432616, 2.1728551137447356, 63.074, 46.12, tensor(0.4053, device='cuda:0', grad_fn=<DivBackward0>), 2.923739433288574, 0.37732934951782227]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [26 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [26][0/196]	Time 0.033 (0.033)	Data 0.171 (0.171)	Loss 2.0079 (2.0079)	Acc@1 67.578 (67.578)	Acc@5 91.797 (91.797)
Epoch: [26][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 1.9668 (2.0858)	Acc@1 69.141 (64.311)	Acc@5 90.625 (91.513)
Epoch: [26][20/196]	Time 0.012 (0.016)	Data 0.010 (0.011)	Loss 2.1238 (2.0689)	Acc@1 63.672 (65.569)	Acc@5 89.062 (91.481)
Epoch: [26][30/196]	Time 0.017 (0.015)	Data 0.001 (0.009)	Loss 1.9527 (2.0641)	Acc@1 65.625 (65.398)	Acc@5 91.016 (91.469)
Epoch: [26][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 2.0482 (2.0785)	Acc@1 68.750 (65.215)	Acc@5 91.797 (91.149)
Epoch: [26][50/196]	Time 0.017 (0.015)	Data 0.000 (0.007)	Loss 2.2466 (2.0890)	Acc@1 63.672 (64.959)	Acc@5 87.891 (91.069)
Epoch: [26][60/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 2.2632 (2.0934)	Acc@1 61.328 (64.716)	Acc@5 89.453 (91.086)
Epoch: [26][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.1126 (2.0886)	Acc@1 63.672 (64.871)	Acc@5 88.672 (90.994)
Epoch: [26][80/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.0228 (2.0907)	Acc@1 66.797 (64.800)	Acc@5 92.188 (90.866)
Epoch: [26][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1540 (2.0961)	Acc@1 62.109 (64.668)	Acc@5 89.844 (90.715)
Epoch: [26][100/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.2136 (2.1011)	Acc@1 66.016 (64.596)	Acc@5 90.625 (90.640)
Epoch: [26][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2237 (2.1114)	Acc@1 62.891 (64.253)	Acc@5 89.453 (90.555)
Epoch: [26][120/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.2530 (2.1183)	Acc@1 58.594 (64.030)	Acc@5 89.062 (90.473)
Epoch: [26][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2199 (2.1263)	Acc@1 59.766 (63.836)	Acc@5 87.500 (90.333)
Epoch: [26][140/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.2211 (2.1287)	Acc@1 61.719 (63.797)	Acc@5 87.500 (90.265)
Epoch: [26][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1681 (2.1339)	Acc@1 64.062 (63.664)	Acc@5 87.891 (90.221)
Epoch: [26][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0218 (2.1358)	Acc@1 68.750 (63.701)	Acc@5 92.188 (90.196)
Epoch: [26][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1103 (2.1386)	Acc@1 65.234 (63.649)	Acc@5 88.672 (90.134)
Epoch: [26][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0852 (2.1435)	Acc@1 66.016 (63.598)	Acc@5 90.625 (90.036)
Epoch: [26][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.3099 (2.1490)	Acc@1 59.375 (63.467)	Acc@5 89.453 (89.956)
num momentum params: 26
[0.1, 2.150768223800659, 1.8596710920333863, 63.444, 51.89, tensor(0.4101, device='cuda:0', grad_fn=<DivBackward0>), 2.919235944747925, 0.3765285015106201]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [27 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [27][0/196]	Time 0.031 (0.031)	Data 0.163 (0.163)	Loss 2.0567 (2.0567)	Acc@1 67.188 (67.188)	Acc@5 91.406 (91.406)
Epoch: [27][10/196]	Time 0.017 (0.017)	Data 0.003 (0.017)	Loss 2.1909 (2.1185)	Acc@1 62.109 (65.412)	Acc@5 89.453 (90.376)
Epoch: [27][20/196]	Time 0.013 (0.016)	Data 0.003 (0.010)	Loss 1.9898 (2.0772)	Acc@1 67.578 (66.164)	Acc@5 89.453 (91.127)
Epoch: [27][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9169 (2.0598)	Acc@1 68.750 (66.192)	Acc@5 92.578 (91.368)
Epoch: [27][40/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.0292 (2.0554)	Acc@1 66.016 (66.235)	Acc@5 91.797 (91.425)
Epoch: [27][50/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.8705 (2.0616)	Acc@1 68.750 (66.016)	Acc@5 94.922 (91.322)
Epoch: [27][60/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0258 (2.0695)	Acc@1 62.109 (65.843)	Acc@5 92.578 (91.137)
Epoch: [27][70/196]	Time 0.027 (0.016)	Data 0.001 (0.005)	Loss 2.1989 (2.0760)	Acc@1 59.375 (65.575)	Acc@5 91.016 (91.087)
Epoch: [27][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2245 (2.0853)	Acc@1 59.766 (65.326)	Acc@5 89.844 (90.885)
Epoch: [27][90/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9981 (2.0896)	Acc@1 68.359 (65.204)	Acc@5 92.578 (90.814)
Epoch: [27][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1502 (2.0895)	Acc@1 61.719 (65.207)	Acc@5 89.453 (90.842)
Epoch: [27][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1593 (2.0940)	Acc@1 59.375 (65.115)	Acc@5 90.625 (90.766)
Epoch: [27][120/196]	Time 0.021 (0.015)	Data 0.000 (0.004)	Loss 2.4583 (2.1062)	Acc@1 56.250 (64.870)	Acc@5 85.938 (90.593)
Epoch: [27][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2345 (2.1167)	Acc@1 66.016 (64.617)	Acc@5 87.891 (90.485)
Epoch: [27][140/196]	Time 0.021 (0.015)	Data 0.000 (0.004)	Loss 2.2384 (2.1234)	Acc@1 61.719 (64.395)	Acc@5 88.281 (90.390)
Epoch: [27][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2263 (2.1273)	Acc@1 64.453 (64.350)	Acc@5 89.062 (90.340)
Epoch: [27][160/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2928 (2.1327)	Acc@1 62.109 (64.261)	Acc@5 85.547 (90.232)
Epoch: [27][170/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 2.1850 (2.1405)	Acc@1 60.547 (64.056)	Acc@5 89.453 (90.120)
Epoch: [27][180/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1603 (2.1405)	Acc@1 65.234 (64.106)	Acc@5 91.797 (90.079)
Epoch: [27][190/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.1660 (2.1437)	Acc@1 62.109 (64.011)	Acc@5 89.062 (90.054)
num momentum params: 26
[0.1, 2.145469255065918, 1.8150658130645752, 63.976, 52.79, tensor(0.4153, device='cuda:0', grad_fn=<DivBackward0>), 3.01401424407959, 0.38986635208129883]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [28 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [28][0/196]	Time 0.034 (0.034)	Data 0.167 (0.167)	Loss 2.1378 (2.1378)	Acc@1 63.281 (63.281)	Acc@5 92.969 (92.969)
Epoch: [28][10/196]	Time 0.016 (0.017)	Data 0.000 (0.017)	Loss 1.9643 (2.1489)	Acc@1 66.797 (64.027)	Acc@5 94.141 (90.412)
Epoch: [28][20/196]	Time 0.012 (0.016)	Data 0.006 (0.011)	Loss 2.1694 (2.1038)	Acc@1 67.188 (65.309)	Acc@5 89.453 (90.737)
Epoch: [28][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0142 (2.0721)	Acc@1 66.797 (66.217)	Acc@5 91.797 (91.280)
Epoch: [28][40/196]	Time 0.012 (0.015)	Data 0.009 (0.007)	Loss 2.2273 (2.0775)	Acc@1 64.844 (66.035)	Acc@5 87.500 (91.197)
Epoch: [28][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1257 (2.0868)	Acc@1 61.719 (65.571)	Acc@5 92.578 (91.138)
Epoch: [28][60/196]	Time 0.013 (0.015)	Data 0.006 (0.005)	Loss 2.0928 (2.0952)	Acc@1 64.844 (65.529)	Acc@5 90.625 (90.958)
Epoch: [28][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1483 (2.1008)	Acc@1 63.672 (65.377)	Acc@5 90.234 (90.812)
Epoch: [28][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.2243 (2.1002)	Acc@1 62.109 (65.543)	Acc@5 91.016 (90.799)
Epoch: [28][90/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0056 (2.0990)	Acc@1 65.625 (65.479)	Acc@5 91.797 (90.848)
Epoch: [28][100/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.1303 (2.1031)	Acc@1 65.234 (65.296)	Acc@5 88.672 (90.760)
Epoch: [28][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2224 (2.1032)	Acc@1 64.062 (65.270)	Acc@5 87.891 (90.762)
Epoch: [28][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1467 (2.1081)	Acc@1 64.062 (65.121)	Acc@5 92.188 (90.728)
Epoch: [28][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1934 (2.1140)	Acc@1 59.375 (64.880)	Acc@5 89.062 (90.679)
Epoch: [28][140/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.1439 (2.1172)	Acc@1 65.625 (64.877)	Acc@5 90.234 (90.619)
Epoch: [28][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9695 (2.1185)	Acc@1 69.922 (64.813)	Acc@5 92.578 (90.581)
Epoch: [28][160/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.0907 (2.1230)	Acc@1 66.016 (64.730)	Acc@5 91.406 (90.516)
Epoch: [28][170/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.2405 (2.1268)	Acc@1 61.719 (64.620)	Acc@5 89.062 (90.470)
Epoch: [28][180/196]	Time 0.013 (0.015)	Data 0.007 (0.003)	Loss 2.1668 (2.1264)	Acc@1 62.109 (64.602)	Acc@5 90.234 (90.500)
Epoch: [28][190/196]	Time 0.017 (0.015)	Data 0.002 (0.003)	Loss 2.1111 (2.1311)	Acc@1 65.234 (64.512)	Acc@5 89.844 (90.375)
num momentum params: 26
[0.1, 2.1325643231964113, 2.0351080763339997, 64.46, 48.73, tensor(0.4212, device='cuda:0', grad_fn=<DivBackward0>), 3.027137517929077, 0.38524675369262695]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [29 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [29][0/196]	Time 0.030 (0.030)	Data 0.173 (0.173)	Loss 2.1107 (2.1107)	Acc@1 62.500 (62.500)	Acc@5 93.359 (93.359)
Epoch: [29][10/196]	Time 0.013 (0.017)	Data 0.002 (0.019)	Loss 2.1161 (2.0518)	Acc@1 66.016 (67.223)	Acc@5 91.016 (92.081)
Epoch: [29][20/196]	Time 0.014 (0.016)	Data 0.003 (0.011)	Loss 2.0006 (2.0386)	Acc@1 67.188 (67.299)	Acc@5 89.844 (91.871)
Epoch: [29][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9933 (2.0261)	Acc@1 64.844 (67.402)	Acc@5 93.750 (92.112)
Epoch: [29][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 2.0708 (2.0307)	Acc@1 70.703 (67.645)	Acc@5 91.797 (91.864)
Epoch: [29][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.1548 (2.0432)	Acc@1 64.844 (67.165)	Acc@5 91.016 (91.759)
Epoch: [29][60/196]	Time 0.017 (0.015)	Data 0.002 (0.005)	Loss 2.1998 (2.0465)	Acc@1 62.109 (66.822)	Acc@5 90.234 (91.726)
Epoch: [29][70/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.1113 (2.0564)	Acc@1 66.797 (66.522)	Acc@5 89.453 (91.538)
Epoch: [29][80/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.0765 (2.0651)	Acc@1 66.797 (66.209)	Acc@5 92.969 (91.401)
Epoch: [29][90/196]	Time 0.011 (0.015)	Data 0.014 (0.004)	Loss 2.1544 (2.0742)	Acc@1 65.625 (65.986)	Acc@5 89.453 (91.230)
Epoch: [29][100/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2497 (2.0784)	Acc@1 60.547 (65.826)	Acc@5 90.234 (91.221)
Epoch: [29][110/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1549 (2.0854)	Acc@1 63.672 (65.618)	Acc@5 88.672 (91.072)
Epoch: [29][120/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0636 (2.0902)	Acc@1 63.672 (65.451)	Acc@5 92.578 (91.087)
Epoch: [29][130/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2304 (2.0969)	Acc@1 62.891 (65.285)	Acc@5 91.016 (91.022)
Epoch: [29][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0213 (2.1026)	Acc@1 66.797 (65.171)	Acc@5 92.578 (90.844)
Epoch: [29][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2403 (2.1088)	Acc@1 63.672 (64.981)	Acc@5 88.281 (90.780)
Epoch: [29][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1870 (2.1121)	Acc@1 64.062 (64.936)	Acc@5 91.016 (90.756)
Epoch: [29][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0918 (2.1175)	Acc@1 68.359 (64.841)	Acc@5 92.188 (90.687)
Epoch: [29][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2141 (2.1260)	Acc@1 60.156 (64.645)	Acc@5 90.625 (90.608)
Epoch: [29][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1051 (2.1316)	Acc@1 63.281 (64.555)	Acc@5 92.969 (90.535)
num momentum params: 26
[0.1, 2.1340934690856934, 1.824223940372467, 64.504, 51.94, tensor(0.4240, device='cuda:0', grad_fn=<DivBackward0>), 2.942784309387207, 0.37738561630249023]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [510]
Non Pruning Epoch - module.bn8.bias: [510]
Non Pruning Epoch - module.fc.weight: [100, 510]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [30 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [510, 512, 3, 3]
Epoch: [30][0/196]	Time 0.032 (0.032)	Data 0.176 (0.176)	Loss 1.8745 (1.8745)	Acc@1 73.828 (73.828)	Acc@5 93.359 (93.359)
Epoch: [30][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 1.9236 (2.0389)	Acc@1 71.875 (68.111)	Acc@5 92.578 (91.264)
Epoch: [30][20/196]	Time 0.013 (0.016)	Data 0.006 (0.011)	Loss 1.9222 (2.0274)	Acc@1 69.531 (68.006)	Acc@5 92.969 (91.797)
Epoch: [30][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.8954 (2.0211)	Acc@1 72.266 (67.918)	Acc@5 92.969 (92.024)
Epoch: [30][40/196]	Time 0.011 (0.016)	Data 0.009 (0.007)	Loss 1.9125 (2.0130)	Acc@1 69.141 (68.112)	Acc@5 92.188 (92.102)
Epoch: [30][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 2.0889 (2.0448)	Acc@1 64.453 (67.203)	Acc@5 89.062 (91.552)
Epoch: [30][60/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 1.9984 (2.0628)	Acc@1 68.359 (66.720)	Acc@5 91.016 (91.329)
Epoch: [30][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 1.9731 (2.0703)	Acc@1 69.922 (66.593)	Acc@5 91.797 (91.241)
Epoch: [30][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 1.9174 (2.0693)	Acc@1 72.656 (66.585)	Acc@5 92.188 (91.237)
Epoch: [30][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1891 (2.0746)	Acc@1 61.719 (66.393)	Acc@5 90.625 (91.209)
Epoch: [30][100/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.2058 (2.0832)	Acc@1 62.891 (66.074)	Acc@5 88.281 (91.159)
Epoch: [30][110/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0849 (2.0888)	Acc@1 62.500 (65.882)	Acc@5 94.141 (91.114)
Epoch: [30][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2681 (2.0969)	Acc@1 66.016 (65.732)	Acc@5 89.062 (91.003)
Epoch: [30][130/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 2.1579 (2.0992)	Acc@1 62.109 (65.679)	Acc@5 87.109 (90.983)
Epoch: [30][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.2501 (2.0996)	Acc@1 62.500 (65.733)	Acc@5 87.109 (90.960)
Epoch: [30][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1038 (2.1020)	Acc@1 60.938 (65.602)	Acc@5 92.578 (90.930)
Epoch: [30][160/196]	Time 0.014 (0.015)	Data 0.006 (0.004)	Loss 2.2232 (2.1049)	Acc@1 63.672 (65.533)	Acc@5 88.672 (90.906)
Epoch: [30][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.3238 (2.1092)	Acc@1 61.719 (65.422)	Acc@5 86.328 (90.835)
Epoch: [30][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1960 (2.1169)	Acc@1 62.500 (65.187)	Acc@5 89.453 (90.731)
Epoch: [30][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2384 (2.1213)	Acc@1 60.547 (65.026)	Acc@5 88.672 (90.678)
num momentum params: 26
[0.1, 2.123501739807129, 1.8111336374282836, 64.962, 51.69, tensor(0.4295, device='cuda:0', grad_fn=<DivBackward0>), 2.957916736602783, 0.37941694259643555]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [510, 512, 3, 3]
Before - module.bn8.weight: [510]
Before - module.bn8.bias: [510]
Before - module.fc.weight: [100, 510]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv8.weight]: [510, 512, 3, 3] >> [485, 512, 3, 3]
[module.bn8.weight]: 510 >> 485
running_mean [485]
running_var [485]
num_batches_tracked []
[module.fc.weight]: [100, 510] >> [100, 485]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [64, 3, 3, 3]
After - module.bn1.weight: [64]
After - module.bn1.bias: [64]
After - module.conv2.weight: [128, 64, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [512, 512, 3, 3]
After - module.bn7.weight: [512]
After - module.bn7.bias: [512]
After - module.conv8.weight: [485, 512, 3, 3]
After - module.bn8.weight: [485]
After - module.bn8.bias: [485]
After - module.fc.weight: [100, 485]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [485, 512, 3, 3]
fc --> [485, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 6865551360, 8939520, 485
fc, 18624000, 48500, 0
===================
FLOP REPORT: 30997342200000.0 60577600000.0 152315252 151444 2725 17.442745208740234
[INFO] Storing checkpoint...

Epoch: [31 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [31][0/196]	Time 0.176 (0.176)	Data 0.185 (0.185)	Loss 2.0512 (2.0512)	Acc@1 66.406 (66.406)	Acc@5 92.578 (92.578)
Epoch: [31][10/196]	Time 0.015 (0.029)	Data 0.002 (0.019)	Loss 2.0968 (2.1295)	Acc@1 62.109 (65.412)	Acc@5 93.359 (91.087)
Epoch: [31][20/196]	Time 0.011 (0.023)	Data 0.006 (0.011)	Loss 1.9717 (2.0953)	Acc@1 69.141 (66.425)	Acc@5 91.016 (91.127)
Epoch: [31][30/196]	Time 0.017 (0.020)	Data 0.001 (0.008)	Loss 2.1033 (2.0777)	Acc@1 64.453 (66.696)	Acc@5 92.188 (91.620)
Epoch: [31][40/196]	Time 0.011 (0.019)	Data 0.018 (0.007)	Loss 2.0642 (2.0652)	Acc@1 67.578 (66.911)	Acc@5 92.188 (91.683)
Epoch: [31][50/196]	Time 0.018 (0.018)	Data 0.000 (0.006)	Loss 1.9331 (2.0654)	Acc@1 69.531 (66.766)	Acc@5 93.750 (91.651)
Epoch: [31][60/196]	Time 0.011 (0.018)	Data 0.018 (0.006)	Loss 2.0354 (2.0643)	Acc@1 67.188 (66.733)	Acc@5 92.188 (91.765)
Epoch: [31][70/196]	Time 0.016 (0.018)	Data 0.001 (0.006)	Loss 2.1701 (2.0727)	Acc@1 64.062 (66.445)	Acc@5 87.891 (91.522)
Epoch: [31][80/196]	Time 0.012 (0.017)	Data 0.019 (0.006)	Loss 2.1437 (2.0803)	Acc@1 64.062 (66.242)	Acc@5 89.062 (91.377)
Epoch: [31][90/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 2.0523 (2.0852)	Acc@1 68.359 (66.132)	Acc@5 93.359 (91.303)
Epoch: [31][100/196]	Time 0.011 (0.017)	Data 0.019 (0.005)	Loss 2.1865 (2.0894)	Acc@1 61.719 (65.931)	Acc@5 90.234 (91.244)
Epoch: [31][110/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 2.1398 (2.0984)	Acc@1 65.234 (65.752)	Acc@5 90.234 (91.153)
Epoch: [31][120/196]	Time 0.011 (0.017)	Data 0.008 (0.005)	Loss 2.3028 (2.1088)	Acc@1 62.109 (65.525)	Acc@5 87.891 (91.025)
Epoch: [31][130/196]	Time 0.020 (0.017)	Data 0.000 (0.005)	Loss 2.1289 (2.1152)	Acc@1 62.500 (65.389)	Acc@5 91.016 (90.917)
Epoch: [31][140/196]	Time 0.011 (0.017)	Data 0.008 (0.005)	Loss 2.1606 (2.1198)	Acc@1 63.672 (65.329)	Acc@5 91.016 (90.841)
Epoch: [31][150/196]	Time 0.016 (0.017)	Data 0.001 (0.005)	Loss 2.1442 (2.1207)	Acc@1 66.797 (65.273)	Acc@5 90.234 (90.819)
Epoch: [31][160/196]	Time 0.014 (0.016)	Data 0.009 (0.004)	Loss 2.0951 (2.1219)	Acc@1 64.844 (65.256)	Acc@5 91.797 (90.766)
Epoch: [31][170/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.2209 (2.1258)	Acc@1 64.844 (65.173)	Acc@5 87.109 (90.710)
Epoch: [31][180/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2068 (2.1302)	Acc@1 64.844 (65.021)	Acc@5 90.625 (90.688)
Epoch: [31][190/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2233 (2.1334)	Acc@1 64.453 (64.915)	Acc@5 87.500 (90.617)
num momentum params: 26
[0.1, 2.1342306118774412, 1.9023849296569824, 64.9, 50.82, tensor(0.4304, device='cuda:0', grad_fn=<DivBackward0>), 3.23685359954834, 0.4034152030944824]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [32 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [32][0/196]	Time 0.030 (0.030)	Data 0.175 (0.175)	Loss 2.1941 (2.1941)	Acc@1 62.500 (62.500)	Acc@5 91.016 (91.016)
Epoch: [32][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 1.9821 (2.0678)	Acc@1 68.359 (65.447)	Acc@5 94.531 (92.045)
Epoch: [32][20/196]	Time 0.012 (0.016)	Data 0.006 (0.011)	Loss 2.2330 (2.0626)	Acc@1 62.500 (66.183)	Acc@5 89.062 (91.871)
Epoch: [32][30/196]	Time 0.014 (0.016)	Data 0.002 (0.008)	Loss 2.0501 (2.0473)	Acc@1 69.531 (66.986)	Acc@5 90.234 (91.885)
Epoch: [32][40/196]	Time 0.017 (0.015)	Data 0.006 (0.007)	Loss 2.0640 (2.0563)	Acc@1 69.531 (66.625)	Acc@5 91.797 (91.740)
Epoch: [32][50/196]	Time 0.018 (0.015)	Data 0.001 (0.006)	Loss 1.9045 (2.0568)	Acc@1 73.047 (66.659)	Acc@5 92.969 (91.774)
Epoch: [32][60/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1506 (2.0616)	Acc@1 67.188 (66.483)	Acc@5 90.625 (91.733)
Epoch: [32][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1750 (2.0669)	Acc@1 64.844 (66.456)	Acc@5 89.062 (91.731)
Epoch: [32][80/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.0421 (2.0698)	Acc@1 66.406 (66.464)	Acc@5 92.188 (91.667)
Epoch: [32][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9382 (2.0710)	Acc@1 68.359 (66.372)	Acc@5 91.797 (91.694)
Epoch: [32][100/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.0672 (2.0737)	Acc@1 66.406 (66.364)	Acc@5 91.406 (91.662)
Epoch: [32][110/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.9539 (2.0749)	Acc@1 67.578 (66.318)	Acc@5 92.969 (91.681)
Epoch: [32][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0391 (2.0805)	Acc@1 67.578 (66.190)	Acc@5 90.625 (91.593)
Epoch: [32][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2124 (2.0845)	Acc@1 62.891 (66.054)	Acc@5 87.891 (91.505)
Epoch: [32][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.3076 (2.0918)	Acc@1 58.984 (65.852)	Acc@5 89.844 (91.417)
Epoch: [32][150/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.1796 (2.1007)	Acc@1 63.281 (65.656)	Acc@5 91.016 (91.238)
Epoch: [32][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1850 (2.1028)	Acc@1 63.672 (65.637)	Acc@5 91.797 (91.222)
Epoch: [32][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.3061 (2.1091)	Acc@1 61.328 (65.490)	Acc@5 85.938 (91.148)
Epoch: [32][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1377 (2.1117)	Acc@1 66.797 (65.485)	Acc@5 89.844 (91.093)
Epoch: [32][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1797 (2.1147)	Acc@1 65.625 (65.486)	Acc@5 89.844 (91.030)
num momentum params: 26
[0.1, 2.115796388015747, 1.7826654303073883, 65.454, 53.41, tensor(0.4366, device='cuda:0', grad_fn=<DivBackward0>), 2.9612948894500732, 0.3773174285888672]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [33 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [33][0/196]	Time 0.031 (0.031)	Data 0.178 (0.178)	Loss 2.1812 (2.1812)	Acc@1 63.672 (63.672)	Acc@5 91.406 (91.406)
Epoch: [33][10/196]	Time 0.016 (0.016)	Data 0.002 (0.018)	Loss 1.9092 (2.0414)	Acc@1 69.141 (68.288)	Acc@5 95.312 (92.543)
Epoch: [33][20/196]	Time 0.015 (0.016)	Data 0.002 (0.011)	Loss 2.1033 (2.0177)	Acc@1 66.797 (68.322)	Acc@5 90.625 (92.374)
Epoch: [33][30/196]	Time 0.013 (0.015)	Data 0.004 (0.008)	Loss 1.9992 (2.0124)	Acc@1 66.406 (68.700)	Acc@5 90.234 (92.225)
Epoch: [33][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 2.0537 (2.0164)	Acc@1 67.578 (68.512)	Acc@5 91.797 (92.140)
Epoch: [33][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 2.0671 (2.0253)	Acc@1 64.844 (68.038)	Acc@5 92.188 (92.149)
Epoch: [33][60/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0518 (2.0274)	Acc@1 65.234 (67.700)	Acc@5 90.625 (92.284)
Epoch: [33][70/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 1.8764 (2.0383)	Acc@1 69.141 (67.430)	Acc@5 96.875 (92.248)
Epoch: [33][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1770 (2.0396)	Acc@1 67.188 (67.395)	Acc@5 90.234 (92.221)
Epoch: [33][90/196]	Time 0.012 (0.015)	Data 0.012 (0.005)	Loss 2.1082 (2.0461)	Acc@1 66.016 (67.252)	Acc@5 92.969 (92.222)
Epoch: [33][100/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1035 (2.0533)	Acc@1 63.672 (67.079)	Acc@5 89.453 (92.071)
Epoch: [33][110/196]	Time 0.013 (0.015)	Data 0.010 (0.005)	Loss 2.0773 (2.0561)	Acc@1 66.016 (67.015)	Acc@5 91.797 (91.983)
Epoch: [33][120/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9848 (2.0569)	Acc@1 69.141 (67.020)	Acc@5 89.844 (91.920)
Epoch: [33][130/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 1.9624 (2.0645)	Acc@1 72.656 (66.845)	Acc@5 91.797 (91.857)
Epoch: [33][140/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1402 (2.0725)	Acc@1 64.453 (66.645)	Acc@5 91.016 (91.744)
Epoch: [33][150/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.0935 (2.0780)	Acc@1 68.359 (66.523)	Acc@5 89.844 (91.665)
Epoch: [33][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1678 (2.0885)	Acc@1 65.234 (66.275)	Acc@5 88.281 (91.486)
Epoch: [33][170/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1285 (2.0949)	Acc@1 64.062 (66.112)	Acc@5 89.844 (91.367)
Epoch: [33][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1952 (2.1029)	Acc@1 63.672 (65.953)	Acc@5 90.625 (91.201)
Epoch: [33][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1595 (2.1087)	Acc@1 62.500 (65.807)	Acc@5 91.406 (91.093)
num momentum params: 26
[0.1, 2.1109694264221193, 1.8899139010906219, 65.726, 51.9, tensor(0.4399, device='cuda:0', grad_fn=<DivBackward0>), 2.9498932361602783, 0.37848377227783203]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [34 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [34][0/196]	Time 0.031 (0.031)	Data 0.182 (0.182)	Loss 2.1829 (2.1829)	Acc@1 66.797 (66.797)	Acc@5 89.844 (89.844)
Epoch: [34][10/196]	Time 0.018 (0.017)	Data 0.000 (0.019)	Loss 2.0690 (2.0256)	Acc@1 65.234 (68.395)	Acc@5 92.578 (92.436)
Epoch: [34][20/196]	Time 0.014 (0.016)	Data 0.003 (0.011)	Loss 1.9664 (2.0278)	Acc@1 70.312 (68.304)	Acc@5 94.141 (92.355)
Epoch: [34][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9879 (2.0128)	Acc@1 67.188 (68.599)	Acc@5 94.531 (92.540)
Epoch: [34][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.1638 (2.0256)	Acc@1 66.016 (68.255)	Acc@5 88.281 (92.178)
Epoch: [34][50/196]	Time 0.020 (0.016)	Data 0.001 (0.006)	Loss 2.0544 (2.0251)	Acc@1 66.797 (68.130)	Acc@5 92.188 (92.318)
Epoch: [34][60/196]	Time 0.011 (0.016)	Data 0.006 (0.006)	Loss 2.1478 (2.0335)	Acc@1 67.188 (68.110)	Acc@5 90.625 (92.162)
Epoch: [34][70/196]	Time 0.019 (0.015)	Data 0.000 (0.005)	Loss 2.0990 (2.0349)	Acc@1 66.797 (67.963)	Acc@5 90.234 (92.248)
Epoch: [34][80/196]	Time 0.011 (0.015)	Data 0.025 (0.005)	Loss 2.2890 (2.0445)	Acc@1 65.625 (67.766)	Acc@5 88.281 (92.081)
Epoch: [34][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2144 (2.0492)	Acc@1 62.891 (67.733)	Acc@5 91.016 (92.063)
Epoch: [34][100/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 2.2381 (2.0604)	Acc@1 57.031 (67.331)	Acc@5 93.750 (91.967)
Epoch: [34][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1384 (2.0711)	Acc@1 65.234 (66.987)	Acc@5 91.406 (91.790)
Epoch: [34][120/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.2422 (2.0812)	Acc@1 59.766 (66.710)	Acc@5 91.016 (91.671)
Epoch: [34][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.3054 (2.0911)	Acc@1 63.672 (66.433)	Acc@5 88.281 (91.576)
Epoch: [34][140/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.1028 (2.0958)	Acc@1 66.016 (66.307)	Acc@5 88.672 (91.420)
Epoch: [34][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2473 (2.1010)	Acc@1 59.375 (66.166)	Acc@5 89.062 (91.311)
Epoch: [34][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1321 (2.1041)	Acc@1 66.016 (66.137)	Acc@5 91.016 (91.249)
Epoch: [34][170/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.1807 (2.1067)	Acc@1 66.406 (66.130)	Acc@5 88.672 (91.214)
Epoch: [34][180/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.1539 (2.1110)	Acc@1 66.406 (66.020)	Acc@5 89.453 (91.160)
Epoch: [34][190/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2568 (2.1144)	Acc@1 64.062 (65.903)	Acc@5 88.672 (91.149)
num momentum params: 26
[0.1, 2.116915149154663, 1.788687754869461, 65.796, 53.06, tensor(0.4416, device='cuda:0', grad_fn=<DivBackward0>), 2.9728193283081055, 0.3822133541107178]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [35 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [35][0/196]	Time 0.032 (0.032)	Data 0.175 (0.175)	Loss 1.9516 (1.9516)	Acc@1 69.531 (69.531)	Acc@5 93.750 (93.750)
Epoch: [35][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 2.0039 (2.0683)	Acc@1 68.750 (66.406)	Acc@5 92.578 (91.761)
Epoch: [35][20/196]	Time 0.012 (0.016)	Data 0.006 (0.011)	Loss 1.9132 (2.0120)	Acc@1 72.266 (68.322)	Acc@5 93.359 (92.578)
Epoch: [35][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 1.9073 (2.0064)	Acc@1 70.703 (68.574)	Acc@5 93.750 (92.767)
Epoch: [35][40/196]	Time 0.012 (0.015)	Data 0.005 (0.007)	Loss 2.1120 (2.0182)	Acc@1 65.625 (68.445)	Acc@5 92.188 (92.626)
Epoch: [35][50/196]	Time 0.018 (0.015)	Data 0.001 (0.006)	Loss 2.1782 (2.0307)	Acc@1 64.062 (67.961)	Acc@5 92.578 (92.402)
Epoch: [35][60/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0952 (2.0393)	Acc@1 66.797 (67.815)	Acc@5 91.016 (92.309)
Epoch: [35][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1361 (2.0379)	Acc@1 66.406 (68.013)	Acc@5 90.234 (92.369)
Epoch: [35][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 1.9970 (2.0453)	Acc@1 68.359 (67.737)	Acc@5 93.359 (92.318)
Epoch: [35][90/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.2500 (2.0477)	Acc@1 66.016 (67.668)	Acc@5 89.844 (92.273)
Epoch: [35][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0314 (2.0447)	Acc@1 68.359 (67.748)	Acc@5 93.359 (92.307)
Epoch: [35][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1385 (2.0486)	Acc@1 63.281 (67.624)	Acc@5 89.844 (92.209)
Epoch: [35][120/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1244 (2.0548)	Acc@1 65.625 (67.468)	Acc@5 91.797 (92.142)
Epoch: [35][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1242 (2.0574)	Acc@1 66.406 (67.381)	Acc@5 92.969 (92.152)
Epoch: [35][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2124 (2.0611)	Acc@1 63.672 (67.260)	Acc@5 90.234 (92.088)
Epoch: [35][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3031 (2.0685)	Acc@1 57.422 (67.123)	Acc@5 90.234 (91.993)
Epoch: [35][160/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1984 (2.0789)	Acc@1 63.672 (66.867)	Acc@5 92.578 (91.891)
Epoch: [35][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0154 (2.0855)	Acc@1 71.484 (66.706)	Acc@5 90.625 (91.817)
Epoch: [35][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.3112 (2.0932)	Acc@1 60.156 (66.497)	Acc@5 89.453 (91.728)
Epoch: [35][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2095 (2.0967)	Acc@1 60.938 (66.365)	Acc@5 92.188 (91.678)
num momentum params: 26
[0.1, 2.0990603576660156, 1.8594174444675446, 66.286, 52.22, tensor(0.4472, device='cuda:0', grad_fn=<DivBackward0>), 2.9900248050689697, 0.3773472309112549]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [36 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [36][0/196]	Time 0.035 (0.035)	Data 0.172 (0.172)	Loss 1.9120 (1.9120)	Acc@1 73.828 (73.828)	Acc@5 92.969 (92.969)
Epoch: [36][10/196]	Time 0.014 (0.017)	Data 0.002 (0.018)	Loss 2.1239 (2.0652)	Acc@1 65.234 (67.116)	Acc@5 92.969 (92.507)
Epoch: [36][20/196]	Time 0.015 (0.016)	Data 0.003 (0.011)	Loss 2.0598 (2.0592)	Acc@1 64.453 (67.132)	Acc@5 92.188 (92.429)
Epoch: [36][30/196]	Time 0.016 (0.016)	Data 0.003 (0.008)	Loss 1.9845 (2.0281)	Acc@1 70.703 (68.145)	Acc@5 94.141 (92.805)
Epoch: [36][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 2.0462 (2.0271)	Acc@1 69.922 (68.245)	Acc@5 93.359 (92.816)
Epoch: [36][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.0105 (2.0299)	Acc@1 69.922 (68.214)	Acc@5 91.797 (92.777)
Epoch: [36][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1244 (2.0418)	Acc@1 66.016 (67.898)	Acc@5 91.797 (92.533)
Epoch: [36][70/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.1997 (2.0492)	Acc@1 61.719 (67.716)	Acc@5 88.672 (92.342)
Epoch: [36][80/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.1292 (2.0556)	Acc@1 64.062 (67.564)	Acc@5 92.969 (92.260)
Epoch: [36][90/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0404 (2.0622)	Acc@1 67.188 (67.449)	Acc@5 91.797 (92.222)
Epoch: [36][100/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1787 (2.0664)	Acc@1 66.406 (67.392)	Acc@5 91.797 (92.160)
Epoch: [36][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2507 (2.0760)	Acc@1 64.062 (67.092)	Acc@5 87.891 (92.036)
Epoch: [36][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2114 (2.0818)	Acc@1 65.234 (66.890)	Acc@5 92.188 (91.991)
Epoch: [36][130/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2231 (2.0842)	Acc@1 64.453 (66.821)	Acc@5 89.453 (91.949)
Epoch: [36][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0039 (2.0826)	Acc@1 67.969 (66.847)	Acc@5 92.969 (91.894)
Epoch: [36][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1438 (2.0849)	Acc@1 65.234 (66.789)	Acc@5 89.062 (91.823)
Epoch: [36][160/196]	Time 0.017 (0.015)	Data 0.002 (0.004)	Loss 2.0948 (2.0863)	Acc@1 67.969 (66.756)	Acc@5 91.797 (91.826)
Epoch: [36][170/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0211 (2.0875)	Acc@1 66.406 (66.692)	Acc@5 93.750 (91.808)
Epoch: [36][180/196]	Time 0.021 (0.015)	Data 0.003 (0.003)	Loss 2.1875 (2.0907)	Acc@1 65.625 (66.603)	Acc@5 89.844 (91.771)
Epoch: [36][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.0336 (2.0929)	Acc@1 67.188 (66.523)	Acc@5 90.234 (91.727)
num momentum params: 26
[0.1, 2.0914736323547363, 1.9177846467494966, 66.552, 51.58, tensor(0.4507, device='cuda:0', grad_fn=<DivBackward0>), 3.0130250453948975, 0.38083815574645996]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [37 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [37][0/196]	Time 0.032 (0.032)	Data 0.175 (0.175)	Loss 1.9677 (1.9677)	Acc@1 70.312 (70.312)	Acc@5 94.531 (94.531)
Epoch: [37][10/196]	Time 0.015 (0.017)	Data 0.003 (0.018)	Loss 1.8685 (2.0266)	Acc@1 71.094 (67.827)	Acc@5 95.312 (93.111)
Epoch: [37][20/196]	Time 0.012 (0.016)	Data 0.004 (0.011)	Loss 2.1253 (2.0368)	Acc@1 66.406 (67.690)	Acc@5 89.453 (92.727)
Epoch: [37][30/196]	Time 0.012 (0.015)	Data 0.017 (0.009)	Loss 2.1734 (2.0452)	Acc@1 64.453 (67.503)	Acc@5 90.234 (92.528)
Epoch: [37][40/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 2.1042 (2.0383)	Acc@1 66.406 (67.530)	Acc@5 92.578 (92.597)
Epoch: [37][50/196]	Time 0.011 (0.015)	Data 0.018 (0.008)	Loss 2.1906 (2.0520)	Acc@1 64.062 (67.203)	Acc@5 92.188 (92.563)
Epoch: [37][60/196]	Time 0.016 (0.015)	Data 0.001 (0.007)	Loss 2.1347 (2.0621)	Acc@1 66.797 (66.989)	Acc@5 89.844 (92.277)
Epoch: [37][70/196]	Time 0.012 (0.015)	Data 0.010 (0.007)	Loss 2.1389 (2.0601)	Acc@1 62.500 (67.000)	Acc@5 94.531 (92.292)
Epoch: [37][80/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.2480 (2.0666)	Acc@1 59.375 (66.845)	Acc@5 90.234 (92.178)
Epoch: [37][90/196]	Time 0.011 (0.015)	Data 0.023 (0.006)	Loss 2.0424 (2.0688)	Acc@1 68.359 (66.763)	Acc@5 92.578 (92.136)
Epoch: [37][100/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.1693 (2.0771)	Acc@1 64.062 (66.631)	Acc@5 91.016 (91.959)
Epoch: [37][110/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.0371 (2.0777)	Acc@1 67.969 (66.698)	Acc@5 89.844 (91.913)
Epoch: [37][120/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.2354 (2.0799)	Acc@1 60.547 (66.613)	Acc@5 88.672 (91.842)
Epoch: [37][130/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.2255 (2.0872)	Acc@1 60.547 (66.430)	Acc@5 89.844 (91.725)
Epoch: [37][140/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1574 (2.0938)	Acc@1 66.016 (66.354)	Acc@5 89.844 (91.581)
Epoch: [37][150/196]	Time 0.011 (0.015)	Data 0.015 (0.005)	Loss 2.1245 (2.0938)	Acc@1 67.969 (66.411)	Acc@5 91.797 (91.608)
Epoch: [37][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9391 (2.0943)	Acc@1 72.656 (66.467)	Acc@5 92.188 (91.562)
Epoch: [37][170/196]	Time 0.011 (0.015)	Data 0.017 (0.005)	Loss 2.1785 (2.0973)	Acc@1 66.406 (66.388)	Acc@5 91.406 (91.504)
Epoch: [37][180/196]	Time 0.023 (0.015)	Data 0.001 (0.005)	Loss 2.2357 (2.1025)	Acc@1 62.500 (66.285)	Acc@5 90.234 (91.465)
Epoch: [37][190/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.3542 (2.1073)	Acc@1 64.844 (66.196)	Acc@5 86.719 (91.388)
num momentum params: 26
[0.1, 2.110203079223633, 2.025692844390869, 66.156, 49.59, tensor(0.4492, device='cuda:0', grad_fn=<DivBackward0>), 3.0219850540161133, 0.37886595726013184]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [38 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [38][0/196]	Time 0.034 (0.034)	Data 0.168 (0.168)	Loss 2.1090 (2.1090)	Acc@1 64.844 (64.844)	Acc@5 91.406 (91.406)
Epoch: [38][10/196]	Time 0.016 (0.018)	Data 0.003 (0.017)	Loss 2.1144 (2.0617)	Acc@1 69.141 (67.649)	Acc@5 90.625 (91.868)
Epoch: [38][20/196]	Time 0.015 (0.017)	Data 0.002 (0.010)	Loss 1.9782 (2.0609)	Acc@1 70.703 (67.690)	Acc@5 94.141 (91.927)
Epoch: [38][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 1.8893 (2.0430)	Acc@1 71.875 (68.107)	Acc@5 92.969 (92.364)
Epoch: [38][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.0009 (2.0329)	Acc@1 70.703 (68.274)	Acc@5 94.141 (92.464)
Epoch: [38][50/196]	Time 0.011 (0.015)	Data 0.020 (0.006)	Loss 2.0322 (2.0385)	Acc@1 67.188 (67.969)	Acc@5 93.750 (92.502)
Epoch: [38][60/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0950 (2.0453)	Acc@1 65.625 (67.879)	Acc@5 90.625 (92.360)
Epoch: [38][70/196]	Time 0.012 (0.015)	Data 0.018 (0.006)	Loss 1.9318 (2.0496)	Acc@1 71.875 (67.776)	Acc@5 93.750 (92.325)
Epoch: [38][80/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0825 (2.0526)	Acc@1 69.531 (67.679)	Acc@5 91.016 (92.298)
Epoch: [38][90/196]	Time 0.011 (0.015)	Data 0.019 (0.006)	Loss 2.1085 (2.0558)	Acc@1 66.406 (67.591)	Acc@5 91.016 (92.346)
Epoch: [38][100/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0436 (2.0570)	Acc@1 69.141 (67.555)	Acc@5 92.578 (92.362)
Epoch: [38][110/196]	Time 0.011 (0.015)	Data 0.019 (0.005)	Loss 1.9252 (2.0655)	Acc@1 72.266 (67.349)	Acc@5 94.531 (92.261)
Epoch: [38][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1672 (2.0712)	Acc@1 66.797 (67.204)	Acc@5 91.016 (92.091)
Epoch: [38][130/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0666 (2.0760)	Acc@1 66.016 (67.140)	Acc@5 92.188 (91.997)
Epoch: [38][140/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0325 (2.0804)	Acc@1 67.578 (67.085)	Acc@5 92.578 (91.897)
Epoch: [38][150/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.1041 (2.0849)	Acc@1 67.578 (66.934)	Acc@5 90.625 (91.815)
Epoch: [38][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.2012 (2.0875)	Acc@1 63.672 (66.884)	Acc@5 89.062 (91.807)
Epoch: [38][170/196]	Time 0.011 (0.015)	Data 0.017 (0.005)	Loss 2.3514 (2.0919)	Acc@1 62.109 (66.863)	Acc@5 86.719 (91.678)
Epoch: [38][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1691 (2.0956)	Acc@1 60.547 (66.723)	Acc@5 90.625 (91.633)
Epoch: [38][190/196]	Time 0.011 (0.015)	Data 0.018 (0.005)	Loss 2.1710 (2.0967)	Acc@1 65.234 (66.676)	Acc@5 90.234 (91.582)
num momentum params: 26
[0.1, 2.0980650778198244, 2.0365230655670166, 66.634, 48.96, tensor(0.4533, device='cuda:0', grad_fn=<DivBackward0>), 2.9817726612091064, 0.37778425216674805]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [39 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [39][0/196]	Time 0.034 (0.034)	Data 0.174 (0.174)	Loss 2.0567 (2.0567)	Acc@1 66.016 (66.016)	Acc@5 91.406 (91.406)
Epoch: [39][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 2.1433 (2.0315)	Acc@1 62.500 (67.756)	Acc@5 92.578 (92.294)
Epoch: [39][20/196]	Time 0.011 (0.015)	Data 0.006 (0.012)	Loss 2.0458 (2.0380)	Acc@1 69.141 (68.136)	Acc@5 89.844 (92.169)
Epoch: [39][30/196]	Time 0.016 (0.015)	Data 0.001 (0.010)	Loss 2.0706 (2.0286)	Acc@1 66.797 (68.485)	Acc@5 92.578 (92.402)
Epoch: [39][40/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 2.1028 (2.0411)	Acc@1 67.969 (68.007)	Acc@5 91.797 (92.292)
Epoch: [39][50/196]	Time 0.017 (0.015)	Data 0.001 (0.008)	Loss 1.9475 (2.0365)	Acc@1 69.922 (68.107)	Acc@5 93.359 (92.448)
Epoch: [39][60/196]	Time 0.017 (0.015)	Data 0.000 (0.007)	Loss 2.0906 (2.0370)	Acc@1 69.141 (68.065)	Acc@5 92.578 (92.591)
Epoch: [39][70/196]	Time 0.017 (0.015)	Data 0.001 (0.007)	Loss 2.0199 (2.0305)	Acc@1 67.578 (68.079)	Acc@5 94.531 (92.655)
Epoch: [39][80/196]	Time 0.018 (0.015)	Data 0.000 (0.006)	Loss 2.0346 (2.0404)	Acc@1 69.922 (67.974)	Acc@5 94.922 (92.496)
Epoch: [39][90/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0294 (2.0407)	Acc@1 66.797 (67.943)	Acc@5 91.797 (92.505)
Epoch: [39][100/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1267 (2.0453)	Acc@1 66.016 (67.922)	Acc@5 89.844 (92.408)
Epoch: [39][110/196]	Time 0.012 (0.015)	Data 0.001 (0.005)	Loss 2.1355 (2.0464)	Acc@1 68.750 (67.870)	Acc@5 89.844 (92.385)
Epoch: [39][120/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0352 (2.0475)	Acc@1 69.922 (67.891)	Acc@5 92.969 (92.268)
Epoch: [39][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2907 (2.0546)	Acc@1 60.156 (67.811)	Acc@5 87.891 (92.173)
Epoch: [39][140/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1563 (2.0613)	Acc@1 62.500 (67.631)	Acc@5 91.016 (92.113)
Epoch: [39][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0939 (2.0656)	Acc@1 64.453 (67.490)	Acc@5 92.188 (92.063)
Epoch: [39][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0244 (2.0684)	Acc@1 67.969 (67.408)	Acc@5 92.969 (92.013)
Epoch: [39][170/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.2997 (2.0760)	Acc@1 61.328 (67.219)	Acc@5 87.109 (91.884)
Epoch: [39][180/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2720 (2.0848)	Acc@1 62.500 (66.991)	Acc@5 89.453 (91.821)
Epoch: [39][190/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1621 (2.0912)	Acc@1 66.406 (66.852)	Acc@5 92.969 (91.768)
num momentum params: 26
[0.1, 2.0929042474365236, 2.1364297497272493, 66.804, 48.77, tensor(0.4567, device='cuda:0', grad_fn=<DivBackward0>), 2.995462417602539, 0.38474273681640625]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [485, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [485]
Non Pruning Epoch - module.bn8.bias: [485]
Non Pruning Epoch - module.fc.weight: [100, 485]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [40 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [485, 512, 3, 3]
Epoch: [40][0/196]	Time 0.034 (0.034)	Data 0.173 (0.173)	Loss 1.9233 (1.9233)	Acc@1 69.141 (69.141)	Acc@5 94.141 (94.141)
Epoch: [40][10/196]	Time 0.019 (0.018)	Data 0.003 (0.018)	Loss 2.0867 (2.0641)	Acc@1 68.359 (67.045)	Acc@5 91.797 (92.720)
Epoch: [40][20/196]	Time 0.016 (0.017)	Data 0.002 (0.010)	Loss 1.9202 (2.0397)	Acc@1 68.750 (67.987)	Acc@5 94.531 (92.783)
Epoch: [40][30/196]	Time 0.017 (0.017)	Data 0.001 (0.008)	Loss 1.9277 (2.0284)	Acc@1 73.828 (68.674)	Acc@5 94.141 (92.666)
Epoch: [40][40/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.0086 (2.0335)	Acc@1 72.266 (68.569)	Acc@5 91.406 (92.502)
Epoch: [40][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0681 (2.0304)	Acc@1 70.312 (68.827)	Acc@5 90.234 (92.532)
Epoch: [40][60/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0202 (2.0258)	Acc@1 71.484 (68.852)	Acc@5 92.969 (92.681)
Epoch: [40][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0023 (2.0334)	Acc@1 69.922 (68.717)	Acc@5 92.969 (92.584)
Epoch: [40][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.1231 (2.0355)	Acc@1 64.062 (68.658)	Acc@5 91.406 (92.554)
Epoch: [40][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9200 (2.0390)	Acc@1 71.484 (68.617)	Acc@5 92.188 (92.475)
Epoch: [40][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.1333 (2.0426)	Acc@1 66.016 (68.487)	Acc@5 88.672 (92.466)
Epoch: [40][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1779 (2.0515)	Acc@1 64.453 (68.261)	Acc@5 92.578 (92.378)
Epoch: [40][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.3770 (2.0601)	Acc@1 63.672 (68.037)	Acc@5 88.672 (92.288)
Epoch: [40][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1468 (2.0664)	Acc@1 65.234 (67.960)	Acc@5 92.188 (92.155)
Epoch: [40][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0795 (2.0718)	Acc@1 68.750 (67.827)	Acc@5 91.797 (92.127)
Epoch: [40][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0641 (2.0733)	Acc@1 71.094 (67.744)	Acc@5 92.578 (92.100)
Epoch: [40][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9805 (2.0755)	Acc@1 71.094 (67.690)	Acc@5 91.797 (92.073)
Epoch: [40][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1499 (2.0756)	Acc@1 63.672 (67.658)	Acc@5 92.188 (92.073)
Epoch: [40][180/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2786 (2.0809)	Acc@1 63.281 (67.466)	Acc@5 88.672 (92.032)
Epoch: [40][190/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.3287 (2.0858)	Acc@1 59.375 (67.372)	Acc@5 89.844 (91.952)
num momentum params: 26
[0.1, 2.087248332748413, 2.5374050557613375, 67.332, 42.63, tensor(0.4599, device='cuda:0', grad_fn=<DivBackward0>), 3.0235447883605957, 0.38759303092956543]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [485, 512, 3, 3]
Before - module.bn8.weight: [485]
Before - module.bn8.bias: [485]
Before - module.fc.weight: [100, 485]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [64, 3, 3, 3] >> [58, 3, 3, 3]
[module.bn1.weight]: 64 >> 58
running_mean [58]
running_var [58]
num_batches_tracked []
[module.conv2.weight]: [128, 64, 3, 3] >> [128, 58, 3, 3]
[module.conv7.weight]: [512, 512, 3, 3] >> [510, 512, 3, 3]
[module.bn7.weight]: 512 >> 510
running_mean [510]
running_var [510]
num_batches_tracked []
[module.conv8.weight]: [485, 512, 3, 3] >> [459, 510, 3, 3]
[module.bn8.weight]: 485 >> 459
running_mean [459]
running_var [459]
num_batches_tracked []
[module.fc.weight]: [100, 485] >> [100, 459]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [58, 3, 3, 3]
After - module.bn1.weight: [58]
After - module.bn1.bias: [58]
After - module.conv2.weight: [128, 58, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [510, 512, 3, 3]
After - module.bn7.weight: [510]
After - module.bn7.bias: [510]
After - module.conv8.weight: [459, 510, 3, 3]
After - module.bn8.weight: [459]
After - module.bn8.bias: [459]
After - module.fc.weight: [100, 459]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [58, 3, 3, 3]
conv2 --> [128, 58, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [510, 512, 3, 3]
conv8 --> [459, 510, 3, 3]
fc --> [459, 100]
1, 642235392, 1603584, 58
2, 7149846528, 17104896, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7219445760, 9400320, 510
8, 6472120320, 8427240, 459
fc, 17625600, 45900, 0
===================
FLOP REPORT: 30517335000000.0 58075200000.0 149828148 145188 2691 17.16232681274414
[INFO] Storing checkpoint...

Epoch: [41 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [41][0/196]	Time 0.269 (0.269)	Data 0.179 (0.179)	Loss 2.0168 (2.0168)	Acc@1 66.406 (66.406)	Acc@5 94.922 (94.922)
Epoch: [41][10/196]	Time 0.015 (0.038)	Data 0.003 (0.018)	Loss 2.0053 (2.0504)	Acc@1 70.312 (67.578)	Acc@5 92.578 (92.649)
Epoch: [41][20/196]	Time 0.015 (0.027)	Data 0.003 (0.011)	Loss 2.0093 (2.0225)	Acc@1 67.188 (68.769)	Acc@5 91.797 (92.615)
Epoch: [41][30/196]	Time 0.017 (0.023)	Data 0.002 (0.008)	Loss 1.8340 (2.0045)	Acc@1 75.391 (69.544)	Acc@5 94.922 (92.994)
Epoch: [41][40/196]	Time 0.016 (0.021)	Data 0.003 (0.007)	Loss 2.0444 (2.0112)	Acc@1 70.703 (69.360)	Acc@5 92.578 (92.931)
Epoch: [41][50/196]	Time 0.017 (0.020)	Data 0.002 (0.006)	Loss 2.0175 (2.0155)	Acc@1 69.531 (69.248)	Acc@5 93.750 (92.915)
Epoch: [41][60/196]	Time 0.013 (0.020)	Data 0.005 (0.005)	Loss 1.9888 (2.0184)	Acc@1 70.312 (69.269)	Acc@5 92.188 (92.815)
Epoch: [41][70/196]	Time 0.015 (0.019)	Data 0.002 (0.005)	Loss 2.0544 (2.0267)	Acc@1 68.750 (69.141)	Acc@5 94.531 (92.721)
Epoch: [41][80/196]	Time 0.013 (0.019)	Data 0.005 (0.005)	Loss 2.0293 (2.0299)	Acc@1 66.797 (68.928)	Acc@5 92.969 (92.713)
Epoch: [41][90/196]	Time 0.014 (0.018)	Data 0.003 (0.005)	Loss 2.0111 (2.0315)	Acc@1 66.406 (68.810)	Acc@5 93.359 (92.707)
Epoch: [41][100/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 2.1247 (2.0354)	Acc@1 66.406 (68.611)	Acc@5 91.797 (92.725)
Epoch: [41][110/196]	Time 0.017 (0.018)	Data 0.000 (0.004)	Loss 2.1968 (2.0479)	Acc@1 62.109 (68.289)	Acc@5 91.406 (92.568)
Epoch: [41][120/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 2.1758 (2.0564)	Acc@1 66.797 (68.085)	Acc@5 91.406 (92.485)
Epoch: [41][130/196]	Time 0.018 (0.017)	Data 0.000 (0.004)	Loss 2.2096 (2.0635)	Acc@1 63.672 (67.960)	Acc@5 89.844 (92.384)
Epoch: [41][140/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 2.1345 (2.0657)	Acc@1 65.625 (67.919)	Acc@5 89.844 (92.329)
Epoch: [41][150/196]	Time 0.017 (0.017)	Data 0.000 (0.004)	Loss 2.3715 (2.0726)	Acc@1 60.156 (67.702)	Acc@5 91.016 (92.252)
Epoch: [41][160/196]	Time 0.014 (0.017)	Data 0.003 (0.004)	Loss 2.1340 (2.0768)	Acc@1 65.625 (67.590)	Acc@5 91.797 (92.229)
Epoch: [41][170/196]	Time 0.017 (0.017)	Data 0.000 (0.004)	Loss 2.2004 (2.0792)	Acc@1 64.844 (67.507)	Acc@5 89.844 (92.167)
Epoch: [41][180/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.8565 (2.0806)	Acc@1 73.438 (67.503)	Acc@5 94.531 (92.138)
Epoch: [41][190/196]	Time 0.017 (0.017)	Data 0.000 (0.004)	Loss 2.2830 (2.0844)	Acc@1 62.109 (67.404)	Acc@5 89.844 (92.085)
num momentum params: 26
[0.1, 2.0861680307769777, 1.8355423951148986, 67.364, 52.4, tensor(0.4615, device='cuda:0', grad_fn=<DivBackward0>), 3.361593246459961, 0.4410288333892822]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [42 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [42][0/196]	Time 0.037 (0.037)	Data 0.206 (0.206)	Loss 1.9196 (1.9196)	Acc@1 72.656 (72.656)	Acc@5 94.141 (94.141)
Epoch: [42][10/196]	Time 0.018 (0.018)	Data 0.000 (0.020)	Loss 1.9032 (1.9336)	Acc@1 71.484 (71.484)	Acc@5 95.703 (94.070)
Epoch: [42][20/196]	Time 0.013 (0.016)	Data 0.004 (0.012)	Loss 1.9833 (1.9404)	Acc@1 70.703 (71.131)	Acc@5 92.969 (93.992)
Epoch: [42][30/196]	Time 0.015 (0.016)	Data 0.002 (0.009)	Loss 2.0425 (1.9336)	Acc@1 69.922 (71.421)	Acc@5 92.578 (94.052)
Epoch: [42][40/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 1.8919 (1.9354)	Acc@1 73.438 (71.313)	Acc@5 93.750 (94.026)
Epoch: [42][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.0154 (1.9565)	Acc@1 71.875 (70.895)	Acc@5 94.531 (93.719)
Epoch: [42][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 1.9983 (1.9739)	Acc@1 70.312 (70.409)	Acc@5 92.969 (93.616)
Epoch: [42][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0679 (1.9839)	Acc@1 66.797 (70.219)	Acc@5 92.188 (93.475)
Epoch: [42][80/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 2.0723 (1.9963)	Acc@1 65.625 (69.840)	Acc@5 91.797 (93.258)
Epoch: [42][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0626 (2.0091)	Acc@1 67.578 (69.527)	Acc@5 90.625 (93.050)
Epoch: [42][100/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.0852 (2.0205)	Acc@1 69.531 (69.245)	Acc@5 92.969 (92.942)
Epoch: [42][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3918 (2.0356)	Acc@1 59.766 (68.912)	Acc@5 88.281 (92.768)
Epoch: [42][120/196]	Time 0.022 (0.016)	Data 0.008 (0.004)	Loss 2.1821 (2.0472)	Acc@1 63.281 (68.656)	Acc@5 91.797 (92.607)
Epoch: [42][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0894 (2.0578)	Acc@1 67.578 (68.267)	Acc@5 90.234 (92.465)
Epoch: [42][140/196]	Time 0.012 (0.016)	Data 0.017 (0.004)	Loss 1.9049 (2.0629)	Acc@1 69.922 (68.116)	Acc@5 93.750 (92.412)
Epoch: [42][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0255 (2.0694)	Acc@1 67.188 (67.914)	Acc@5 92.969 (92.294)
Epoch: [42][160/196]	Time 0.011 (0.016)	Data 0.018 (0.004)	Loss 2.2049 (2.0724)	Acc@1 65.234 (67.816)	Acc@5 89.453 (92.253)
Epoch: [42][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0905 (2.0753)	Acc@1 69.141 (67.727)	Acc@5 94.141 (92.265)
Epoch: [42][180/196]	Time 0.011 (0.016)	Data 0.016 (0.004)	Loss 2.1796 (2.0812)	Acc@1 65.625 (67.600)	Acc@5 90.625 (92.149)
Epoch: [42][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.2979 (2.0882)	Acc@1 62.500 (67.468)	Acc@5 89.453 (92.050)
num momentum params: 26
[0.1, 2.0904076483917238, 2.29386284828186, 67.396, 44.94, tensor(0.4623, device='cuda:0', grad_fn=<DivBackward0>), 3.0558621883392334, 0.38431239128112793]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [43 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [43][0/196]	Time 0.037 (0.037)	Data 0.195 (0.195)	Loss 1.9498 (1.9498)	Acc@1 70.312 (70.312)	Acc@5 93.750 (93.750)
Epoch: [43][10/196]	Time 0.017 (0.018)	Data 0.001 (0.020)	Loss 2.1400 (2.0544)	Acc@1 68.750 (68.572)	Acc@5 89.844 (92.401)
Epoch: [43][20/196]	Time 0.012 (0.017)	Data 0.006 (0.011)	Loss 1.9627 (2.0549)	Acc@1 71.484 (68.155)	Acc@5 93.750 (92.522)
Epoch: [43][30/196]	Time 0.017 (0.017)	Data 0.000 (0.009)	Loss 1.9582 (2.0280)	Acc@1 71.484 (68.775)	Acc@5 93.359 (92.981)
Epoch: [43][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 1.7577 (2.0077)	Acc@1 77.344 (69.541)	Acc@5 95.703 (93.197)
Epoch: [43][50/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 2.1209 (2.0101)	Acc@1 66.406 (69.447)	Acc@5 91.406 (93.160)
Epoch: [43][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0021 (2.0058)	Acc@1 71.484 (69.480)	Acc@5 92.969 (93.154)
Epoch: [43][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1727 (2.0184)	Acc@1 63.672 (69.174)	Acc@5 89.844 (92.886)
Epoch: [43][80/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.0428 (2.0203)	Acc@1 66.016 (69.145)	Acc@5 91.406 (92.776)
Epoch: [43][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0376 (2.0240)	Acc@1 67.969 (68.973)	Acc@5 92.188 (92.664)
Epoch: [43][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0937 (2.0251)	Acc@1 68.359 (68.940)	Acc@5 91.797 (92.675)
Epoch: [43][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2245 (2.0277)	Acc@1 60.938 (68.922)	Acc@5 90.234 (92.701)
Epoch: [43][120/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0302 (2.0334)	Acc@1 72.266 (68.750)	Acc@5 90.625 (92.594)
Epoch: [43][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1333 (2.0382)	Acc@1 67.188 (68.649)	Acc@5 91.406 (92.566)
Epoch: [43][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1769 (2.0414)	Acc@1 64.844 (68.545)	Acc@5 91.016 (92.467)
Epoch: [43][150/196]	Time 0.020 (0.016)	Data 0.000 (0.004)	Loss 2.1376 (2.0483)	Acc@1 64.844 (68.359)	Acc@5 92.188 (92.451)
Epoch: [43][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0577 (2.0540)	Acc@1 68.359 (68.260)	Acc@5 91.406 (92.369)
Epoch: [43][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0333 (2.0596)	Acc@1 70.703 (68.133)	Acc@5 93.359 (92.288)
Epoch: [43][180/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0055 (2.0672)	Acc@1 72.266 (67.980)	Acc@5 93.359 (92.170)
Epoch: [43][190/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2514 (2.0738)	Acc@1 63.672 (67.864)	Acc@5 88.281 (92.053)
num momentum params: 26
[0.1, 2.0753054273986815, 1.8379701220989226, 67.84, 52.17, tensor(0.4667, device='cuda:0', grad_fn=<DivBackward0>), 3.0957016944885254, 0.3886089324951172]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [44 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [44][0/196]	Time 0.037 (0.037)	Data 0.172 (0.172)	Loss 1.8595 (1.8595)	Acc@1 74.609 (74.609)	Acc@5 92.969 (92.969)
Epoch: [44][10/196]	Time 0.017 (0.018)	Data 0.001 (0.017)	Loss 1.9957 (1.9690)	Acc@1 69.141 (70.384)	Acc@5 94.531 (93.928)
Epoch: [44][20/196]	Time 0.012 (0.017)	Data 0.008 (0.011)	Loss 1.8253 (1.9564)	Acc@1 76.172 (70.796)	Acc@5 95.703 (93.955)
Epoch: [44][30/196]	Time 0.016 (0.017)	Data 0.001 (0.008)	Loss 2.0359 (1.9564)	Acc@1 67.969 (70.602)	Acc@5 92.578 (93.914)
Epoch: [44][40/196]	Time 0.012 (0.016)	Data 0.009 (0.007)	Loss 1.9923 (1.9725)	Acc@1 70.703 (70.246)	Acc@5 93.750 (93.531)
Epoch: [44][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.0777 (1.9781)	Acc@1 66.797 (70.060)	Acc@5 93.359 (93.681)
Epoch: [44][60/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.1274 (1.9837)	Acc@1 67.969 (69.819)	Acc@5 92.188 (93.622)
Epoch: [44][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9565 (1.9881)	Acc@1 70.312 (69.729)	Acc@5 93.359 (93.546)
Epoch: [44][80/196]	Time 0.012 (0.016)	Data 0.017 (0.005)	Loss 2.2586 (2.0000)	Acc@1 64.453 (69.560)	Acc@5 90.234 (93.412)
Epoch: [44][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0308 (2.0049)	Acc@1 68.750 (69.394)	Acc@5 92.969 (93.299)
Epoch: [44][100/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2005 (2.0161)	Acc@1 64.062 (69.164)	Acc@5 89.844 (93.205)
Epoch: [44][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9952 (2.0224)	Acc@1 67.578 (68.944)	Acc@5 94.141 (93.124)
Epoch: [44][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0783 (2.0307)	Acc@1 66.016 (68.682)	Acc@5 94.922 (92.998)
Epoch: [44][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0890 (2.0330)	Acc@1 66.797 (68.631)	Acc@5 91.797 (92.993)
Epoch: [44][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1772 (2.0364)	Acc@1 67.578 (68.600)	Acc@5 89.844 (92.911)
Epoch: [44][150/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1171 (2.0447)	Acc@1 65.234 (68.432)	Acc@5 91.406 (92.772)
Epoch: [44][160/196]	Time 0.013 (0.016)	Data 0.007 (0.004)	Loss 2.0868 (2.0480)	Acc@1 70.312 (68.403)	Acc@5 92.188 (92.721)
Epoch: [44][170/196]	Time 0.015 (0.016)	Data 0.000 (0.004)	Loss 1.9860 (2.0522)	Acc@1 72.266 (68.321)	Acc@5 93.359 (92.665)
Epoch: [44][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1064 (2.0577)	Acc@1 67.969 (68.258)	Acc@5 91.016 (92.572)
Epoch: [44][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.2057 (2.0631)	Acc@1 60.938 (68.112)	Acc@5 94.141 (92.498)
num momentum params: 26
[0.1, 2.0659695307922363, 2.1736635994911193, 68.034, 47.15, tensor(0.4701, device='cuda:0', grad_fn=<DivBackward0>), 3.0694520473480225, 0.38965749740600586]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [45 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [45][0/196]	Time 0.043 (0.043)	Data 0.179 (0.179)	Loss 1.9660 (1.9660)	Acc@1 72.266 (72.266)	Acc@5 92.188 (92.188)
Epoch: [45][10/196]	Time 0.017 (0.020)	Data 0.001 (0.018)	Loss 2.0092 (2.0841)	Acc@1 70.312 (68.004)	Acc@5 92.578 (91.974)
Epoch: [45][20/196]	Time 0.015 (0.018)	Data 0.002 (0.010)	Loss 1.9559 (2.0559)	Acc@1 74.609 (69.103)	Acc@5 93.750 (92.541)
Epoch: [45][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9946 (2.0376)	Acc@1 71.875 (69.292)	Acc@5 94.141 (93.032)
Epoch: [45][40/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9815 (2.0291)	Acc@1 73.828 (69.550)	Acc@5 95.703 (93.407)
Epoch: [45][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0980 (2.0230)	Acc@1 66.406 (69.761)	Acc@5 91.016 (93.413)
Epoch: [45][60/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9705 (2.0222)	Acc@1 67.578 (69.621)	Acc@5 94.531 (93.436)
Epoch: [45][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0457 (2.0282)	Acc@1 67.188 (69.366)	Acc@5 92.578 (93.288)
Epoch: [45][80/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0763 (2.0341)	Acc@1 69.922 (69.247)	Acc@5 90.625 (93.138)
Epoch: [45][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.8930 (2.0389)	Acc@1 71.094 (68.935)	Acc@5 94.531 (93.033)
Epoch: [45][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0675 (2.0422)	Acc@1 70.312 (68.831)	Acc@5 91.797 (92.930)
Epoch: [45][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1281 (2.0448)	Acc@1 65.625 (68.743)	Acc@5 91.797 (92.874)
Epoch: [45][120/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1152 (2.0494)	Acc@1 64.844 (68.618)	Acc@5 94.141 (92.765)
Epoch: [45][130/196]	Time 0.025 (0.016)	Data 0.001 (0.004)	Loss 2.2824 (2.0575)	Acc@1 66.406 (68.362)	Acc@5 89.453 (92.656)
Epoch: [45][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0504 (2.0661)	Acc@1 69.922 (68.116)	Acc@5 92.969 (92.553)
Epoch: [45][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0558 (2.0654)	Acc@1 68.750 (68.111)	Acc@5 94.141 (92.586)
Epoch: [45][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0539 (2.0654)	Acc@1 67.969 (68.090)	Acc@5 93.359 (92.544)
Epoch: [45][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1390 (2.0663)	Acc@1 62.500 (68.062)	Acc@5 91.016 (92.530)
Epoch: [45][180/196]	Time 0.011 (0.016)	Data 0.017 (0.004)	Loss 2.3163 (2.0695)	Acc@1 62.109 (67.984)	Acc@5 89.453 (92.475)
Epoch: [45][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1373 (2.0769)	Acc@1 66.797 (67.779)	Acc@5 90.234 (92.396)
num momentum params: 26
[0.1, 2.077416142120361, 1.8112518107891082, 67.776, 53.72, tensor(0.4697, device='cuda:0', grad_fn=<DivBackward0>), 3.07130765914917, 0.3920323848724365]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [46 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [46][0/196]	Time 0.046 (0.046)	Data 0.164 (0.164)	Loss 2.0439 (2.0439)	Acc@1 68.359 (68.359)	Acc@5 93.750 (93.750)
Epoch: [46][10/196]	Time 0.015 (0.018)	Data 0.002 (0.017)	Loss 1.9500 (2.0081)	Acc@1 71.484 (70.064)	Acc@5 93.359 (93.324)
Epoch: [46][20/196]	Time 0.015 (0.017)	Data 0.002 (0.010)	Loss 2.0028 (1.9771)	Acc@1 71.484 (70.945)	Acc@5 94.531 (93.731)
Epoch: [46][30/196]	Time 0.012 (0.016)	Data 0.006 (0.008)	Loss 2.0125 (1.9773)	Acc@1 71.094 (71.119)	Acc@5 92.969 (93.485)
Epoch: [46][40/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.8426 (1.9746)	Acc@1 73.828 (70.998)	Acc@5 95.703 (93.540)
Epoch: [46][50/196]	Time 0.012 (0.016)	Data 0.017 (0.006)	Loss 2.0906 (1.9782)	Acc@1 65.625 (70.841)	Acc@5 92.188 (93.520)
Epoch: [46][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0203 (1.9740)	Acc@1 69.531 (70.921)	Acc@5 93.750 (93.526)
Epoch: [46][70/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.0073 (1.9801)	Acc@1 69.531 (70.588)	Acc@5 92.969 (93.458)
Epoch: [46][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0704 (1.9837)	Acc@1 67.969 (70.399)	Acc@5 93.750 (93.446)
Epoch: [46][90/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.2395 (1.9970)	Acc@1 64.844 (70.076)	Acc@5 90.234 (93.282)
Epoch: [46][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1972 (2.0077)	Acc@1 64.453 (69.732)	Acc@5 91.406 (93.093)
Epoch: [46][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9809 (2.0134)	Acc@1 69.531 (69.496)	Acc@5 92.969 (93.064)
Epoch: [46][120/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1904 (2.0236)	Acc@1 64.453 (69.260)	Acc@5 91.797 (92.959)
Epoch: [46][130/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2773 (2.0311)	Acc@1 62.109 (69.081)	Acc@5 91.016 (92.888)
Epoch: [46][140/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2141 (2.0370)	Acc@1 60.938 (68.961)	Acc@5 87.891 (92.794)
Epoch: [46][150/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.2185 (2.0422)	Acc@1 66.406 (68.887)	Acc@5 91.016 (92.733)
Epoch: [46][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0971 (2.0485)	Acc@1 67.188 (68.767)	Acc@5 91.797 (92.622)
Epoch: [46][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0042 (2.0529)	Acc@1 69.922 (68.647)	Acc@5 94.141 (92.590)
Epoch: [46][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1494 (2.0602)	Acc@1 67.188 (68.485)	Acc@5 90.625 (92.500)
Epoch: [46][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0551 (2.0633)	Acc@1 66.406 (68.355)	Acc@5 92.578 (92.462)
num momentum params: 26
[0.1, 2.065952716445923, 1.802892906665802, 68.274, 53.21, tensor(0.4741, device='cuda:0', grad_fn=<DivBackward0>), 3.0122745037078857, 0.3994638919830322]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [47 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [47][0/196]	Time 0.047 (0.047)	Data 0.173 (0.173)	Loss 2.0049 (2.0049)	Acc@1 66.797 (66.797)	Acc@5 93.359 (93.359)
Epoch: [47][10/196]	Time 0.017 (0.018)	Data 0.001 (0.018)	Loss 2.0708 (2.0399)	Acc@1 67.188 (69.141)	Acc@5 91.797 (93.182)
Epoch: [47][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 2.0237 (2.0304)	Acc@1 71.094 (69.327)	Acc@5 93.359 (92.950)
Epoch: [47][30/196]	Time 0.014 (0.016)	Data 0.004 (0.008)	Loss 1.9840 (2.0190)	Acc@1 69.922 (69.519)	Acc@5 93.359 (92.994)
Epoch: [47][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 1.9771 (2.0072)	Acc@1 68.750 (69.731)	Acc@5 94.531 (93.150)
Epoch: [47][50/196]	Time 0.015 (0.016)	Data 0.004 (0.006)	Loss 2.0064 (2.0089)	Acc@1 67.969 (69.830)	Acc@5 93.750 (93.160)
Epoch: [47][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9060 (2.0057)	Acc@1 72.656 (70.018)	Acc@5 95.703 (93.225)
Epoch: [47][70/196]	Time 0.013 (0.015)	Data 0.016 (0.005)	Loss 2.1826 (2.0242)	Acc@1 64.844 (69.465)	Acc@5 91.406 (92.963)
Epoch: [47][80/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0079 (2.0266)	Acc@1 71.875 (69.406)	Acc@5 91.406 (92.882)
Epoch: [47][90/196]	Time 0.011 (0.015)	Data 0.017 (0.005)	Loss 2.1920 (2.0305)	Acc@1 63.672 (69.342)	Acc@5 90.625 (92.831)
Epoch: [47][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2204 (2.0371)	Acc@1 62.109 (69.079)	Acc@5 90.625 (92.764)
Epoch: [47][110/196]	Time 0.012 (0.015)	Data 0.016 (0.005)	Loss 2.1183 (2.0393)	Acc@1 67.578 (69.032)	Acc@5 94.141 (92.789)
Epoch: [47][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0856 (2.0404)	Acc@1 67.578 (68.970)	Acc@5 94.922 (92.782)
Epoch: [47][130/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.2051 (2.0489)	Acc@1 64.062 (68.738)	Acc@5 92.188 (92.650)
Epoch: [47][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0173 (2.0484)	Acc@1 71.484 (68.772)	Acc@5 92.578 (92.636)
Epoch: [47][150/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0494 (2.0514)	Acc@1 67.578 (68.734)	Acc@5 92.578 (92.594)
Epoch: [47][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1065 (2.0509)	Acc@1 67.969 (68.721)	Acc@5 90.234 (92.556)
Epoch: [47][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1683 (2.0543)	Acc@1 66.016 (68.624)	Acc@5 92.188 (92.519)
Epoch: [47][180/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.0952 (2.0552)	Acc@1 67.578 (68.612)	Acc@5 92.969 (92.505)
Epoch: [47][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0828 (2.0564)	Acc@1 69.531 (68.584)	Acc@5 92.188 (92.509)
num momentum params: 26
[0.1, 2.0580979981994627, 2.0344324779510496, 68.532, 50.11, tensor(0.4770, device='cuda:0', grad_fn=<DivBackward0>), 3.0072431564331055, 0.3978970050811767]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [48 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [48][0/196]	Time 0.047 (0.047)	Data 0.172 (0.172)	Loss 2.0582 (2.0582)	Acc@1 65.625 (65.625)	Acc@5 96.875 (96.875)
Epoch: [48][10/196]	Time 0.013 (0.018)	Data 0.002 (0.018)	Loss 2.0649 (2.0356)	Acc@1 69.531 (69.531)	Acc@5 89.844 (92.649)
Epoch: [48][20/196]	Time 0.011 (0.017)	Data 0.006 (0.011)	Loss 2.0252 (2.0463)	Acc@1 71.094 (69.494)	Acc@5 92.969 (92.429)
Epoch: [48][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.0460 (2.0283)	Acc@1 70.703 (69.997)	Acc@5 91.406 (92.755)
Epoch: [48][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.0753 (2.0210)	Acc@1 67.578 (70.065)	Acc@5 89.844 (92.664)
Epoch: [48][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0463 (2.0351)	Acc@1 69.141 (69.508)	Acc@5 92.578 (92.570)
Epoch: [48][60/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 2.0699 (2.0397)	Acc@1 66.797 (69.307)	Acc@5 91.016 (92.629)
Epoch: [48][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0628 (2.0345)	Acc@1 67.578 (69.377)	Acc@5 92.188 (92.776)
Epoch: [48][80/196]	Time 0.011 (0.016)	Data 0.005 (0.005)	Loss 2.0573 (2.0393)	Acc@1 70.312 (69.131)	Acc@5 91.016 (92.699)
Epoch: [48][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1000 (2.0426)	Acc@1 67.969 (69.020)	Acc@5 90.625 (92.668)
Epoch: [48][100/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0999 (2.0504)	Acc@1 67.188 (68.785)	Acc@5 91.797 (92.590)
Epoch: [48][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1346 (2.0533)	Acc@1 64.844 (68.683)	Acc@5 92.578 (92.589)
Epoch: [48][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.0689 (2.0544)	Acc@1 67.188 (68.656)	Acc@5 93.359 (92.565)
Epoch: [48][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1552 (2.0583)	Acc@1 66.016 (68.544)	Acc@5 92.188 (92.524)
Epoch: [48][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1012 (2.0587)	Acc@1 69.531 (68.551)	Acc@5 91.016 (92.506)
Epoch: [48][150/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.2012 (2.0630)	Acc@1 63.672 (68.455)	Acc@5 91.016 (92.464)
Epoch: [48][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1133 (2.0674)	Acc@1 66.406 (68.325)	Acc@5 93.750 (92.420)
Epoch: [48][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1548 (2.0696)	Acc@1 64.844 (68.302)	Acc@5 92.578 (92.361)
Epoch: [48][180/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.0992 (2.0720)	Acc@1 66.406 (68.280)	Acc@5 92.969 (92.336)
Epoch: [48][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2431 (2.0761)	Acc@1 63.281 (68.134)	Acc@5 90.625 (92.300)
num momentum params: 26
[0.1, 2.076939588775635, 1.7428867995738984, 68.108, 55.02, tensor(0.4736, device='cuda:0', grad_fn=<DivBackward0>), 2.9999747276306152, 0.3928148746490478]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [49 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [49][0/196]	Time 0.044 (0.044)	Data 0.168 (0.168)	Loss 1.9317 (1.9317)	Acc@1 70.312 (70.312)	Acc@5 93.359 (93.359)
Epoch: [49][10/196]	Time 0.016 (0.019)	Data 0.002 (0.018)	Loss 1.9467 (1.9897)	Acc@1 72.266 (70.384)	Acc@5 93.359 (93.466)
Epoch: [49][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 2.0253 (1.9847)	Acc@1 68.359 (70.573)	Acc@5 92.188 (93.266)
Epoch: [49][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.8845 (1.9797)	Acc@1 73.438 (70.552)	Acc@5 93.359 (93.372)
Epoch: [49][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 1.9879 (1.9689)	Acc@1 69.531 (70.856)	Acc@5 92.969 (93.464)
Epoch: [49][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0838 (1.9643)	Acc@1 69.922 (70.810)	Acc@5 93.359 (93.536)
Epoch: [49][60/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9985 (1.9675)	Acc@1 69.141 (70.767)	Acc@5 94.141 (93.584)
Epoch: [49][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.9738 (1.9732)	Acc@1 71.875 (70.555)	Acc@5 92.969 (93.513)
Epoch: [49][80/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.0479 (1.9894)	Acc@1 69.531 (70.129)	Acc@5 91.016 (93.282)
Epoch: [49][90/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0238 (1.9955)	Acc@1 71.484 (70.025)	Acc@5 92.969 (93.209)
Epoch: [49][100/196]	Time 0.012 (0.016)	Data 0.020 (0.004)	Loss 1.9831 (1.9985)	Acc@1 67.969 (69.937)	Acc@5 92.969 (93.209)
Epoch: [49][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9181 (2.0072)	Acc@1 73.047 (69.676)	Acc@5 95.312 (93.155)
Epoch: [49][120/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 1.9639 (2.0134)	Acc@1 70.312 (69.463)	Acc@5 94.531 (93.104)
Epoch: [49][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2565 (2.0197)	Acc@1 66.016 (69.334)	Acc@5 89.062 (93.049)
Epoch: [49][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1483 (2.0251)	Acc@1 65.625 (69.240)	Acc@5 92.188 (92.966)
Epoch: [49][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0061 (2.0339)	Acc@1 69.531 (69.016)	Acc@5 94.141 (92.899)
Epoch: [49][160/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0673 (2.0391)	Acc@1 71.484 (68.934)	Acc@5 92.188 (92.838)
Epoch: [49][170/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.1946 (2.0450)	Acc@1 67.188 (68.798)	Acc@5 88.281 (92.775)
Epoch: [49][180/196]	Time 0.013 (0.015)	Data 0.004 (0.003)	Loss 2.2237 (2.0483)	Acc@1 61.719 (68.744)	Acc@5 92.969 (92.744)
Epoch: [49][190/196]	Time 0.014 (0.015)	Data 0.003 (0.003)	Loss 2.3807 (2.0514)	Acc@1 62.109 (68.693)	Acc@5 89.844 (92.725)
num momentum params: 26
[0.1, 2.0528135410308836, 1.85166921377182, 68.68, 52.28, tensor(0.4801, device='cuda:0', grad_fn=<DivBackward0>), 3.0364930629730225, 0.40244293212890625]
Non Pruning Epoch - module.conv1.weight: [58, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [58]
Non Pruning Epoch - module.bn1.bias: [58]
Non Pruning Epoch - module.conv2.weight: [128, 58, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [510]
Non Pruning Epoch - module.bn7.bias: [510]
Non Pruning Epoch - module.conv8.weight: [459, 510, 3, 3]
Non Pruning Epoch - module.bn8.weight: [459]
Non Pruning Epoch - module.bn8.bias: [459]
Non Pruning Epoch - module.fc.weight: [100, 459]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [50 | 180] LR: 0.100000
module.conv1.weight [58, 3, 3, 3]
module.conv2.weight [128, 58, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [510, 512, 3, 3]
module.conv8.weight [459, 510, 3, 3]
Epoch: [50][0/196]	Time 0.045 (0.045)	Data 0.171 (0.171)	Loss 2.0228 (2.0228)	Acc@1 71.484 (71.484)	Acc@5 91.797 (91.797)
Epoch: [50][10/196]	Time 0.014 (0.018)	Data 0.004 (0.017)	Loss 1.9352 (2.0356)	Acc@1 72.656 (69.531)	Acc@5 92.188 (92.720)
Epoch: [50][20/196]	Time 0.017 (0.017)	Data 0.003 (0.010)	Loss 1.8479 (2.0028)	Acc@1 76.953 (70.257)	Acc@5 93.750 (93.322)
Epoch: [50][30/196]	Time 0.011 (0.016)	Data 0.007 (0.008)	Loss 1.9695 (1.9806)	Acc@1 71.875 (71.043)	Acc@5 92.188 (93.422)
Epoch: [50][40/196]	Time 0.019 (0.016)	Data 0.001 (0.007)	Loss 1.9843 (1.9696)	Acc@1 69.141 (71.218)	Acc@5 93.750 (93.521)
Epoch: [50][50/196]	Time 0.013 (0.016)	Data 0.007 (0.006)	Loss 1.9617 (1.9659)	Acc@1 72.266 (71.163)	Acc@5 93.750 (93.811)
Epoch: [50][60/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0700 (1.9712)	Acc@1 67.969 (71.030)	Acc@5 92.969 (93.878)
Epoch: [50][70/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.2490 (1.9779)	Acc@1 62.891 (70.901)	Acc@5 89.062 (93.656)
Epoch: [50][80/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.0973 (1.9866)	Acc@1 67.969 (70.669)	Acc@5 93.359 (93.552)
Epoch: [50][90/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1293 (1.9897)	Acc@1 65.625 (70.531)	Acc@5 92.188 (93.535)
Epoch: [50][100/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2799 (2.0032)	Acc@1 66.016 (70.216)	Acc@5 89.453 (93.352)
Epoch: [50][110/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9870 (2.0053)	Acc@1 71.484 (70.235)	Acc@5 93.750 (93.293)
Epoch: [50][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9952 (2.0114)	Acc@1 69.531 (70.038)	Acc@5 94.141 (93.214)
Epoch: [50][130/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.1644 (2.0207)	Acc@1 66.406 (69.829)	Acc@5 90.625 (93.133)
Epoch: [50][140/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1992 (2.0347)	Acc@1 67.188 (69.407)	Acc@5 88.281 (92.922)
Epoch: [50][150/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0967 (2.0411)	Acc@1 66.797 (69.231)	Acc@5 92.578 (92.832)
Epoch: [50][160/196]	Time 0.020 (0.016)	Data 0.001 (0.004)	Loss 2.1484 (2.0445)	Acc@1 69.531 (69.099)	Acc@5 90.234 (92.816)
Epoch: [50][170/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 1.9208 (2.0467)	Acc@1 71.094 (69.031)	Acc@5 94.531 (92.768)
Epoch: [50][180/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 2.0708 (2.0508)	Acc@1 69.531 (68.927)	Acc@5 93.750 (92.721)
Epoch: [50][190/196]	Time 0.012 (0.016)	Data 0.005 (0.003)	Loss 2.2933 (2.0535)	Acc@1 62.109 (68.803)	Acc@5 90.234 (92.701)
num momentum params: 26
[0.1, 2.0548428755187986, 1.9236974680423737, 68.756, 52.76, tensor(0.4809, device='cuda:0', grad_fn=<DivBackward0>), 3.1027510166168213, 0.4007546901702881]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [58, 3, 3, 3]
Before - module.bn1.weight: [58]
Before - module.bn1.bias: [58]
Before - module.conv2.weight: [128, 58, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [510, 512, 3, 3]
Before - module.bn7.weight: [510]
Before - module.bn7.bias: [510]
Before - module.conv8.weight: [459, 510, 3, 3]
Before - module.bn8.weight: [459]
Before - module.bn8.bias: [459]
Before - module.fc.weight: [100, 459]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [58, 3, 3, 3] >> [56, 3, 3, 3]
[module.bn1.weight]: 58 >> 56
running_mean [56]
running_var [56]
num_batches_tracked []
[module.conv2.weight]: [128, 58, 3, 3] >> [128, 56, 3, 3]
[module.conv6.weight]: [512, 512, 3, 3] >> [511, 512, 3, 3]
[module.bn6.weight]: 512 >> 511
running_mean [511]
running_var [511]
num_batches_tracked []
[module.conv7.weight]: [510, 512, 3, 3] >> [506, 511, 3, 3]
[module.bn7.weight]: 510 >> 506
running_mean [506]
running_var [506]
num_batches_tracked []
[module.conv8.weight]: [459, 510, 3, 3] >> [419, 506, 3, 3]
[module.bn8.weight]: 459 >> 419
running_mean [419]
running_var [419]
num_batches_tracked []
[module.fc.weight]: [100, 459] >> [100, 419]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [56, 3, 3, 3]
After - module.bn1.weight: [56]
After - module.bn1.bias: [56]
After - module.conv2.weight: [128, 56, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [511, 512, 3, 3]
After - module.bn6.weight: [511]
After - module.bn6.bias: [511]
After - module.conv7.weight: [506, 511, 3, 3]
After - module.bn7.weight: [506]
After - module.bn7.bias: [506]
After - module.conv8.weight: [419, 506, 3, 3]
After - module.bn8.weight: [419]
After - module.bn8.bias: [419]
After - module.fc.weight: [100, 419]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [56, 3, 3, 3]
conv2 --> [128, 56, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [511, 512, 3, 3]
conv7 --> [506, 511, 3, 3]
conv8 --> [419, 506, 3, 3]
fc --> [419, 100]
1, 620089344, 1548288, 56
2, 6903300096, 16515072, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20495204352, 37675008, 511
7, 7148832768, 9308376, 506
8, 5861763072, 7632504, 419
fc, 16089600, 41900, 0
===================
FLOP REPORT: 30130105800000.0 57179200000.0 148218620 142948 2644 16.718442916870117
[INFO] Storing checkpoint...

Epoch: [51 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [51][0/196]	Time 0.559 (0.559)	Data 0.186 (0.186)	Loss 2.0222 (2.0222)	Acc@1 69.141 (69.141)	Acc@5 92.969 (92.969)
Epoch: [51][10/196]	Time 0.018 (0.066)	Data 0.002 (0.018)	Loss 2.0597 (2.0310)	Acc@1 70.312 (69.283)	Acc@5 92.188 (93.111)
Epoch: [51][20/196]	Time 0.015 (0.042)	Data 0.002 (0.011)	Loss 1.9213 (2.0034)	Acc@1 72.656 (70.071)	Acc@5 94.141 (93.452)
Epoch: [51][30/196]	Time 0.013 (0.033)	Data 0.004 (0.008)	Loss 1.7795 (1.9954)	Acc@1 75.781 (70.350)	Acc@5 94.922 (93.611)
Epoch: [51][40/196]	Time 0.016 (0.029)	Data 0.001 (0.007)	Loss 1.9723 (1.9873)	Acc@1 68.750 (70.408)	Acc@5 95.312 (93.721)
Epoch: [51][50/196]	Time 0.011 (0.026)	Data 0.015 (0.006)	Loss 1.8694 (1.9944)	Acc@1 72.656 (70.259)	Acc@5 94.922 (93.666)
Epoch: [51][60/196]	Time 0.023 (0.025)	Data 0.001 (0.006)	Loss 2.0998 (2.0039)	Acc@1 68.359 (70.031)	Acc@5 92.969 (93.680)
Epoch: [51][70/196]	Time 0.011 (0.023)	Data 0.017 (0.005)	Loss 2.0714 (2.0076)	Acc@1 67.188 (69.790)	Acc@5 93.359 (93.645)
Epoch: [51][80/196]	Time 0.017 (0.022)	Data 0.000 (0.005)	Loss 2.1368 (2.0109)	Acc@1 66.797 (69.734)	Acc@5 91.406 (93.605)
Epoch: [51][90/196]	Time 0.011 (0.022)	Data 0.009 (0.005)	Loss 1.9908 (2.0168)	Acc@1 69.531 (69.613)	Acc@5 93.359 (93.535)
Epoch: [51][100/196]	Time 0.016 (0.021)	Data 0.001 (0.005)	Loss 2.1928 (2.0220)	Acc@1 65.625 (69.504)	Acc@5 91.797 (93.468)
Epoch: [51][110/196]	Time 0.011 (0.020)	Data 0.006 (0.005)	Loss 2.1302 (2.0273)	Acc@1 66.016 (69.380)	Acc@5 91.797 (93.345)
Epoch: [51][120/196]	Time 0.017 (0.020)	Data 0.002 (0.004)	Loss 2.1813 (2.0346)	Acc@1 65.625 (69.241)	Acc@5 90.625 (93.233)
Epoch: [51][130/196]	Time 0.013 (0.020)	Data 0.004 (0.004)	Loss 2.1586 (2.0439)	Acc@1 70.312 (69.123)	Acc@5 91.016 (93.049)
Epoch: [51][140/196]	Time 0.012 (0.019)	Data 0.005 (0.004)	Loss 2.2374 (2.0504)	Acc@1 66.016 (68.963)	Acc@5 88.672 (92.944)
Epoch: [51][150/196]	Time 0.011 (0.019)	Data 0.007 (0.004)	Loss 2.0089 (2.0524)	Acc@1 71.094 (68.874)	Acc@5 92.578 (92.896)
Epoch: [51][160/196]	Time 0.014 (0.019)	Data 0.004 (0.004)	Loss 2.1992 (2.0561)	Acc@1 65.625 (68.796)	Acc@5 92.188 (92.821)
Epoch: [51][170/196]	Time 0.016 (0.019)	Data 0.007 (0.004)	Loss 2.2786 (2.0637)	Acc@1 60.547 (68.570)	Acc@5 90.234 (92.729)
Epoch: [51][180/196]	Time 0.011 (0.018)	Data 0.017 (0.004)	Loss 2.1688 (2.0679)	Acc@1 66.797 (68.526)	Acc@5 90.234 (92.677)
Epoch: [51][190/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 2.1103 (2.0727)	Acc@1 66.406 (68.441)	Acc@5 91.797 (92.592)
num momentum params: 26
[0.1, 2.0743032092285154, 1.811810177564621, 68.432, 53.68, tensor(0.4782, device='cuda:0', grad_fn=<DivBackward0>), 3.76928448677063, 0.48137521743774414]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [52 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [52][0/196]	Time 0.042 (0.042)	Data 0.181 (0.181)	Loss 2.0861 (2.0861)	Acc@1 64.844 (64.844)	Acc@5 93.750 (93.750)
Epoch: [52][10/196]	Time 0.018 (0.019)	Data 0.001 (0.018)	Loss 2.0150 (2.0089)	Acc@1 70.312 (69.957)	Acc@5 92.188 (93.643)
Epoch: [52][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.9537 (1.9942)	Acc@1 71.484 (70.387)	Acc@5 92.969 (93.620)
Epoch: [52][30/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 1.9343 (1.9870)	Acc@1 71.875 (70.628)	Acc@5 94.531 (93.788)
Epoch: [52][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 2.0406 (1.9793)	Acc@1 71.484 (70.760)	Acc@5 92.969 (93.760)
Epoch: [52][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0681 (1.9758)	Acc@1 66.406 (70.872)	Acc@5 92.969 (93.926)
Epoch: [52][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9828 (1.9840)	Acc@1 69.922 (70.748)	Acc@5 95.703 (93.808)
Epoch: [52][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9613 (1.9866)	Acc@1 69.922 (70.599)	Acc@5 92.578 (93.700)
Epoch: [52][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1520 (1.9965)	Acc@1 65.234 (70.337)	Acc@5 90.625 (93.601)
Epoch: [52][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0168 (2.0070)	Acc@1 70.312 (70.012)	Acc@5 91.406 (93.458)
Epoch: [52][100/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0124 (2.0115)	Acc@1 69.531 (69.829)	Acc@5 94.531 (93.402)
Epoch: [52][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1669 (2.0154)	Acc@1 71.094 (69.764)	Acc@5 91.016 (93.331)
Epoch: [52][120/196]	Time 0.014 (0.016)	Data 0.007 (0.004)	Loss 2.2400 (2.0225)	Acc@1 64.062 (69.554)	Acc@5 90.234 (93.246)
Epoch: [52][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0162 (2.0269)	Acc@1 71.094 (69.498)	Acc@5 93.750 (93.201)
Epoch: [52][140/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.1552 (2.0314)	Acc@1 64.062 (69.404)	Acc@5 93.359 (93.135)
Epoch: [52][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1563 (2.0364)	Acc@1 65.234 (69.267)	Acc@5 91.797 (93.072)
Epoch: [52][160/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.1362 (2.0364)	Acc@1 66.016 (69.267)	Acc@5 90.625 (93.020)
Epoch: [52][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0851 (2.0394)	Acc@1 67.188 (69.184)	Acc@5 92.188 (92.998)
Epoch: [52][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1685 (2.0434)	Acc@1 67.188 (69.104)	Acc@5 92.188 (92.932)
Epoch: [52][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9114 (2.0484)	Acc@1 74.219 (69.002)	Acc@5 94.141 (92.848)
num momentum params: 26
[0.1, 2.051160822906494, 1.9994884192943574, 68.928, 50.91, tensor(0.4836, device='cuda:0', grad_fn=<DivBackward0>), 2.9927890300750732, 0.39682722091674805]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [53 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [53][0/196]	Time 0.041 (0.041)	Data 0.182 (0.182)	Loss 1.8579 (1.8579)	Acc@1 74.219 (74.219)	Acc@5 95.312 (95.312)
Epoch: [53][10/196]	Time 0.015 (0.018)	Data 0.003 (0.019)	Loss 2.1274 (2.0092)	Acc@1 67.578 (70.348)	Acc@5 91.406 (93.714)
Epoch: [53][20/196]	Time 0.016 (0.017)	Data 0.003 (0.011)	Loss 1.9156 (1.9681)	Acc@1 71.094 (71.038)	Acc@5 95.312 (94.103)
Epoch: [53][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9078 (1.9547)	Acc@1 71.094 (71.497)	Acc@5 96.484 (94.292)
Epoch: [53][40/196]	Time 0.019 (0.016)	Data 0.008 (0.007)	Loss 2.0328 (1.9617)	Acc@1 69.922 (71.665)	Acc@5 92.188 (94.074)
Epoch: [53][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9013 (1.9576)	Acc@1 71.094 (71.576)	Acc@5 95.312 (94.240)
Epoch: [53][60/196]	Time 0.011 (0.016)	Data 0.008 (0.006)	Loss 2.1203 (1.9620)	Acc@1 66.016 (71.363)	Acc@5 90.625 (94.211)
Epoch: [53][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0592 (1.9756)	Acc@1 66.797 (70.901)	Acc@5 93.750 (94.058)
Epoch: [53][80/196]	Time 0.011 (0.016)	Data 0.012 (0.005)	Loss 2.0322 (1.9850)	Acc@1 72.656 (70.795)	Acc@5 92.188 (93.875)
Epoch: [53][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0102 (1.9899)	Acc@1 69.922 (70.652)	Acc@5 93.359 (93.823)
Epoch: [53][100/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0402 (1.9983)	Acc@1 70.703 (70.398)	Acc@5 93.750 (93.723)
Epoch: [53][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0898 (2.0096)	Acc@1 69.141 (70.073)	Acc@5 89.062 (93.532)
Epoch: [53][120/196]	Time 0.013 (0.016)	Data 0.023 (0.004)	Loss 2.1573 (2.0147)	Acc@1 67.969 (69.909)	Acc@5 91.016 (93.421)
Epoch: [53][130/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.0559 (2.0189)	Acc@1 66.797 (69.871)	Acc@5 94.922 (93.344)
Epoch: [53][140/196]	Time 0.013 (0.016)	Data 0.006 (0.004)	Loss 2.0978 (2.0299)	Acc@1 66.016 (69.578)	Acc@5 93.750 (93.240)
Epoch: [53][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1792 (2.0389)	Acc@1 67.969 (69.361)	Acc@5 89.453 (93.093)
Epoch: [53][160/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.2560 (2.0463)	Acc@1 63.672 (69.085)	Acc@5 92.188 (92.978)
Epoch: [53][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2251 (2.0547)	Acc@1 67.969 (68.830)	Acc@5 90.625 (92.880)
Epoch: [53][180/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2259 (2.0616)	Acc@1 63.672 (68.601)	Acc@5 89.844 (92.766)
Epoch: [53][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.1236 (2.0665)	Acc@1 71.094 (68.517)	Acc@5 92.188 (92.703)
num momentum params: 26
[0.1, 2.068668553314209, 1.7995396196842193, 68.452, 54.55, tensor(0.4805, device='cuda:0', grad_fn=<DivBackward0>), 3.0765254497528076, 0.3903164863586426]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [54 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [54][0/196]	Time 0.041 (0.041)	Data 0.183 (0.183)	Loss 1.9709 (1.9709)	Acc@1 71.484 (71.484)	Acc@5 93.750 (93.750)
Epoch: [54][10/196]	Time 0.017 (0.019)	Data 0.002 (0.019)	Loss 2.0265 (1.9990)	Acc@1 69.531 (70.490)	Acc@5 92.578 (93.572)
Epoch: [54][20/196]	Time 0.014 (0.017)	Data 0.004 (0.011)	Loss 1.8881 (1.9629)	Acc@1 73.828 (71.168)	Acc@5 94.922 (93.936)
Epoch: [54][30/196]	Time 0.017 (0.017)	Data 0.002 (0.008)	Loss 2.0115 (1.9424)	Acc@1 67.578 (71.850)	Acc@5 94.531 (94.241)
Epoch: [54][40/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 2.0094 (1.9460)	Acc@1 69.141 (71.456)	Acc@5 93.750 (94.245)
Epoch: [54][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9350 (1.9534)	Acc@1 70.312 (71.270)	Acc@5 96.875 (94.248)
Epoch: [54][60/196]	Time 0.014 (0.017)	Data 0.003 (0.005)	Loss 2.0964 (1.9629)	Acc@1 67.578 (70.959)	Acc@5 90.625 (94.102)
Epoch: [54][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0728 (1.9710)	Acc@1 69.531 (70.846)	Acc@5 93.750 (94.036)
Epoch: [54][80/196]	Time 0.012 (0.016)	Data 0.004 (0.005)	Loss 2.0718 (1.9810)	Acc@1 69.531 (70.611)	Acc@5 94.141 (93.895)
Epoch: [54][90/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0316 (1.9857)	Acc@1 70.703 (70.497)	Acc@5 94.141 (93.836)
Epoch: [54][100/196]	Time 0.011 (0.016)	Data 0.020 (0.004)	Loss 2.1496 (1.9947)	Acc@1 65.625 (70.200)	Acc@5 92.578 (93.727)
Epoch: [54][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0696 (2.0031)	Acc@1 70.703 (69.982)	Acc@5 92.969 (93.690)
Epoch: [54][120/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.1622 (2.0140)	Acc@1 63.672 (69.706)	Acc@5 94.141 (93.608)
Epoch: [54][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9862 (2.0172)	Acc@1 70.312 (69.624)	Acc@5 94.531 (93.550)
Epoch: [54][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0189 (2.0277)	Acc@1 71.094 (69.409)	Acc@5 91.797 (93.431)
Epoch: [54][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1382 (2.0382)	Acc@1 64.453 (69.172)	Acc@5 94.531 (93.284)
Epoch: [54][160/196]	Time 0.015 (0.016)	Data 0.001 (0.004)	Loss 2.0131 (2.0395)	Acc@1 68.359 (69.150)	Acc@5 93.359 (93.287)
Epoch: [54][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1755 (2.0463)	Acc@1 66.797 (68.992)	Acc@5 89.844 (93.163)
Epoch: [54][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1114 (2.0501)	Acc@1 64.453 (68.925)	Acc@5 92.969 (93.116)
Epoch: [54][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1087 (2.0515)	Acc@1 67.969 (68.918)	Acc@5 94.531 (93.063)
num momentum params: 26
[0.1, 2.0537831760406493, 1.7773652350902558, 68.866, 54.9, tensor(0.4856, device='cuda:0', grad_fn=<DivBackward0>), 3.1063520908355713, 0.3917710781097412]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [55 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [55][0/196]	Time 0.038 (0.038)	Data 0.175 (0.175)	Loss 1.9892 (1.9892)	Acc@1 70.312 (70.312)	Acc@5 93.750 (93.750)
Epoch: [55][10/196]	Time 0.017 (0.017)	Data 0.001 (0.019)	Loss 1.9753 (1.9585)	Acc@1 69.141 (71.058)	Acc@5 92.578 (93.857)
Epoch: [55][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 2.0284 (1.9487)	Acc@1 67.969 (71.596)	Acc@5 94.531 (94.159)
Epoch: [55][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.8870 (1.9463)	Acc@1 74.609 (71.812)	Acc@5 94.141 (94.279)
Epoch: [55][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.0375 (1.9578)	Acc@1 70.312 (71.694)	Acc@5 93.359 (94.188)
Epoch: [55][50/196]	Time 0.018 (0.015)	Data 0.001 (0.006)	Loss 1.8870 (1.9614)	Acc@1 71.094 (71.385)	Acc@5 95.312 (94.187)
Epoch: [55][60/196]	Time 0.011 (0.015)	Data 0.020 (0.006)	Loss 2.1100 (1.9697)	Acc@1 66.406 (71.171)	Acc@5 91.406 (94.045)
Epoch: [55][70/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 1.9922 (1.9768)	Acc@1 73.047 (71.182)	Acc@5 95.312 (93.833)
Epoch: [55][80/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 2.0648 (1.9856)	Acc@1 62.500 (70.857)	Acc@5 94.922 (93.682)
Epoch: [55][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1601 (1.9989)	Acc@1 64.453 (70.403)	Acc@5 92.578 (93.505)
Epoch: [55][100/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.0401 (2.0071)	Acc@1 71.484 (70.239)	Acc@5 93.359 (93.433)
Epoch: [55][110/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 1.9324 (2.0114)	Acc@1 75.781 (70.200)	Acc@5 95.312 (93.352)
Epoch: [55][120/196]	Time 0.011 (0.015)	Data 0.016 (0.005)	Loss 1.9132 (2.0110)	Acc@1 73.828 (70.264)	Acc@5 94.141 (93.324)
Epoch: [55][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0655 (2.0163)	Acc@1 72.656 (70.148)	Acc@5 91.797 (93.285)
Epoch: [55][140/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.0034 (2.0190)	Acc@1 72.656 (70.052)	Acc@5 93.359 (93.287)
Epoch: [55][150/196]	Time 0.019 (0.015)	Data 0.001 (0.004)	Loss 2.0804 (2.0246)	Acc@1 66.406 (69.912)	Acc@5 92.578 (93.230)
Epoch: [55][160/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.0555 (2.0269)	Acc@1 67.578 (69.810)	Acc@5 94.531 (93.221)
Epoch: [55][170/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.1597 (2.0304)	Acc@1 64.062 (69.712)	Acc@5 91.406 (93.174)
Epoch: [55][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1232 (2.0344)	Acc@1 67.578 (69.609)	Acc@5 92.969 (93.120)
Epoch: [55][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2053 (2.0374)	Acc@1 66.797 (69.564)	Acc@5 89.453 (93.053)
num momentum params: 26
[0.1, 2.0400364237213133, 2.0359195220470427, 69.504, 50.37, tensor(0.4890, device='cuda:0', grad_fn=<DivBackward0>), 2.9760942459106445, 0.40210771560668945]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [56 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [56][0/196]	Time 0.037 (0.037)	Data 0.171 (0.171)	Loss 2.0657 (2.0657)	Acc@1 65.625 (65.625)	Acc@5 93.359 (93.359)
Epoch: [56][10/196]	Time 0.016 (0.017)	Data 0.003 (0.018)	Loss 1.9270 (1.9555)	Acc@1 73.828 (71.768)	Acc@5 93.750 (93.963)
Epoch: [56][20/196]	Time 0.014 (0.016)	Data 0.004 (0.011)	Loss 1.9475 (1.9649)	Acc@1 69.141 (71.856)	Acc@5 93.359 (93.806)
Epoch: [56][30/196]	Time 0.015 (0.016)	Data 0.003 (0.008)	Loss 2.0148 (1.9596)	Acc@1 72.266 (72.177)	Acc@5 92.578 (93.876)
Epoch: [56][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 1.8743 (1.9623)	Acc@1 71.875 (71.875)	Acc@5 94.922 (93.817)
Epoch: [56][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9489 (1.9667)	Acc@1 71.875 (71.507)	Acc@5 95.312 (93.788)
Epoch: [56][60/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.9336 (1.9658)	Acc@1 72.656 (71.465)	Acc@5 94.531 (93.782)
Epoch: [56][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0277 (1.9694)	Acc@1 68.359 (71.402)	Acc@5 93.359 (93.794)
Epoch: [56][80/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9965 (1.9787)	Acc@1 67.578 (71.142)	Acc@5 95.703 (93.740)
Epoch: [56][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1132 (1.9938)	Acc@1 66.797 (70.613)	Acc@5 88.281 (93.531)
Epoch: [56][100/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0844 (2.0018)	Acc@1 66.016 (70.343)	Acc@5 94.141 (93.499)
Epoch: [56][110/196]	Time 0.013 (0.016)	Data 0.002 (0.005)	Loss 2.0474 (2.0079)	Acc@1 71.484 (70.147)	Acc@5 90.625 (93.405)
Epoch: [56][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0714 (2.0127)	Acc@1 69.141 (69.986)	Acc@5 93.750 (93.350)
Epoch: [56][130/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1789 (2.0171)	Acc@1 66.797 (69.987)	Acc@5 92.969 (93.303)
Epoch: [56][140/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1438 (2.0235)	Acc@1 67.188 (69.783)	Acc@5 90.625 (93.221)
Epoch: [56][150/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0634 (2.0272)	Acc@1 70.703 (69.730)	Acc@5 92.188 (93.142)
Epoch: [56][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0533 (2.0323)	Acc@1 71.484 (69.665)	Acc@5 90.625 (93.039)
Epoch: [56][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1493 (2.0336)	Acc@1 62.891 (69.545)	Acc@5 92.578 (93.056)
Epoch: [56][180/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.2146 (2.0365)	Acc@1 65.234 (69.479)	Acc@5 91.406 (93.042)
Epoch: [56][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1861 (2.0427)	Acc@1 65.234 (69.310)	Acc@5 91.406 (92.963)
num momentum params: 26
[0.1, 2.0461267667388916, 2.116306710243225, 69.22, 49.79, tensor(0.4881, device='cuda:0', grad_fn=<DivBackward0>), 3.043193817138672, 0.4062156677246093]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [57 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [57][0/196]	Time 0.046 (0.046)	Data 0.179 (0.179)	Loss 1.9895 (1.9895)	Acc@1 70.312 (70.312)	Acc@5 94.531 (94.531)
Epoch: [57][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 2.1596 (2.0588)	Acc@1 66.016 (68.679)	Acc@5 92.969 (93.537)
Epoch: [57][20/196]	Time 0.019 (0.017)	Data 0.000 (0.012)	Loss 1.7644 (2.0231)	Acc@1 77.734 (69.810)	Acc@5 96.875 (93.787)
Epoch: [57][30/196]	Time 0.016 (0.016)	Data 0.001 (0.010)	Loss 1.9320 (2.0163)	Acc@1 73.438 (70.073)	Acc@5 92.969 (93.876)
Epoch: [57][40/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9814 (2.0063)	Acc@1 68.750 (70.427)	Acc@5 95.312 (93.921)
Epoch: [57][50/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 2.1011 (2.0078)	Acc@1 67.578 (70.274)	Acc@5 92.578 (93.934)
Epoch: [57][60/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 2.0266 (2.0159)	Acc@1 68.359 (69.986)	Acc@5 93.750 (93.846)
Epoch: [57][70/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.0339 (2.0216)	Acc@1 70.703 (69.944)	Acc@5 94.531 (93.711)
Epoch: [57][80/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 2.0146 (2.0221)	Acc@1 69.141 (69.912)	Acc@5 96.094 (93.721)
Epoch: [57][90/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9575 (2.0191)	Acc@1 75.391 (70.051)	Acc@5 95.312 (93.741)
Epoch: [57][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9502 (2.0229)	Acc@1 72.656 (69.972)	Acc@5 94.922 (93.665)
Epoch: [57][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0371 (2.0289)	Acc@1 69.531 (69.841)	Acc@5 93.750 (93.599)
Epoch: [57][120/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0942 (2.0308)	Acc@1 65.625 (69.790)	Acc@5 95.312 (93.621)
Epoch: [57][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.2083 (2.0340)	Acc@1 64.453 (69.692)	Acc@5 92.188 (93.604)
Epoch: [57][140/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0560 (2.0399)	Acc@1 68.359 (69.528)	Acc@5 91.797 (93.498)
Epoch: [57][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0736 (2.0413)	Acc@1 65.234 (69.495)	Acc@5 92.969 (93.465)
Epoch: [57][160/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9631 (2.0450)	Acc@1 71.875 (69.415)	Acc@5 94.922 (93.413)
Epoch: [57][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0839 (2.0498)	Acc@1 67.188 (69.294)	Acc@5 91.797 (93.375)
Epoch: [57][180/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2101 (2.0548)	Acc@1 66.797 (69.126)	Acc@5 87.500 (93.312)
Epoch: [57][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1846 (2.0585)	Acc@1 64.453 (69.098)	Acc@5 93.359 (93.233)
num momentum params: 26
[0.1, 2.0609068576049805, 1.9876381659507751, 69.038, 50.43, tensor(0.4868, device='cuda:0', grad_fn=<DivBackward0>), 3.041846752166748, 0.40171766281127924]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [58 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [58][0/196]	Time 0.048 (0.048)	Data 0.170 (0.170)	Loss 2.0239 (2.0239)	Acc@1 68.359 (68.359)	Acc@5 93.750 (93.750)
Epoch: [58][10/196]	Time 0.016 (0.019)	Data 0.002 (0.017)	Loss 1.8546 (2.0096)	Acc@1 75.781 (70.170)	Acc@5 96.094 (93.643)
Epoch: [58][20/196]	Time 0.015 (0.018)	Data 0.002 (0.010)	Loss 1.9872 (1.9852)	Acc@1 71.484 (70.964)	Acc@5 91.406 (93.620)
Epoch: [58][30/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.9703 (1.9867)	Acc@1 73.828 (71.132)	Acc@5 94.922 (93.536)
Epoch: [58][40/196]	Time 0.012 (0.016)	Data 0.002 (0.006)	Loss 1.9008 (1.9906)	Acc@1 73.438 (71.037)	Acc@5 95.703 (93.512)
Epoch: [58][50/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0093 (1.9929)	Acc@1 69.531 (70.856)	Acc@5 93.359 (93.528)
Epoch: [58][60/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.8506 (1.9916)	Acc@1 74.609 (71.023)	Acc@5 92.969 (93.532)
Epoch: [58][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9704 (1.9923)	Acc@1 70.703 (71.017)	Acc@5 94.922 (93.519)
Epoch: [58][80/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0843 (1.9905)	Acc@1 66.797 (71.007)	Acc@5 95.703 (93.601)
Epoch: [58][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2339 (1.9972)	Acc@1 65.625 (70.875)	Acc@5 89.844 (93.535)
Epoch: [58][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0550 (2.0051)	Acc@1 69.531 (70.831)	Acc@5 91.406 (93.414)
Epoch: [58][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9735 (2.0085)	Acc@1 69.531 (70.693)	Acc@5 93.359 (93.416)
Epoch: [58][120/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 1.9487 (2.0111)	Acc@1 75.391 (70.606)	Acc@5 92.969 (93.443)
Epoch: [58][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0459 (2.0134)	Acc@1 69.531 (70.557)	Acc@5 93.750 (93.404)
Epoch: [58][140/196]	Time 0.013 (0.016)	Data 0.006 (0.004)	Loss 2.0344 (2.0148)	Acc@1 67.578 (70.504)	Acc@5 95.312 (93.406)
Epoch: [58][150/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0107 (2.0184)	Acc@1 69.922 (70.382)	Acc@5 93.750 (93.383)
Epoch: [58][160/196]	Time 0.011 (0.016)	Data 0.006 (0.003)	Loss 1.9566 (2.0222)	Acc@1 70.703 (70.242)	Acc@5 93.750 (93.342)
Epoch: [58][170/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.1225 (2.0278)	Acc@1 67.969 (70.066)	Acc@5 92.578 (93.284)
Epoch: [58][180/196]	Time 0.013 (0.015)	Data 0.004 (0.003)	Loss 2.3220 (2.0357)	Acc@1 60.156 (69.788)	Acc@5 91.406 (93.200)
Epoch: [58][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2322 (2.0415)	Acc@1 66.406 (69.629)	Acc@5 91.016 (93.159)
num momentum params: 26
[0.1, 2.042686901321411, 1.8556011855602264, 69.594, 52.43, tensor(0.4915, device='cuda:0', grad_fn=<DivBackward0>), 3.0449423789978027, 0.3969576358795166]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [59 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [59][0/196]	Time 0.043 (0.043)	Data 0.168 (0.168)	Loss 1.8830 (1.8830)	Acc@1 71.094 (71.094)	Acc@5 96.875 (96.875)
Epoch: [59][10/196]	Time 0.014 (0.018)	Data 0.003 (0.018)	Loss 1.8357 (1.9691)	Acc@1 74.219 (71.520)	Acc@5 95.312 (93.999)
Epoch: [59][20/196]	Time 0.013 (0.017)	Data 0.005 (0.010)	Loss 1.9320 (1.9874)	Acc@1 72.656 (71.150)	Acc@5 94.141 (94.010)
Epoch: [59][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 1.8610 (1.9843)	Acc@1 74.219 (71.472)	Acc@5 95.703 (94.002)
Epoch: [59][40/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.9315 (1.9830)	Acc@1 73.828 (71.551)	Acc@5 96.484 (93.960)
Epoch: [59][50/196]	Time 0.012 (0.016)	Data 0.008 (0.007)	Loss 1.9857 (1.9781)	Acc@1 71.484 (71.569)	Acc@5 92.969 (94.003)
Epoch: [59][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9660 (1.9779)	Acc@1 69.141 (71.593)	Acc@5 94.922 (94.006)
Epoch: [59][70/196]	Time 0.011 (0.016)	Data 0.018 (0.006)	Loss 2.1495 (1.9884)	Acc@1 66.406 (71.325)	Acc@5 92.188 (93.954)
Epoch: [59][80/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9546 (1.9932)	Acc@1 71.094 (71.094)	Acc@5 93.359 (93.871)
Epoch: [59][90/196]	Time 0.012 (0.016)	Data 0.017 (0.006)	Loss 2.0448 (1.9966)	Acc@1 67.188 (70.952)	Acc@5 94.922 (93.797)
Epoch: [59][100/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 2.1281 (2.0016)	Acc@1 68.750 (70.761)	Acc@5 92.578 (93.754)
Epoch: [59][110/196]	Time 0.012 (0.016)	Data 0.017 (0.005)	Loss 2.0690 (2.0060)	Acc@1 65.625 (70.608)	Acc@5 92.969 (93.694)
Epoch: [59][120/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0124 (2.0073)	Acc@1 68.359 (70.548)	Acc@5 93.750 (93.702)
Epoch: [59][130/196]	Time 0.012 (0.016)	Data 0.020 (0.005)	Loss 2.1113 (2.0109)	Acc@1 66.016 (70.453)	Acc@5 93.750 (93.661)
Epoch: [59][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0932 (2.0196)	Acc@1 67.969 (70.229)	Acc@5 92.188 (93.559)
Epoch: [59][150/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 2.0595 (2.0231)	Acc@1 68.359 (70.075)	Acc@5 93.750 (93.515)
Epoch: [59][160/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9907 (2.0279)	Acc@1 73.047 (69.997)	Acc@5 91.797 (93.410)
Epoch: [59][170/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0128 (2.0305)	Acc@1 73.438 (69.956)	Acc@5 93.359 (93.355)
Epoch: [59][180/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0075 (2.0323)	Acc@1 71.875 (69.881)	Acc@5 91.406 (93.342)
Epoch: [59][190/196]	Time 0.012 (0.016)	Data 0.016 (0.005)	Loss 2.1917 (2.0356)	Acc@1 67.578 (69.832)	Acc@5 90.234 (93.300)
num momentum params: 26
[0.1, 2.037061957168579, 1.881975120306015, 69.84, 52.3, tensor(0.4928, device='cuda:0', grad_fn=<DivBackward0>), 3.077058792114258, 0.397899866104126]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [511, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [511]
Non Pruning Epoch - module.bn6.bias: [511]
Non Pruning Epoch - module.conv7.weight: [506, 511, 3, 3]
Non Pruning Epoch - module.bn7.weight: [506]
Non Pruning Epoch - module.bn7.bias: [506]
Non Pruning Epoch - module.conv8.weight: [419, 506, 3, 3]
Non Pruning Epoch - module.bn8.weight: [419]
Non Pruning Epoch - module.bn8.bias: [419]
Non Pruning Epoch - module.fc.weight: [100, 419]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [60 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [511, 512, 3, 3]
module.conv7.weight [506, 511, 3, 3]
module.conv8.weight [419, 506, 3, 3]
Epoch: [60][0/196]	Time 0.046 (0.046)	Data 0.176 (0.176)	Loss 1.9107 (1.9107)	Acc@1 72.656 (72.656)	Acc@5 94.922 (94.922)
Epoch: [60][10/196]	Time 0.012 (0.018)	Data 0.006 (0.018)	Loss 2.0656 (1.9614)	Acc@1 69.141 (71.768)	Acc@5 94.531 (93.928)
Epoch: [60][20/196]	Time 0.014 (0.016)	Data 0.004 (0.011)	Loss 1.8794 (1.9543)	Acc@1 73.828 (71.819)	Acc@5 95.312 (93.899)
Epoch: [60][30/196]	Time 0.011 (0.016)	Data 0.016 (0.008)	Loss 1.7735 (1.9548)	Acc@1 76.562 (71.900)	Acc@5 96.875 (93.876)
Epoch: [60][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9666 (1.9578)	Acc@1 71.094 (71.865)	Acc@5 94.141 (93.950)
Epoch: [60][50/196]	Time 0.013 (0.016)	Data 0.009 (0.006)	Loss 1.9669 (1.9588)	Acc@1 73.438 (71.906)	Acc@5 92.969 (93.903)
Epoch: [60][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0483 (1.9718)	Acc@1 69.531 (71.478)	Acc@5 92.969 (93.820)
Epoch: [60][70/196]	Time 0.011 (0.016)	Data 0.017 (0.006)	Loss 1.9214 (1.9801)	Acc@1 69.531 (71.088)	Acc@5 96.484 (93.844)
Epoch: [60][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9523 (1.9816)	Acc@1 76.172 (71.234)	Acc@5 92.578 (93.784)
Epoch: [60][90/196]	Time 0.011 (0.016)	Data 0.017 (0.005)	Loss 2.0202 (1.9822)	Acc@1 69.922 (71.188)	Acc@5 92.969 (93.741)
Epoch: [60][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0706 (1.9845)	Acc@1 68.359 (71.082)	Acc@5 92.578 (93.680)
Epoch: [60][110/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.0886 (1.9876)	Acc@1 69.922 (71.048)	Acc@5 89.062 (93.606)
Epoch: [60][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0319 (1.9943)	Acc@1 71.875 (70.835)	Acc@5 91.016 (93.498)
Epoch: [60][130/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0930 (2.0053)	Acc@1 67.969 (70.485)	Acc@5 92.969 (93.440)
Epoch: [60][140/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1444 (2.0105)	Acc@1 69.531 (70.340)	Acc@5 89.453 (93.382)
Epoch: [60][150/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0802 (2.0155)	Acc@1 69.531 (70.230)	Acc@5 92.188 (93.352)
Epoch: [60][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0274 (2.0210)	Acc@1 70.703 (70.075)	Acc@5 93.359 (93.267)
Epoch: [60][170/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0291 (2.0299)	Acc@1 71.094 (69.920)	Acc@5 91.016 (93.154)
Epoch: [60][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9862 (2.0364)	Acc@1 73.438 (69.741)	Acc@5 91.797 (93.059)
Epoch: [60][190/196]	Time 0.013 (0.015)	Data 0.002 (0.004)	Loss 1.9706 (2.0414)	Acc@1 69.531 (69.580)	Acc@5 94.531 (93.018)
num momentum params: 26
[0.1, 2.044214272918701, 1.8612179088592529, 69.492, 53.08, tensor(0.4916, device='cuda:0', grad_fn=<DivBackward0>), 3.005955219268799, 0.4135503768920899]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [56, 3, 3, 3]
Before - module.bn1.weight: [56]
Before - module.bn1.bias: [56]
Before - module.conv2.weight: [128, 56, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [511, 512, 3, 3]
Before - module.bn6.weight: [511]
Before - module.bn6.bias: [511]
Before - module.conv7.weight: [506, 511, 3, 3]
Before - module.bn7.weight: [506]
Before - module.bn7.bias: [506]
Before - module.conv8.weight: [419, 506, 3, 3]
Before - module.bn8.weight: [419]
Before - module.bn8.bias: [419]
Before - module.fc.weight: [100, 419]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [56, 3, 3, 3] >> [53, 3, 3, 3]
[module.bn1.weight]: 56 >> 53
running_mean [53]
running_var [53]
num_batches_tracked []
[module.conv2.weight]: [128, 56, 3, 3] >> [128, 53, 3, 3]
[module.conv6.weight]: [511, 512, 3, 3] >> [510, 512, 3, 3]
[module.bn6.weight]: 511 >> 510
running_mean [510]
running_var [510]
num_batches_tracked []
[module.conv7.weight]: [506, 511, 3, 3] >> [501, 510, 3, 3]
[module.bn7.weight]: 506 >> 501
running_mean [501]
running_var [501]
num_batches_tracked []
[module.conv8.weight]: [419, 506, 3, 3] >> [390, 501, 3, 3]
[module.bn8.weight]: 419 >> 390
running_mean [390]
running_var [390]
num_batches_tracked []
[module.fc.weight]: [100, 419] >> [100, 390]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [53, 3, 3, 3]
After - module.bn1.weight: [53]
After - module.bn1.bias: [53]
After - module.conv2.weight: [128, 53, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [510, 512, 3, 3]
After - module.bn6.weight: [510]
After - module.bn6.bias: [510]
After - module.conv7.weight: [501, 510, 3, 3]
After - module.bn7.weight: [501]
After - module.bn7.bias: [501]
After - module.conv8.weight: [390, 501, 3, 3]
After - module.bn8.weight: [390]
After - module.bn8.bias: [390]
After - module.fc.weight: [100, 390]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [53, 3, 3, 3]
conv2 --> [128, 53, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [510, 512, 3, 3]
conv7 --> [501, 510, 3, 3]
conv8 --> [390, 501, 3, 3]
fc --> [390, 100]
1, 586870272, 1465344, 53
2, 6533480448, 15630336, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20455096320, 37601280, 510
7, 7064340480, 9198360, 501
8, 5402142720, 7034040, 390
fc, 14976000, 39000, 0
===================
FLOP REPORT: 29744022600000.0 55889600000.0 146465832 139724 2606 16.359418869018555
[INFO] Storing checkpoint...

Epoch: [61 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [61][0/196]	Time 0.555 (0.555)	Data 0.190 (0.190)	Loss 1.9742 (1.9742)	Acc@1 73.828 (73.828)	Acc@5 95.703 (95.703)
Epoch: [61][10/196]	Time 0.016 (0.065)	Data 0.002 (0.019)	Loss 1.8933 (1.9580)	Acc@1 72.266 (73.047)	Acc@5 94.531 (94.531)
Epoch: [61][20/196]	Time 0.015 (0.041)	Data 0.002 (0.011)	Loss 1.9261 (1.9460)	Acc@1 77.734 (73.196)	Acc@5 92.969 (94.382)
Epoch: [61][30/196]	Time 0.013 (0.033)	Data 0.004 (0.008)	Loss 1.8990 (1.9587)	Acc@1 73.828 (72.555)	Acc@5 97.266 (94.267)
Epoch: [61][40/196]	Time 0.014 (0.029)	Data 0.003 (0.007)	Loss 1.8625 (1.9597)	Acc@1 72.656 (72.247)	Acc@5 96.094 (94.379)
Epoch: [61][50/196]	Time 0.014 (0.026)	Data 0.003 (0.006)	Loss 1.9436 (1.9615)	Acc@1 73.438 (72.128)	Acc@5 93.359 (94.210)
Epoch: [61][60/196]	Time 0.017 (0.024)	Data 0.001 (0.005)	Loss 2.0065 (1.9716)	Acc@1 69.141 (71.728)	Acc@5 94.922 (94.147)
Epoch: [61][70/196]	Time 0.015 (0.023)	Data 0.002 (0.005)	Loss 1.9583 (1.9861)	Acc@1 72.266 (71.275)	Acc@5 94.141 (93.998)
Epoch: [61][80/196]	Time 0.017 (0.022)	Data 0.001 (0.005)	Loss 2.0559 (1.9933)	Acc@1 65.625 (70.988)	Acc@5 95.312 (93.986)
Epoch: [61][90/196]	Time 0.019 (0.021)	Data 0.002 (0.004)	Loss 1.8482 (1.9958)	Acc@1 75.000 (70.879)	Acc@5 96.484 (93.939)
Epoch: [61][100/196]	Time 0.015 (0.020)	Data 0.002 (0.004)	Loss 2.1990 (1.9987)	Acc@1 64.844 (70.804)	Acc@5 91.797 (93.959)
Epoch: [61][110/196]	Time 0.014 (0.020)	Data 0.003 (0.004)	Loss 1.8605 (2.0021)	Acc@1 76.172 (70.678)	Acc@5 94.531 (93.891)
Epoch: [61][120/196]	Time 0.012 (0.019)	Data 0.014 (0.004)	Loss 2.0094 (2.0074)	Acc@1 69.141 (70.532)	Acc@5 93.750 (93.782)
Epoch: [61][130/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 2.0842 (2.0150)	Acc@1 66.406 (70.360)	Acc@5 93.359 (93.649)
Epoch: [61][140/196]	Time 0.014 (0.019)	Data 0.003 (0.004)	Loss 2.0128 (2.0179)	Acc@1 70.703 (70.337)	Acc@5 92.188 (93.650)
Epoch: [61][150/196]	Time 0.016 (0.019)	Data 0.001 (0.004)	Loss 2.0815 (2.0204)	Acc@1 70.312 (70.235)	Acc@5 92.969 (93.603)
Epoch: [61][160/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 2.0708 (2.0255)	Acc@1 70.703 (70.104)	Acc@5 93.359 (93.536)
Epoch: [61][170/196]	Time 0.016 (0.018)	Data 0.000 (0.004)	Loss 2.0474 (2.0298)	Acc@1 67.969 (69.997)	Acc@5 94.141 (93.478)
Epoch: [61][180/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 2.1995 (2.0375)	Acc@1 67.188 (69.848)	Acc@5 91.406 (93.383)
Epoch: [61][190/196]	Time 0.018 (0.018)	Data 0.000 (0.004)	Loss 2.2681 (2.0426)	Acc@1 66.016 (69.695)	Acc@5 90.625 (93.288)
num momentum params: 26
[0.1, 2.0439897862243654, 1.7656032598018647, 69.676, 54.77, tensor(0.4928, device='cuda:0', grad_fn=<DivBackward0>), 3.6935322284698486, 0.48095345497131353]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [62 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [62][0/196]	Time 0.042 (0.042)	Data 0.174 (0.174)	Loss 1.9228 (1.9228)	Acc@1 73.047 (73.047)	Acc@5 95.703 (95.703)
Epoch: [62][10/196]	Time 0.018 (0.018)	Data 0.000 (0.018)	Loss 1.9536 (1.9631)	Acc@1 71.094 (71.875)	Acc@5 94.922 (94.176)
Epoch: [62][20/196]	Time 0.011 (0.017)	Data 0.008 (0.011)	Loss 1.9353 (1.9659)	Acc@1 73.047 (71.801)	Acc@5 94.531 (94.215)
Epoch: [62][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.9378 (1.9607)	Acc@1 74.219 (72.001)	Acc@5 92.969 (94.178)
Epoch: [62][40/196]	Time 0.011 (0.016)	Data 0.009 (0.007)	Loss 1.9317 (1.9606)	Acc@1 72.266 (71.865)	Acc@5 94.531 (94.093)
Epoch: [62][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9935 (1.9619)	Acc@1 68.750 (71.768)	Acc@5 93.750 (94.072)
Epoch: [62][60/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 1.9307 (1.9607)	Acc@1 70.312 (71.856)	Acc@5 93.359 (94.057)
Epoch: [62][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.9456 (1.9565)	Acc@1 73.438 (71.914)	Acc@5 94.531 (94.135)
Epoch: [62][80/196]	Time 0.011 (0.016)	Data 0.011 (0.005)	Loss 2.1173 (1.9666)	Acc@1 64.453 (71.595)	Acc@5 92.188 (94.054)
Epoch: [62][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1352 (1.9789)	Acc@1 67.969 (71.205)	Acc@5 90.234 (93.866)
Epoch: [62][100/196]	Time 0.012 (0.016)	Data 0.011 (0.005)	Loss 2.0332 (1.9870)	Acc@1 70.312 (71.071)	Acc@5 92.969 (93.781)
Epoch: [62][110/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1834 (1.9979)	Acc@1 64.453 (70.812)	Acc@5 91.406 (93.641)
Epoch: [62][120/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.0317 (2.0008)	Acc@1 69.922 (70.735)	Acc@5 93.359 (93.624)
Epoch: [62][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9906 (2.0063)	Acc@1 74.219 (70.629)	Acc@5 93.359 (93.616)
Epoch: [62][140/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 1.9799 (2.0062)	Acc@1 69.531 (70.598)	Acc@5 95.703 (93.672)
Epoch: [62][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0989 (2.0115)	Acc@1 67.578 (70.483)	Acc@5 91.797 (93.641)
Epoch: [62][160/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1903 (2.0179)	Acc@1 67.188 (70.247)	Acc@5 92.969 (93.558)
Epoch: [62][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0208 (2.0219)	Acc@1 71.094 (70.155)	Acc@5 92.578 (93.490)
Epoch: [62][180/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.0799 (2.0253)	Acc@1 71.484 (70.120)	Acc@5 91.797 (93.413)
Epoch: [62][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1756 (2.0305)	Acc@1 67.188 (70.010)	Acc@5 93.359 (93.341)
num momentum params: 26
[0.1, 2.031883935546875, 2.4348532593250276, 69.976, 44.38, tensor(0.4964, device='cuda:0', grad_fn=<DivBackward0>), 3.0526223182678223, 0.39923214912414545]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [63 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [63][0/196]	Time 0.045 (0.045)	Data 0.176 (0.176)	Loss 2.1016 (2.1016)	Acc@1 67.578 (67.578)	Acc@5 93.750 (93.750)
Epoch: [63][10/196]	Time 0.013 (0.018)	Data 0.004 (0.019)	Loss 1.9725 (2.0158)	Acc@1 72.266 (70.241)	Acc@5 92.578 (94.141)
Epoch: [63][20/196]	Time 0.014 (0.016)	Data 0.002 (0.011)	Loss 2.0086 (2.0038)	Acc@1 74.219 (70.480)	Acc@5 92.578 (93.862)
Epoch: [63][30/196]	Time 0.013 (0.016)	Data 0.021 (0.009)	Loss 1.9642 (1.9995)	Acc@1 71.094 (70.602)	Acc@5 92.578 (93.838)
Epoch: [63][40/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0780 (1.9919)	Acc@1 69.922 (70.817)	Acc@5 91.797 (93.912)
Epoch: [63][50/196]	Time 0.011 (0.015)	Data 0.017 (0.007)	Loss 1.9767 (1.9886)	Acc@1 71.875 (70.956)	Acc@5 94.531 (93.934)
Epoch: [63][60/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 2.1415 (1.9883)	Acc@1 67.578 (70.934)	Acc@5 89.844 (93.942)
Epoch: [63][70/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.1462 (1.9959)	Acc@1 65.625 (70.775)	Acc@5 92.188 (93.915)
Epoch: [63][80/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0875 (1.9963)	Acc@1 67.578 (70.751)	Acc@5 92.578 (93.861)
Epoch: [63][90/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.8989 (1.9933)	Acc@1 72.266 (70.845)	Acc@5 96.094 (93.874)
Epoch: [63][100/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0396 (1.9956)	Acc@1 72.656 (70.885)	Acc@5 92.969 (93.862)
Epoch: [63][110/196]	Time 0.014 (0.015)	Data 0.007 (0.005)	Loss 2.1996 (2.0007)	Acc@1 67.969 (70.710)	Acc@5 91.797 (93.856)
Epoch: [63][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0377 (2.0050)	Acc@1 67.578 (70.590)	Acc@5 94.141 (93.763)
Epoch: [63][130/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 1.9684 (2.0099)	Acc@1 69.922 (70.459)	Acc@5 94.531 (93.655)
Epoch: [63][140/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9873 (2.0151)	Acc@1 74.219 (70.340)	Acc@5 93.750 (93.611)
Epoch: [63][150/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.0535 (2.0217)	Acc@1 71.094 (70.243)	Acc@5 93.359 (93.489)
Epoch: [63][160/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1998 (2.0259)	Acc@1 66.406 (70.138)	Acc@5 91.406 (93.456)
Epoch: [63][170/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0868 (2.0312)	Acc@1 67.578 (70.011)	Acc@5 92.188 (93.389)
Epoch: [63][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2081 (2.0342)	Acc@1 62.500 (69.918)	Acc@5 91.406 (93.359)
Epoch: [63][190/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0130 (2.0378)	Acc@1 70.703 (69.820)	Acc@5 93.359 (93.343)
num momentum params: 26
[0.1, 2.03834798248291, 1.7633865535259248, 69.83, 54.27, tensor(0.4962, device='cuda:0', grad_fn=<DivBackward0>), 3.0413284301757812, 0.3958868980407715]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [64 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [64][0/196]	Time 0.046 (0.046)	Data 0.191 (0.191)	Loss 2.0829 (2.0829)	Acc@1 68.750 (68.750)	Acc@5 93.750 (93.750)
Epoch: [64][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.9872 (1.9412)	Acc@1 71.484 (72.372)	Acc@5 94.531 (94.602)
Epoch: [64][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 1.8877 (1.9374)	Acc@1 75.000 (72.824)	Acc@5 93.359 (94.438)
Epoch: [64][30/196]	Time 0.013 (0.017)	Data 0.004 (0.008)	Loss 1.8826 (1.9394)	Acc@1 73.828 (72.543)	Acc@5 94.141 (94.481)
Epoch: [64][40/196]	Time 0.014 (0.016)	Data 0.002 (0.007)	Loss 1.9446 (1.9420)	Acc@1 71.875 (72.370)	Acc@5 94.141 (94.360)
Epoch: [64][50/196]	Time 0.012 (0.016)	Data 0.008 (0.007)	Loss 1.9879 (1.9467)	Acc@1 70.703 (72.358)	Acc@5 94.531 (94.294)
Epoch: [64][60/196]	Time 0.020 (0.016)	Data 0.000 (0.006)	Loss 2.1248 (1.9518)	Acc@1 68.359 (72.240)	Acc@5 92.969 (94.326)
Epoch: [64][70/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 1.9629 (1.9619)	Acc@1 70.703 (71.814)	Acc@5 94.141 (94.328)
Epoch: [64][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0121 (1.9713)	Acc@1 69.141 (71.504)	Acc@5 94.922 (94.232)
Epoch: [64][90/196]	Time 0.012 (0.016)	Data 0.015 (0.005)	Loss 1.9130 (1.9789)	Acc@1 71.875 (71.313)	Acc@5 92.969 (94.098)
Epoch: [64][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9271 (1.9848)	Acc@1 74.219 (71.245)	Acc@5 94.531 (93.978)
Epoch: [64][110/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 1.9966 (1.9898)	Acc@1 70.312 (71.101)	Acc@5 94.141 (93.912)
Epoch: [64][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1736 (1.9987)	Acc@1 67.578 (70.871)	Acc@5 91.016 (93.773)
Epoch: [64][130/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 2.1844 (2.0088)	Acc@1 66.406 (70.506)	Acc@5 91.406 (93.604)
Epoch: [64][140/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9905 (2.0145)	Acc@1 73.047 (70.357)	Acc@5 92.578 (93.498)
Epoch: [64][150/196]	Time 0.011 (0.016)	Data 0.011 (0.004)	Loss 2.2785 (2.0231)	Acc@1 66.797 (70.183)	Acc@5 88.281 (93.370)
Epoch: [64][160/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0188 (2.0275)	Acc@1 74.219 (70.082)	Acc@5 91.797 (93.311)
Epoch: [64][170/196]	Time 0.011 (0.016)	Data 0.015 (0.004)	Loss 2.0022 (2.0310)	Acc@1 72.266 (69.947)	Acc@5 92.188 (93.273)
Epoch: [64][180/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1225 (2.0344)	Acc@1 68.750 (69.851)	Acc@5 94.141 (93.247)
Epoch: [64][190/196]	Time 0.011 (0.016)	Data 0.020 (0.004)	Loss 1.8944 (2.0360)	Acc@1 74.219 (69.793)	Acc@5 94.531 (93.241)
num momentum params: 26
[0.1, 2.037368440246582, 2.056616940498352, 69.732, 49.85, tensor(0.4969, device='cuda:0', grad_fn=<DivBackward0>), 3.0384509563446045, 0.3941078186035157]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [65 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [65][0/196]	Time 0.048 (0.048)	Data 0.183 (0.183)	Loss 1.8955 (1.8955)	Acc@1 73.828 (73.828)	Acc@5 94.922 (94.922)
Epoch: [65][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.9477 (1.9190)	Acc@1 73.438 (72.479)	Acc@5 94.922 (94.957)
Epoch: [65][20/196]	Time 0.014 (0.017)	Data 0.004 (0.011)	Loss 1.8342 (1.9299)	Acc@1 78.125 (72.842)	Acc@5 98.047 (94.810)
Epoch: [65][30/196]	Time 0.012 (0.016)	Data 0.005 (0.008)	Loss 1.9065 (1.9605)	Acc@1 73.828 (72.278)	Acc@5 95.703 (94.241)
Epoch: [65][40/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 2.0873 (1.9695)	Acc@1 69.141 (71.970)	Acc@5 92.578 (94.179)
Epoch: [65][50/196]	Time 0.015 (0.016)	Data 0.004 (0.006)	Loss 1.9375 (1.9801)	Acc@1 69.922 (71.553)	Acc@5 95.312 (94.194)
Epoch: [65][60/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 1.9321 (1.9751)	Acc@1 72.656 (71.811)	Acc@5 94.531 (94.205)
Epoch: [65][70/196]	Time 0.012 (0.016)	Data 0.004 (0.005)	Loss 1.9973 (1.9753)	Acc@1 72.266 (71.765)	Acc@5 91.797 (94.157)
Epoch: [65][80/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.7971 (1.9737)	Acc@1 76.562 (71.783)	Acc@5 95.703 (94.198)
Epoch: [65][90/196]	Time 0.013 (0.016)	Data 0.006 (0.004)	Loss 2.0343 (1.9759)	Acc@1 69.922 (71.613)	Acc@5 92.969 (94.175)
Epoch: [65][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1301 (1.9792)	Acc@1 66.406 (71.434)	Acc@5 95.703 (94.203)
Epoch: [65][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9090 (1.9886)	Acc@1 73.047 (71.277)	Acc@5 94.141 (94.049)
Epoch: [65][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1017 (1.9994)	Acc@1 65.625 (70.997)	Acc@5 94.141 (93.924)
Epoch: [65][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9327 (2.0042)	Acc@1 71.484 (70.864)	Acc@5 94.922 (93.828)
Epoch: [65][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1282 (2.0116)	Acc@1 67.969 (70.684)	Acc@5 91.797 (93.750)
Epoch: [65][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1716 (2.0194)	Acc@1 66.406 (70.512)	Acc@5 91.797 (93.628)
Epoch: [65][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9917 (2.0266)	Acc@1 70.703 (70.283)	Acc@5 94.141 (93.546)
Epoch: [65][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1828 (2.0285)	Acc@1 64.453 (70.235)	Acc@5 92.188 (93.506)
Epoch: [65][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2074 (2.0293)	Acc@1 64.062 (70.246)	Acc@5 92.969 (93.482)
Epoch: [65][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1151 (2.0311)	Acc@1 69.531 (70.188)	Acc@5 90.625 (93.417)
num momentum params: 26
[0.1, 2.0324360247039794, 1.9347479391098021, 70.128, 53.18, tensor(0.4996, device='cuda:0', grad_fn=<DivBackward0>), 3.0291924476623535, 0.39715409278869623]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [66 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [66][0/196]	Time 0.046 (0.046)	Data 0.189 (0.189)	Loss 1.9271 (1.9271)	Acc@1 71.094 (71.094)	Acc@5 94.141 (94.141)
Epoch: [66][10/196]	Time 0.016 (0.018)	Data 0.001 (0.019)	Loss 1.8623 (1.9697)	Acc@1 73.828 (72.301)	Acc@5 94.141 (94.105)
Epoch: [66][20/196]	Time 0.012 (0.017)	Data 0.006 (0.011)	Loss 2.0968 (1.9498)	Acc@1 69.141 (72.824)	Acc@5 92.188 (94.271)
Epoch: [66][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.8504 (1.9356)	Acc@1 74.219 (73.009)	Acc@5 95.703 (94.493)
Epoch: [66][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 1.9515 (1.9281)	Acc@1 73.047 (73.095)	Acc@5 93.750 (94.646)
Epoch: [66][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.8824 (1.9302)	Acc@1 76.562 (72.917)	Acc@5 93.359 (94.677)
Epoch: [66][60/196]	Time 0.014 (0.016)	Data 0.007 (0.005)	Loss 2.0406 (1.9382)	Acc@1 69.141 (72.618)	Acc@5 91.797 (94.563)
Epoch: [66][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9010 (1.9465)	Acc@1 76.172 (72.282)	Acc@5 93.359 (94.471)
Epoch: [66][80/196]	Time 0.015 (0.016)	Data 0.004 (0.005)	Loss 1.8364 (1.9513)	Acc@1 73.828 (72.160)	Acc@5 94.922 (94.377)
Epoch: [66][90/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 2.0519 (1.9570)	Acc@1 71.094 (72.068)	Acc@5 92.188 (94.304)
Epoch: [66][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2133 (1.9712)	Acc@1 65.234 (71.709)	Acc@5 92.188 (94.117)
Epoch: [66][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1381 (1.9799)	Acc@1 64.844 (71.491)	Acc@5 92.969 (93.979)
Epoch: [66][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9268 (1.9824)	Acc@1 76.172 (71.442)	Acc@5 91.406 (93.899)
Epoch: [66][130/196]	Time 0.012 (0.016)	Data 0.012 (0.004)	Loss 2.2458 (1.9891)	Acc@1 64.844 (71.261)	Acc@5 92.188 (93.813)
Epoch: [66][140/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.3524 (1.9999)	Acc@1 62.891 (70.922)	Acc@5 87.500 (93.645)
Epoch: [66][150/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 2.0660 (2.0066)	Acc@1 71.875 (70.757)	Acc@5 92.969 (93.579)
Epoch: [66][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0783 (2.0112)	Acc@1 69.922 (70.698)	Acc@5 91.016 (93.524)
Epoch: [66][170/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.0637 (2.0142)	Acc@1 72.266 (70.694)	Acc@5 92.969 (93.501)
Epoch: [66][180/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.0664 (2.0186)	Acc@1 69.141 (70.580)	Acc@5 91.016 (93.495)
Epoch: [66][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0273 (2.0210)	Acc@1 67.969 (70.515)	Acc@5 94.922 (93.427)
num momentum params: 26
[0.1, 2.0207391455078123, 1.9831040787696839, 70.486, 50.6, tensor(0.5026, device='cuda:0', grad_fn=<DivBackward0>), 3.105532169342041, 0.3954412937164306]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [67 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [67][0/196]	Time 0.044 (0.044)	Data 0.179 (0.179)	Loss 2.0467 (2.0467)	Acc@1 66.797 (66.797)	Acc@5 94.922 (94.922)
Epoch: [67][10/196]	Time 0.014 (0.018)	Data 0.004 (0.019)	Loss 1.9472 (1.9656)	Acc@1 73.438 (71.911)	Acc@5 96.875 (94.389)
Epoch: [67][20/196]	Time 0.012 (0.016)	Data 0.007 (0.011)	Loss 2.0952 (1.9699)	Acc@1 66.406 (71.466)	Acc@5 93.359 (94.308)
Epoch: [67][30/196]	Time 0.012 (0.016)	Data 0.007 (0.009)	Loss 1.9306 (1.9477)	Acc@1 74.219 (72.316)	Acc@5 92.969 (94.317)
Epoch: [67][40/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 1.9124 (1.9487)	Acc@1 69.141 (72.208)	Acc@5 96.484 (94.331)
Epoch: [67][50/196]	Time 0.012 (0.015)	Data 0.030 (0.007)	Loss 1.9078 (1.9410)	Acc@1 73.438 (72.610)	Acc@5 94.141 (94.432)
Epoch: [67][60/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.1554 (1.9454)	Acc@1 67.578 (72.451)	Acc@5 94.141 (94.416)
Epoch: [67][70/196]	Time 0.011 (0.015)	Data 0.018 (0.006)	Loss 2.1762 (1.9554)	Acc@1 65.625 (72.079)	Acc@5 93.359 (94.344)
Epoch: [67][80/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0550 (1.9605)	Acc@1 67.578 (72.005)	Acc@5 92.969 (94.213)
Epoch: [67][90/196]	Time 0.011 (0.015)	Data 0.017 (0.006)	Loss 2.0018 (1.9727)	Acc@1 73.047 (71.751)	Acc@5 94.922 (94.123)
Epoch: [67][100/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0478 (1.9787)	Acc@1 68.359 (71.562)	Acc@5 93.750 (94.056)
Epoch: [67][110/196]	Time 0.011 (0.015)	Data 0.018 (0.006)	Loss 2.2247 (1.9844)	Acc@1 66.797 (71.463)	Acc@5 90.234 (94.007)
Epoch: [67][120/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.0547 (1.9895)	Acc@1 71.094 (71.355)	Acc@5 94.922 (93.963)
Epoch: [67][130/196]	Time 0.011 (0.015)	Data 0.019 (0.005)	Loss 2.0461 (1.9968)	Acc@1 69.922 (71.201)	Acc@5 93.359 (93.863)
Epoch: [67][140/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 2.0749 (2.0015)	Acc@1 67.578 (71.133)	Acc@5 94.531 (93.783)
Epoch: [67][150/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 2.0263 (2.0062)	Acc@1 69.922 (70.936)	Acc@5 94.922 (93.734)
Epoch: [67][160/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2094 (2.0113)	Acc@1 66.797 (70.800)	Acc@5 91.016 (93.655)
Epoch: [67][170/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0932 (2.0146)	Acc@1 67.578 (70.747)	Acc@5 92.969 (93.599)
Epoch: [67][180/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1438 (2.0181)	Acc@1 68.750 (70.651)	Acc@5 90.625 (93.584)
Epoch: [67][190/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0943 (2.0220)	Acc@1 69.141 (70.556)	Acc@5 91.406 (93.511)
num momentum params: 26
[0.1, 2.024945054092407, 1.9384002935886384, 70.456, 51.07, tensor(0.5024, device='cuda:0', grad_fn=<DivBackward0>), 3.0207231044769287, 0.4016604423522949]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [68 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [68][0/196]	Time 0.044 (0.044)	Data 0.192 (0.192)	Loss 2.0397 (2.0397)	Acc@1 67.578 (67.578)	Acc@5 94.531 (94.531)
Epoch: [68][10/196]	Time 0.017 (0.018)	Data 0.003 (0.019)	Loss 1.9814 (1.9777)	Acc@1 71.094 (71.520)	Acc@5 94.141 (93.821)
Epoch: [68][20/196]	Time 0.015 (0.017)	Data 0.003 (0.011)	Loss 2.0225 (1.9802)	Acc@1 72.266 (71.726)	Acc@5 94.141 (94.159)
Epoch: [68][30/196]	Time 0.018 (0.016)	Data 0.000 (0.008)	Loss 1.9965 (1.9730)	Acc@1 72.656 (71.749)	Acc@5 95.312 (94.191)
Epoch: [68][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.0583 (1.9788)	Acc@1 70.703 (71.618)	Acc@5 91.797 (94.255)
Epoch: [68][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0594 (1.9798)	Acc@1 69.141 (71.630)	Acc@5 92.188 (94.171)
Epoch: [68][60/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 1.9709 (1.9866)	Acc@1 71.875 (71.472)	Acc@5 92.969 (93.968)
Epoch: [68][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1636 (1.9923)	Acc@1 63.672 (71.385)	Acc@5 92.578 (93.866)
Epoch: [68][80/196]	Time 0.013 (0.016)	Data 0.011 (0.005)	Loss 2.0007 (1.9952)	Acc@1 72.266 (71.287)	Acc@5 95.312 (93.813)
Epoch: [68][90/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1749 (1.9952)	Acc@1 64.844 (71.287)	Acc@5 92.969 (93.819)
Epoch: [68][100/196]	Time 0.014 (0.016)	Data 0.011 (0.004)	Loss 1.9752 (1.9983)	Acc@1 73.047 (71.152)	Acc@5 92.969 (93.796)
Epoch: [68][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1328 (2.0024)	Acc@1 65.625 (70.886)	Acc@5 91.406 (93.803)
Epoch: [68][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9413 (2.0038)	Acc@1 71.875 (70.826)	Acc@5 94.922 (93.827)
Epoch: [68][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.2111 (2.0098)	Acc@1 67.188 (70.703)	Acc@5 90.625 (93.714)
Epoch: [68][140/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0948 (2.0156)	Acc@1 67.969 (70.479)	Acc@5 94.531 (93.639)
Epoch: [68][150/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 2.0745 (2.0196)	Acc@1 67.969 (70.406)	Acc@5 92.188 (93.584)
Epoch: [68][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1892 (2.0240)	Acc@1 66.016 (70.259)	Acc@5 91.406 (93.553)
Epoch: [68][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1850 (2.0295)	Acc@1 68.359 (70.111)	Acc@5 88.672 (93.448)
Epoch: [68][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1300 (2.0336)	Acc@1 63.281 (69.933)	Acc@5 92.578 (93.400)
Epoch: [68][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1096 (2.0364)	Acc@1 69.531 (69.838)	Acc@5 90.234 (93.363)
num momentum params: 26
[0.1, 2.039132437362671, 1.9935083520412444, 69.744, 50.29, tensor(0.5000, device='cuda:0', grad_fn=<DivBackward0>), 3.0158839225769043, 0.4079904556274414]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [69 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [69][0/196]	Time 0.049 (0.049)	Data 0.184 (0.184)	Loss 2.1083 (2.1083)	Acc@1 67.188 (67.188)	Acc@5 93.750 (93.750)
Epoch: [69][10/196]	Time 0.015 (0.019)	Data 0.002 (0.018)	Loss 2.1349 (2.0245)	Acc@1 65.234 (69.602)	Acc@5 93.750 (93.466)
Epoch: [69][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 1.9378 (1.9885)	Acc@1 71.484 (70.889)	Acc@5 96.094 (94.178)
Epoch: [69][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9919 (1.9944)	Acc@1 72.266 (71.006)	Acc@5 93.359 (94.027)
Epoch: [69][40/196]	Time 0.011 (0.016)	Data 0.004 (0.007)	Loss 1.9332 (1.9884)	Acc@1 71.094 (71.141)	Acc@5 95.312 (94.169)
Epoch: [69][50/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 1.9722 (1.9886)	Acc@1 71.094 (71.063)	Acc@5 93.750 (94.087)
Epoch: [69][60/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9099 (1.9955)	Acc@1 74.219 (70.818)	Acc@5 94.922 (94.032)
Epoch: [69][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0710 (1.9851)	Acc@1 70.312 (71.132)	Acc@5 91.406 (94.124)
Epoch: [69][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0284 (1.9840)	Acc@1 71.094 (71.243)	Acc@5 92.578 (94.088)
Epoch: [69][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1782 (1.9967)	Acc@1 64.844 (70.862)	Acc@5 94.531 (93.995)
Epoch: [69][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 1.9212 (1.9984)	Acc@1 73.047 (70.850)	Acc@5 96.484 (93.982)
Epoch: [69][110/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0824 (2.0088)	Acc@1 66.016 (70.481)	Acc@5 92.188 (93.849)
Epoch: [69][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1378 (2.0113)	Acc@1 66.406 (70.390)	Acc@5 92.188 (93.818)
Epoch: [69][130/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0755 (2.0180)	Acc@1 71.484 (70.256)	Acc@5 93.359 (93.726)
Epoch: [69][140/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 1.9857 (2.0229)	Acc@1 71.094 (70.168)	Acc@5 93.359 (93.614)
Epoch: [69][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0932 (2.0211)	Acc@1 69.531 (70.230)	Acc@5 92.188 (93.621)
Epoch: [69][160/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0688 (2.0258)	Acc@1 66.797 (70.065)	Acc@5 92.578 (93.561)
Epoch: [69][170/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.1630 (2.0292)	Acc@1 67.578 (69.972)	Acc@5 90.625 (93.522)
Epoch: [69][180/196]	Time 0.011 (0.015)	Data 0.013 (0.004)	Loss 2.2304 (2.0351)	Acc@1 64.844 (69.810)	Acc@5 94.141 (93.489)
Epoch: [69][190/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 2.0893 (2.0375)	Acc@1 70.312 (69.795)	Acc@5 91.797 (93.468)
num momentum params: 26
[0.1, 2.04024138168335, 1.832711445093155, 69.75, 53.42, tensor(0.5011, device='cuda:0', grad_fn=<DivBackward0>), 2.9504587650299072, 0.39453792572021484]
Non Pruning Epoch - module.conv1.weight: [53, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [53]
Non Pruning Epoch - module.bn1.bias: [53]
Non Pruning Epoch - module.conv2.weight: [128, 53, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [510, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [501, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [501]
Non Pruning Epoch - module.bn7.bias: [501]
Non Pruning Epoch - module.conv8.weight: [390, 501, 3, 3]
Non Pruning Epoch - module.bn8.weight: [390]
Non Pruning Epoch - module.bn8.bias: [390]
Non Pruning Epoch - module.fc.weight: [100, 390]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [70 | 180] LR: 0.100000
module.conv1.weight [53, 3, 3, 3]
module.conv2.weight [128, 53, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [510, 512, 3, 3]
module.conv7.weight [501, 510, 3, 3]
module.conv8.weight [390, 501, 3, 3]
Epoch: [70][0/196]	Time 0.045 (0.045)	Data 0.177 (0.177)	Loss 2.1687 (2.1687)	Acc@1 69.141 (69.141)	Acc@5 92.188 (92.188)
Epoch: [70][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 1.8860 (2.0328)	Acc@1 73.047 (69.851)	Acc@5 95.703 (93.679)
Epoch: [70][20/196]	Time 0.013 (0.017)	Data 0.007 (0.011)	Loss 1.9706 (2.0081)	Acc@1 71.094 (70.871)	Acc@5 92.578 (93.508)
Epoch: [70][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 1.9387 (1.9780)	Acc@1 77.734 (71.850)	Acc@5 93.750 (94.002)
Epoch: [70][40/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 1.9774 (1.9703)	Acc@1 66.797 (71.818)	Acc@5 96.875 (94.341)
Epoch: [70][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0969 (1.9748)	Acc@1 69.531 (71.737)	Acc@5 93.750 (94.401)
Epoch: [70][60/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9851 (1.9695)	Acc@1 73.047 (71.984)	Acc@5 93.359 (94.422)
Epoch: [70][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 2.0586 (1.9743)	Acc@1 70.312 (71.897)	Acc@5 94.141 (94.355)
Epoch: [70][80/196]	Time 0.015 (0.016)	Data 0.008 (0.005)	Loss 2.0441 (1.9795)	Acc@1 68.750 (71.716)	Acc@5 92.188 (94.237)
Epoch: [70][90/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 1.9206 (1.9802)	Acc@1 71.094 (71.720)	Acc@5 95.703 (94.214)
Epoch: [70][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9523 (1.9804)	Acc@1 75.781 (71.782)	Acc@5 93.359 (94.191)
Epoch: [70][110/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9003 (1.9814)	Acc@1 74.219 (71.713)	Acc@5 93.750 (94.165)
Epoch: [70][120/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 1.9355 (1.9870)	Acc@1 75.000 (71.504)	Acc@5 94.141 (94.041)
Epoch: [70][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1212 (1.9920)	Acc@1 67.578 (71.329)	Acc@5 93.750 (94.009)
Epoch: [70][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0990 (1.9953)	Acc@1 68.359 (71.202)	Acc@5 92.188 (93.969)
Epoch: [70][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.2480 (2.0023)	Acc@1 67.188 (71.047)	Acc@5 92.578 (93.903)
Epoch: [70][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2059 (2.0060)	Acc@1 65.234 (70.897)	Acc@5 92.578 (93.876)
Epoch: [70][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.4670 (2.0149)	Acc@1 60.156 (70.689)	Acc@5 91.016 (93.835)
Epoch: [70][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2392 (2.0227)	Acc@1 64.453 (70.511)	Acc@5 90.625 (93.715)
Epoch: [70][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1605 (2.0306)	Acc@1 66.797 (70.288)	Acc@5 91.406 (93.601)
num momentum params: 26
[0.1, 2.0347797870635986, 1.8396095037460327, 70.148, 52.95, tensor(0.5026, device='cuda:0', grad_fn=<DivBackward0>), 3.0302319526672363, 0.4064490795135498]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [53, 3, 3, 3]
Before - module.bn1.weight: [53]
Before - module.bn1.bias: [53]
Before - module.conv2.weight: [128, 53, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [510, 512, 3, 3]
Before - module.bn6.weight: [510]
Before - module.bn6.bias: [510]
Before - module.conv7.weight: [501, 510, 3, 3]
Before - module.bn7.weight: [501]
Before - module.bn7.bias: [501]
Before - module.conv8.weight: [390, 501, 3, 3]
Before - module.bn8.weight: [390]
Before - module.bn8.bias: [390]
Before - module.fc.weight: [100, 390]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [53, 3, 3, 3] >> [51, 3, 3, 3]
[module.bn1.weight]: 53 >> 51
running_mean [51]
running_var [51]
num_batches_tracked []
[module.conv2.weight]: [128, 53, 3, 3] >> [128, 51, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [255, 128, 3, 3]
[module.bn3.weight]: 256 >> 255
running_mean [255]
running_var [255]
num_batches_tracked []
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 255, 3, 3]
[module.conv6.weight]: [510, 512, 3, 3] >> [509, 512, 3, 3]
[module.bn6.weight]: 510 >> 509
running_mean [509]
running_var [509]
num_batches_tracked []
[module.conv7.weight]: [501, 510, 3, 3] >> [492, 509, 3, 3]
[module.bn7.weight]: 501 >> 492
running_mean [492]
running_var [492]
num_batches_tracked []
[module.conv8.weight]: [390, 501, 3, 3] >> [375, 492, 3, 3]
[module.bn8.weight]: 390 >> 375
running_mean [375]
running_var [375]
num_batches_tracked []
[module.fc.weight]: [100, 390] >> [100, 375]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [51, 3, 3, 3]
After - module.bn1.weight: [51]
After - module.bn1.bias: [51]
After - module.conv2.weight: [128, 51, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [255, 128, 3, 3]
After - module.bn3.weight: [255]
After - module.bn3.bias: [255]
After - module.conv4.weight: [256, 255, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [509, 512, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [492, 509, 3, 3]
After - module.bn7.weight: [492]
After - module.bn7.bias: [492]
After - module.conv8.weight: [375, 492, 3, 3]
After - module.bn8.weight: [375]
After - module.bn8.bias: [375]
After - module.fc.weight: [100, 375]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [51, 3, 3, 3]
conv2 --> [128, 51, 3, 3]
conv3 --> [255, 128, 3, 3]
conv4 --> [256, 255, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [509, 512, 3, 3]
conv7 --> [492, 509, 3, 3]
conv8 --> [375, 492, 3, 3]
fc --> [375, 100]
1, 564724224, 1410048, 51
2, 6286934016, 15040512, 128
3, 8573091840, 18800640, 255
4, 17146183680, 37601280, 256
5, 10267656192, 18874368, 512
6, 20414988288, 37527552, 509
7, 6923833344, 9015408, 492
8, 5101056000, 6642000, 375
fc, 14400000, 37500, 0
===================
FLOP REPORT: 29411276400000.0 55000000000.0 144949308 137500 2578 16.062419891357422
[INFO] Storing checkpoint...

Epoch: [71 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [71][0/196]	Time 0.638 (0.638)	Data 0.179 (0.179)	Loss 2.0612 (2.0612)	Acc@1 71.875 (71.875)	Acc@5 94.531 (94.531)
Epoch: [71][10/196]	Time 0.014 (0.072)	Data 0.002 (0.018)	Loss 1.8581 (1.9699)	Acc@1 77.734 (72.301)	Acc@5 96.875 (94.922)
Epoch: [71][20/196]	Time 0.016 (0.045)	Data 0.002 (0.011)	Loss 1.8857 (1.9603)	Acc@1 74.219 (72.433)	Acc@5 94.531 (94.699)
Epoch: [71][30/196]	Time 0.014 (0.035)	Data 0.002 (0.008)	Loss 1.8749 (1.9420)	Acc@1 73.047 (72.707)	Acc@5 96.094 (94.972)
Epoch: [71][40/196]	Time 0.015 (0.030)	Data 0.003 (0.007)	Loss 1.9937 (1.9436)	Acc@1 71.094 (72.799)	Acc@5 95.312 (94.874)
Epoch: [71][50/196]	Time 0.015 (0.027)	Data 0.002 (0.006)	Loss 2.0617 (1.9473)	Acc@1 69.141 (72.549)	Acc@5 92.969 (94.776)
Epoch: [71][60/196]	Time 0.015 (0.025)	Data 0.002 (0.005)	Loss 1.9085 (1.9520)	Acc@1 73.438 (72.426)	Acc@5 93.359 (94.614)
Epoch: [71][70/196]	Time 0.016 (0.024)	Data 0.000 (0.005)	Loss 1.9714 (1.9552)	Acc@1 74.219 (72.370)	Acc@5 95.703 (94.487)
Epoch: [71][80/196]	Time 0.019 (0.023)	Data 0.002 (0.005)	Loss 2.0206 (1.9700)	Acc@1 70.312 (72.111)	Acc@5 95.312 (94.367)
Epoch: [71][90/196]	Time 0.016 (0.022)	Data 0.000 (0.004)	Loss 2.0553 (1.9761)	Acc@1 68.750 (71.909)	Acc@5 94.922 (94.269)
Epoch: [71][100/196]	Time 0.016 (0.021)	Data 0.001 (0.004)	Loss 2.0583 (1.9870)	Acc@1 69.922 (71.639)	Acc@5 90.625 (94.148)
Epoch: [71][110/196]	Time 0.017 (0.021)	Data 0.000 (0.004)	Loss 1.9433 (1.9923)	Acc@1 69.531 (71.393)	Acc@5 94.141 (94.091)
Epoch: [71][120/196]	Time 0.017 (0.020)	Data 0.000 (0.004)	Loss 2.0835 (1.9952)	Acc@1 70.703 (71.310)	Acc@5 88.672 (94.028)
Epoch: [71][130/196]	Time 0.016 (0.020)	Data 0.000 (0.004)	Loss 1.9774 (2.0046)	Acc@1 72.656 (71.025)	Acc@5 96.484 (93.872)
Epoch: [71][140/196]	Time 0.016 (0.019)	Data 0.001 (0.004)	Loss 2.3139 (2.0174)	Acc@1 62.500 (70.639)	Acc@5 90.234 (93.708)
Epoch: [71][150/196]	Time 0.016 (0.019)	Data 0.002 (0.004)	Loss 2.1084 (2.0256)	Acc@1 67.188 (70.416)	Acc@5 93.750 (93.623)
Epoch: [71][160/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 2.1435 (2.0340)	Acc@1 65.625 (70.157)	Acc@5 91.797 (93.493)
Epoch: [71][170/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 2.1597 (2.0361)	Acc@1 67.969 (70.173)	Acc@5 90.625 (93.483)
Epoch: [71][180/196]	Time 0.016 (0.018)	Data 0.000 (0.003)	Loss 2.1929 (2.0394)	Acc@1 67.578 (70.101)	Acc@5 91.406 (93.415)
Epoch: [71][190/196]	Time 0.017 (0.018)	Data 0.000 (0.003)	Loss 2.0916 (2.0401)	Acc@1 69.141 (70.047)	Acc@5 91.797 (93.413)
num momentum params: 26
[0.1, 2.0414935353088377, 2.0032542049884796, 70.024, 51.21, tensor(0.5016, device='cuda:0', grad_fn=<DivBackward0>), 3.7732937335968018, 0.47935390472412115]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [72 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [72][0/196]	Time 0.042 (0.042)	Data 0.187 (0.187)	Loss 1.9519 (1.9519)	Acc@1 72.656 (72.656)	Acc@5 96.875 (96.875)
Epoch: [72][10/196]	Time 0.017 (0.019)	Data 0.002 (0.019)	Loss 1.9622 (1.9910)	Acc@1 73.438 (71.804)	Acc@5 96.094 (95.028)
Epoch: [72][20/196]	Time 0.015 (0.018)	Data 0.006 (0.011)	Loss 1.9655 (1.9681)	Acc@1 71.094 (72.452)	Acc@5 93.750 (94.885)
Epoch: [72][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9069 (1.9557)	Acc@1 73.438 (72.820)	Acc@5 95.312 (94.821)
Epoch: [72][40/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 2.0329 (1.9568)	Acc@1 72.266 (72.656)	Acc@5 94.531 (94.684)
Epoch: [72][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9552 (1.9488)	Acc@1 71.875 (72.855)	Acc@5 94.922 (94.707)
Epoch: [72][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.0155 (1.9498)	Acc@1 69.141 (72.765)	Acc@5 93.750 (94.698)
Epoch: [72][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0950 (1.9545)	Acc@1 66.406 (72.568)	Acc@5 93.359 (94.674)
Epoch: [72][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.8207 (1.9552)	Acc@1 75.391 (72.536)	Acc@5 95.312 (94.637)
Epoch: [72][90/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9285 (1.9582)	Acc@1 73.438 (72.412)	Acc@5 93.359 (94.587)
Epoch: [72][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0934 (1.9652)	Acc@1 65.234 (72.219)	Acc@5 89.453 (94.427)
Epoch: [72][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9723 (1.9701)	Acc@1 73.438 (72.030)	Acc@5 94.141 (94.383)
Epoch: [72][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1081 (1.9748)	Acc@1 68.359 (71.846)	Acc@5 92.578 (94.325)
Epoch: [72][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0901 (1.9838)	Acc@1 68.359 (71.595)	Acc@5 91.406 (94.239)
Epoch: [72][140/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1284 (1.9922)	Acc@1 68.359 (71.365)	Acc@5 92.578 (94.171)
Epoch: [72][150/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.0568 (1.9994)	Acc@1 68.750 (71.187)	Acc@5 94.531 (94.068)
Epoch: [72][160/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.2591 (2.0070)	Acc@1 64.844 (70.941)	Acc@5 90.234 (93.939)
Epoch: [72][170/196]	Time 0.011 (0.016)	Data 0.013 (0.004)	Loss 2.0930 (2.0108)	Acc@1 71.094 (70.856)	Acc@5 92.578 (93.887)
Epoch: [72][180/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1631 (2.0164)	Acc@1 66.797 (70.714)	Acc@5 93.359 (93.826)
Epoch: [72][190/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0871 (2.0229)	Acc@1 66.016 (70.564)	Acc@5 92.969 (93.746)
num momentum params: 26
[0.1, 2.0239349178314208, 2.062938767671585, 70.51, 49.76, tensor(0.5053, device='cuda:0', grad_fn=<DivBackward0>), 3.0385303497314453, 0.37845659255981445]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [73 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [73][0/196]	Time 0.046 (0.046)	Data 0.181 (0.181)	Loss 1.9906 (1.9906)	Acc@1 72.266 (72.266)	Acc@5 94.141 (94.141)
Epoch: [73][10/196]	Time 0.016 (0.019)	Data 0.001 (0.018)	Loss 2.0077 (2.0000)	Acc@1 66.797 (70.881)	Acc@5 94.531 (94.247)
Epoch: [73][20/196]	Time 0.012 (0.017)	Data 0.006 (0.011)	Loss 1.9437 (1.9623)	Acc@1 73.047 (72.005)	Acc@5 94.141 (94.327)
Epoch: [73][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.8450 (1.9420)	Acc@1 75.781 (72.644)	Acc@5 94.141 (94.544)
Epoch: [73][40/196]	Time 0.011 (0.016)	Data 0.005 (0.007)	Loss 1.7536 (1.9381)	Acc@1 79.688 (72.818)	Acc@5 97.266 (94.655)
Epoch: [73][50/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.9298 (1.9433)	Acc@1 72.266 (72.541)	Acc@5 94.141 (94.600)
Epoch: [73][60/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9472 (1.9541)	Acc@1 71.094 (72.138)	Acc@5 94.531 (94.499)
Epoch: [73][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9393 (1.9573)	Acc@1 74.219 (72.073)	Acc@5 95.312 (94.487)
Epoch: [73][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0073 (1.9656)	Acc@1 70.703 (71.904)	Acc@5 93.750 (94.300)
Epoch: [73][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9355 (1.9648)	Acc@1 76.172 (72.042)	Acc@5 92.969 (94.269)
Epoch: [73][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0811 (1.9691)	Acc@1 69.922 (71.964)	Acc@5 90.234 (94.272)
Epoch: [73][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0574 (1.9729)	Acc@1 69.141 (71.826)	Acc@5 92.188 (94.281)
Epoch: [73][120/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.2660 (1.9800)	Acc@1 67.578 (71.701)	Acc@5 90.234 (94.218)
Epoch: [73][130/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0536 (1.9844)	Acc@1 73.438 (71.610)	Acc@5 94.141 (94.179)
Epoch: [73][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9986 (1.9923)	Acc@1 68.750 (71.340)	Acc@5 94.531 (94.102)
Epoch: [73][150/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 1.9814 (1.9964)	Acc@1 72.656 (71.231)	Acc@5 93.750 (94.055)
Epoch: [73][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1054 (1.9997)	Acc@1 65.625 (71.089)	Acc@5 94.141 (94.017)
Epoch: [73][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9333 (1.9995)	Acc@1 72.656 (71.096)	Acc@5 94.922 (94.036)
Epoch: [73][180/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0765 (2.0061)	Acc@1 71.484 (70.915)	Acc@5 92.969 (93.951)
Epoch: [73][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0773 (2.0126)	Acc@1 69.531 (70.779)	Acc@5 92.969 (93.838)
num momentum params: 26
[0.1, 2.0156896477508544, 2.245825316905975, 70.7, 46.77, tensor(0.5077, device='cuda:0', grad_fn=<DivBackward0>), 3.0083515644073486, 0.3821291923522949]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [74 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [74][0/196]	Time 0.045 (0.045)	Data 0.182 (0.182)	Loss 1.8659 (1.8659)	Acc@1 73.828 (73.828)	Acc@5 96.094 (96.094)
Epoch: [74][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 1.8517 (1.9607)	Acc@1 73.438 (71.058)	Acc@5 95.312 (94.496)
Epoch: [74][20/196]	Time 0.011 (0.017)	Data 0.006 (0.011)	Loss 2.0396 (1.9576)	Acc@1 72.266 (71.931)	Acc@5 93.750 (94.550)
Epoch: [74][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.0077 (1.9638)	Acc@1 73.047 (71.938)	Acc@5 93.359 (94.317)
Epoch: [74][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 1.9659 (1.9746)	Acc@1 74.219 (71.513)	Acc@5 94.531 (94.274)
Epoch: [74][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9619 (1.9805)	Acc@1 76.172 (71.477)	Acc@5 92.578 (94.225)
Epoch: [74][60/196]	Time 0.014 (0.016)	Data 0.006 (0.006)	Loss 1.9785 (1.9798)	Acc@1 67.578 (71.292)	Acc@5 95.312 (94.326)
Epoch: [74][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9709 (1.9859)	Acc@1 69.141 (71.099)	Acc@5 96.875 (94.267)
Epoch: [74][80/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9213 (1.9889)	Acc@1 74.219 (71.007)	Acc@5 95.312 (94.189)
Epoch: [74][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9999 (1.9936)	Acc@1 72.656 (70.875)	Acc@5 95.312 (94.154)
Epoch: [74][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0197 (2.0028)	Acc@1 69.531 (70.626)	Acc@5 92.969 (94.059)
Epoch: [74][110/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0253 (2.0101)	Acc@1 70.312 (70.538)	Acc@5 94.531 (94.007)
Epoch: [74][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0740 (2.0136)	Acc@1 68.750 (70.497)	Acc@5 92.969 (93.950)
Epoch: [74][130/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0533 (2.0184)	Acc@1 67.578 (70.432)	Acc@5 92.969 (93.893)
Epoch: [74][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0883 (2.0207)	Acc@1 67.578 (70.357)	Acc@5 93.359 (93.891)
Epoch: [74][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 1.9790 (2.0268)	Acc@1 71.094 (70.199)	Acc@5 96.484 (93.812)
Epoch: [74][160/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.2220 (2.0315)	Acc@1 64.453 (70.133)	Acc@5 91.406 (93.762)
Epoch: [74][170/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1472 (2.0344)	Acc@1 66.406 (70.050)	Acc@5 91.016 (93.727)
Epoch: [74][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.2218 (2.0370)	Acc@1 62.500 (69.987)	Acc@5 89.844 (93.672)
Epoch: [74][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1775 (2.0401)	Acc@1 62.500 (69.822)	Acc@5 93.359 (93.658)
num momentum params: 26
[0.1, 2.043959656677246, 2.098724763393402, 69.744, 49.28, tensor(0.5014, device='cuda:0', grad_fn=<DivBackward0>), 2.9511265754699707, 0.3811368942260742]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [75 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [75][0/196]	Time 0.050 (0.050)	Data 0.192 (0.192)	Loss 1.9943 (1.9943)	Acc@1 71.484 (71.484)	Acc@5 92.578 (92.578)
Epoch: [75][10/196]	Time 0.014 (0.019)	Data 0.003 (0.019)	Loss 2.0201 (1.9625)	Acc@1 68.359 (71.839)	Acc@5 94.531 (94.354)
Epoch: [75][20/196]	Time 0.013 (0.017)	Data 0.004 (0.011)	Loss 2.0000 (1.9714)	Acc@1 71.094 (71.875)	Acc@5 96.484 (94.327)
Epoch: [75][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9482 (1.9731)	Acc@1 72.266 (71.913)	Acc@5 93.359 (94.254)
Epoch: [75][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 1.9081 (1.9691)	Acc@1 73.047 (72.266)	Acc@5 95.703 (94.293)
Epoch: [75][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.0217 (1.9648)	Acc@1 67.969 (72.403)	Acc@5 94.141 (94.363)
Epoch: [75][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9167 (1.9664)	Acc@1 74.609 (72.349)	Acc@5 96.875 (94.422)
Epoch: [75][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9478 (1.9642)	Acc@1 71.094 (72.233)	Acc@5 94.141 (94.416)
Epoch: [75][80/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.9616 (1.9633)	Acc@1 72.656 (72.343)	Acc@5 92.578 (94.367)
Epoch: [75][90/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 1.9646 (1.9638)	Acc@1 71.484 (72.248)	Acc@5 94.531 (94.360)
Epoch: [75][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9738 (1.9638)	Acc@1 75.391 (72.331)	Acc@5 95.312 (94.384)
Epoch: [75][110/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0336 (1.9680)	Acc@1 68.359 (72.135)	Acc@5 90.625 (94.274)
Epoch: [75][120/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.8786 (1.9681)	Acc@1 71.875 (72.133)	Acc@5 94.922 (94.289)
Epoch: [75][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9867 (1.9706)	Acc@1 68.359 (72.069)	Acc@5 92.578 (94.227)
Epoch: [75][140/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0273 (1.9761)	Acc@1 67.188 (71.850)	Acc@5 92.969 (94.177)
Epoch: [75][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0248 (1.9811)	Acc@1 71.094 (71.722)	Acc@5 92.969 (94.123)
Epoch: [75][160/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9693 (1.9899)	Acc@1 71.094 (71.472)	Acc@5 94.922 (94.000)
Epoch: [75][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1415 (1.9962)	Acc@1 69.531 (71.288)	Acc@5 92.578 (93.949)
Epoch: [75][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0683 (2.0023)	Acc@1 69.922 (71.087)	Acc@5 91.797 (93.871)
Epoch: [75][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1228 (2.0107)	Acc@1 66.016 (70.877)	Acc@5 94.141 (93.762)
num momentum params: 26
[0.1, 2.013883672180176, 1.9078661394119263, 70.796, 52.05, tensor(0.5088, device='cuda:0', grad_fn=<DivBackward0>), 2.9944920539855957, 0.3919100761413575]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [76 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [76][0/196]	Time 0.046 (0.046)	Data 0.192 (0.192)	Loss 1.8678 (1.8678)	Acc@1 71.484 (71.484)	Acc@5 96.484 (96.484)
Epoch: [76][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.8794 (1.9580)	Acc@1 74.219 (71.697)	Acc@5 93.359 (94.531)
Epoch: [76][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 2.0322 (1.9487)	Acc@1 69.531 (72.247)	Acc@5 96.094 (94.847)
Epoch: [76][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9212 (1.9307)	Acc@1 72.656 (73.085)	Acc@5 95.703 (94.985)
Epoch: [76][40/196]	Time 0.016 (0.016)	Data 0.004 (0.007)	Loss 1.8716 (1.9306)	Acc@1 74.219 (72.942)	Acc@5 96.484 (94.950)
Epoch: [76][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8645 (1.9340)	Acc@1 73.438 (72.802)	Acc@5 94.922 (94.914)
Epoch: [76][60/196]	Time 0.011 (0.016)	Data 0.008 (0.006)	Loss 1.9415 (1.9482)	Acc@1 71.094 (72.330)	Acc@5 94.531 (94.691)
Epoch: [76][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9362 (1.9558)	Acc@1 73.047 (72.260)	Acc@5 95.312 (94.636)
Epoch: [76][80/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 1.9098 (1.9604)	Acc@1 71.875 (72.184)	Acc@5 95.703 (94.647)
Epoch: [76][90/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0209 (1.9719)	Acc@1 71.094 (71.828)	Acc@5 94.141 (94.557)
Epoch: [76][100/196]	Time 0.012 (0.016)	Data 0.011 (0.004)	Loss 1.9120 (1.9785)	Acc@1 72.656 (71.662)	Acc@5 94.141 (94.419)
Epoch: [76][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8810 (1.9770)	Acc@1 73.438 (71.713)	Acc@5 94.922 (94.457)
Epoch: [76][120/196]	Time 0.015 (0.016)	Data 0.009 (0.004)	Loss 2.0979 (1.9799)	Acc@1 67.578 (71.665)	Acc@5 93.359 (94.418)
Epoch: [76][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.2018 (1.9884)	Acc@1 67.188 (71.404)	Acc@5 90.234 (94.296)
Epoch: [76][140/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.2968 (2.0011)	Acc@1 64.453 (71.019)	Acc@5 89.453 (94.163)
Epoch: [76][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0551 (2.0083)	Acc@1 67.969 (70.866)	Acc@5 94.141 (94.032)
Epoch: [76][160/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.0734 (2.0153)	Acc@1 70.312 (70.749)	Acc@5 91.406 (93.944)
Epoch: [76][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9966 (2.0195)	Acc@1 65.625 (70.616)	Acc@5 94.922 (93.889)
Epoch: [76][180/196]	Time 0.011 (0.016)	Data 0.005 (0.004)	Loss 2.1502 (2.0248)	Acc@1 67.969 (70.485)	Acc@5 92.578 (93.843)
Epoch: [76][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9831 (2.0260)	Acc@1 71.875 (70.488)	Acc@5 93.750 (93.822)
num momentum params: 26
[0.1, 2.026508339614868, 1.7546435713768005, 70.498, 54.77, tensor(0.5071, device='cuda:0', grad_fn=<DivBackward0>), 3.0931618213653564, 0.3867182731628418]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [77 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [77][0/196]	Time 0.046 (0.046)	Data 0.190 (0.190)	Loss 1.7941 (1.7941)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [77][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.8505 (1.9084)	Acc@1 73.047 (72.869)	Acc@5 95.703 (95.206)
Epoch: [77][20/196]	Time 0.013 (0.017)	Data 0.004 (0.011)	Loss 1.8908 (1.8991)	Acc@1 74.609 (73.251)	Acc@5 94.922 (95.350)
Epoch: [77][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 1.7930 (1.8962)	Acc@1 73.828 (73.387)	Acc@5 94.922 (95.363)
Epoch: [77][40/196]	Time 0.012 (0.016)	Data 0.009 (0.007)	Loss 1.9252 (1.8952)	Acc@1 73.047 (73.590)	Acc@5 95.312 (95.579)
Epoch: [77][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0804 (1.9070)	Acc@1 71.875 (73.537)	Acc@5 91.406 (95.320)
Epoch: [77][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 1.9907 (1.9146)	Acc@1 73.047 (73.297)	Acc@5 92.578 (95.172)
Epoch: [77][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9048 (1.9265)	Acc@1 72.266 (73.063)	Acc@5 96.094 (94.999)
Epoch: [77][80/196]	Time 0.012 (0.015)	Data 0.014 (0.005)	Loss 2.0166 (1.9358)	Acc@1 69.531 (72.772)	Acc@5 94.531 (94.893)
Epoch: [77][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0188 (1.9453)	Acc@1 69.531 (72.527)	Acc@5 93.750 (94.780)
Epoch: [77][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.9943 (1.9544)	Acc@1 68.359 (72.239)	Acc@5 95.312 (94.678)
Epoch: [77][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2739 (1.9634)	Acc@1 65.625 (72.044)	Acc@5 91.016 (94.584)
Epoch: [77][120/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.0067 (1.9706)	Acc@1 72.656 (71.894)	Acc@5 92.969 (94.470)
Epoch: [77][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0256 (1.9793)	Acc@1 70.312 (71.657)	Acc@5 92.578 (94.349)
Epoch: [77][140/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.1785 (1.9875)	Acc@1 68.750 (71.446)	Acc@5 91.406 (94.213)
Epoch: [77][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0773 (1.9914)	Acc@1 67.578 (71.355)	Acc@5 92.578 (94.161)
Epoch: [77][160/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 1.9509 (1.9964)	Acc@1 74.219 (71.244)	Acc@5 94.531 (94.102)
Epoch: [77][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0683 (2.0042)	Acc@1 72.656 (71.062)	Acc@5 92.969 (94.024)
Epoch: [77][180/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 1.9534 (2.0071)	Acc@1 71.875 (71.020)	Acc@5 95.312 (93.916)
Epoch: [77][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0926 (2.0133)	Acc@1 71.875 (70.877)	Acc@5 91.406 (93.797)
num momentum params: 26
[0.1, 2.0162716818237305, 2.029202032089233, 70.828, 50.16, tensor(0.5100, device='cuda:0', grad_fn=<DivBackward0>), 2.9698760509490967, 0.3784360885620117]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [78 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [78][0/196]	Time 0.048 (0.048)	Data 0.192 (0.192)	Loss 1.8509 (1.8509)	Acc@1 78.516 (78.516)	Acc@5 94.922 (94.922)
Epoch: [78][10/196]	Time 0.016 (0.019)	Data 0.002 (0.020)	Loss 1.8700 (1.9630)	Acc@1 73.828 (71.626)	Acc@5 95.703 (94.744)
Epoch: [78][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 1.9192 (1.9508)	Acc@1 73.047 (72.135)	Acc@5 95.312 (94.885)
Epoch: [78][30/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 1.8370 (1.9384)	Acc@1 75.391 (72.770)	Acc@5 96.875 (94.997)
Epoch: [78][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.0300 (1.9363)	Acc@1 70.703 (72.666)	Acc@5 94.141 (94.769)
Epoch: [78][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.8320 (1.9308)	Acc@1 74.609 (72.825)	Acc@5 96.094 (94.838)
Epoch: [78][60/196]	Time 0.014 (0.016)	Data 0.018 (0.006)	Loss 1.8431 (1.9347)	Acc@1 76.953 (72.836)	Acc@5 94.531 (94.704)
Epoch: [78][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1685 (1.9416)	Acc@1 66.016 (72.722)	Acc@5 93.359 (94.592)
Epoch: [78][80/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.0811 (1.9522)	Acc@1 69.141 (72.463)	Acc@5 92.969 (94.478)
Epoch: [78][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0903 (1.9568)	Acc@1 68.750 (72.343)	Acc@5 92.188 (94.428)
Epoch: [78][100/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0955 (1.9611)	Acc@1 70.312 (72.273)	Acc@5 92.969 (94.369)
Epoch: [78][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8642 (1.9649)	Acc@1 73.438 (72.149)	Acc@5 97.266 (94.327)
Epoch: [78][120/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.0672 (1.9707)	Acc@1 68.359 (72.017)	Acc@5 93.750 (94.234)
Epoch: [78][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0234 (1.9809)	Acc@1 67.969 (71.690)	Acc@5 91.406 (94.123)
Epoch: [78][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1052 (1.9895)	Acc@1 68.359 (71.448)	Acc@5 90.234 (94.027)
Epoch: [78][150/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1190 (1.9969)	Acc@1 65.625 (71.249)	Acc@5 92.188 (93.931)
Epoch: [78][160/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0978 (2.0025)	Acc@1 71.094 (71.154)	Acc@5 91.797 (93.823)
Epoch: [78][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0643 (2.0060)	Acc@1 69.922 (71.037)	Acc@5 93.359 (93.777)
Epoch: [78][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0653 (2.0097)	Acc@1 70.312 (70.953)	Acc@5 94.141 (93.737)
Epoch: [78][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9870 (2.0103)	Acc@1 73.047 (70.977)	Acc@5 94.922 (93.732)
num momentum params: 26
[0.1, 2.011442937469482, 1.8536811399459838, 70.938, 53.4, tensor(0.5113, device='cuda:0', grad_fn=<DivBackward0>), 3.0380172729492188, 0.38417935371398926]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [79 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [79][0/196]	Time 0.050 (0.050)	Data 0.184 (0.184)	Loss 1.9282 (1.9282)	Acc@1 74.219 (74.219)	Acc@5 94.531 (94.531)
Epoch: [79][10/196]	Time 0.016 (0.019)	Data 0.002 (0.018)	Loss 1.8832 (1.9412)	Acc@1 75.781 (73.153)	Acc@5 95.312 (94.886)
Epoch: [79][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.8723 (1.9464)	Acc@1 70.312 (73.065)	Acc@5 96.094 (94.866)
Epoch: [79][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.9392 (1.9343)	Acc@1 70.703 (73.538)	Acc@5 94.141 (94.808)
Epoch: [79][40/196]	Time 0.013 (0.016)	Data 0.002 (0.007)	Loss 2.1111 (1.9418)	Acc@1 68.359 (73.256)	Acc@5 92.578 (94.655)
Epoch: [79][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.8421 (1.9444)	Acc@1 76.172 (72.955)	Acc@5 95.312 (94.623)
Epoch: [79][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8786 (1.9495)	Acc@1 73.047 (72.675)	Acc@5 94.531 (94.550)
Epoch: [79][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.8011 (1.9551)	Acc@1 77.734 (72.651)	Acc@5 96.094 (94.509)
Epoch: [79][80/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9550 (1.9647)	Acc@1 73.828 (72.362)	Acc@5 96.094 (94.420)
Epoch: [79][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0303 (1.9699)	Acc@1 67.578 (72.184)	Acc@5 95.312 (94.372)
Epoch: [79][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0421 (1.9763)	Acc@1 70.703 (72.049)	Acc@5 92.969 (94.214)
Epoch: [79][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.1227 (1.9808)	Acc@1 65.234 (71.882)	Acc@5 93.359 (94.151)
Epoch: [79][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1011 (1.9878)	Acc@1 67.578 (71.752)	Acc@5 93.359 (94.083)
Epoch: [79][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.2317 (1.9956)	Acc@1 64.844 (71.517)	Acc@5 95.312 (94.030)
Epoch: [79][140/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.1367 (2.0021)	Acc@1 66.016 (71.362)	Acc@5 91.797 (93.913)
Epoch: [79][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0757 (2.0067)	Acc@1 70.312 (71.210)	Acc@5 91.797 (93.877)
Epoch: [79][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2126 (2.0130)	Acc@1 62.891 (71.045)	Acc@5 90.625 (93.782)
Epoch: [79][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0389 (2.0142)	Acc@1 70.312 (71.005)	Acc@5 92.188 (93.764)
Epoch: [79][180/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9986 (2.0180)	Acc@1 71.094 (70.882)	Acc@5 95.703 (93.763)
Epoch: [79][190/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9442 (2.0186)	Acc@1 73.438 (70.871)	Acc@5 93.750 (93.725)
num momentum params: 26
[0.1, 2.021550209350586, 1.9214179122447967, 70.824, 52.85, tensor(0.5096, device='cuda:0', grad_fn=<DivBackward0>), 3.0689008235931396, 0.3830230236053467]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [492, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [492]
Non Pruning Epoch - module.bn7.bias: [492]
Non Pruning Epoch - module.conv8.weight: [375, 492, 3, 3]
Non Pruning Epoch - module.bn8.weight: [375]
Non Pruning Epoch - module.bn8.bias: [375]
Non Pruning Epoch - module.fc.weight: [100, 375]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [80 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [492, 509, 3, 3]
module.conv8.weight [375, 492, 3, 3]
Epoch: [80][0/196]	Time 0.050 (0.050)	Data 0.185 (0.185)	Loss 1.8149 (1.8149)	Acc@1 75.781 (75.781)	Acc@5 95.703 (95.703)
Epoch: [80][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.9183 (1.9630)	Acc@1 75.391 (72.621)	Acc@5 91.406 (93.928)
Epoch: [80][20/196]	Time 0.018 (0.017)	Data 0.002 (0.011)	Loss 2.0067 (2.0099)	Acc@1 71.875 (71.168)	Acc@5 95.312 (93.620)
Epoch: [80][30/196]	Time 0.018 (0.017)	Data 0.000 (0.008)	Loss 1.8818 (1.9921)	Acc@1 76.953 (71.686)	Acc@5 95.703 (94.078)
Epoch: [80][40/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 1.8441 (1.9882)	Acc@1 77.734 (71.865)	Acc@5 94.141 (94.074)
Epoch: [80][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9946 (1.9885)	Acc@1 69.922 (71.867)	Acc@5 94.141 (94.141)
Epoch: [80][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9401 (1.9868)	Acc@1 73.438 (71.926)	Acc@5 92.969 (94.064)
Epoch: [80][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0559 (1.9845)	Acc@1 69.922 (71.996)	Acc@5 92.969 (94.064)
Epoch: [80][80/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 1.9569 (1.9832)	Acc@1 71.094 (71.981)	Acc@5 94.922 (94.136)
Epoch: [80][90/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9871 (1.9833)	Acc@1 71.094 (72.017)	Acc@5 93.750 (94.132)
Epoch: [80][100/196]	Time 0.012 (0.016)	Data 0.001 (0.004)	Loss 2.0336 (1.9891)	Acc@1 69.922 (71.809)	Acc@5 94.531 (94.083)
Epoch: [80][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1419 (1.9941)	Acc@1 67.969 (71.632)	Acc@5 92.188 (94.053)
Epoch: [80][120/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1221 (1.9990)	Acc@1 68.750 (71.497)	Acc@5 94.141 (94.011)
Epoch: [80][130/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9608 (2.0006)	Acc@1 71.875 (71.419)	Acc@5 93.750 (94.027)
Epoch: [80][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8718 (2.0030)	Acc@1 76.172 (71.421)	Acc@5 94.531 (93.958)
Epoch: [80][150/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0958 (2.0079)	Acc@1 67.188 (71.293)	Acc@5 95.312 (93.931)
Epoch: [80][160/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.1356 (2.0105)	Acc@1 66.016 (71.220)	Acc@5 93.359 (93.915)
Epoch: [80][170/196]	Time 0.011 (0.016)	Data 0.018 (0.004)	Loss 1.9818 (2.0128)	Acc@1 71.484 (71.144)	Acc@5 92.578 (93.846)
Epoch: [80][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1689 (2.0173)	Acc@1 66.797 (71.018)	Acc@5 91.406 (93.819)
Epoch: [80][190/196]	Time 0.011 (0.016)	Data 0.019 (0.004)	Loss 2.0980 (2.0234)	Acc@1 68.750 (70.846)	Acc@5 91.016 (93.756)
num momentum params: 26
[0.1, 2.0256357801055906, 1.9483528470993041, 70.808, 53.3, tensor(0.5094, device='cuda:0', grad_fn=<DivBackward0>), 3.0585272312164307, 0.3946211338043213]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [51, 3, 3, 3]
Before - module.bn1.weight: [51]
Before - module.bn1.bias: [51]
Before - module.conv2.weight: [128, 51, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [255, 128, 3, 3]
Before - module.bn3.weight: [255]
Before - module.bn3.bias: [255]
Before - module.conv4.weight: [256, 255, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [509, 512, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [492, 509, 3, 3]
Before - module.bn7.weight: [492]
Before - module.bn7.bias: [492]
Before - module.conv8.weight: [375, 492, 3, 3]
Before - module.bn8.weight: [375]
Before - module.bn8.bias: [375]
Before - module.fc.weight: [100, 375]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [51, 3, 3, 3] >> [49, 3, 3, 3]
[module.bn1.weight]: 51 >> 49
running_mean [49]
running_var [49]
num_batches_tracked []
[module.conv2.weight]: [128, 51, 3, 3] >> [128, 49, 3, 3]
[module.conv7.weight]: [492, 509, 3, 3] >> [489, 509, 3, 3]
[module.bn7.weight]: 492 >> 489
running_mean [489]
running_var [489]
num_batches_tracked []
[module.conv8.weight]: [375, 492, 3, 3] >> [359, 489, 3, 3]
[module.bn8.weight]: 375 >> 359
running_mean [359]
running_var [359]
num_batches_tracked []
[module.fc.weight]: [100, 375] >> [100, 359]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [49, 3, 3, 3]
After - module.bn1.weight: [49]
After - module.bn1.bias: [49]
After - module.conv2.weight: [128, 49, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [255, 128, 3, 3]
After - module.bn3.weight: [255]
After - module.bn3.bias: [255]
After - module.conv4.weight: [256, 255, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [509, 512, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [489, 509, 3, 3]
After - module.bn7.weight: [489]
After - module.bn7.bias: [489]
After - module.conv8.weight: [359, 489, 3, 3]
After - module.bn8.weight: [359]
After - module.bn8.bias: [359]
After - module.fc.weight: [100, 359]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [49, 3, 3, 3]
conv2 --> [128, 49, 3, 3]
conv3 --> [255, 128, 3, 3]
conv4 --> [256, 255, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [509, 512, 3, 3]
conv7 --> [489, 509, 3, 3]
conv8 --> [359, 489, 3, 3]
fc --> [359, 100]
1, 542578176, 1354752, 49
2, 6040387584, 14450688, 128
3, 8573091840, 18800640, 255
4, 17146183680, 37601280, 256
5, 10267656192, 18874368, 512
6, 20414988288, 37527552, 509
7, 6881614848, 8960436, 489
8, 4853634048, 6319836, 359
fc, 13785600, 35900, 0
===================
FLOP REPORT: 29192937600000.0 54150400000.0 143925452 135376 2557 15.874967575073242
[INFO] Storing checkpoint...

Epoch: [81 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [81][0/196]	Time 0.379 (0.379)	Data 0.197 (0.197)	Loss 1.9044 (1.9044)	Acc@1 72.656 (72.656)	Acc@5 95.703 (95.703)
Epoch: [81][10/196]	Time 0.014 (0.049)	Data 0.003 (0.020)	Loss 1.8786 (1.9792)	Acc@1 74.219 (72.514)	Acc@5 96.094 (94.389)
Epoch: [81][20/196]	Time 0.015 (0.033)	Data 0.003 (0.012)	Loss 1.9307 (1.9524)	Acc@1 74.609 (73.456)	Acc@5 94.922 (94.643)
Epoch: [81][30/196]	Time 0.017 (0.027)	Data 0.002 (0.009)	Loss 1.8793 (1.9544)	Acc@1 75.781 (73.311)	Acc@5 94.922 (94.506)
Epoch: [81][40/196]	Time 0.014 (0.024)	Data 0.003 (0.007)	Loss 2.0890 (1.9486)	Acc@1 67.969 (73.619)	Acc@5 93.750 (94.512)
Epoch: [81][50/196]	Time 0.014 (0.022)	Data 0.002 (0.006)	Loss 1.9559 (1.9607)	Acc@1 72.656 (73.162)	Acc@5 93.750 (94.393)
Epoch: [81][60/196]	Time 0.015 (0.021)	Data 0.002 (0.006)	Loss 2.0621 (1.9687)	Acc@1 71.484 (72.816)	Acc@5 92.188 (94.217)
Epoch: [81][70/196]	Time 0.011 (0.020)	Data 0.006 (0.005)	Loss 2.0488 (1.9762)	Acc@1 69.922 (72.502)	Acc@5 95.312 (94.185)
Epoch: [81][80/196]	Time 0.015 (0.019)	Data 0.002 (0.005)	Loss 1.8960 (1.9809)	Acc@1 74.219 (72.333)	Acc@5 95.703 (94.107)
Epoch: [81][90/196]	Time 0.014 (0.019)	Data 0.003 (0.005)	Loss 2.0624 (1.9801)	Acc@1 68.750 (72.356)	Acc@5 94.531 (94.149)
Epoch: [81][100/196]	Time 0.015 (0.018)	Data 0.001 (0.005)	Loss 2.0060 (1.9848)	Acc@1 70.312 (72.146)	Acc@5 93.750 (94.110)
Epoch: [81][110/196]	Time 0.016 (0.018)	Data 0.002 (0.005)	Loss 2.0635 (1.9864)	Acc@1 69.141 (72.044)	Acc@5 91.797 (94.102)
Epoch: [81][120/196]	Time 0.017 (0.018)	Data 0.000 (0.005)	Loss 1.9488 (1.9865)	Acc@1 75.391 (72.024)	Acc@5 93.359 (94.141)
Epoch: [81][130/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 1.9102 (1.9926)	Acc@1 75.000 (71.881)	Acc@5 94.531 (94.069)
Epoch: [81][140/196]	Time 0.016 (0.017)	Data 0.001 (0.004)	Loss 2.0631 (1.9979)	Acc@1 71.484 (71.748)	Acc@5 92.188 (93.994)
Epoch: [81][150/196]	Time 0.014 (0.017)	Data 0.003 (0.004)	Loss 2.0149 (2.0019)	Acc@1 69.141 (71.653)	Acc@5 92.969 (93.952)
Epoch: [81][160/196]	Time 0.015 (0.017)	Data 0.001 (0.004)	Loss 2.1089 (2.0095)	Acc@1 67.969 (71.402)	Acc@5 92.578 (93.886)
Epoch: [81][170/196]	Time 0.013 (0.017)	Data 0.004 (0.004)	Loss 1.9760 (2.0151)	Acc@1 69.141 (71.222)	Acc@5 94.141 (93.839)
Epoch: [81][180/196]	Time 0.017 (0.017)	Data 0.001 (0.004)	Loss 2.1091 (2.0206)	Acc@1 72.656 (71.096)	Acc@5 92.578 (93.780)
Epoch: [81][190/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 1.9544 (2.0220)	Acc@1 74.219 (71.036)	Acc@5 95.312 (93.756)
num momentum params: 26
[0.1, 2.0229323934555055, 1.8941907608509063, 70.986, 53.38, tensor(0.5106, device='cuda:0', grad_fn=<DivBackward0>), 3.405484437942505, 0.4635579586029053]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [82 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [82][0/196]	Time 0.042 (0.042)	Data 0.194 (0.194)	Loss 1.7939 (1.7939)	Acc@1 79.688 (79.688)	Acc@5 95.703 (95.703)
Epoch: [82][10/196]	Time 0.012 (0.018)	Data 0.004 (0.020)	Loss 1.9144 (1.9215)	Acc@1 73.438 (73.970)	Acc@5 96.094 (95.135)
Epoch: [82][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 1.9185 (1.9122)	Acc@1 74.219 (74.014)	Acc@5 95.703 (94.959)
Epoch: [82][30/196]	Time 0.013 (0.016)	Data 0.004 (0.009)	Loss 1.9241 (1.9227)	Acc@1 73.047 (73.828)	Acc@5 95.312 (94.909)
Epoch: [82][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 1.8557 (1.9181)	Acc@1 75.000 (73.771)	Acc@5 95.312 (95.017)
Epoch: [82][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.8688 (1.9126)	Acc@1 76.172 (73.759)	Acc@5 95.312 (95.121)
Epoch: [82][60/196]	Time 0.012 (0.015)	Data 0.004 (0.006)	Loss 1.9488 (1.9157)	Acc@1 69.922 (73.610)	Acc@5 94.922 (95.031)
Epoch: [82][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0753 (1.9277)	Acc@1 71.484 (73.278)	Acc@5 94.141 (94.834)
Epoch: [82][80/196]	Time 0.015 (0.015)	Data 0.008 (0.005)	Loss 1.9266 (1.9325)	Acc@1 76.953 (73.187)	Acc@5 94.531 (94.806)
Epoch: [82][90/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 1.9996 (1.9401)	Acc@1 68.750 (72.884)	Acc@5 95.312 (94.785)
Epoch: [82][100/196]	Time 0.014 (0.015)	Data 0.011 (0.005)	Loss 2.1385 (1.9509)	Acc@1 71.094 (72.641)	Acc@5 91.406 (94.616)
Epoch: [82][110/196]	Time 0.015 (0.015)	Data 0.001 (0.005)	Loss 2.0189 (1.9600)	Acc@1 70.312 (72.378)	Acc@5 92.969 (94.521)
Epoch: [82][120/196]	Time 0.013 (0.015)	Data 0.007 (0.004)	Loss 2.0098 (1.9668)	Acc@1 70.703 (72.178)	Acc@5 94.531 (94.483)
Epoch: [82][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9491 (1.9692)	Acc@1 73.047 (72.125)	Acc@5 94.141 (94.469)
Epoch: [82][140/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9055 (1.9719)	Acc@1 75.000 (72.005)	Acc@5 94.531 (94.437)
Epoch: [82][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2216 (1.9813)	Acc@1 66.016 (71.712)	Acc@5 93.359 (94.327)
Epoch: [82][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0383 (1.9868)	Acc@1 72.656 (71.603)	Acc@5 92.188 (94.238)
Epoch: [82][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1024 (1.9979)	Acc@1 70.312 (71.382)	Acc@5 91.797 (94.061)
Epoch: [82][180/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.1279 (2.0076)	Acc@1 67.578 (71.120)	Acc@5 89.062 (93.910)
Epoch: [82][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0621 (2.0124)	Acc@1 71.094 (71.045)	Acc@5 92.578 (93.850)
num momentum params: 26
[0.1, 2.0139287571716307, 1.8639010882377625, 70.992, 52.84, tensor(0.5133, device='cuda:0', grad_fn=<DivBackward0>), 2.9958226680755615, 0.38905835151672363]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [83 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [83][0/196]	Time 0.047 (0.047)	Data 0.203 (0.203)	Loss 1.9949 (1.9949)	Acc@1 73.047 (73.047)	Acc@5 95.703 (95.703)
Epoch: [83][10/196]	Time 0.016 (0.019)	Data 0.002 (0.020)	Loss 1.9084 (1.9878)	Acc@1 72.656 (71.484)	Acc@5 96.484 (94.780)
Epoch: [83][20/196]	Time 0.014 (0.017)	Data 0.004 (0.012)	Loss 1.8494 (1.9602)	Acc@1 73.047 (72.173)	Acc@5 98.438 (95.033)
Epoch: [83][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.9557 (1.9325)	Acc@1 75.391 (72.845)	Acc@5 91.406 (95.136)
Epoch: [83][40/196]	Time 0.012 (0.017)	Data 0.005 (0.007)	Loss 1.8124 (1.9305)	Acc@1 75.781 (72.866)	Acc@5 98.047 (95.217)
Epoch: [83][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9245 (1.9350)	Acc@1 73.438 (72.832)	Acc@5 94.531 (95.113)
Epoch: [83][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 1.8775 (1.9412)	Acc@1 73.047 (72.611)	Acc@5 95.312 (94.960)
Epoch: [83][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9938 (1.9420)	Acc@1 71.484 (72.711)	Acc@5 95.703 (94.955)
Epoch: [83][80/196]	Time 0.012 (0.016)	Data 0.018 (0.005)	Loss 1.8855 (1.9459)	Acc@1 73.438 (72.526)	Acc@5 96.094 (94.970)
Epoch: [83][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0718 (1.9576)	Acc@1 67.188 (72.193)	Acc@5 92.969 (94.780)
Epoch: [83][100/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.1113 (1.9700)	Acc@1 68.750 (71.945)	Acc@5 91.797 (94.678)
Epoch: [83][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1186 (1.9831)	Acc@1 68.750 (71.643)	Acc@5 93.750 (94.486)
Epoch: [83][120/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 2.1411 (1.9902)	Acc@1 66.406 (71.507)	Acc@5 93.359 (94.421)
Epoch: [83][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9683 (1.9922)	Acc@1 73.438 (71.434)	Acc@5 94.141 (94.421)
Epoch: [83][140/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1362 (2.0014)	Acc@1 69.922 (71.199)	Acc@5 92.969 (94.323)
Epoch: [83][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2008 (2.0043)	Acc@1 67.188 (71.076)	Acc@5 92.969 (94.283)
Epoch: [83][160/196]	Time 0.012 (0.016)	Data 0.019 (0.004)	Loss 2.0312 (2.0052)	Acc@1 72.266 (71.043)	Acc@5 94.141 (94.257)
Epoch: [83][170/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1030 (2.0107)	Acc@1 64.062 (70.948)	Acc@5 94.922 (94.202)
Epoch: [83][180/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.0461 (2.0153)	Acc@1 69.141 (70.837)	Acc@5 94.531 (94.100)
Epoch: [83][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0043 (2.0175)	Acc@1 69.141 (70.695)	Acc@5 93.750 (94.061)
num momentum params: 26
[0.1, 2.0184953555297853, 1.9100191962718964, 70.688, 52.57, tensor(0.5127, device='cuda:0', grad_fn=<DivBackward0>), 3.1147780418395996, 0.3907794952392578]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [84 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [84][0/196]	Time 0.049 (0.049)	Data 0.213 (0.213)	Loss 1.8302 (1.8302)	Acc@1 78.125 (78.125)	Acc@5 96.875 (96.875)
Epoch: [84][10/196]	Time 0.016 (0.019)	Data 0.002 (0.021)	Loss 1.9546 (1.9749)	Acc@1 73.828 (72.053)	Acc@5 94.531 (95.312)
Epoch: [84][20/196]	Time 0.011 (0.017)	Data 0.006 (0.012)	Loss 1.9993 (1.9597)	Acc@1 70.312 (72.656)	Acc@5 95.312 (95.052)
Epoch: [84][30/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 1.8730 (1.9652)	Acc@1 75.781 (72.719)	Acc@5 95.312 (94.796)
Epoch: [84][40/196]	Time 0.012 (0.017)	Data 0.006 (0.007)	Loss 1.8143 (1.9568)	Acc@1 79.688 (73.085)	Acc@5 96.484 (94.827)
Epoch: [84][50/196]	Time 0.018 (0.017)	Data 0.002 (0.006)	Loss 1.8607 (1.9549)	Acc@1 73.047 (73.009)	Acc@5 95.312 (94.815)
Epoch: [84][60/196]	Time 0.018 (0.016)	Data 0.002 (0.006)	Loss 1.9504 (1.9544)	Acc@1 70.703 (72.900)	Acc@5 95.703 (94.762)
Epoch: [84][70/196]	Time 0.012 (0.016)	Data 0.004 (0.005)	Loss 1.8681 (1.9552)	Acc@1 73.438 (72.739)	Acc@5 98.047 (94.762)
Epoch: [84][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1846 (1.9605)	Acc@1 67.188 (72.536)	Acc@5 93.359 (94.613)
Epoch: [84][90/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.0155 (1.9657)	Acc@1 72.656 (72.317)	Acc@5 93.750 (94.536)
Epoch: [84][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0235 (1.9710)	Acc@1 70.703 (72.196)	Acc@5 92.578 (94.446)
Epoch: [84][110/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.1766 (1.9812)	Acc@1 66.406 (71.896)	Acc@5 91.016 (94.320)
Epoch: [84][120/196]	Time 0.013 (0.016)	Data 0.009 (0.004)	Loss 2.0347 (1.9854)	Acc@1 74.609 (71.836)	Acc@5 93.750 (94.273)
Epoch: [84][130/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0700 (1.9886)	Acc@1 67.578 (71.777)	Acc@5 95.703 (94.233)
Epoch: [84][140/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9505 (1.9898)	Acc@1 71.094 (71.739)	Acc@5 94.922 (94.221)
Epoch: [84][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9017 (1.9903)	Acc@1 73.828 (71.777)	Acc@5 95.312 (94.257)
Epoch: [84][160/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0029 (1.9992)	Acc@1 72.656 (71.562)	Acc@5 94.141 (94.143)
Epoch: [84][170/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 1.9611 (2.0071)	Acc@1 74.609 (71.388)	Acc@5 91.797 (94.031)
Epoch: [84][180/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0207 (2.0095)	Acc@1 69.141 (71.260)	Acc@5 92.188 (94.009)
Epoch: [84][190/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.2579 (2.0160)	Acc@1 64.062 (71.098)	Acc@5 91.016 (93.948)
num momentum params: 26
[0.1, 2.0194952059936524, 2.4056231343746184, 71.034, 44.34, tensor(0.5125, device='cuda:0', grad_fn=<DivBackward0>), 3.0533316135406494, 0.39235568046569824]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [85 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [85][0/196]	Time 0.049 (0.049)	Data 0.184 (0.184)	Loss 1.9360 (1.9360)	Acc@1 73.047 (73.047)	Acc@5 93.750 (93.750)
Epoch: [85][10/196]	Time 0.015 (0.018)	Data 0.004 (0.019)	Loss 1.9897 (2.0326)	Acc@1 72.656 (71.626)	Acc@5 95.312 (93.679)
Epoch: [85][20/196]	Time 0.014 (0.017)	Data 0.008 (0.011)	Loss 1.9216 (1.9924)	Acc@1 69.922 (71.708)	Acc@5 96.094 (94.420)
Epoch: [85][30/196]	Time 0.018 (0.017)	Data 0.001 (0.009)	Loss 1.8581 (1.9633)	Acc@1 75.391 (72.379)	Acc@5 95.703 (94.771)
Epoch: [85][40/196]	Time 0.013 (0.016)	Data 0.010 (0.007)	Loss 1.8982 (1.9553)	Acc@1 72.656 (72.618)	Acc@5 97.266 (94.712)
Epoch: [85][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8914 (1.9504)	Acc@1 73.828 (72.718)	Acc@5 96.484 (94.753)
Epoch: [85][60/196]	Time 0.011 (0.016)	Data 0.007 (0.006)	Loss 2.1049 (1.9583)	Acc@1 69.141 (72.458)	Acc@5 92.188 (94.614)
Epoch: [85][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.9858 (1.9591)	Acc@1 71.094 (72.403)	Acc@5 96.094 (94.553)
Epoch: [85][80/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9751 (1.9625)	Acc@1 69.922 (72.246)	Acc@5 93.750 (94.551)
Epoch: [85][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0534 (1.9641)	Acc@1 69.141 (72.236)	Acc@5 94.531 (94.531)
Epoch: [85][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0071 (1.9714)	Acc@1 70.703 (72.072)	Acc@5 94.141 (94.473)
Epoch: [85][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0008 (1.9799)	Acc@1 69.531 (71.879)	Acc@5 95.703 (94.405)
Epoch: [85][120/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 2.0687 (1.9859)	Acc@1 69.141 (71.665)	Acc@5 94.922 (94.370)
Epoch: [85][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9772 (1.9932)	Acc@1 71.094 (71.446)	Acc@5 94.922 (94.296)
Epoch: [85][140/196]	Time 0.013 (0.016)	Data 0.009 (0.004)	Loss 1.9352 (1.9943)	Acc@1 70.703 (71.401)	Acc@5 94.922 (94.265)
Epoch: [85][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0717 (1.9993)	Acc@1 71.094 (71.272)	Acc@5 93.750 (94.210)
Epoch: [85][160/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 1.9772 (2.0062)	Acc@1 73.828 (71.094)	Acc@5 93.750 (94.121)
Epoch: [85][170/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.1045 (2.0117)	Acc@1 68.750 (70.973)	Acc@5 93.359 (94.017)
Epoch: [85][180/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 1.9460 (2.0139)	Acc@1 72.266 (70.889)	Acc@5 94.531 (94.015)
Epoch: [85][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0253 (2.0174)	Acc@1 73.438 (70.787)	Acc@5 94.141 (93.969)
num momentum params: 26
[0.1, 2.0205418296813966, 1.8511768066883088, 70.702, 53.48, tensor(0.5129, device='cuda:0', grad_fn=<DivBackward0>), 3.023355722427368, 0.38536882400512695]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [86 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [86][0/196]	Time 0.047 (0.047)	Data 0.185 (0.185)	Loss 1.9418 (1.9418)	Acc@1 73.828 (73.828)	Acc@5 94.531 (94.531)
Epoch: [86][10/196]	Time 0.017 (0.019)	Data 0.002 (0.019)	Loss 1.8394 (1.9407)	Acc@1 73.047 (72.869)	Acc@5 96.094 (94.709)
Epoch: [86][20/196]	Time 0.016 (0.017)	Data 0.003 (0.011)	Loss 1.9627 (1.9451)	Acc@1 72.266 (72.805)	Acc@5 94.922 (94.475)
Epoch: [86][30/196]	Time 0.019 (0.016)	Data 0.000 (0.008)	Loss 1.9457 (1.9452)	Acc@1 71.484 (72.833)	Acc@5 94.922 (94.695)
Epoch: [86][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 1.8335 (1.9458)	Acc@1 77.344 (72.952)	Acc@5 94.531 (94.674)
Epoch: [86][50/196]	Time 0.015 (0.016)	Data 0.000 (0.006)	Loss 1.9788 (1.9462)	Acc@1 68.359 (72.886)	Acc@5 95.312 (94.723)
Epoch: [86][60/196]	Time 0.013 (0.016)	Data 0.016 (0.006)	Loss 2.1744 (1.9567)	Acc@1 62.500 (72.541)	Acc@5 94.141 (94.691)
Epoch: [86][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9722 (1.9520)	Acc@1 70.312 (72.717)	Acc@5 93.359 (94.707)
Epoch: [86][80/196]	Time 0.012 (0.016)	Data 0.023 (0.006)	Loss 1.9689 (1.9579)	Acc@1 70.312 (72.545)	Acc@5 94.141 (94.589)
Epoch: [86][90/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.0414 (1.9613)	Acc@1 69.531 (72.510)	Acc@5 93.750 (94.566)
Epoch: [86][100/196]	Time 0.012 (0.016)	Data 0.018 (0.006)	Loss 1.9222 (1.9715)	Acc@1 74.609 (72.208)	Acc@5 93.750 (94.469)
Epoch: [86][110/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1523 (1.9771)	Acc@1 68.359 (72.040)	Acc@5 91.797 (94.383)
Epoch: [86][120/196]	Time 0.012 (0.016)	Data 0.016 (0.006)	Loss 2.0191 (1.9800)	Acc@1 67.188 (71.952)	Acc@5 95.703 (94.360)
Epoch: [86][130/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9446 (1.9837)	Acc@1 71.484 (71.800)	Acc@5 95.703 (94.326)
Epoch: [86][140/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 1.9466 (1.9875)	Acc@1 72.656 (71.714)	Acc@5 94.922 (94.290)
Epoch: [86][150/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9708 (1.9889)	Acc@1 70.703 (71.611)	Acc@5 95.312 (94.327)
Epoch: [86][160/196]	Time 0.014 (0.016)	Data 0.008 (0.005)	Loss 2.0791 (1.9927)	Acc@1 67.578 (71.470)	Acc@5 94.531 (94.262)
Epoch: [86][170/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0191 (1.9964)	Acc@1 69.922 (71.363)	Acc@5 92.578 (94.202)
Epoch: [86][180/196]	Time 0.013 (0.016)	Data 0.009 (0.005)	Loss 2.1335 (2.0000)	Acc@1 69.922 (71.286)	Acc@5 93.750 (94.156)
Epoch: [86][190/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0449 (2.0053)	Acc@1 71.094 (71.151)	Acc@5 92.578 (94.087)
num momentum params: 26
[0.1, 2.0069937564849853, 1.8755863881111146, 71.112, 53.66, tensor(0.5160, device='cuda:0', grad_fn=<DivBackward0>), 3.0592241287231445, 0.38866400718688965]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [87 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [87][0/196]	Time 0.049 (0.049)	Data 0.186 (0.186)	Loss 1.9438 (1.9438)	Acc@1 75.000 (75.000)	Acc@5 93.750 (93.750)
Epoch: [87][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 2.0594 (2.0285)	Acc@1 68.750 (71.165)	Acc@5 92.578 (94.105)
Epoch: [87][20/196]	Time 0.013 (0.017)	Data 0.004 (0.011)	Loss 1.8188 (1.9886)	Acc@1 75.000 (71.410)	Acc@5 96.484 (94.587)
Epoch: [87][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 1.9223 (1.9744)	Acc@1 76.172 (71.774)	Acc@5 96.484 (94.720)
Epoch: [87][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 1.9029 (1.9707)	Acc@1 73.438 (71.923)	Acc@5 95.312 (94.808)
Epoch: [87][50/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 1.8393 (1.9676)	Acc@1 73.047 (71.982)	Acc@5 94.531 (94.799)
Epoch: [87][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1780 (1.9742)	Acc@1 67.578 (71.945)	Acc@5 92.969 (94.659)
Epoch: [87][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9818 (1.9810)	Acc@1 71.484 (71.781)	Acc@5 94.141 (94.465)
Epoch: [87][80/196]	Time 0.017 (0.015)	Data 0.002 (0.005)	Loss 2.0622 (1.9846)	Acc@1 69.141 (71.629)	Acc@5 94.141 (94.430)
Epoch: [87][90/196]	Time 0.020 (0.016)	Data 0.003 (0.004)	Loss 1.9138 (1.9859)	Acc@1 73.828 (71.575)	Acc@5 94.141 (94.437)
Epoch: [87][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0894 (1.9847)	Acc@1 68.750 (71.627)	Acc@5 92.578 (94.466)
Epoch: [87][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1092 (1.9926)	Acc@1 67.188 (71.474)	Acc@5 92.188 (94.334)
Epoch: [87][120/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0569 (1.9920)	Acc@1 71.484 (71.575)	Acc@5 94.141 (94.328)
Epoch: [87][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0719 (1.9955)	Acc@1 68.359 (71.481)	Acc@5 94.141 (94.302)
Epoch: [87][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1116 (1.9976)	Acc@1 67.188 (71.362)	Acc@5 92.188 (94.310)
Epoch: [87][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1602 (2.0063)	Acc@1 67.969 (71.177)	Acc@5 92.188 (94.185)
Epoch: [87][160/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1929 (2.0116)	Acc@1 68.359 (71.101)	Acc@5 89.844 (94.099)
Epoch: [87][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0245 (2.0154)	Acc@1 71.875 (71.023)	Acc@5 93.359 (94.054)
Epoch: [87][180/196]	Time 0.024 (0.016)	Data 0.001 (0.004)	Loss 2.3658 (2.0232)	Acc@1 64.844 (70.843)	Acc@5 89.453 (93.966)
Epoch: [87][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1536 (2.0291)	Acc@1 69.141 (70.715)	Acc@5 94.531 (93.907)
num momentum params: 26
[0.1, 2.031207833404541, 1.8547929155826568, 70.642, 53.67, tensor(0.5109, device='cuda:0', grad_fn=<DivBackward0>), 3.064023017883301, 0.39741849899291987]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [88 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [88][0/196]	Time 0.054 (0.054)	Data 0.176 (0.176)	Loss 1.9399 (1.9399)	Acc@1 74.609 (74.609)	Acc@5 92.578 (92.578)
Epoch: [88][10/196]	Time 0.014 (0.019)	Data 0.004 (0.018)	Loss 1.9516 (1.9578)	Acc@1 71.484 (73.118)	Acc@5 95.312 (94.141)
Epoch: [88][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 1.9775 (1.9651)	Acc@1 72.656 (72.619)	Acc@5 96.094 (94.420)
Epoch: [88][30/196]	Time 0.013 (0.016)	Data 0.004 (0.008)	Loss 1.9147 (1.9648)	Acc@1 73.438 (72.593)	Acc@5 95.312 (94.531)
Epoch: [88][40/196]	Time 0.016 (0.016)	Data 0.003 (0.007)	Loss 1.8824 (1.9497)	Acc@1 74.609 (72.761)	Acc@5 94.922 (94.731)
Epoch: [88][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0293 (1.9499)	Acc@1 71.875 (72.763)	Acc@5 95.312 (94.784)
Epoch: [88][60/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.8445 (1.9520)	Acc@1 78.125 (72.650)	Acc@5 95.703 (94.730)
Epoch: [88][70/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 1.9781 (1.9535)	Acc@1 71.875 (72.618)	Acc@5 95.312 (94.718)
Epoch: [88][80/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0631 (1.9532)	Acc@1 69.141 (72.603)	Acc@5 93.750 (94.734)
Epoch: [88][90/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 1.9891 (1.9579)	Acc@1 72.266 (72.424)	Acc@5 94.531 (94.742)
Epoch: [88][100/196]	Time 0.012 (0.015)	Data 0.012 (0.005)	Loss 1.9406 (1.9654)	Acc@1 72.656 (72.246)	Acc@5 94.141 (94.616)
Epoch: [88][110/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0752 (1.9754)	Acc@1 68.750 (71.924)	Acc@5 93.359 (94.503)
Epoch: [88][120/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.0570 (1.9783)	Acc@1 68.750 (71.804)	Acc@5 92.188 (94.473)
Epoch: [88][130/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.1592 (1.9844)	Acc@1 66.016 (71.678)	Acc@5 92.188 (94.436)
Epoch: [88][140/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.0745 (1.9890)	Acc@1 71.484 (71.548)	Acc@5 92.578 (94.368)
Epoch: [88][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1678 (1.9928)	Acc@1 64.062 (71.425)	Acc@5 91.797 (94.340)
Epoch: [88][160/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 2.0726 (1.9981)	Acc@1 69.531 (71.341)	Acc@5 90.625 (94.243)
Epoch: [88][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0803 (2.0027)	Acc@1 69.531 (71.247)	Acc@5 94.531 (94.184)
Epoch: [88][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 1.8876 (2.0047)	Acc@1 76.953 (71.195)	Acc@5 95.312 (94.171)
Epoch: [88][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0741 (2.0077)	Acc@1 67.188 (71.122)	Acc@5 95.703 (94.145)
num momentum params: 26
[0.1, 2.010056303405762, 2.1045166754722597, 71.036, 50.21, tensor(0.5163, device='cuda:0', grad_fn=<DivBackward0>), 2.927175283432007, 0.4099924564361573]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [89 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [89][0/196]	Time 0.057 (0.057)	Data 0.187 (0.187)	Loss 1.9466 (1.9466)	Acc@1 72.266 (72.266)	Acc@5 95.312 (95.312)
Epoch: [89][10/196]	Time 0.015 (0.019)	Data 0.003 (0.019)	Loss 1.9274 (1.9326)	Acc@1 74.609 (73.402)	Acc@5 96.484 (95.028)
Epoch: [89][20/196]	Time 0.019 (0.017)	Data 0.002 (0.011)	Loss 1.8935 (1.9310)	Acc@1 72.266 (73.344)	Acc@5 96.875 (95.071)
Epoch: [89][30/196]	Time 0.019 (0.017)	Data 0.002 (0.008)	Loss 2.0042 (1.9255)	Acc@1 70.703 (73.286)	Acc@5 93.750 (95.161)
Epoch: [89][40/196]	Time 0.017 (0.017)	Data 0.002 (0.007)	Loss 1.8848 (1.9192)	Acc@1 74.609 (73.523)	Acc@5 96.094 (95.274)
Epoch: [89][50/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.9836 (1.9176)	Acc@1 67.578 (73.430)	Acc@5 96.484 (95.312)
Epoch: [89][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9455 (1.9220)	Acc@1 74.219 (73.399)	Acc@5 95.312 (95.248)
Epoch: [89][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0730 (1.9328)	Acc@1 71.484 (73.069)	Acc@5 91.406 (95.103)
Epoch: [89][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.8344 (1.9374)	Acc@1 76.562 (73.013)	Acc@5 94.922 (94.907)
Epoch: [89][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9778 (1.9441)	Acc@1 73.828 (72.944)	Acc@5 92.969 (94.875)
Epoch: [89][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0612 (1.9535)	Acc@1 70.703 (72.703)	Acc@5 93.359 (94.767)
Epoch: [89][110/196]	Time 0.014 (0.016)	Data 0.001 (0.004)	Loss 2.0204 (1.9586)	Acc@1 71.875 (72.603)	Acc@5 94.922 (94.686)
Epoch: [89][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8918 (1.9688)	Acc@1 74.609 (72.411)	Acc@5 94.141 (94.531)
Epoch: [89][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0810 (1.9738)	Acc@1 69.531 (72.278)	Acc@5 91.797 (94.451)
Epoch: [89][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9659 (1.9777)	Acc@1 71.875 (72.171)	Acc@5 94.141 (94.432)
Epoch: [89][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2006 (1.9824)	Acc@1 66.406 (71.986)	Acc@5 91.016 (94.358)
Epoch: [89][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0137 (1.9873)	Acc@1 73.438 (71.790)	Acc@5 92.578 (94.320)
Epoch: [89][170/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0894 (1.9921)	Acc@1 65.234 (71.722)	Acc@5 94.531 (94.257)
Epoch: [89][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1771 (1.9979)	Acc@1 65.234 (71.566)	Acc@5 94.141 (94.151)
Epoch: [89][190/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 2.1830 (2.0017)	Acc@1 68.750 (71.488)	Acc@5 90.625 (94.092)
num momentum params: 26
[0.1, 2.004858109283447, 1.769184980392456, 71.394, 54.58, tensor(0.5179, device='cuda:0', grad_fn=<DivBackward0>), 3.0310428142547607, 0.4050371646881104]
Non Pruning Epoch - module.conv1.weight: [49, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [49]
Non Pruning Epoch - module.bn1.bias: [49]
Non Pruning Epoch - module.conv2.weight: [128, 49, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [255, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [255]
Non Pruning Epoch - module.bn3.bias: [255]
Non Pruning Epoch - module.conv4.weight: [256, 255, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [489, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [489]
Non Pruning Epoch - module.bn7.bias: [489]
Non Pruning Epoch - module.conv8.weight: [359, 489, 3, 3]
Non Pruning Epoch - module.bn8.weight: [359]
Non Pruning Epoch - module.bn8.bias: [359]
Non Pruning Epoch - module.fc.weight: [100, 359]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [90 | 180] LR: 0.100000
module.conv1.weight [49, 3, 3, 3]
module.conv2.weight [128, 49, 3, 3]
module.conv3.weight [255, 128, 3, 3]
module.conv4.weight [256, 255, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [489, 509, 3, 3]
module.conv8.weight [359, 489, 3, 3]
Epoch: [90][0/196]	Time 0.055 (0.055)	Data 0.197 (0.197)	Loss 1.8193 (1.8193)	Acc@1 76.953 (76.953)	Acc@5 98.047 (98.047)
Epoch: [90][10/196]	Time 0.017 (0.020)	Data 0.002 (0.020)	Loss 1.8691 (1.9192)	Acc@1 76.953 (74.112)	Acc@5 94.922 (95.455)
Epoch: [90][20/196]	Time 0.015 (0.018)	Data 0.003 (0.011)	Loss 1.8640 (1.9219)	Acc@1 74.219 (73.196)	Acc@5 95.312 (95.499)
Epoch: [90][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.8544 (1.9136)	Acc@1 77.734 (73.526)	Acc@5 95.703 (95.514)
Epoch: [90][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.9785 (1.9124)	Acc@1 71.094 (73.571)	Acc@5 94.922 (95.303)
Epoch: [90][50/196]	Time 0.017 (0.016)	Data 0.002 (0.006)	Loss 2.0588 (1.9284)	Acc@1 66.797 (72.970)	Acc@5 94.922 (95.167)
Epoch: [90][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9596 (1.9426)	Acc@1 71.484 (72.656)	Acc@5 94.922 (94.999)
Epoch: [90][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0012 (1.9500)	Acc@1 71.875 (72.530)	Acc@5 94.531 (94.889)
Epoch: [90][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0626 (1.9542)	Acc@1 69.531 (72.487)	Acc@5 92.578 (94.830)
Epoch: [90][90/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1028 (1.9643)	Acc@1 66.406 (72.266)	Acc@5 92.578 (94.639)
Epoch: [90][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1607 (1.9746)	Acc@1 66.797 (72.022)	Acc@5 92.188 (94.535)
Epoch: [90][110/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0243 (1.9805)	Acc@1 71.875 (71.850)	Acc@5 93.359 (94.475)
Epoch: [90][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1855 (1.9900)	Acc@1 64.062 (71.530)	Acc@5 91.406 (94.325)
Epoch: [90][130/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1258 (1.9980)	Acc@1 66.797 (71.347)	Acc@5 91.406 (94.266)
Epoch: [90][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1773 (2.0055)	Acc@1 68.359 (71.146)	Acc@5 92.188 (94.166)
Epoch: [90][150/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 2.0787 (2.0131)	Acc@1 68.359 (70.936)	Acc@5 93.750 (94.086)
Epoch: [90][160/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0695 (2.0151)	Acc@1 68.359 (70.926)	Acc@5 93.359 (94.022)
Epoch: [90][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1495 (2.0201)	Acc@1 67.969 (70.790)	Acc@5 92.578 (93.953)
Epoch: [90][180/196]	Time 0.017 (0.015)	Data 0.002 (0.004)	Loss 2.1558 (2.0238)	Acc@1 69.531 (70.686)	Acc@5 89.844 (93.867)
Epoch: [90][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9789 (2.0272)	Acc@1 71.875 (70.552)	Acc@5 94.531 (93.832)
num momentum params: 26
[0.1, 2.03064184173584, 1.7699313902854918, 70.456, 55.22, tensor(0.5124, device='cuda:0', grad_fn=<DivBackward0>), 2.9721217155456543, 0.39110875129699707]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [49, 3, 3, 3]
Before - module.bn1.weight: [49]
Before - module.bn1.bias: [49]
Before - module.conv2.weight: [128, 49, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [255, 128, 3, 3]
Before - module.bn3.weight: [255]
Before - module.bn3.bias: [255]
Before - module.conv4.weight: [256, 255, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [509, 512, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [489, 509, 3, 3]
Before - module.bn7.weight: [489]
Before - module.bn7.bias: [489]
Before - module.conv8.weight: [359, 489, 3, 3]
Before - module.bn8.weight: [359]
Before - module.bn8.bias: [359]
Before - module.fc.weight: [100, 359]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [49, 3, 3, 3] >> [47, 3, 3, 3]
[module.bn1.weight]: 49 >> 47
running_mean [47]
running_var [47]
num_batches_tracked []
[module.conv2.weight]: [128, 49, 3, 3] >> [128, 47, 3, 3]
[module.conv3.weight]: [255, 128, 3, 3] >> [253, 128, 3, 3]
[module.bn3.weight]: 255 >> 253
running_mean [253]
running_var [253]
num_batches_tracked []
[module.conv4.weight]: [256, 255, 3, 3] >> [256, 253, 3, 3]
[module.conv7.weight]: [489, 509, 3, 3] >> [481, 509, 3, 3]
[module.bn7.weight]: 489 >> 481
running_mean [481]
running_var [481]
num_batches_tracked []
[module.conv8.weight]: [359, 489, 3, 3] >> [343, 481, 3, 3]
[module.bn8.weight]: 359 >> 343
running_mean [343]
running_var [343]
num_batches_tracked []
[module.fc.weight]: [100, 359] >> [100, 343]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [47, 3, 3, 3]
After - module.bn1.weight: [47]
After - module.bn1.bias: [47]
After - module.conv2.weight: [128, 47, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [253, 128, 3, 3]
After - module.bn3.weight: [253]
After - module.bn3.bias: [253]
After - module.conv4.weight: [256, 253, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [509, 512, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [481, 509, 3, 3]
After - module.bn7.weight: [481]
After - module.bn7.bias: [481]
After - module.conv8.weight: [343, 481, 3, 3]
After - module.bn8.weight: [343]
After - module.bn8.bias: [343]
After - module.fc.weight: [100, 343]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [47, 3, 3, 3]
conv2 --> [128, 47, 3, 3]
conv3 --> [253, 128, 3, 3]
conv4 --> [256, 253, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [509, 512, 3, 3]
conv7 --> [481, 509, 3, 3]
conv8 --> [343, 481, 3, 3]
fc --> [343, 100]
1, 520432128, 1299456, 47
2, 5793841152, 13860864, 128
3, 8505851904, 18653184, 253
4, 17011703808, 37306368, 256
5, 10267656192, 18874368, 512
6, 20414988288, 37527552, 509
7, 6769032192, 8813844, 481
8, 4561449984, 5939388, 343
fc, 13171200, 34300, 0
===================
FLOP REPORT: 28850830800000.0 53241600000.0 142309324 133104 2529 15.602838516235352
[INFO] Storing checkpoint...

Epoch: [91 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [91][0/196]	Time 0.500 (0.500)	Data 0.193 (0.193)	Loss 1.9087 (1.9087)	Acc@1 75.781 (75.781)	Acc@5 94.531 (94.531)
Epoch: [91][10/196]	Time 0.015 (0.060)	Data 0.002 (0.019)	Loss 1.8839 (1.9264)	Acc@1 75.000 (74.290)	Acc@5 96.094 (94.886)
Epoch: [91][20/196]	Time 0.014 (0.038)	Data 0.003 (0.011)	Loss 2.0106 (1.9418)	Acc@1 73.438 (73.717)	Acc@5 92.578 (94.736)
Epoch: [91][30/196]	Time 0.018 (0.031)	Data 0.000 (0.008)	Loss 1.9144 (1.9257)	Acc@1 75.391 (74.131)	Acc@5 95.312 (95.186)
Epoch: [91][40/196]	Time 0.017 (0.027)	Data 0.001 (0.007)	Loss 2.0452 (1.9313)	Acc@1 69.531 (73.752)	Acc@5 94.141 (95.151)
Epoch: [91][50/196]	Time 0.017 (0.025)	Data 0.000 (0.006)	Loss 1.8531 (1.9358)	Acc@1 75.391 (73.591)	Acc@5 95.703 (95.044)
Epoch: [91][60/196]	Time 0.015 (0.023)	Data 0.002 (0.006)	Loss 1.8938 (1.9378)	Acc@1 73.828 (73.585)	Acc@5 95.312 (94.935)
Epoch: [91][70/196]	Time 0.016 (0.022)	Data 0.000 (0.005)	Loss 2.0645 (1.9514)	Acc@1 68.750 (73.179)	Acc@5 93.359 (94.762)
Epoch: [91][80/196]	Time 0.017 (0.021)	Data 0.001 (0.005)	Loss 1.9971 (1.9589)	Acc@1 71.094 (72.994)	Acc@5 93.750 (94.604)
Epoch: [91][90/196]	Time 0.016 (0.021)	Data 0.001 (0.005)	Loss 1.8917 (1.9609)	Acc@1 73.047 (72.892)	Acc@5 95.312 (94.613)
Epoch: [91][100/196]	Time 0.016 (0.020)	Data 0.001 (0.005)	Loss 2.0417 (1.9655)	Acc@1 70.312 (72.753)	Acc@5 95.703 (94.566)
Epoch: [91][110/196]	Time 0.016 (0.020)	Data 0.001 (0.005)	Loss 2.0408 (1.9678)	Acc@1 73.047 (72.695)	Acc@5 92.578 (94.559)
Epoch: [91][120/196]	Time 0.016 (0.019)	Data 0.000 (0.005)	Loss 1.9055 (1.9668)	Acc@1 74.609 (72.718)	Acc@5 94.922 (94.564)
Epoch: [91][130/196]	Time 0.016 (0.019)	Data 0.000 (0.005)	Loss 2.2417 (1.9720)	Acc@1 65.234 (72.609)	Acc@5 91.406 (94.507)
Epoch: [91][140/196]	Time 0.016 (0.019)	Data 0.001 (0.005)	Loss 2.2448 (1.9798)	Acc@1 61.328 (72.307)	Acc@5 88.281 (94.404)
Epoch: [91][150/196]	Time 0.017 (0.018)	Data 0.000 (0.005)	Loss 2.1302 (1.9899)	Acc@1 68.750 (72.038)	Acc@5 92.578 (94.267)
Epoch: [91][160/196]	Time 0.016 (0.018)	Data 0.001 (0.005)	Loss 2.1384 (1.9993)	Acc@1 69.141 (71.788)	Acc@5 91.797 (94.184)
Epoch: [91][170/196]	Time 0.017 (0.018)	Data 0.001 (0.005)	Loss 1.9710 (2.0070)	Acc@1 74.609 (71.592)	Acc@5 94.531 (94.081)
Epoch: [91][180/196]	Time 0.016 (0.018)	Data 0.000 (0.005)	Loss 2.1159 (2.0134)	Acc@1 69.531 (71.424)	Acc@5 92.188 (94.031)
Epoch: [91][190/196]	Time 0.016 (0.018)	Data 0.000 (0.005)	Loss 2.1577 (2.0165)	Acc@1 65.625 (71.300)	Acc@5 91.016 (93.977)
num momentum params: 26
[0.1, 2.018213466720581, 1.6945687651634216, 71.286, 57.55, tensor(0.5161, device='cuda:0', grad_fn=<DivBackward0>), 3.6718218326568604, 0.4661076068878174]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [92 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [92][0/196]	Time 0.056 (0.056)	Data 0.198 (0.198)	Loss 1.8729 (1.8729)	Acc@1 75.781 (75.781)	Acc@5 94.141 (94.141)
Epoch: [92][10/196]	Time 0.015 (0.020)	Data 0.002 (0.020)	Loss 1.8896 (1.9235)	Acc@1 73.047 (73.970)	Acc@5 94.922 (95.206)
Epoch: [92][20/196]	Time 0.016 (0.018)	Data 0.002 (0.011)	Loss 1.8663 (1.9235)	Acc@1 73.828 (73.754)	Acc@5 97.266 (95.182)
Epoch: [92][30/196]	Time 0.013 (0.017)	Data 0.004 (0.009)	Loss 1.8594 (1.9203)	Acc@1 76.953 (73.929)	Acc@5 94.922 (95.123)
Epoch: [92][40/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.9310 (1.9273)	Acc@1 72.656 (73.657)	Acc@5 95.312 (95.179)
Epoch: [92][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9257 (1.9300)	Acc@1 70.312 (73.545)	Acc@5 94.922 (95.106)
Epoch: [92][60/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 1.9926 (1.9408)	Acc@1 75.000 (73.367)	Acc@5 94.141 (94.915)
Epoch: [92][70/196]	Time 0.020 (0.016)	Data 0.001 (0.006)	Loss 1.9180 (1.9388)	Acc@1 70.312 (73.327)	Acc@5 96.484 (94.927)
Epoch: [92][80/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0519 (1.9392)	Acc@1 69.141 (73.331)	Acc@5 92.969 (94.888)
Epoch: [92][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9338 (1.9405)	Acc@1 74.219 (73.240)	Acc@5 95.312 (94.879)
Epoch: [92][100/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.0255 (1.9426)	Acc@1 71.484 (73.167)	Acc@5 94.141 (94.845)
Epoch: [92][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9548 (1.9467)	Acc@1 71.484 (73.089)	Acc@5 95.312 (94.778)
Epoch: [92][120/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 2.0680 (1.9536)	Acc@1 72.266 (72.982)	Acc@5 91.406 (94.677)
Epoch: [92][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0607 (1.9596)	Acc@1 69.141 (72.731)	Acc@5 91.016 (94.621)
Epoch: [92][140/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 2.0328 (1.9661)	Acc@1 69.531 (72.612)	Acc@5 93.750 (94.495)
Epoch: [92][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0045 (1.9736)	Acc@1 72.656 (72.356)	Acc@5 93.359 (94.376)
Epoch: [92][160/196]	Time 0.012 (0.015)	Data 0.013 (0.005)	Loss 2.0066 (1.9792)	Acc@1 70.312 (72.205)	Acc@5 94.141 (94.286)
Epoch: [92][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0353 (1.9852)	Acc@1 69.531 (72.046)	Acc@5 93.750 (94.216)
Epoch: [92][180/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 2.1880 (1.9919)	Acc@1 63.672 (71.836)	Acc@5 92.188 (94.167)
Epoch: [92][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9455 (1.9987)	Acc@1 75.000 (71.670)	Acc@5 94.141 (94.120)
num momentum params: 26
[0.1, 2.0009268316650393, 2.1998124849796294, 71.614, 49.02, tensor(0.5204, device='cuda:0', grad_fn=<DivBackward0>), 3.024806499481201, 0.3996279239654541]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [93 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [93][0/196]	Time 0.050 (0.050)	Data 0.188 (0.188)	Loss 2.0012 (2.0012)	Acc@1 71.094 (71.094)	Acc@5 93.359 (93.359)
Epoch: [93][10/196]	Time 0.019 (0.019)	Data 0.002 (0.019)	Loss 1.9603 (2.0014)	Acc@1 70.312 (71.271)	Acc@5 94.141 (94.460)
Epoch: [93][20/196]	Time 0.012 (0.017)	Data 0.004 (0.011)	Loss 1.9590 (1.9515)	Acc@1 73.828 (72.861)	Acc@5 93.750 (94.866)
Epoch: [93][30/196]	Time 0.015 (0.016)	Data 0.003 (0.008)	Loss 1.8010 (1.9288)	Acc@1 75.781 (73.450)	Acc@5 96.094 (94.997)
Epoch: [93][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.8522 (1.9170)	Acc@1 73.438 (73.657)	Acc@5 95.703 (95.179)
Epoch: [93][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.8712 (1.9138)	Acc@1 76.562 (73.759)	Acc@5 94.531 (95.190)
Epoch: [93][60/196]	Time 0.014 (0.016)	Data 0.005 (0.006)	Loss 2.0038 (1.9170)	Acc@1 72.266 (73.770)	Acc@5 92.578 (95.056)
Epoch: [93][70/196]	Time 0.015 (0.016)	Data 0.004 (0.005)	Loss 1.9051 (1.9234)	Acc@1 73.828 (73.619)	Acc@5 95.312 (95.010)
Epoch: [93][80/196]	Time 0.015 (0.016)	Data 0.005 (0.005)	Loss 1.8755 (1.9293)	Acc@1 73.828 (73.529)	Acc@5 96.094 (94.946)
Epoch: [93][90/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.8750 (1.9439)	Acc@1 78.516 (73.103)	Acc@5 96.094 (94.776)
Epoch: [93][100/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 1.9240 (1.9516)	Acc@1 74.219 (72.908)	Acc@5 94.141 (94.686)
Epoch: [93][110/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.0135 (1.9564)	Acc@1 71.875 (72.783)	Acc@5 91.797 (94.637)
Epoch: [93][120/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 1.9209 (1.9621)	Acc@1 73.047 (72.601)	Acc@5 94.141 (94.615)
Epoch: [93][130/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0575 (1.9666)	Acc@1 69.531 (72.453)	Acc@5 92.578 (94.627)
Epoch: [93][140/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 1.8430 (1.9697)	Acc@1 75.781 (72.324)	Acc@5 96.094 (94.623)
Epoch: [93][150/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1393 (1.9747)	Acc@1 69.922 (72.219)	Acc@5 91.797 (94.557)
Epoch: [93][160/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 1.9181 (1.9778)	Acc@1 75.391 (72.072)	Acc@5 94.922 (94.553)
Epoch: [93][170/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 1.9950 (1.9841)	Acc@1 75.781 (71.971)	Acc@5 94.141 (94.463)
Epoch: [93][180/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.0602 (1.9902)	Acc@1 69.531 (71.836)	Acc@5 93.750 (94.404)
Epoch: [93][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1121 (1.9974)	Acc@1 69.922 (71.681)	Acc@5 93.359 (94.310)
num momentum params: 26
[0.1, 1.9995279844665528, 1.8823736345767974, 71.642, 54.13, tensor(0.5206, device='cuda:0', grad_fn=<DivBackward0>), 2.996640920639038, 0.39394593238830566]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [94 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [94][0/196]	Time 0.048 (0.048)	Data 0.190 (0.190)	Loss 1.8985 (1.8985)	Acc@1 73.047 (73.047)	Acc@5 96.094 (96.094)
Epoch: [94][10/196]	Time 0.017 (0.019)	Data 0.001 (0.020)	Loss 1.8984 (1.9550)	Acc@1 73.438 (72.443)	Acc@5 94.531 (95.241)
Epoch: [94][20/196]	Time 0.014 (0.018)	Data 0.002 (0.011)	Loss 2.0355 (1.9521)	Acc@1 68.359 (72.507)	Acc@5 95.312 (95.164)
Epoch: [94][30/196]	Time 0.018 (0.017)	Data 0.001 (0.009)	Loss 1.9436 (1.9381)	Acc@1 71.875 (73.110)	Acc@5 95.703 (95.212)
Epoch: [94][40/196]	Time 0.017 (0.017)	Data 0.002 (0.007)	Loss 1.7526 (1.9341)	Acc@1 80.469 (73.190)	Acc@5 96.094 (95.122)
Epoch: [94][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.7917 (1.9290)	Acc@1 78.125 (73.407)	Acc@5 97.656 (95.067)
Epoch: [94][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0716 (1.9340)	Acc@1 72.266 (73.380)	Acc@5 94.531 (94.992)
Epoch: [94][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.9717 (1.9407)	Acc@1 71.875 (73.173)	Acc@5 93.750 (94.911)
Epoch: [94][80/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.0431 (1.9519)	Acc@1 71.875 (72.849)	Acc@5 94.922 (94.743)
Epoch: [94][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.1381 (1.9586)	Acc@1 66.406 (72.605)	Acc@5 92.578 (94.733)
Epoch: [94][100/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9302 (1.9667)	Acc@1 75.781 (72.463)	Acc@5 93.750 (94.663)
Epoch: [94][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0431 (1.9719)	Acc@1 72.266 (72.332)	Acc@5 95.312 (94.570)
Epoch: [94][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0442 (1.9774)	Acc@1 73.047 (72.182)	Acc@5 93.750 (94.522)
Epoch: [94][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1364 (1.9845)	Acc@1 70.312 (72.054)	Acc@5 92.969 (94.436)
Epoch: [94][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9913 (1.9870)	Acc@1 71.875 (71.925)	Acc@5 96.484 (94.409)
Epoch: [94][150/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1315 (1.9896)	Acc@1 69.531 (71.852)	Acc@5 89.844 (94.366)
Epoch: [94][160/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0044 (1.9955)	Acc@1 73.438 (71.742)	Acc@5 94.141 (94.313)
Epoch: [94][170/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0029 (1.9999)	Acc@1 68.359 (71.589)	Acc@5 96.484 (94.262)
Epoch: [94][180/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0702 (2.0048)	Acc@1 69.922 (71.469)	Acc@5 93.750 (94.199)
Epoch: [94][190/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0743 (2.0068)	Acc@1 68.359 (71.360)	Acc@5 91.797 (94.149)
num momentum params: 26
[0.1, 2.0078026400375366, 1.736952395439148, 71.356, 56.05, tensor(0.5192, device='cuda:0', grad_fn=<DivBackward0>), 3.082493543624878, 0.39653491973876953]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [95 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [95][0/196]	Time 0.054 (0.054)	Data 0.182 (0.182)	Loss 1.8471 (1.8471)	Acc@1 76.562 (76.562)	Acc@5 95.703 (95.703)
Epoch: [95][10/196]	Time 0.015 (0.019)	Data 0.003 (0.019)	Loss 1.8987 (1.8874)	Acc@1 74.609 (74.751)	Acc@5 95.312 (95.277)
Epoch: [95][20/196]	Time 0.014 (0.017)	Data 0.004 (0.011)	Loss 1.9381 (1.8867)	Acc@1 73.047 (74.870)	Acc@5 95.703 (95.238)
Epoch: [95][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 2.0009 (1.8904)	Acc@1 69.141 (74.609)	Acc@5 96.094 (95.439)
Epoch: [95][40/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 1.9387 (1.8995)	Acc@1 72.656 (74.457)	Acc@5 96.875 (95.370)
Epoch: [95][50/196]	Time 0.014 (0.016)	Data 0.002 (0.007)	Loss 2.0148 (1.9078)	Acc@1 72.656 (74.119)	Acc@5 93.750 (95.274)
Epoch: [95][60/196]	Time 0.015 (0.016)	Data 0.000 (0.006)	Loss 1.7435 (1.9017)	Acc@1 78.516 (74.347)	Acc@5 97.266 (95.312)
Epoch: [95][70/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.9321 (1.9084)	Acc@1 69.531 (74.037)	Acc@5 94.531 (95.263)
Epoch: [95][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.8755 (1.9182)	Acc@1 75.781 (73.732)	Acc@5 97.266 (95.202)
Epoch: [95][90/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0011 (1.9251)	Acc@1 69.531 (73.553)	Acc@5 96.875 (95.145)
Epoch: [95][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.8051 (1.9295)	Acc@1 75.000 (73.387)	Acc@5 96.875 (95.123)
Epoch: [95][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1468 (1.9346)	Acc@1 67.578 (73.244)	Acc@5 92.578 (95.024)
Epoch: [95][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.8458 (1.9406)	Acc@1 75.781 (73.105)	Acc@5 96.484 (94.957)
Epoch: [95][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.8693 (1.9444)	Acc@1 77.344 (72.981)	Acc@5 94.531 (94.877)
Epoch: [95][140/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0909 (1.9518)	Acc@1 68.359 (72.814)	Acc@5 95.312 (94.797)
Epoch: [95][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0510 (1.9602)	Acc@1 67.578 (72.573)	Acc@5 95.312 (94.702)
Epoch: [95][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0942 (1.9677)	Acc@1 67.578 (72.433)	Acc@5 93.359 (94.604)
Epoch: [95][170/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0224 (1.9746)	Acc@1 67.578 (72.234)	Acc@5 97.266 (94.522)
Epoch: [95][180/196]	Time 0.011 (0.015)	Data 0.001 (0.004)	Loss 2.0695 (1.9806)	Acc@1 69.531 (72.061)	Acc@5 91.797 (94.423)
Epoch: [95][190/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1792 (1.9870)	Acc@1 66.406 (71.857)	Acc@5 93.359 (94.351)
num momentum params: 26
[0.1, 1.9888460089874267, 1.8021889507770539, 71.828, 54.96, tensor(0.5240, device='cuda:0', grad_fn=<DivBackward0>), 2.992305278778076, 0.39187264442443853]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [96 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [96][0/196]	Time 0.054 (0.054)	Data 0.196 (0.196)	Loss 1.8705 (1.8705)	Acc@1 75.000 (75.000)	Acc@5 95.703 (95.703)
Epoch: [96][10/196]	Time 0.016 (0.019)	Data 0.001 (0.020)	Loss 1.8930 (1.9377)	Acc@1 75.000 (73.118)	Acc@5 95.703 (95.312)
Epoch: [96][20/196]	Time 0.012 (0.017)	Data 0.009 (0.012)	Loss 1.9371 (1.9646)	Acc@1 72.656 (72.489)	Acc@5 95.703 (94.978)
Epoch: [96][30/196]	Time 0.016 (0.017)	Data 0.001 (0.009)	Loss 1.9723 (1.9508)	Acc@1 70.312 (72.833)	Acc@5 95.703 (94.909)
Epoch: [96][40/196]	Time 0.012 (0.017)	Data 0.013 (0.008)	Loss 1.9591 (1.9440)	Acc@1 69.141 (72.742)	Acc@5 97.266 (95.103)
Epoch: [96][50/196]	Time 0.017 (0.017)	Data 0.001 (0.007)	Loss 1.8581 (1.9461)	Acc@1 74.219 (72.710)	Acc@5 97.266 (95.075)
Epoch: [96][60/196]	Time 0.012 (0.016)	Data 0.018 (0.007)	Loss 2.0564 (1.9537)	Acc@1 67.969 (72.509)	Acc@5 94.531 (95.050)
Epoch: [96][70/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.9126 (1.9596)	Acc@1 75.781 (72.337)	Acc@5 94.141 (94.933)
Epoch: [96][80/196]	Time 0.012 (0.016)	Data 0.018 (0.006)	Loss 1.8522 (1.9670)	Acc@1 76.172 (72.150)	Acc@5 96.094 (94.821)
Epoch: [96][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9169 (1.9706)	Acc@1 73.047 (71.991)	Acc@5 94.922 (94.789)
Epoch: [96][100/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 1.9839 (1.9683)	Acc@1 69.922 (72.053)	Acc@5 94.141 (94.744)
Epoch: [96][110/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9180 (1.9717)	Acc@1 76.562 (71.959)	Acc@5 93.750 (94.718)
Epoch: [96][120/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 1.8548 (1.9763)	Acc@1 77.344 (71.856)	Acc@5 95.312 (94.615)
Epoch: [96][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0512 (1.9825)	Acc@1 69.531 (71.684)	Acc@5 92.969 (94.522)
Epoch: [96][140/196]	Time 0.011 (0.016)	Data 0.013 (0.005)	Loss 2.0809 (1.9920)	Acc@1 67.969 (71.398)	Acc@5 92.969 (94.357)
Epoch: [96][150/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1538 (2.0031)	Acc@1 66.797 (71.169)	Acc@5 92.188 (94.221)
Epoch: [96][160/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0587 (2.0074)	Acc@1 71.875 (71.111)	Acc@5 91.406 (94.165)
Epoch: [96][170/196]	Time 0.027 (0.016)	Data 0.001 (0.005)	Loss 2.0594 (2.0123)	Acc@1 68.359 (70.989)	Acc@5 95.312 (94.154)
Epoch: [96][180/196]	Time 0.016 (0.016)	Data 0.021 (0.005)	Loss 2.1292 (2.0162)	Acc@1 70.312 (70.912)	Acc@5 92.578 (94.080)
Epoch: [96][190/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1014 (2.0205)	Acc@1 68.359 (70.785)	Acc@5 94.531 (94.051)
num momentum params: 26
[0.1, 2.022357006378174, 1.9276160788536072, 70.766, 51.95, tensor(0.5159, device='cuda:0', grad_fn=<DivBackward0>), 3.1021857261657715, 0.39873385429382324]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [97 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [97][0/196]	Time 0.059 (0.059)	Data 0.191 (0.191)	Loss 1.8975 (1.8975)	Acc@1 75.391 (75.391)	Acc@5 94.531 (94.531)
Epoch: [97][10/196]	Time 0.014 (0.019)	Data 0.003 (0.019)	Loss 1.8370 (1.9745)	Acc@1 75.000 (73.189)	Acc@5 97.266 (94.460)
Epoch: [97][20/196]	Time 0.014 (0.017)	Data 0.005 (0.011)	Loss 2.1093 (1.9790)	Acc@1 70.312 (72.433)	Acc@5 94.141 (94.624)
Epoch: [97][30/196]	Time 0.018 (0.017)	Data 0.004 (0.009)	Loss 1.9752 (1.9541)	Acc@1 71.484 (72.984)	Acc@5 94.531 (94.972)
Epoch: [97][40/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 1.8998 (1.9414)	Acc@1 74.219 (73.466)	Acc@5 96.094 (95.093)
Epoch: [97][50/196]	Time 0.012 (0.016)	Data 0.018 (0.007)	Loss 1.8738 (1.9251)	Acc@1 74.609 (73.866)	Acc@5 95.703 (95.190)
Epoch: [97][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.8719 (1.9263)	Acc@1 75.391 (73.822)	Acc@5 96.484 (95.127)
Epoch: [97][70/196]	Time 0.012 (0.016)	Data 0.018 (0.006)	Loss 1.7515 (1.9171)	Acc@1 77.734 (73.977)	Acc@5 96.484 (95.235)
Epoch: [97][80/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9179 (1.9252)	Acc@1 73.828 (73.736)	Acc@5 96.484 (95.177)
Epoch: [97][90/196]	Time 0.013 (0.016)	Data 0.020 (0.006)	Loss 1.8785 (1.9234)	Acc@1 73.047 (73.721)	Acc@5 96.094 (95.205)
Epoch: [97][100/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9099 (1.9291)	Acc@1 73.828 (73.646)	Acc@5 95.703 (95.142)
Epoch: [97][110/196]	Time 0.013 (0.016)	Data 0.020 (0.006)	Loss 1.9206 (1.9362)	Acc@1 72.656 (73.402)	Acc@5 96.094 (95.087)
Epoch: [97][120/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0864 (1.9483)	Acc@1 69.922 (73.086)	Acc@5 93.359 (94.935)
Epoch: [97][130/196]	Time 0.012 (0.016)	Data 0.017 (0.006)	Loss 1.9774 (1.9556)	Acc@1 71.875 (72.934)	Acc@5 94.922 (94.818)
Epoch: [97][140/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1101 (1.9615)	Acc@1 64.062 (72.786)	Acc@5 94.141 (94.689)
Epoch: [97][150/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.1157 (1.9682)	Acc@1 66.797 (72.553)	Acc@5 93.359 (94.578)
Epoch: [97][160/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9310 (1.9737)	Acc@1 71.484 (72.392)	Acc@5 96.875 (94.536)
Epoch: [97][170/196]	Time 0.013 (0.016)	Data 0.014 (0.005)	Loss 2.1413 (1.9818)	Acc@1 66.797 (72.133)	Acc@5 93.359 (94.479)
Epoch: [97][180/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1373 (1.9907)	Acc@1 67.969 (71.931)	Acc@5 92.969 (94.356)
Epoch: [97][190/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.1961 (1.9996)	Acc@1 65.625 (71.709)	Acc@5 90.234 (94.227)
num momentum params: 26
[0.1, 2.0018495503234863, 1.9054516243934632, 71.66, 52.98, tensor(0.5213, device='cuda:0', grad_fn=<DivBackward0>), 3.027419328689575, 0.39239501953125006]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [98 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [98][0/196]	Time 0.053 (0.053)	Data 0.183 (0.183)	Loss 1.9686 (1.9686)	Acc@1 71.875 (71.875)	Acc@5 96.094 (96.094)
Epoch: [98][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 1.9180 (1.9381)	Acc@1 74.609 (72.798)	Acc@5 93.750 (95.419)
Epoch: [98][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.8564 (1.9094)	Acc@1 73.828 (73.661)	Acc@5 96.484 (95.480)
Epoch: [98][30/196]	Time 0.016 (0.017)	Data 0.001 (0.008)	Loss 1.7559 (1.8977)	Acc@1 78.516 (74.496)	Acc@5 98.047 (95.590)
Epoch: [98][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.7481 (1.8867)	Acc@1 79.688 (74.581)	Acc@5 94.922 (95.522)
Epoch: [98][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0341 (1.8916)	Acc@1 71.875 (74.372)	Acc@5 92.188 (95.466)
Epoch: [98][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8976 (1.9059)	Acc@1 72.656 (73.995)	Acc@5 96.094 (95.293)
Epoch: [98][70/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9183 (1.9157)	Acc@1 73.828 (73.718)	Acc@5 95.703 (95.235)
Epoch: [98][80/196]	Time 0.013 (0.016)	Data 0.002 (0.005)	Loss 1.9356 (1.9272)	Acc@1 73.047 (73.356)	Acc@5 95.703 (95.110)
Epoch: [98][90/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0157 (1.9390)	Acc@1 71.094 (73.017)	Acc@5 93.750 (95.055)
Epoch: [98][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0161 (1.9444)	Acc@1 74.609 (72.896)	Acc@5 92.578 (94.984)
Epoch: [98][110/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.1485 (1.9493)	Acc@1 66.797 (72.860)	Acc@5 92.969 (94.908)
Epoch: [98][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 1.9318 (1.9556)	Acc@1 73.438 (72.727)	Acc@5 95.703 (94.864)
Epoch: [98][130/196]	Time 0.013 (0.015)	Data 0.006 (0.004)	Loss 2.1109 (1.9610)	Acc@1 68.359 (72.644)	Acc@5 92.188 (94.776)
Epoch: [98][140/196]	Time 0.011 (0.015)	Data 0.005 (0.004)	Loss 2.1122 (1.9680)	Acc@1 68.359 (72.421)	Acc@5 92.578 (94.697)
Epoch: [98][150/196]	Time 0.013 (0.015)	Data 0.009 (0.004)	Loss 2.0929 (1.9773)	Acc@1 71.875 (72.183)	Acc@5 93.359 (94.609)
Epoch: [98][160/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.0836 (1.9819)	Acc@1 66.406 (72.057)	Acc@5 93.359 (94.524)
Epoch: [98][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0454 (1.9859)	Acc@1 73.438 (71.980)	Acc@5 93.359 (94.476)
Epoch: [98][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.0666 (1.9939)	Acc@1 69.141 (71.774)	Acc@5 93.359 (94.354)
Epoch: [98][190/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0687 (1.9990)	Acc@1 70.312 (71.691)	Acc@5 93.750 (94.265)
num momentum params: 26
[0.1, 2.0022174131774904, 1.9786074924468995, 71.6, 51.19, tensor(0.5217, device='cuda:0', grad_fn=<DivBackward0>), 2.881171226501465, 0.4057483673095703]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [99 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [99][0/196]	Time 0.064 (0.064)	Data 0.183 (0.183)	Loss 1.8071 (1.8071)	Acc@1 78.516 (78.516)	Acc@5 97.266 (97.266)
Epoch: [99][10/196]	Time 0.015 (0.020)	Data 0.002 (0.019)	Loss 1.8095 (1.9607)	Acc@1 74.609 (72.372)	Acc@5 98.047 (94.780)
Epoch: [99][20/196]	Time 0.014 (0.018)	Data 0.004 (0.011)	Loss 1.8826 (1.9346)	Acc@1 75.000 (73.270)	Acc@5 93.750 (95.145)
Epoch: [99][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9259 (1.9151)	Acc@1 73.828 (73.614)	Acc@5 96.484 (95.439)
Epoch: [99][40/196]	Time 0.012 (0.016)	Data 0.004 (0.007)	Loss 1.8541 (1.8999)	Acc@1 77.344 (73.962)	Acc@5 94.141 (95.665)
Epoch: [99][50/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 2.0744 (1.9111)	Acc@1 70.703 (73.782)	Acc@5 92.969 (95.389)
Epoch: [99][60/196]	Time 0.014 (0.016)	Data 0.012 (0.006)	Loss 2.1210 (1.9251)	Acc@1 64.844 (73.309)	Acc@5 91.797 (95.255)
Epoch: [99][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8758 (1.9320)	Acc@1 75.000 (73.080)	Acc@5 96.484 (95.208)
Epoch: [99][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0557 (1.9405)	Acc@1 70.703 (72.873)	Acc@5 94.922 (95.095)
Epoch: [99][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0110 (1.9470)	Acc@1 73.047 (72.789)	Acc@5 91.797 (94.926)
Epoch: [99][100/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.9562 (1.9508)	Acc@1 73.047 (72.676)	Acc@5 93.750 (94.895)
Epoch: [99][110/196]	Time 0.021 (0.016)	Data 0.001 (0.004)	Loss 1.9942 (1.9510)	Acc@1 71.094 (72.684)	Acc@5 94.531 (94.880)
Epoch: [99][120/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 1.8699 (1.9512)	Acc@1 75.000 (72.634)	Acc@5 95.703 (94.861)
Epoch: [99][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8860 (1.9570)	Acc@1 76.562 (72.531)	Acc@5 96.094 (94.782)
Epoch: [99][140/196]	Time 0.013 (0.016)	Data 0.019 (0.004)	Loss 2.0363 (1.9639)	Acc@1 69.141 (72.371)	Acc@5 97.266 (94.714)
Epoch: [99][150/196]	Time 0.015 (0.016)	Data 0.001 (0.004)	Loss 2.0226 (1.9711)	Acc@1 69.922 (72.235)	Acc@5 92.969 (94.591)
Epoch: [99][160/196]	Time 0.016 (0.016)	Data 0.008 (0.004)	Loss 2.1217 (1.9780)	Acc@1 64.453 (72.052)	Acc@5 92.188 (94.517)
Epoch: [99][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0398 (1.9861)	Acc@1 71.875 (71.859)	Acc@5 92.188 (94.422)
Epoch: [99][180/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 2.1209 (1.9924)	Acc@1 69.141 (71.631)	Acc@5 94.531 (94.322)
Epoch: [99][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1722 (1.9971)	Acc@1 67.188 (71.464)	Acc@5 91.797 (94.263)
num momentum params: 26
[0.1, 1.9990505406188965, 1.882982929944992, 71.414, 54.33, tensor(0.5226, device='cuda:0', grad_fn=<DivBackward0>), 3.0554986000061035, 0.3959383964538574]
Non Pruning Epoch - module.conv1.weight: [47, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [47]
Non Pruning Epoch - module.bn1.bias: [47]
Non Pruning Epoch - module.conv2.weight: [128, 47, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [509, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [481, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [343, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [343]
Non Pruning Epoch - module.bn8.bias: [343]
Non Pruning Epoch - module.fc.weight: [100, 343]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [100 | 180] LR: 0.100000
module.conv1.weight [47, 3, 3, 3]
module.conv2.weight [128, 47, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [509, 512, 3, 3]
module.conv7.weight [481, 509, 3, 3]
module.conv8.weight [343, 481, 3, 3]
Epoch: [100][0/196]	Time 0.057 (0.057)	Data 0.186 (0.186)	Loss 1.8853 (1.8853)	Acc@1 73.828 (73.828)	Acc@5 94.531 (94.531)
Epoch: [100][10/196]	Time 0.013 (0.019)	Data 0.004 (0.019)	Loss 1.9227 (2.0093)	Acc@1 73.438 (70.987)	Acc@5 94.922 (94.389)
Epoch: [100][20/196]	Time 0.015 (0.018)	Data 0.004 (0.011)	Loss 2.0063 (1.9963)	Acc@1 70.312 (71.596)	Acc@5 93.359 (94.327)
Epoch: [100][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.8947 (1.9738)	Acc@1 74.219 (72.177)	Acc@5 94.531 (94.493)
Epoch: [100][40/196]	Time 0.015 (0.017)	Data 0.004 (0.007)	Loss 1.8238 (1.9551)	Acc@1 78.125 (72.923)	Acc@5 94.531 (94.560)
Epoch: [100][50/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 1.9117 (1.9437)	Acc@1 75.000 (73.376)	Acc@5 94.141 (94.654)
Epoch: [100][60/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 2.0271 (1.9408)	Acc@1 72.656 (73.373)	Acc@5 94.922 (94.775)
Epoch: [100][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9165 (1.9432)	Acc@1 73.828 (73.256)	Acc@5 96.094 (94.751)
Epoch: [100][80/196]	Time 0.013 (0.016)	Data 0.018 (0.005)	Loss 1.9899 (1.9462)	Acc@1 72.656 (73.105)	Acc@5 92.578 (94.729)
Epoch: [100][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9655 (1.9511)	Acc@1 69.531 (73.013)	Acc@5 94.922 (94.703)
Epoch: [100][100/196]	Time 0.012 (0.015)	Data 0.018 (0.005)	Loss 2.0928 (1.9576)	Acc@1 69.922 (72.846)	Acc@5 92.969 (94.640)
Epoch: [100][110/196]	Time 0.013 (0.016)	Data 0.001 (0.005)	Loss 2.1307 (1.9675)	Acc@1 69.141 (72.561)	Acc@5 91.797 (94.535)
Epoch: [100][120/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.0084 (1.9734)	Acc@1 69.922 (72.424)	Acc@5 94.141 (94.476)
Epoch: [100][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.2029 (1.9810)	Acc@1 64.844 (72.149)	Acc@5 92.188 (94.406)
Epoch: [100][140/196]	Time 0.013 (0.015)	Data 0.011 (0.005)	Loss 2.0106 (1.9833)	Acc@1 72.656 (72.066)	Acc@5 93.359 (94.396)
Epoch: [100][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.2217 (1.9893)	Acc@1 64.062 (71.909)	Acc@5 90.234 (94.329)
Epoch: [100][160/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.9808 (1.9923)	Acc@1 74.219 (71.848)	Acc@5 92.969 (94.315)
Epoch: [100][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1298 (1.9963)	Acc@1 65.625 (71.672)	Acc@5 93.359 (94.314)
Epoch: [100][180/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9882 (1.9981)	Acc@1 67.969 (71.594)	Acc@5 95.703 (94.294)
Epoch: [100][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2300 (2.0039)	Acc@1 66.797 (71.466)	Acc@5 89.062 (94.192)
num momentum params: 26
[0.1, 2.006566015701294, 1.8845497870445251, 71.398, 52.93, tensor(0.5203, device='cuda:0', grad_fn=<DivBackward0>), 3.0086333751678467, 0.39391589164733887]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [47, 3, 3, 3]
Before - module.bn1.weight: [47]
Before - module.bn1.bias: [47]
Before - module.conv2.weight: [128, 47, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [253, 128, 3, 3]
Before - module.bn3.weight: [253]
Before - module.bn3.bias: [253]
Before - module.conv4.weight: [256, 253, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [509, 512, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [481, 509, 3, 3]
Before - module.bn7.weight: [481]
Before - module.bn7.bias: [481]
Before - module.conv8.weight: [343, 481, 3, 3]
Before - module.bn8.weight: [343]
Before - module.bn8.bias: [343]
Before - module.fc.weight: [100, 343]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [47, 3, 3, 3] >> [45, 3, 3, 3]
[module.bn1.weight]: 47 >> 45
running_mean [45]
running_var [45]
num_batches_tracked []
[module.conv2.weight]: [128, 47, 3, 3] >> [128, 45, 3, 3]
[module.conv3.weight]: [253, 128, 3, 3] >> [252, 128, 3, 3]
[module.bn3.weight]: 253 >> 252
running_mean [252]
running_var [252]
num_batches_tracked []
[module.conv4.weight]: [256, 253, 3, 3] >> [256, 252, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [511, 256, 3, 3]
[module.bn5.weight]: 512 >> 511
running_mean [511]
running_var [511]
num_batches_tracked []
[module.conv6.weight]: [509, 512, 3, 3] >> [509, 511, 3, 3]
[module.conv7.weight]: [481, 509, 3, 3] >> [477, 509, 3, 3]
[module.bn7.weight]: 481 >> 477
running_mean [477]
running_var [477]
num_batches_tracked []
[module.conv8.weight]: [343, 481, 3, 3] >> [323, 477, 3, 3]
[module.bn8.weight]: 343 >> 323
running_mean [323]
running_var [323]
num_batches_tracked []
[module.fc.weight]: [100, 343] >> [100, 323]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [45, 3, 3, 3]
After - module.bn1.weight: [45]
After - module.bn1.bias: [45]
After - module.conv2.weight: [128, 45, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [252, 128, 3, 3]
After - module.bn3.weight: [252]
After - module.bn3.bias: [252]
After - module.conv4.weight: [256, 252, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [511, 256, 3, 3]
After - module.bn5.weight: [511]
After - module.bn5.bias: [511]
After - module.conv6.weight: [509, 511, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [477, 509, 3, 3]
After - module.bn7.weight: [477]
After - module.bn7.bias: [477]
After - module.conv8.weight: [323, 477, 3, 3]
After - module.bn8.weight: [323]
After - module.bn8.bias: [323]
After - module.fc.weight: [100, 323]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [128, 45, 3, 3]
conv3 --> [252, 128, 3, 3]
conv4 --> [256, 252, 3, 3]
conv5 --> [511, 256, 3, 3]
conv6 --> [509, 511, 3, 3]
conv7 --> [477, 509, 3, 3]
conv8 --> [323, 477, 3, 3]
fc --> [323, 100]
1, 498286080, 1244160, 45
2, 5547294720, 13271040, 128
3, 8472231936, 18579456, 252
4, 16944463872, 37158912, 256
5, 10247602176, 18837504, 511
6, 20375115264, 37454256, 509
7, 6712740864, 8740548, 477
8, 4259755008, 5546556, 323
fc, 12403200, 32300, 0
===================
FLOP REPORT: 28542927000000.0 52352000000.0 140864732 130880 2501 15.35244369506836
[INFO] Storing checkpoint...

Epoch: [101 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [101][0/196]	Time 0.755 (0.755)	Data 0.195 (0.195)	Loss 1.7845 (1.7845)	Acc@1 76.562 (76.562)	Acc@5 95.312 (95.312)
Epoch: [101][10/196]	Time 0.014 (0.083)	Data 0.002 (0.019)	Loss 1.9398 (1.9186)	Acc@1 74.609 (73.722)	Acc@5 94.531 (94.922)
Epoch: [101][20/196]	Time 0.015 (0.051)	Data 0.003 (0.011)	Loss 1.9539 (1.9105)	Acc@1 73.047 (73.996)	Acc@5 93.359 (95.015)
Epoch: [101][30/196]	Time 0.014 (0.040)	Data 0.004 (0.008)	Loss 1.8838 (1.9282)	Acc@1 73.438 (73.173)	Acc@5 96.094 (95.149)
Epoch: [101][40/196]	Time 0.014 (0.034)	Data 0.002 (0.007)	Loss 1.9510 (1.9249)	Acc@1 75.391 (73.485)	Acc@5 94.141 (95.151)
Epoch: [101][50/196]	Time 0.014 (0.030)	Data 0.004 (0.006)	Loss 2.0575 (1.9315)	Acc@1 73.438 (73.384)	Acc@5 91.016 (95.037)
Epoch: [101][60/196]	Time 0.012 (0.027)	Data 0.025 (0.006)	Loss 1.9406 (1.9293)	Acc@1 74.219 (73.521)	Acc@5 94.531 (95.069)
Epoch: [101][70/196]	Time 0.017 (0.026)	Data 0.001 (0.006)	Loss 1.9101 (1.9323)	Acc@1 73.047 (73.432)	Acc@5 97.266 (95.109)
Epoch: [101][80/196]	Time 0.012 (0.024)	Data 0.019 (0.006)	Loss 2.1882 (1.9375)	Acc@1 69.141 (73.192)	Acc@5 91.406 (95.071)
Epoch: [101][90/196]	Time 0.016 (0.023)	Data 0.001 (0.005)	Loss 2.1741 (1.9452)	Acc@1 65.625 (72.888)	Acc@5 90.234 (94.986)
Epoch: [101][100/196]	Time 0.011 (0.023)	Data 0.020 (0.005)	Loss 2.0443 (1.9535)	Acc@1 69.922 (72.679)	Acc@5 94.141 (94.918)
Epoch: [101][110/196]	Time 0.018 (0.022)	Data 0.000 (0.005)	Loss 2.0349 (1.9612)	Acc@1 70.703 (72.420)	Acc@5 94.531 (94.855)
Epoch: [101][120/196]	Time 0.012 (0.021)	Data 0.020 (0.005)	Loss 2.0515 (1.9655)	Acc@1 72.266 (72.311)	Acc@5 94.922 (94.825)
Epoch: [101][130/196]	Time 0.017 (0.021)	Data 0.000 (0.005)	Loss 2.1490 (1.9751)	Acc@1 67.969 (72.060)	Acc@5 92.969 (94.716)
Epoch: [101][140/196]	Time 0.012 (0.021)	Data 0.013 (0.005)	Loss 2.1235 (1.9799)	Acc@1 68.750 (71.950)	Acc@5 89.453 (94.631)
Epoch: [101][150/196]	Time 0.016 (0.020)	Data 0.001 (0.005)	Loss 2.0291 (1.9826)	Acc@1 68.359 (71.891)	Acc@5 95.312 (94.586)
Epoch: [101][160/196]	Time 0.011 (0.020)	Data 0.008 (0.005)	Loss 1.9388 (1.9916)	Acc@1 74.219 (71.640)	Acc@5 94.141 (94.446)
Epoch: [101][170/196]	Time 0.016 (0.020)	Data 0.001 (0.005)	Loss 2.0537 (1.9961)	Acc@1 68.359 (71.557)	Acc@5 95.312 (94.378)
Epoch: [101][180/196]	Time 0.012 (0.019)	Data 0.007 (0.005)	Loss 2.0462 (1.9984)	Acc@1 70.703 (71.521)	Acc@5 94.141 (94.320)
Epoch: [101][190/196]	Time 0.017 (0.019)	Data 0.000 (0.005)	Loss 2.0151 (2.0016)	Acc@1 67.969 (71.460)	Acc@5 94.922 (94.304)
num momentum params: 26
[0.1, 2.006237780647278, 1.8896455109119414, 71.324, 53.36, tensor(0.5210, device='cuda:0', grad_fn=<DivBackward0>), 4.0307981967926025, 0.5183298587799072]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [102 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [102][0/196]	Time 0.058 (0.058)	Data 0.190 (0.190)	Loss 1.9580 (1.9580)	Acc@1 72.266 (72.266)	Acc@5 95.703 (95.703)
Epoch: [102][10/196]	Time 0.015 (0.020)	Data 0.002 (0.019)	Loss 1.8628 (1.9721)	Acc@1 75.000 (73.011)	Acc@5 95.703 (94.318)
Epoch: [102][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.9025 (1.9480)	Acc@1 74.609 (73.307)	Acc@5 95.703 (94.847)
Epoch: [102][30/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 1.9018 (1.9349)	Acc@1 71.484 (73.211)	Acc@5 94.531 (95.048)
Epoch: [102][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 1.9825 (1.9245)	Acc@1 70.703 (73.418)	Acc@5 94.922 (95.112)
Epoch: [102][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0293 (1.9222)	Acc@1 75.000 (73.545)	Acc@5 92.578 (95.098)
Epoch: [102][60/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0545 (1.9260)	Acc@1 71.094 (73.361)	Acc@5 94.141 (95.140)
Epoch: [102][70/196]	Time 0.023 (0.016)	Data 0.000 (0.005)	Loss 1.8806 (1.9226)	Acc@1 73.438 (73.426)	Acc@5 96.875 (95.213)
Epoch: [102][80/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 1.8786 (1.9236)	Acc@1 73.047 (73.351)	Acc@5 96.094 (95.255)
Epoch: [102][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0086 (1.9231)	Acc@1 68.359 (73.429)	Acc@5 95.312 (95.274)
Epoch: [102][100/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0211 (1.9322)	Acc@1 71.484 (73.263)	Acc@5 93.359 (95.092)
Epoch: [102][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.8836 (1.9389)	Acc@1 71.875 (73.117)	Acc@5 95.312 (95.020)
Epoch: [102][120/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0072 (1.9454)	Acc@1 73.438 (72.986)	Acc@5 97.266 (94.954)
Epoch: [102][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.9463 (1.9503)	Acc@1 75.000 (72.841)	Acc@5 93.359 (94.847)
Epoch: [102][140/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 2.1621 (1.9567)	Acc@1 66.797 (72.678)	Acc@5 90.234 (94.728)
Epoch: [102][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9263 (1.9598)	Acc@1 72.266 (72.573)	Acc@5 92.969 (94.674)
Epoch: [102][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0124 (1.9627)	Acc@1 73.438 (72.530)	Acc@5 94.141 (94.621)
Epoch: [102][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1975 (1.9702)	Acc@1 64.844 (72.339)	Acc@5 92.578 (94.566)
Epoch: [102][180/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1230 (1.9768)	Acc@1 72.266 (72.117)	Acc@5 92.188 (94.497)
Epoch: [102][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2236 (1.9858)	Acc@1 64.062 (71.857)	Acc@5 89.844 (94.376)
num momentum params: 26
[0.1, 1.9896273838043212, 1.8965046274662019, 71.772, 52.66, tensor(0.5257, device='cuda:0', grad_fn=<DivBackward0>), 2.9621846675872803, 0.39829063415527344]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [103 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [103][0/196]	Time 0.049 (0.049)	Data 0.196 (0.196)	Loss 2.0703 (2.0703)	Acc@1 70.703 (70.703)	Acc@5 94.141 (94.141)
Epoch: [103][10/196]	Time 0.018 (0.019)	Data 0.001 (0.020)	Loss 1.9877 (2.0166)	Acc@1 73.438 (71.413)	Acc@5 95.703 (94.957)
Epoch: [103][20/196]	Time 0.014 (0.017)	Data 0.003 (0.012)	Loss 1.8900 (1.9685)	Acc@1 74.609 (72.935)	Acc@5 96.094 (95.182)
Epoch: [103][30/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 1.8465 (1.9591)	Acc@1 75.000 (72.946)	Acc@5 94.922 (95.161)
Epoch: [103][40/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.9179 (1.9489)	Acc@1 72.656 (73.075)	Acc@5 93.750 (95.036)
Epoch: [103][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.8806 (1.9545)	Acc@1 78.125 (73.123)	Acc@5 95.703 (94.945)
Epoch: [103][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9448 (1.9624)	Acc@1 74.219 (72.944)	Acc@5 94.531 (94.794)
Epoch: [103][70/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 1.9762 (1.9566)	Acc@1 72.656 (73.063)	Acc@5 96.094 (94.894)
Epoch: [103][80/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 1.9978 (1.9639)	Acc@1 69.531 (72.690)	Acc@5 96.484 (94.888)
Epoch: [103][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9668 (1.9668)	Acc@1 71.875 (72.532)	Acc@5 95.312 (94.870)
Epoch: [103][100/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 2.0059 (1.9691)	Acc@1 69.531 (72.424)	Acc@5 93.750 (94.848)
Epoch: [103][110/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.9825 (1.9735)	Acc@1 73.828 (72.336)	Acc@5 95.312 (94.802)
Epoch: [103][120/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.0552 (1.9756)	Acc@1 71.094 (72.204)	Acc@5 92.188 (94.735)
Epoch: [103][130/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 1.9862 (1.9794)	Acc@1 70.703 (72.057)	Acc@5 93.359 (94.671)
Epoch: [103][140/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0948 (1.9858)	Acc@1 68.359 (71.925)	Acc@5 93.359 (94.592)
Epoch: [103][150/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.0306 (1.9951)	Acc@1 73.438 (71.689)	Acc@5 92.969 (94.469)
Epoch: [103][160/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.8723 (1.9981)	Acc@1 74.609 (71.659)	Acc@5 96.094 (94.432)
Epoch: [103][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0731 (2.0043)	Acc@1 69.531 (71.464)	Acc@5 91.406 (94.339)
Epoch: [103][180/196]	Time 0.024 (0.015)	Data 0.001 (0.005)	Loss 2.1094 (2.0089)	Acc@1 69.531 (71.355)	Acc@5 94.141 (94.298)
Epoch: [103][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0313 (2.0106)	Acc@1 75.781 (71.353)	Acc@5 92.578 (94.280)
num momentum params: 26
[0.1, 2.011975420150757, 1.9270729279518128, 71.318, 53.14, tensor(0.5213, device='cuda:0', grad_fn=<DivBackward0>), 2.943763494491577, 0.4095258712768554]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [104 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [104][0/196]	Time 0.055 (0.055)	Data 0.202 (0.202)	Loss 1.8434 (1.8434)	Acc@1 73.828 (73.828)	Acc@5 94.141 (94.141)
Epoch: [104][10/196]	Time 0.016 (0.020)	Data 0.002 (0.020)	Loss 1.8702 (1.9653)	Acc@1 75.391 (72.656)	Acc@5 96.484 (94.993)
Epoch: [104][20/196]	Time 0.012 (0.018)	Data 0.006 (0.012)	Loss 1.9221 (1.9556)	Acc@1 74.609 (73.047)	Acc@5 96.875 (95.145)
Epoch: [104][30/196]	Time 0.016 (0.017)	Data 0.002 (0.009)	Loss 1.8363 (1.9422)	Acc@1 74.609 (73.198)	Acc@5 97.266 (95.287)
Epoch: [104][40/196]	Time 0.012 (0.017)	Data 0.005 (0.007)	Loss 1.9475 (1.9355)	Acc@1 72.266 (73.314)	Acc@5 95.703 (95.351)
Epoch: [104][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8527 (1.9322)	Acc@1 76.172 (73.468)	Acc@5 97.266 (95.274)
Epoch: [104][60/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 1.9168 (1.9352)	Acc@1 74.219 (73.405)	Acc@5 93.359 (95.293)
Epoch: [104][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0166 (1.9397)	Acc@1 71.094 (73.388)	Acc@5 94.531 (95.191)
Epoch: [104][80/196]	Time 0.012 (0.016)	Data 0.020 (0.005)	Loss 2.0073 (1.9443)	Acc@1 72.266 (73.274)	Acc@5 94.141 (95.153)
Epoch: [104][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0655 (1.9473)	Acc@1 70.312 (73.193)	Acc@5 93.359 (95.055)
Epoch: [104][100/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 1.9596 (1.9506)	Acc@1 73.438 (73.151)	Acc@5 95.703 (94.976)
Epoch: [104][110/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 1.9686 (1.9553)	Acc@1 73.828 (73.036)	Acc@5 94.141 (94.901)
Epoch: [104][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1215 (1.9648)	Acc@1 61.328 (72.714)	Acc@5 93.750 (94.825)
Epoch: [104][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1897 (1.9676)	Acc@1 69.922 (72.734)	Acc@5 90.625 (94.755)
Epoch: [104][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8568 (1.9697)	Acc@1 75.391 (72.642)	Acc@5 95.703 (94.736)
Epoch: [104][150/196]	Time 0.013 (0.015)	Data 0.000 (0.004)	Loss 2.1773 (1.9761)	Acc@1 67.188 (72.470)	Acc@5 93.750 (94.692)
Epoch: [104][160/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 1.9525 (1.9838)	Acc@1 71.094 (72.239)	Acc@5 94.531 (94.604)
Epoch: [104][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1420 (1.9901)	Acc@1 68.750 (72.103)	Acc@5 94.141 (94.568)
Epoch: [104][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1562 (1.9995)	Acc@1 67.578 (71.869)	Acc@5 92.969 (94.464)
Epoch: [104][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0955 (2.0042)	Acc@1 66.016 (71.724)	Acc@5 92.969 (94.380)
num momentum params: 26
[0.1, 2.006379788169861, 1.8637763619422913, 71.662, 54.07, tensor(0.5231, device='cuda:0', grad_fn=<DivBackward0>), 2.9982805252075195, 0.4017212390899658]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [105 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [105][0/196]	Time 0.053 (0.053)	Data 0.187 (0.187)	Loss 2.0090 (2.0090)	Acc@1 69.141 (69.141)	Acc@5 96.094 (96.094)
Epoch: [105][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.8277 (1.9104)	Acc@1 77.734 (74.751)	Acc@5 96.094 (95.561)
Epoch: [105][20/196]	Time 0.013 (0.017)	Data 0.005 (0.011)	Loss 1.9313 (1.9359)	Acc@1 72.266 (73.642)	Acc@5 96.094 (95.294)
Epoch: [105][30/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 2.0348 (1.9407)	Acc@1 72.266 (73.614)	Acc@5 93.359 (95.086)
Epoch: [105][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 1.9486 (1.9335)	Acc@1 73.047 (73.638)	Acc@5 92.578 (95.093)
Epoch: [105][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 1.8273 (1.9232)	Acc@1 75.391 (73.782)	Acc@5 95.312 (95.205)
Epoch: [105][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0057 (1.9222)	Acc@1 72.656 (73.745)	Acc@5 94.141 (95.287)
Epoch: [105][70/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 1.9092 (1.9226)	Acc@1 75.000 (73.784)	Acc@5 96.094 (95.230)
Epoch: [105][80/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.8569 (1.9318)	Acc@1 73.438 (73.486)	Acc@5 95.703 (95.115)
Epoch: [105][90/196]	Time 0.012 (0.015)	Data 0.016 (0.005)	Loss 2.0700 (1.9462)	Acc@1 70.312 (73.073)	Acc@5 93.750 (94.948)
Epoch: [105][100/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.8653 (1.9518)	Acc@1 73.047 (73.004)	Acc@5 97.266 (94.899)
Epoch: [105][110/196]	Time 0.012 (0.015)	Data 0.019 (0.005)	Loss 2.0373 (1.9549)	Acc@1 70.703 (72.822)	Acc@5 92.969 (94.841)
Epoch: [105][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0975 (1.9630)	Acc@1 71.484 (72.630)	Acc@5 94.922 (94.802)
Epoch: [105][130/196]	Time 0.011 (0.015)	Data 0.018 (0.005)	Loss 1.9980 (1.9662)	Acc@1 69.531 (72.573)	Acc@5 95.312 (94.716)
Epoch: [105][140/196]	Time 0.020 (0.015)	Data 0.001 (0.005)	Loss 2.0959 (1.9715)	Acc@1 67.969 (72.388)	Acc@5 94.531 (94.678)
Epoch: [105][150/196]	Time 0.016 (0.015)	Data 0.009 (0.005)	Loss 2.1400 (1.9738)	Acc@1 70.312 (72.273)	Acc@5 93.359 (94.630)
Epoch: [105][160/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1200 (1.9784)	Acc@1 66.797 (72.154)	Acc@5 91.016 (94.541)
Epoch: [105][170/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.0844 (1.9846)	Acc@1 69.531 (72.035)	Acc@5 95.312 (94.474)
Epoch: [105][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0441 (1.9893)	Acc@1 72.266 (71.955)	Acc@5 93.359 (94.441)
Epoch: [105][190/196]	Time 0.014 (0.015)	Data 0.022 (0.005)	Loss 2.0707 (1.9943)	Acc@1 69.922 (71.820)	Acc@5 92.969 (94.392)
num momentum params: 26
[0.1, 1.9958209880065918, 1.8223061108589171, 71.798, 54.6, tensor(0.5256, device='cuda:0', grad_fn=<DivBackward0>), 2.980621576309204, 0.39929747581481934]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [106 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [106][0/196]	Time 0.047 (0.047)	Data 0.193 (0.193)	Loss 1.9216 (1.9216)	Acc@1 74.609 (74.609)	Acc@5 95.703 (95.703)
Epoch: [106][10/196]	Time 0.017 (0.018)	Data 0.000 (0.020)	Loss 1.9126 (1.9677)	Acc@1 75.781 (72.869)	Acc@5 94.531 (94.247)
Epoch: [106][20/196]	Time 0.016 (0.017)	Data 0.003 (0.011)	Loss 1.9394 (1.9342)	Acc@1 72.656 (73.289)	Acc@5 96.484 (94.996)
Epoch: [106][30/196]	Time 0.018 (0.017)	Data 0.001 (0.009)	Loss 1.8694 (1.9340)	Acc@1 73.047 (73.236)	Acc@5 95.312 (95.035)
Epoch: [106][40/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 2.0251 (1.9297)	Acc@1 71.094 (73.514)	Acc@5 95.312 (95.065)
Epoch: [106][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0250 (1.9253)	Acc@1 67.578 (73.468)	Acc@5 94.141 (95.167)
Epoch: [106][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9365 (1.9173)	Acc@1 73.047 (73.867)	Acc@5 94.141 (95.191)
Epoch: [106][70/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0602 (1.9190)	Acc@1 70.312 (73.680)	Acc@5 93.359 (95.202)
Epoch: [106][80/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.0970 (1.9304)	Acc@1 67.969 (73.423)	Acc@5 94.141 (95.168)
Epoch: [106][90/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 1.9599 (1.9378)	Acc@1 72.656 (73.197)	Acc@5 93.750 (95.042)
Epoch: [106][100/196]	Time 0.012 (0.015)	Data 0.019 (0.005)	Loss 1.9649 (1.9444)	Acc@1 68.750 (72.931)	Acc@5 96.484 (95.007)
Epoch: [106][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.9729 (1.9542)	Acc@1 73.828 (72.688)	Acc@5 96.484 (94.950)
Epoch: [106][120/196]	Time 0.011 (0.015)	Data 0.019 (0.005)	Loss 2.1611 (1.9621)	Acc@1 68.750 (72.553)	Acc@5 94.531 (94.838)
Epoch: [106][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0388 (1.9665)	Acc@1 69.922 (72.430)	Acc@5 93.359 (94.788)
Epoch: [106][140/196]	Time 0.011 (0.015)	Data 0.015 (0.005)	Loss 2.0558 (1.9701)	Acc@1 68.359 (72.302)	Acc@5 93.750 (94.700)
Epoch: [106][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1055 (1.9784)	Acc@1 64.844 (72.079)	Acc@5 93.750 (94.627)
Epoch: [106][160/196]	Time 0.014 (0.015)	Data 0.018 (0.005)	Loss 1.9626 (1.9876)	Acc@1 70.703 (71.865)	Acc@5 96.094 (94.500)
Epoch: [106][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9892 (1.9913)	Acc@1 72.656 (71.786)	Acc@5 95.312 (94.438)
Epoch: [106][180/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.0674 (1.9967)	Acc@1 70.703 (71.702)	Acc@5 91.406 (94.339)
Epoch: [106][190/196]	Time 0.019 (0.015)	Data 0.000 (0.005)	Loss 1.8676 (2.0004)	Acc@1 76.562 (71.636)	Acc@5 96.875 (94.306)
num momentum params: 26
[0.1, 2.0031253031921388, 1.8317596662044524, 71.552, 54.31, tensor(0.5236, device='cuda:0', grad_fn=<DivBackward0>), 3.0205976963043213, 0.40143465995788574]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [107 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [107][0/196]	Time 0.052 (0.052)	Data 0.201 (0.201)	Loss 1.9938 (1.9938)	Acc@1 71.484 (71.484)	Acc@5 92.188 (92.188)
Epoch: [107][10/196]	Time 0.016 (0.020)	Data 0.002 (0.020)	Loss 1.9333 (1.9839)	Acc@1 75.781 (71.768)	Acc@5 94.531 (94.744)
Epoch: [107][20/196]	Time 0.015 (0.018)	Data 0.006 (0.012)	Loss 1.8326 (1.9323)	Acc@1 78.516 (73.661)	Acc@5 96.094 (95.126)
Epoch: [107][30/196]	Time 0.018 (0.018)	Data 0.001 (0.009)	Loss 1.8283 (1.9232)	Acc@1 73.438 (73.564)	Acc@5 97.656 (95.186)
Epoch: [107][40/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 1.8897 (1.9222)	Acc@1 73.047 (73.666)	Acc@5 94.922 (95.151)
Epoch: [107][50/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 1.9524 (1.9162)	Acc@1 70.703 (73.851)	Acc@5 95.312 (95.312)
Epoch: [107][60/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8519 (1.9147)	Acc@1 76.172 (73.867)	Acc@5 94.531 (95.312)
Epoch: [107][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9029 (1.9111)	Acc@1 75.391 (74.092)	Acc@5 94.531 (95.340)
Epoch: [107][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0629 (1.9122)	Acc@1 70.703 (74.055)	Acc@5 96.094 (95.351)
Epoch: [107][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9794 (1.9236)	Acc@1 70.703 (73.708)	Acc@5 93.359 (95.218)
Epoch: [107][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1038 (1.9444)	Acc@1 69.531 (73.213)	Acc@5 92.578 (95.015)
Epoch: [107][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9242 (1.9553)	Acc@1 75.000 (72.991)	Acc@5 94.531 (94.859)
Epoch: [107][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1214 (1.9668)	Acc@1 66.406 (72.663)	Acc@5 91.797 (94.718)
Epoch: [107][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0306 (1.9731)	Acc@1 71.484 (72.477)	Acc@5 94.141 (94.636)
Epoch: [107][140/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1659 (1.9799)	Acc@1 67.578 (72.277)	Acc@5 91.016 (94.601)
Epoch: [107][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0103 (1.9844)	Acc@1 72.266 (72.110)	Acc@5 94.531 (94.557)
Epoch: [107][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0331 (1.9903)	Acc@1 71.875 (71.991)	Acc@5 92.578 (94.483)
Epoch: [107][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9663 (1.9972)	Acc@1 69.141 (71.829)	Acc@5 95.703 (94.376)
Epoch: [107][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0434 (2.0026)	Acc@1 70.312 (71.720)	Acc@5 94.922 (94.320)
Epoch: [107][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1510 (2.0082)	Acc@1 67.969 (71.613)	Acc@5 93.359 (94.233)
num momentum params: 26
[0.1, 2.0102828897857665, 1.944353882074356, 71.544, 51.94, tensor(0.5224, device='cuda:0', grad_fn=<DivBackward0>), 3.092064619064331, 0.4081237316131592]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [108 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [108][0/196]	Time 0.058 (0.058)	Data 0.200 (0.200)	Loss 2.1106 (2.1106)	Acc@1 70.703 (70.703)	Acc@5 93.359 (93.359)
Epoch: [108][10/196]	Time 0.017 (0.021)	Data 0.002 (0.020)	Loss 1.8521 (1.9365)	Acc@1 73.047 (72.869)	Acc@5 94.922 (95.384)
Epoch: [108][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 1.8384 (1.9199)	Acc@1 76.562 (73.624)	Acc@5 96.484 (95.424)
Epoch: [108][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.8027 (1.9101)	Acc@1 78.125 (73.904)	Acc@5 97.656 (95.451)
Epoch: [108][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.8571 (1.9104)	Acc@1 74.609 (74.276)	Acc@5 96.875 (95.474)
Epoch: [108][50/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.9242 (1.9091)	Acc@1 73.438 (74.219)	Acc@5 95.703 (95.389)
Epoch: [108][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9889 (1.9194)	Acc@1 70.703 (73.905)	Acc@5 94.141 (95.293)
Epoch: [108][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0250 (1.9233)	Acc@1 71.094 (73.751)	Acc@5 94.922 (95.307)
Epoch: [108][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9537 (1.9244)	Acc@1 73.047 (73.823)	Acc@5 94.531 (95.269)
Epoch: [108][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8636 (1.9288)	Acc@1 76.953 (73.635)	Acc@5 96.484 (95.231)
Epoch: [108][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0343 (1.9381)	Acc@1 67.969 (73.337)	Acc@5 94.531 (95.100)
Epoch: [108][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0827 (1.9442)	Acc@1 72.266 (73.163)	Acc@5 94.531 (94.999)
Epoch: [108][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1964 (1.9563)	Acc@1 67.969 (72.885)	Acc@5 91.016 (94.870)
Epoch: [108][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9880 (1.9601)	Acc@1 72.266 (72.764)	Acc@5 94.141 (94.859)
Epoch: [108][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9941 (1.9675)	Acc@1 70.703 (72.593)	Acc@5 92.188 (94.742)
Epoch: [108][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.8994 (1.9701)	Acc@1 77.344 (72.486)	Acc@5 93.750 (94.684)
Epoch: [108][160/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2114 (1.9725)	Acc@1 68.750 (72.399)	Acc@5 91.406 (94.672)
Epoch: [108][170/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.9217 (1.9799)	Acc@1 73.047 (72.266)	Acc@5 95.703 (94.593)
Epoch: [108][180/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1330 (1.9885)	Acc@1 67.969 (72.082)	Acc@5 92.969 (94.508)
Epoch: [108][190/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1633 (1.9951)	Acc@1 66.797 (71.883)	Acc@5 92.578 (94.423)
num momentum params: 26
[0.1, 1.9978053564453124, 1.9201211369037627, 71.806, 52.71, tensor(0.5252, device='cuda:0', grad_fn=<DivBackward0>), 3.0663397312164307, 0.39621877670288086]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [109 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [109][0/196]	Time 0.052 (0.052)	Data 0.197 (0.197)	Loss 1.9493 (1.9493)	Acc@1 73.828 (73.828)	Acc@5 96.094 (96.094)
Epoch: [109][10/196]	Time 0.017 (0.019)	Data 0.002 (0.020)	Loss 1.9150 (1.9732)	Acc@1 74.609 (73.544)	Acc@5 94.531 (94.318)
Epoch: [109][20/196]	Time 0.014 (0.017)	Data 0.004 (0.012)	Loss 1.9602 (1.9475)	Acc@1 70.312 (73.679)	Acc@5 96.875 (94.773)
Epoch: [109][30/196]	Time 0.017 (0.016)	Data 0.000 (0.009)	Loss 1.8019 (1.9220)	Acc@1 77.734 (74.433)	Acc@5 96.875 (94.985)
Epoch: [109][40/196]	Time 0.011 (0.016)	Data 0.008 (0.008)	Loss 1.9938 (1.9073)	Acc@1 71.094 (74.781)	Acc@5 93.359 (95.189)
Epoch: [109][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 2.0053 (1.8992)	Acc@1 70.703 (74.862)	Acc@5 94.531 (95.267)
Epoch: [109][60/196]	Time 0.011 (0.016)	Data 0.006 (0.006)	Loss 2.0388 (1.9015)	Acc@1 73.438 (74.866)	Acc@5 92.969 (95.280)
Epoch: [109][70/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.9330 (1.9132)	Acc@1 71.875 (74.516)	Acc@5 95.703 (95.098)
Epoch: [109][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0018 (1.9178)	Acc@1 73.047 (74.334)	Acc@5 93.359 (95.124)
Epoch: [109][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0900 (1.9219)	Acc@1 67.578 (74.197)	Acc@5 91.797 (95.064)
Epoch: [109][100/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0400 (1.9365)	Acc@1 69.141 (73.728)	Acc@5 91.797 (94.864)
Epoch: [109][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.8974 (1.9439)	Acc@1 74.609 (73.536)	Acc@5 96.484 (94.767)
Epoch: [109][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0859 (1.9521)	Acc@1 68.750 (73.305)	Acc@5 95.312 (94.689)
Epoch: [109][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1847 (1.9585)	Acc@1 67.188 (73.109)	Acc@5 91.797 (94.645)
Epoch: [109][140/196]	Time 0.015 (0.015)	Data 0.001 (0.005)	Loss 2.4534 (1.9716)	Acc@1 58.984 (72.767)	Acc@5 87.109 (94.481)
Epoch: [109][150/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1060 (1.9818)	Acc@1 69.922 (72.454)	Acc@5 92.578 (94.392)
Epoch: [109][160/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 1.9792 (1.9865)	Acc@1 69.141 (72.249)	Acc@5 95.703 (94.378)
Epoch: [109][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0276 (1.9900)	Acc@1 72.656 (72.149)	Acc@5 92.188 (94.314)
Epoch: [109][180/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0844 (1.9923)	Acc@1 69.141 (72.084)	Acc@5 93.359 (94.298)
Epoch: [109][190/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1512 (1.9960)	Acc@1 68.359 (71.951)	Acc@5 92.188 (94.233)
num momentum params: 26
[0.1, 1.9980650110626221, 2.010892493724823, 71.864, 51.03, tensor(0.5254, device='cuda:0', grad_fn=<DivBackward0>), 2.9891414642333984, 0.4046158790588379]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [509, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [477, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [477]
Non Pruning Epoch - module.bn7.bias: [477]
Non Pruning Epoch - module.conv8.weight: [323, 477, 3, 3]
Non Pruning Epoch - module.bn8.weight: [323]
Non Pruning Epoch - module.bn8.bias: [323]
Non Pruning Epoch - module.fc.weight: [100, 323]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [110 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [509, 511, 3, 3]
module.conv7.weight [477, 509, 3, 3]
module.conv8.weight [323, 477, 3, 3]
Epoch: [110][0/196]	Time 0.055 (0.055)	Data 0.202 (0.202)	Loss 1.9028 (1.9028)	Acc@1 74.219 (74.219)	Acc@5 94.922 (94.922)
Epoch: [110][10/196]	Time 0.015 (0.020)	Data 0.003 (0.020)	Loss 1.8862 (1.9467)	Acc@1 73.828 (73.295)	Acc@5 96.094 (95.312)
Epoch: [110][20/196]	Time 0.013 (0.017)	Data 0.004 (0.012)	Loss 1.9597 (1.9187)	Acc@1 71.875 (74.051)	Acc@5 94.922 (95.480)
Epoch: [110][30/196]	Time 0.014 (0.017)	Data 0.004 (0.009)	Loss 1.8293 (1.8977)	Acc@1 77.344 (74.685)	Acc@5 96.875 (95.590)
Epoch: [110][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.8163 (1.8848)	Acc@1 77.344 (75.000)	Acc@5 96.875 (95.751)
Epoch: [110][50/196]	Time 0.012 (0.016)	Data 0.005 (0.006)	Loss 1.9777 (1.8832)	Acc@1 71.875 (74.770)	Acc@5 96.484 (95.741)
Epoch: [110][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 1.8710 (1.8893)	Acc@1 73.438 (74.782)	Acc@5 97.656 (95.633)
Epoch: [110][70/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.8751 (1.9032)	Acc@1 73.828 (74.373)	Acc@5 97.266 (95.549)
Epoch: [110][80/196]	Time 0.012 (0.016)	Data 0.002 (0.005)	Loss 2.2192 (1.9199)	Acc@1 66.797 (73.915)	Acc@5 93.359 (95.356)
Epoch: [110][90/196]	Time 0.012 (0.015)	Data 0.020 (0.005)	Loss 1.8407 (1.9336)	Acc@1 76.953 (73.493)	Acc@5 97.266 (95.248)
Epoch: [110][100/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0870 (1.9421)	Acc@1 71.484 (73.387)	Acc@5 93.750 (95.166)
Epoch: [110][110/196]	Time 0.012 (0.015)	Data 0.019 (0.005)	Loss 1.9719 (1.9444)	Acc@1 70.703 (73.195)	Acc@5 96.094 (95.161)
Epoch: [110][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0644 (1.9520)	Acc@1 68.750 (72.973)	Acc@5 92.969 (95.032)
Epoch: [110][130/196]	Time 0.012 (0.015)	Data 0.015 (0.005)	Loss 1.7995 (1.9572)	Acc@1 75.781 (72.814)	Acc@5 96.094 (94.955)
Epoch: [110][140/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9667 (1.9604)	Acc@1 73.047 (72.764)	Acc@5 94.141 (94.869)
Epoch: [110][150/196]	Time 0.013 (0.015)	Data 0.018 (0.005)	Loss 2.0606 (1.9644)	Acc@1 69.922 (72.672)	Acc@5 92.969 (94.813)
Epoch: [110][160/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.0859 (1.9677)	Acc@1 69.531 (72.557)	Acc@5 92.969 (94.796)
Epoch: [110][170/196]	Time 0.014 (0.015)	Data 0.008 (0.005)	Loss 1.9550 (1.9724)	Acc@1 71.875 (72.444)	Acc@5 94.531 (94.741)
Epoch: [110][180/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9785 (1.9771)	Acc@1 73.828 (72.326)	Acc@5 94.531 (94.667)
Epoch: [110][190/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.1677 (1.9834)	Acc@1 68.750 (72.190)	Acc@5 93.750 (94.580)
num momentum params: 26
[0.1, 1.9861804774856568, 1.8608755993843078, 72.11, 54.18, tensor(0.5281, device='cuda:0', grad_fn=<DivBackward0>), 3.0271570682525635, 0.4087240695953369]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [45, 3, 3, 3]
Before - module.bn1.weight: [45]
Before - module.bn1.bias: [45]
Before - module.conv2.weight: [128, 45, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [252, 128, 3, 3]
Before - module.bn3.weight: [252]
Before - module.bn3.bias: [252]
Before - module.conv4.weight: [256, 252, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [511, 256, 3, 3]
Before - module.bn5.weight: [511]
Before - module.bn5.bias: [511]
Before - module.conv6.weight: [509, 511, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [477, 509, 3, 3]
Before - module.bn7.weight: [477]
Before - module.bn7.bias: [477]
Before - module.conv8.weight: [323, 477, 3, 3]
Before - module.bn8.weight: [323]
Before - module.bn8.bias: [323]
Before - module.fc.weight: [100, 323]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [43, 3, 3, 3]
[module.bn1.weight]: 45 >> 43
running_mean [43]
running_var [43]
num_batches_tracked []
[module.conv2.weight]: [128, 45, 3, 3] >> [128, 43, 3, 3]
[module.conv5.weight]: [511, 256, 3, 3] >> [510, 256, 3, 3]
[module.bn5.weight]: 511 >> 510
running_mean [510]
running_var [510]
num_batches_tracked []
[module.conv6.weight]: [509, 511, 3, 3] >> [509, 510, 3, 3]
[module.conv7.weight]: [477, 509, 3, 3] >> [475, 509, 3, 3]
[module.bn7.weight]: 477 >> 475
running_mean [475]
running_var [475]
num_batches_tracked []
[module.conv8.weight]: [323, 477, 3, 3] >> [310, 475, 3, 3]
[module.bn8.weight]: 323 >> 310
running_mean [310]
running_var [310]
num_batches_tracked []
[module.fc.weight]: [100, 323] >> [100, 310]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [43, 3, 3, 3]
After - module.bn1.weight: [43]
After - module.bn1.bias: [43]
After - module.conv2.weight: [128, 43, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [252, 128, 3, 3]
After - module.bn3.weight: [252]
After - module.bn3.bias: [252]
After - module.conv4.weight: [256, 252, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [509, 510, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [475, 509, 3, 3]
After - module.bn7.weight: [475]
After - module.bn7.bias: [475]
After - module.conv8.weight: [310, 475, 3, 3]
After - module.bn8.weight: [310]
After - module.bn8.bias: [310]
After - module.fc.weight: [100, 310]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [43, 3, 3, 3]
conv2 --> [128, 43, 3, 3]
conv3 --> [252, 128, 3, 3]
conv4 --> [256, 252, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [509, 510, 3, 3]
conv7 --> [475, 509, 3, 3]
conv8 --> [310, 475, 3, 3]
fc --> [310, 100]
1, 476140032, 1188864, 43
2, 5300748288, 12681216, 128
3, 8472231936, 18579456, 252
4, 16944463872, 37158912, 256
5, 10227548160, 18800640, 510
6, 20335242240, 37380960, 509
7, 6684595200, 8703900, 475
8, 4071168000, 5301000, 310
fc, 11904000, 31000, 0
===================
FLOP REPORT: 28329703800000.0 51502400000.0 139825948 128756 2483 15.197710037231445
[INFO] Storing checkpoint...

Epoch: [111 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [111][0/196]	Time 0.620 (0.620)	Data 0.212 (0.212)	Loss 1.8445 (1.8445)	Acc@1 76.953 (76.953)	Acc@5 97.656 (97.656)
Epoch: [111][10/196]	Time 0.018 (0.071)	Data 0.003 (0.021)	Loss 1.8708 (1.8805)	Acc@1 75.781 (75.249)	Acc@5 94.531 (95.845)
Epoch: [111][20/196]	Time 0.015 (0.044)	Data 0.002 (0.012)	Loss 1.8668 (1.8891)	Acc@1 73.828 (74.888)	Acc@5 96.484 (95.740)
Epoch: [111][30/196]	Time 0.015 (0.034)	Data 0.002 (0.009)	Loss 1.8771 (1.8859)	Acc@1 74.219 (75.013)	Acc@5 96.094 (95.716)
Epoch: [111][40/196]	Time 0.016 (0.030)	Data 0.001 (0.008)	Loss 1.8806 (1.8912)	Acc@1 74.609 (74.657)	Acc@5 96.094 (95.579)
Epoch: [111][50/196]	Time 0.015 (0.027)	Data 0.002 (0.006)	Loss 1.9345 (1.8891)	Acc@1 71.484 (74.617)	Acc@5 96.484 (95.695)
Epoch: [111][60/196]	Time 0.016 (0.025)	Data 0.001 (0.006)	Loss 1.9596 (1.8975)	Acc@1 70.703 (74.180)	Acc@5 94.922 (95.690)
Epoch: [111][70/196]	Time 0.015 (0.024)	Data 0.003 (0.006)	Loss 1.9810 (1.9079)	Acc@1 71.875 (73.927)	Acc@5 92.188 (95.549)
Epoch: [111][80/196]	Time 0.017 (0.023)	Data 0.000 (0.005)	Loss 1.9714 (1.9191)	Acc@1 72.656 (73.558)	Acc@5 95.703 (95.404)
Epoch: [111][90/196]	Time 0.015 (0.022)	Data 0.003 (0.005)	Loss 1.9866 (1.9263)	Acc@1 73.438 (73.442)	Acc@5 94.531 (95.282)
Epoch: [111][100/196]	Time 0.016 (0.021)	Data 0.000 (0.005)	Loss 2.0155 (1.9380)	Acc@1 72.656 (73.302)	Acc@5 94.922 (95.131)
Epoch: [111][110/196]	Time 0.015 (0.020)	Data 0.002 (0.005)	Loss 2.0876 (1.9574)	Acc@1 70.703 (72.832)	Acc@5 91.016 (94.883)
Epoch: [111][120/196]	Time 0.016 (0.020)	Data 0.000 (0.005)	Loss 2.2309 (1.9698)	Acc@1 62.500 (72.475)	Acc@5 94.141 (94.751)
Epoch: [111][130/196]	Time 0.014 (0.020)	Data 0.003 (0.005)	Loss 2.1153 (1.9756)	Acc@1 68.750 (72.310)	Acc@5 92.578 (94.674)
Epoch: [111][140/196]	Time 0.016 (0.019)	Data 0.000 (0.005)	Loss 2.1048 (1.9826)	Acc@1 72.656 (72.155)	Acc@5 93.359 (94.562)
Epoch: [111][150/196]	Time 0.016 (0.019)	Data 0.000 (0.004)	Loss 2.0605 (1.9876)	Acc@1 68.359 (72.051)	Acc@5 92.969 (94.474)
Epoch: [111][160/196]	Time 0.017 (0.019)	Data 0.000 (0.004)	Loss 2.1174 (1.9930)	Acc@1 67.188 (71.907)	Acc@5 96.094 (94.449)
Epoch: [111][170/196]	Time 0.017 (0.018)	Data 0.000 (0.004)	Loss 2.1642 (1.9961)	Acc@1 69.531 (71.811)	Acc@5 91.016 (94.390)
Epoch: [111][180/196]	Time 0.016 (0.018)	Data 0.000 (0.004)	Loss 2.0227 (2.0003)	Acc@1 71.094 (71.683)	Acc@5 92.578 (94.322)
Epoch: [111][190/196]	Time 0.016 (0.018)	Data 0.000 (0.004)	Loss 2.1292 (2.0050)	Acc@1 67.188 (71.613)	Acc@5 91.016 (94.243)
num momentum params: 26
[0.1, 2.0074535694122315, 1.9773946237564086, 71.572, 51.88, tensor(0.5236, device='cuda:0', grad_fn=<DivBackward0>), 3.729436159133911, 0.48184204101562494]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [112 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [112][0/196]	Time 0.055 (0.055)	Data 0.204 (0.204)	Loss 1.9997 (1.9997)	Acc@1 72.656 (72.656)	Acc@5 95.312 (95.312)
Epoch: [112][10/196]	Time 0.016 (0.020)	Data 0.002 (0.020)	Loss 1.7621 (1.9144)	Acc@1 80.859 (75.355)	Acc@5 97.266 (95.597)
Epoch: [112][20/196]	Time 0.012 (0.018)	Data 0.005 (0.012)	Loss 1.9557 (1.9107)	Acc@1 75.781 (74.777)	Acc@5 94.141 (95.443)
Epoch: [112][30/196]	Time 0.014 (0.017)	Data 0.002 (0.009)	Loss 1.8870 (1.9054)	Acc@1 73.828 (74.647)	Acc@5 95.703 (95.413)
Epoch: [112][40/196]	Time 0.012 (0.016)	Data 0.004 (0.007)	Loss 1.9090 (1.8973)	Acc@1 75.391 (74.952)	Acc@5 96.875 (95.474)
Epoch: [112][50/196]	Time 0.017 (0.016)	Data 0.003 (0.007)	Loss 2.0444 (1.9014)	Acc@1 69.922 (74.694)	Acc@5 94.922 (95.458)
Epoch: [112][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0643 (1.9072)	Acc@1 69.141 (74.404)	Acc@5 96.094 (95.505)
Epoch: [112][70/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 2.0519 (1.9140)	Acc@1 72.656 (74.180)	Acc@5 92.969 (95.351)
Epoch: [112][80/196]	Time 0.014 (0.015)	Data 0.010 (0.006)	Loss 2.0049 (1.9236)	Acc@1 70.703 (73.929)	Acc@5 95.312 (95.250)
Epoch: [112][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1033 (1.9339)	Acc@1 67.969 (73.669)	Acc@5 92.578 (95.145)
Epoch: [112][100/196]	Time 0.011 (0.015)	Data 0.018 (0.005)	Loss 2.0443 (1.9462)	Acc@1 68.359 (73.306)	Acc@5 96.094 (94.976)
Epoch: [112][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0732 (1.9561)	Acc@1 69.922 (73.036)	Acc@5 94.531 (94.947)
Epoch: [112][120/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 1.9696 (1.9649)	Acc@1 71.094 (72.805)	Acc@5 96.094 (94.815)
Epoch: [112][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1922 (1.9779)	Acc@1 65.625 (72.498)	Acc@5 91.797 (94.671)
Epoch: [112][140/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0519 (1.9836)	Acc@1 69.141 (72.390)	Acc@5 93.750 (94.589)
Epoch: [112][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0511 (1.9886)	Acc@1 71.875 (72.240)	Acc@5 93.359 (94.542)
Epoch: [112][160/196]	Time 0.015 (0.015)	Data 0.019 (0.005)	Loss 2.0229 (1.9934)	Acc@1 73.828 (72.195)	Acc@5 94.141 (94.517)
Epoch: [112][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1989 (1.9990)	Acc@1 65.625 (72.044)	Acc@5 90.234 (94.447)
Epoch: [112][180/196]	Time 0.020 (0.015)	Data 0.020 (0.005)	Loss 2.0770 (2.0036)	Acc@1 71.484 (71.946)	Acc@5 93.359 (94.395)
Epoch: [112][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1696 (2.0088)	Acc@1 69.531 (71.793)	Acc@5 92.578 (94.355)
num momentum params: 26
[0.1, 2.0110840656280518, 1.8956272113323211, 71.734, 53.06, tensor(0.5234, device='cuda:0', grad_fn=<DivBackward0>), 3.0040783882141113, 0.39003419876098633]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [113 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [113][0/196]	Time 0.049 (0.049)	Data 0.206 (0.206)	Loss 1.8490 (1.8490)	Acc@1 74.219 (74.219)	Acc@5 96.484 (96.484)
Epoch: [113][10/196]	Time 0.018 (0.019)	Data 0.001 (0.021)	Loss 1.8602 (1.9620)	Acc@1 75.391 (73.082)	Acc@5 95.703 (94.922)
Epoch: [113][20/196]	Time 0.016 (0.017)	Data 0.007 (0.012)	Loss 1.9119 (1.9606)	Acc@1 78.125 (73.586)	Acc@5 94.531 (94.587)
Epoch: [113][30/196]	Time 0.016 (0.017)	Data 0.000 (0.009)	Loss 1.9499 (1.9480)	Acc@1 72.656 (73.853)	Acc@5 96.484 (94.897)
Epoch: [113][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 1.7529 (1.9387)	Acc@1 76.953 (73.647)	Acc@5 97.266 (95.227)
Epoch: [113][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9617 (1.9372)	Acc@1 67.578 (73.560)	Acc@5 95.703 (95.144)
Epoch: [113][60/196]	Time 0.013 (0.016)	Data 0.018 (0.006)	Loss 1.9358 (1.9329)	Acc@1 72.656 (73.777)	Acc@5 92.578 (95.216)
Epoch: [113][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8718 (1.9311)	Acc@1 76.953 (73.867)	Acc@5 94.922 (95.186)
Epoch: [113][80/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 2.1620 (1.9359)	Acc@1 67.578 (73.640)	Acc@5 95.312 (95.240)
Epoch: [113][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0527 (1.9438)	Acc@1 68.750 (73.279)	Acc@5 95.312 (95.179)
Epoch: [113][100/196]	Time 0.014 (0.016)	Data 0.011 (0.005)	Loss 1.9152 (1.9471)	Acc@1 73.438 (73.113)	Acc@5 96.094 (95.162)
Epoch: [113][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8585 (1.9482)	Acc@1 78.516 (73.096)	Acc@5 95.703 (95.108)
Epoch: [113][120/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 1.9399 (1.9516)	Acc@1 72.656 (72.982)	Acc@5 94.922 (95.019)
Epoch: [113][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0122 (1.9536)	Acc@1 73.047 (72.993)	Acc@5 94.141 (94.976)
Epoch: [113][140/196]	Time 0.012 (0.015)	Data 0.012 (0.004)	Loss 2.0886 (1.9599)	Acc@1 67.578 (72.820)	Acc@5 93.750 (94.922)
Epoch: [113][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1512 (1.9721)	Acc@1 67.188 (72.501)	Acc@5 91.797 (94.733)
Epoch: [113][160/196]	Time 0.011 (0.015)	Data 0.005 (0.004)	Loss 2.0939 (1.9780)	Acc@1 69.141 (72.346)	Acc@5 94.531 (94.682)
Epoch: [113][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9429 (1.9818)	Acc@1 73.047 (72.247)	Acc@5 94.531 (94.613)
Epoch: [113][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9848 (1.9851)	Acc@1 73.047 (72.140)	Acc@5 95.312 (94.598)
Epoch: [113][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0987 (1.9894)	Acc@1 71.094 (72.092)	Acc@5 92.578 (94.541)
num momentum params: 26
[0.1, 1.9919689622497558, 1.7624611842632294, 72.008, 55.46, tensor(0.5281, device='cuda:0', grad_fn=<DivBackward0>), 2.966395378112793, 0.40159368515014654]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [114 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [114][0/196]	Time 0.050 (0.050)	Data 0.205 (0.205)	Loss 2.0139 (2.0139)	Acc@1 72.266 (72.266)	Acc@5 95.312 (95.312)
Epoch: [114][10/196]	Time 0.017 (0.019)	Data 0.001 (0.021)	Loss 1.9740 (1.9778)	Acc@1 74.609 (73.082)	Acc@5 94.922 (94.744)
Epoch: [114][20/196]	Time 0.013 (0.017)	Data 0.013 (0.013)	Loss 1.8585 (1.9626)	Acc@1 77.344 (72.991)	Acc@5 95.703 (94.847)
Epoch: [114][30/196]	Time 0.017 (0.017)	Data 0.000 (0.009)	Loss 1.7848 (1.9292)	Acc@1 76.953 (73.753)	Acc@5 97.266 (95.048)
Epoch: [114][40/196]	Time 0.011 (0.016)	Data 0.010 (0.008)	Loss 1.9188 (1.9288)	Acc@1 70.312 (73.619)	Acc@5 95.312 (95.151)
Epoch: [114][50/196]	Time 0.018 (0.016)	Data 0.000 (0.007)	Loss 1.8439 (1.9204)	Acc@1 76.953 (73.951)	Acc@5 95.312 (95.198)
Epoch: [114][60/196]	Time 0.011 (0.016)	Data 0.007 (0.006)	Loss 1.9022 (1.9186)	Acc@1 72.656 (74.091)	Acc@5 95.703 (95.223)
Epoch: [114][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8225 (1.9168)	Acc@1 75.391 (74.131)	Acc@5 96.094 (95.268)
Epoch: [114][80/196]	Time 0.014 (0.016)	Data 0.006 (0.005)	Loss 1.9403 (1.9174)	Acc@1 75.000 (74.084)	Acc@5 93.359 (95.235)
Epoch: [114][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9589 (1.9252)	Acc@1 71.875 (73.742)	Acc@5 95.703 (95.141)
Epoch: [114][100/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 2.0821 (1.9316)	Acc@1 69.141 (73.670)	Acc@5 92.188 (94.988)
Epoch: [114][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.8591 (1.9356)	Acc@1 75.391 (73.529)	Acc@5 95.312 (95.017)
Epoch: [114][120/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9499 (1.9429)	Acc@1 73.047 (73.325)	Acc@5 94.922 (94.961)
Epoch: [114][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0308 (1.9524)	Acc@1 70.312 (73.095)	Acc@5 93.750 (94.791)
Epoch: [114][140/196]	Time 0.012 (0.016)	Data 0.019 (0.004)	Loss 1.9990 (1.9598)	Acc@1 69.922 (72.831)	Acc@5 95.703 (94.714)
Epoch: [114][150/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1424 (1.9696)	Acc@1 69.922 (72.563)	Acc@5 91.016 (94.562)
Epoch: [114][160/196]	Time 0.011 (0.016)	Data 0.019 (0.004)	Loss 2.0467 (1.9777)	Acc@1 73.047 (72.351)	Acc@5 92.578 (94.463)
Epoch: [114][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0065 (1.9842)	Acc@1 69.531 (72.188)	Acc@5 92.969 (94.392)
Epoch: [114][180/196]	Time 0.012 (0.016)	Data 0.020 (0.005)	Loss 2.0613 (1.9894)	Acc@1 67.969 (72.056)	Acc@5 94.922 (94.333)
Epoch: [114][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0233 (1.9961)	Acc@1 72.656 (71.889)	Acc@5 93.750 (94.261)
num momentum params: 26
[0.1, 1.99946773979187, 1.802984105348587, 71.828, 54.12, tensor(0.5262, device='cuda:0', grad_fn=<DivBackward0>), 3.0411200523376465, 0.40550994873046875]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [115 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [115][0/196]	Time 0.053 (0.053)	Data 0.204 (0.204)	Loss 1.8451 (1.8451)	Acc@1 76.562 (76.562)	Acc@5 96.094 (96.094)
Epoch: [115][10/196]	Time 0.014 (0.019)	Data 0.002 (0.021)	Loss 2.0139 (1.9477)	Acc@1 68.750 (73.793)	Acc@5 95.312 (94.780)
Epoch: [115][20/196]	Time 0.013 (0.017)	Data 0.004 (0.012)	Loss 1.9765 (1.9469)	Acc@1 71.875 (73.140)	Acc@5 95.703 (95.108)
Epoch: [115][30/196]	Time 0.015 (0.017)	Data 0.004 (0.009)	Loss 1.8479 (1.9261)	Acc@1 76.562 (73.967)	Acc@5 97.656 (95.325)
Epoch: [115][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 1.9320 (1.9152)	Acc@1 72.656 (74.095)	Acc@5 94.141 (95.246)
Epoch: [115][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.8809 (1.9135)	Acc@1 76.562 (74.257)	Acc@5 95.312 (95.198)
Epoch: [115][60/196]	Time 0.014 (0.016)	Data 0.010 (0.006)	Loss 1.8099 (1.9207)	Acc@1 78.906 (74.097)	Acc@5 95.312 (95.146)
Epoch: [115][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9257 (1.9240)	Acc@1 72.266 (73.977)	Acc@5 96.875 (95.087)
Epoch: [115][80/196]	Time 0.012 (0.016)	Data 0.011 (0.005)	Loss 2.0939 (1.9288)	Acc@1 68.750 (73.765)	Acc@5 94.531 (95.100)
Epoch: [115][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8422 (1.9273)	Acc@1 75.781 (73.802)	Acc@5 95.703 (95.141)
Epoch: [115][100/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9719 (1.9284)	Acc@1 70.312 (73.689)	Acc@5 92.578 (95.150)
Epoch: [115][110/196]	Time 0.019 (0.016)	Data 0.000 (0.005)	Loss 2.1056 (1.9342)	Acc@1 70.703 (73.550)	Acc@5 92.969 (95.045)
Epoch: [115][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1402 (1.9441)	Acc@1 71.094 (73.328)	Acc@5 92.578 (94.961)
Epoch: [115][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1951 (1.9522)	Acc@1 67.188 (73.133)	Acc@5 94.141 (94.868)
Epoch: [115][140/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 2.0400 (1.9616)	Acc@1 69.922 (72.828)	Acc@5 96.094 (94.789)
Epoch: [115][150/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.2218 (1.9715)	Acc@1 67.969 (72.504)	Acc@5 92.188 (94.684)
Epoch: [115][160/196]	Time 0.012 (0.016)	Data 0.004 (0.004)	Loss 2.1258 (1.9785)	Acc@1 66.797 (72.314)	Acc@5 94.531 (94.597)
Epoch: [115][170/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9676 (1.9824)	Acc@1 76.562 (72.179)	Acc@5 93.359 (94.543)
Epoch: [115][180/196]	Time 0.017 (0.016)	Data 0.004 (0.004)	Loss 2.0321 (1.9888)	Acc@1 68.359 (71.996)	Acc@5 92.969 (94.475)
Epoch: [115][190/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2485 (1.9960)	Acc@1 65.234 (71.773)	Acc@5 93.359 (94.406)
num momentum params: 26
[0.1, 1.9971964149856567, 1.8798742663860322, 71.744, 53.93, tensor(0.5274, device='cuda:0', grad_fn=<DivBackward0>), 3.0331101417541504, 0.39639496803283686]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [116 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [116][0/196]	Time 0.063 (0.063)	Data 0.196 (0.196)	Loss 2.0282 (2.0282)	Acc@1 72.266 (72.266)	Acc@5 94.531 (94.531)
Epoch: [116][10/196]	Time 0.016 (0.020)	Data 0.001 (0.020)	Loss 1.8784 (1.9519)	Acc@1 76.953 (73.189)	Acc@5 94.141 (94.993)
Epoch: [116][20/196]	Time 0.012 (0.018)	Data 0.008 (0.012)	Loss 1.7679 (1.9368)	Acc@1 78.125 (73.549)	Acc@5 97.266 (95.108)
Epoch: [116][30/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 1.9527 (1.9416)	Acc@1 69.922 (73.324)	Acc@5 95.312 (95.136)
Epoch: [116][40/196]	Time 0.012 (0.017)	Data 0.010 (0.008)	Loss 1.9547 (1.9282)	Acc@1 74.219 (73.866)	Acc@5 95.703 (95.198)
Epoch: [116][50/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 1.8574 (1.9212)	Acc@1 75.000 (73.935)	Acc@5 96.484 (95.404)
Epoch: [116][60/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 1.8418 (1.9153)	Acc@1 76.562 (74.123)	Acc@5 95.312 (95.473)
Epoch: [116][70/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.1006 (1.9161)	Acc@1 70.703 (73.977)	Acc@5 94.141 (95.461)
Epoch: [116][80/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 1.9172 (1.9186)	Acc@1 75.000 (73.997)	Acc@5 94.922 (95.428)
Epoch: [116][90/196]	Time 0.015 (0.016)	Data 0.007 (0.005)	Loss 1.9442 (1.9267)	Acc@1 76.562 (73.768)	Acc@5 93.359 (95.338)
Epoch: [116][100/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.0183 (1.9273)	Acc@1 68.359 (73.712)	Acc@5 94.922 (95.355)
Epoch: [116][110/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.2614 (1.9345)	Acc@1 64.844 (73.515)	Acc@5 91.406 (95.267)
Epoch: [116][120/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 2.0652 (1.9468)	Acc@1 69.141 (73.195)	Acc@5 92.188 (95.090)
Epoch: [116][130/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.3843 (1.9551)	Acc@1 61.328 (72.993)	Acc@5 90.234 (94.952)
Epoch: [116][140/196]	Time 0.020 (0.016)	Data 0.002 (0.004)	Loss 2.0581 (1.9628)	Acc@1 73.047 (72.800)	Acc@5 94.141 (94.878)
Epoch: [116][150/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0553 (1.9718)	Acc@1 67.969 (72.555)	Acc@5 94.141 (94.774)
Epoch: [116][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1888 (1.9805)	Acc@1 68.750 (72.360)	Acc@5 90.625 (94.655)
Epoch: [116][170/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.1102 (1.9850)	Acc@1 69.531 (72.304)	Acc@5 92.188 (94.611)
Epoch: [116][180/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0701 (1.9905)	Acc@1 71.484 (72.160)	Acc@5 91.797 (94.518)
Epoch: [116][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1531 (1.9974)	Acc@1 71.484 (71.965)	Acc@5 92.578 (94.409)
num momentum params: 26
[0.1, 1.9995395364379882, 1.8480736756324767, 71.934, 53.87, tensor(0.5263, device='cuda:0', grad_fn=<DivBackward0>), 3.015892744064331, 0.40822362899780273]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [117 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [117][0/196]	Time 0.062 (0.062)	Data 0.203 (0.203)	Loss 1.8166 (1.8166)	Acc@1 78.516 (78.516)	Acc@5 95.703 (95.703)
Epoch: [117][10/196]	Time 0.014 (0.020)	Data 0.002 (0.020)	Loss 2.0652 (1.9559)	Acc@1 69.141 (72.727)	Acc@5 93.359 (95.028)
Epoch: [117][20/196]	Time 0.015 (0.018)	Data 0.003 (0.012)	Loss 1.8904 (1.9101)	Acc@1 73.438 (74.070)	Acc@5 97.266 (95.554)
Epoch: [117][30/196]	Time 0.027 (0.017)	Data 0.001 (0.009)	Loss 2.0276 (1.9089)	Acc@1 72.266 (74.269)	Acc@5 93.359 (95.615)
Epoch: [117][40/196]	Time 0.014 (0.017)	Data 0.003 (0.007)	Loss 1.8518 (1.9065)	Acc@1 73.047 (74.162)	Acc@5 96.484 (95.579)
Epoch: [117][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.8768 (1.9122)	Acc@1 74.219 (74.020)	Acc@5 95.703 (95.473)
Epoch: [117][60/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0586 (1.9222)	Acc@1 71.484 (73.694)	Acc@5 92.188 (95.421)
Epoch: [117][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8328 (1.9231)	Acc@1 77.734 (73.685)	Acc@5 92.969 (95.417)
Epoch: [117][80/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0263 (1.9279)	Acc@1 71.484 (73.640)	Acc@5 96.094 (95.346)
Epoch: [117][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0149 (1.9349)	Acc@1 73.047 (73.532)	Acc@5 93.750 (95.252)
Epoch: [117][100/196]	Time 0.017 (0.016)	Data 0.003 (0.005)	Loss 1.9824 (1.9368)	Acc@1 71.094 (73.387)	Acc@5 95.312 (95.258)
Epoch: [117][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0377 (1.9442)	Acc@1 73.047 (73.219)	Acc@5 92.578 (95.144)
Epoch: [117][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0883 (1.9522)	Acc@1 70.312 (73.031)	Acc@5 94.922 (95.054)
Epoch: [117][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0378 (1.9584)	Acc@1 71.875 (72.889)	Acc@5 94.531 (94.946)
Epoch: [117][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1674 (1.9655)	Acc@1 68.359 (72.634)	Acc@5 94.531 (94.875)
Epoch: [117][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1906 (1.9749)	Acc@1 68.359 (72.439)	Acc@5 92.578 (94.767)
Epoch: [117][160/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 1.9872 (1.9791)	Acc@1 74.609 (72.382)	Acc@5 94.531 (94.650)
Epoch: [117][170/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 2.1379 (1.9822)	Acc@1 68.750 (72.314)	Acc@5 91.797 (94.568)
Epoch: [117][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0841 (1.9844)	Acc@1 73.438 (72.248)	Acc@5 94.141 (94.559)
Epoch: [117][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0472 (1.9874)	Acc@1 69.922 (72.204)	Acc@5 91.797 (94.496)
num momentum params: 26
[0.1, 1.9885962984466552, 2.1009668457508086, 72.148, 51.27, tensor(0.5288, device='cuda:0', grad_fn=<DivBackward0>), 3.049039363861084, 0.4014475345611572]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [118 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [118][0/196]	Time 0.066 (0.066)	Data 0.195 (0.195)	Loss 1.8487 (1.8487)	Acc@1 78.906 (78.906)	Acc@5 94.922 (94.922)
Epoch: [118][10/196]	Time 0.016 (0.021)	Data 0.002 (0.019)	Loss 1.8409 (1.9163)	Acc@1 77.734 (74.396)	Acc@5 96.875 (95.384)
Epoch: [118][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.7680 (1.9235)	Acc@1 79.297 (74.405)	Acc@5 96.875 (94.773)
Epoch: [118][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9153 (1.9072)	Acc@1 75.781 (74.672)	Acc@5 95.703 (95.413)
Epoch: [118][40/196]	Time 0.015 (0.017)	Data 0.004 (0.007)	Loss 1.8596 (1.8962)	Acc@1 75.391 (74.943)	Acc@5 95.703 (95.503)
Epoch: [118][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9988 (1.9060)	Acc@1 70.312 (74.525)	Acc@5 96.484 (95.404)
Epoch: [118][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.8132 (1.9078)	Acc@1 76.953 (74.449)	Acc@5 95.312 (95.364)
Epoch: [118][70/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 1.9171 (1.9172)	Acc@1 69.141 (74.147)	Acc@5 95.703 (95.274)
Epoch: [118][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8997 (1.9198)	Acc@1 73.828 (74.040)	Acc@5 95.312 (95.288)
Epoch: [118][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1469 (1.9255)	Acc@1 66.406 (73.923)	Acc@5 93.359 (95.278)
Epoch: [118][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.8928 (1.9273)	Acc@1 75.000 (73.913)	Acc@5 93.750 (95.258)
Epoch: [118][110/196]	Time 0.014 (0.016)	Data 0.001 (0.004)	Loss 1.9298 (1.9358)	Acc@1 75.391 (73.694)	Acc@5 94.922 (95.140)
Epoch: [118][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9782 (1.9416)	Acc@1 74.609 (73.550)	Acc@5 94.141 (95.032)
Epoch: [118][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0672 (1.9470)	Acc@1 71.484 (73.411)	Acc@5 91.797 (95.011)
Epoch: [118][140/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0658 (1.9564)	Acc@1 68.359 (73.183)	Acc@5 94.922 (94.936)
Epoch: [118][150/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0458 (1.9601)	Acc@1 72.656 (73.073)	Acc@5 94.922 (94.899)
Epoch: [118][160/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9462 (1.9622)	Acc@1 76.953 (73.032)	Acc@5 91.016 (94.885)
Epoch: [118][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9989 (1.9671)	Acc@1 70.703 (72.896)	Acc@5 94.922 (94.837)
Epoch: [118][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1601 (1.9721)	Acc@1 67.578 (72.719)	Acc@5 94.141 (94.788)
Epoch: [118][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0916 (1.9756)	Acc@1 69.141 (72.640)	Acc@5 92.969 (94.740)
num momentum params: 26
[0.1, 1.9776500165557862, 2.0116611647605898, 72.584, 50.53, tensor(0.5310, device='cuda:0', grad_fn=<DivBackward0>), 3.0238873958587646, 0.394193172454834]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [119 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [119][0/196]	Time 0.063 (0.063)	Data 0.195 (0.195)	Loss 1.8627 (1.8627)	Acc@1 75.781 (75.781)	Acc@5 94.922 (94.922)
Epoch: [119][10/196]	Time 0.018 (0.021)	Data 0.002 (0.019)	Loss 2.0188 (2.0216)	Acc@1 71.484 (71.023)	Acc@5 93.750 (94.354)
Epoch: [119][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.9244 (1.9952)	Acc@1 72.266 (72.117)	Acc@5 94.531 (94.401)
Epoch: [119][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.8541 (1.9532)	Acc@1 76.172 (73.072)	Acc@5 95.312 (94.947)
Epoch: [119][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.8548 (1.9437)	Acc@1 73.438 (73.342)	Acc@5 94.141 (94.950)
Epoch: [119][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0163 (1.9274)	Acc@1 72.656 (73.828)	Acc@5 96.094 (95.090)
Epoch: [119][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0211 (1.9346)	Acc@1 70.312 (73.591)	Acc@5 92.969 (95.127)
Epoch: [119][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8862 (1.9308)	Acc@1 74.219 (73.685)	Acc@5 96.484 (95.114)
Epoch: [119][80/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 2.0338 (1.9375)	Acc@1 68.750 (73.442)	Acc@5 92.578 (95.057)
Epoch: [119][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9368 (1.9393)	Acc@1 74.219 (73.523)	Acc@5 94.531 (94.999)
Epoch: [119][100/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0430 (1.9456)	Acc@1 71.484 (73.329)	Acc@5 94.922 (94.937)
Epoch: [119][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.9266 (1.9508)	Acc@1 76.172 (73.170)	Acc@5 96.094 (94.925)
Epoch: [119][120/196]	Time 0.026 (0.015)	Data 0.001 (0.005)	Loss 1.9121 (1.9578)	Acc@1 73.438 (72.963)	Acc@5 96.094 (94.831)
Epoch: [119][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0162 (1.9649)	Acc@1 69.922 (72.802)	Acc@5 93.359 (94.740)
Epoch: [119][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0215 (1.9753)	Acc@1 71.875 (72.551)	Acc@5 93.750 (94.634)
Epoch: [119][150/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9641 (1.9757)	Acc@1 74.609 (72.529)	Acc@5 92.578 (94.614)
Epoch: [119][160/196]	Time 0.024 (0.015)	Data 0.000 (0.005)	Loss 2.0358 (1.9794)	Acc@1 69.531 (72.440)	Acc@5 92.578 (94.563)
Epoch: [119][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2195 (1.9856)	Acc@1 66.016 (72.311)	Acc@5 91.797 (94.481)
Epoch: [119][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0734 (1.9889)	Acc@1 70.703 (72.251)	Acc@5 91.797 (94.417)
Epoch: [119][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9900 (1.9918)	Acc@1 71.875 (72.186)	Acc@5 94.531 (94.359)
num momentum params: 26
[0.1, 1.9956934938049316, 1.8395126628875733, 72.054, 54.09, tensor(0.5267, device='cuda:0', grad_fn=<DivBackward0>), 3.0162088871002197, 0.4035489559173584]
Non Pruning Epoch - module.conv1.weight: [43, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [43]
Non Pruning Epoch - module.bn1.bias: [43]
Non Pruning Epoch - module.conv2.weight: [128, 43, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [252, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [252]
Non Pruning Epoch - module.bn3.bias: [252]
Non Pruning Epoch - module.conv4.weight: [256, 252, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [475, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [475]
Non Pruning Epoch - module.bn7.bias: [475]
Non Pruning Epoch - module.conv8.weight: [310, 475, 3, 3]
Non Pruning Epoch - module.bn8.weight: [310]
Non Pruning Epoch - module.bn8.bias: [310]
Non Pruning Epoch - module.fc.weight: [100, 310]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [120 | 180] LR: 0.100000
module.conv1.weight [43, 3, 3, 3]
module.conv2.weight [128, 43, 3, 3]
module.conv3.weight [252, 128, 3, 3]
module.conv4.weight [256, 252, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [475, 509, 3, 3]
module.conv8.weight [310, 475, 3, 3]
Epoch: [120][0/196]	Time 0.060 (0.060)	Data 0.189 (0.189)	Loss 1.8070 (1.8070)	Acc@1 75.781 (75.781)	Acc@5 94.922 (94.922)
Epoch: [120][10/196]	Time 0.013 (0.019)	Data 0.003 (0.020)	Loss 2.1900 (1.9683)	Acc@1 68.750 (73.366)	Acc@5 91.797 (94.744)
Epoch: [120][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 1.8917 (1.9573)	Acc@1 76.172 (73.363)	Acc@5 96.484 (94.903)
Epoch: [120][30/196]	Time 0.016 (0.017)	Data 0.002 (0.009)	Loss 1.8687 (1.9361)	Acc@1 76.562 (74.105)	Acc@5 96.875 (95.199)
Epoch: [120][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.8240 (1.9275)	Acc@1 75.391 (74.371)	Acc@5 96.875 (95.322)
Epoch: [120][50/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 1.9899 (1.9302)	Acc@1 70.312 (74.203)	Acc@5 94.141 (95.190)
Epoch: [120][60/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 1.8374 (1.9224)	Acc@1 75.000 (74.232)	Acc@5 98.047 (95.293)
Epoch: [120][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.0042 (1.9292)	Acc@1 73.047 (74.054)	Acc@5 94.531 (95.158)
Epoch: [120][80/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.8555 (1.9275)	Acc@1 77.344 (74.045)	Acc@5 95.312 (95.134)
Epoch: [120][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0332 (1.9362)	Acc@1 74.219 (73.845)	Acc@5 93.359 (95.072)
Epoch: [120][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0310 (1.9457)	Acc@1 69.531 (73.507)	Acc@5 93.750 (95.007)
Epoch: [120][110/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.0368 (1.9516)	Acc@1 67.969 (73.335)	Acc@5 97.656 (94.950)
Epoch: [120][120/196]	Time 0.017 (0.015)	Data 0.002 (0.004)	Loss 1.9535 (1.9572)	Acc@1 73.828 (73.160)	Acc@5 93.359 (94.880)
Epoch: [120][130/196]	Time 0.013 (0.015)	Data 0.029 (0.004)	Loss 1.9852 (1.9640)	Acc@1 69.922 (73.011)	Acc@5 96.875 (94.770)
Epoch: [120][140/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0912 (1.9740)	Acc@1 68.359 (72.792)	Acc@5 91.797 (94.684)
Epoch: [120][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 1.9891 (1.9808)	Acc@1 72.656 (72.623)	Acc@5 93.359 (94.591)
Epoch: [120][160/196]	Time 0.020 (0.015)	Data 0.001 (0.004)	Loss 2.1991 (1.9886)	Acc@1 69.141 (72.411)	Acc@5 93.359 (94.553)
Epoch: [120][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2564 (1.9943)	Acc@1 68.750 (72.302)	Acc@5 89.453 (94.454)
Epoch: [120][180/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1027 (2.0002)	Acc@1 70.703 (72.117)	Acc@5 92.188 (94.378)
Epoch: [120][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1666 (2.0049)	Acc@1 72.656 (71.996)	Acc@5 93.359 (94.329)
num momentum params: 26
[0.1, 2.00554675201416, 1.9582078683376312, 71.964, 52.51, tensor(0.5249, device='cuda:0', grad_fn=<DivBackward0>), 2.997804880142212, 0.4054365158081055]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [43, 3, 3, 3]
Before - module.bn1.weight: [43]
Before - module.bn1.bias: [43]
Before - module.conv2.weight: [128, 43, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [252, 128, 3, 3]
Before - module.bn3.weight: [252]
Before - module.bn3.bias: [252]
Before - module.conv4.weight: [256, 252, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [509, 510, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [475, 509, 3, 3]
Before - module.bn7.weight: [475]
Before - module.bn7.bias: [475]
Before - module.conv8.weight: [310, 475, 3, 3]
Before - module.bn8.weight: [310]
Before - module.bn8.bias: [310]
Before - module.fc.weight: [100, 310]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [43, 3, 3, 3] >> [42, 3, 3, 3]
[module.bn1.weight]: 43 >> 42
running_mean [42]
running_var [42]
num_batches_tracked []
[module.conv2.weight]: [128, 43, 3, 3] >> [128, 42, 3, 3]
[module.conv3.weight]: [252, 128, 3, 3] >> [250, 128, 3, 3]
[module.bn3.weight]: 252 >> 250
running_mean [250]
running_var [250]
num_batches_tracked []
[module.conv4.weight]: [256, 252, 3, 3] >> [256, 250, 3, 3]
[module.conv5.weight]: [510, 256, 3, 3] >> [509, 256, 3, 3]
[module.bn5.weight]: 510 >> 509
running_mean [509]
running_var [509]
num_batches_tracked []
[module.conv6.weight]: [509, 510, 3, 3] >> [509, 509, 3, 3]
[module.conv7.weight]: [475, 509, 3, 3] >> [474, 509, 3, 3]
[module.bn7.weight]: 475 >> 474
running_mean [474]
running_var [474]
num_batches_tracked []
[module.conv8.weight]: [310, 475, 3, 3] >> [306, 474, 3, 3]
[module.bn8.weight]: 310 >> 306
running_mean [306]
running_var [306]
num_batches_tracked []
[module.fc.weight]: [100, 310] >> [100, 306]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [250, 128, 3, 3]
After - module.bn3.weight: [250]
After - module.bn3.bias: [250]
After - module.conv4.weight: [256, 250, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [509, 256, 3, 3]
After - module.bn5.weight: [509]
After - module.bn5.bias: [509]
After - module.conv6.weight: [509, 509, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [474, 509, 3, 3]
After - module.bn7.weight: [474]
After - module.bn7.bias: [474]
After - module.conv8.weight: [306, 474, 3, 3]
After - module.bn8.weight: [306]
After - module.bn8.bias: [306]
After - module.fc.weight: [100, 306]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [250, 128, 3, 3]
conv4 --> [256, 250, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [509, 509, 3, 3]
conv7 --> [474, 509, 3, 3]
conv8 --> [306, 474, 3, 3]
fc --> [306, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8404992000, 18432000, 250
4, 16809984000, 36864000, 256
5, 10207494144, 18763776, 509
6, 20295369216, 37307664, 509
7, 6670522368, 8685576, 474
8, 4010176512, 5221584, 306
fc, 11750400, 30600, 0
===================
FLOP REPORT: 28145637000000.0 51027200000.0 138852720 127568 2474 15.12175178527832
[INFO] Storing checkpoint...

Epoch: [121 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [121][0/196]	Time 0.778 (0.778)	Data 0.211 (0.211)	Loss 1.8382 (1.8382)	Acc@1 77.734 (77.734)	Acc@5 94.141 (94.141)
Epoch: [121][10/196]	Time 0.015 (0.085)	Data 0.002 (0.021)	Loss 1.9909 (1.9475)	Acc@1 77.344 (74.290)	Acc@5 93.750 (94.354)
Epoch: [121][20/196]	Time 0.014 (0.052)	Data 0.002 (0.012)	Loss 1.9855 (1.9586)	Acc@1 73.047 (73.140)	Acc@5 95.312 (94.699)
Epoch: [121][30/196]	Time 0.014 (0.040)	Data 0.007 (0.009)	Loss 1.8761 (1.9368)	Acc@1 75.000 (73.891)	Acc@5 96.484 (95.060)
Epoch: [121][40/196]	Time 0.018 (0.034)	Data 0.001 (0.008)	Loss 1.9089 (1.9310)	Acc@1 73.828 (73.876)	Acc@5 96.875 (95.265)
Epoch: [121][50/196]	Time 0.011 (0.030)	Data 0.010 (0.007)	Loss 1.9099 (1.9270)	Acc@1 74.219 (73.966)	Acc@5 95.312 (95.175)
Epoch: [121][60/196]	Time 0.017 (0.028)	Data 0.001 (0.006)	Loss 1.9400 (1.9333)	Acc@1 73.438 (73.867)	Acc@5 95.703 (95.050)
Epoch: [121][70/196]	Time 0.012 (0.026)	Data 0.008 (0.006)	Loss 1.8352 (1.9331)	Acc@1 75.391 (73.779)	Acc@5 95.312 (95.087)
Epoch: [121][80/196]	Time 0.017 (0.025)	Data 0.000 (0.005)	Loss 1.8529 (1.9321)	Acc@1 75.000 (73.751)	Acc@5 97.266 (95.129)
Epoch: [121][90/196]	Time 0.013 (0.024)	Data 0.004 (0.005)	Loss 1.9300 (1.9362)	Acc@1 69.922 (73.631)	Acc@5 96.484 (95.064)
Epoch: [121][100/196]	Time 0.013 (0.023)	Data 0.004 (0.005)	Loss 1.9462 (1.9404)	Acc@1 71.094 (73.410)	Acc@5 95.312 (95.011)
Epoch: [121][110/196]	Time 0.013 (0.022)	Data 0.004 (0.004)	Loss 1.8148 (1.9501)	Acc@1 75.781 (73.124)	Acc@5 95.703 (94.837)
Epoch: [121][120/196]	Time 0.018 (0.022)	Data 0.001 (0.004)	Loss 2.0004 (1.9538)	Acc@1 71.875 (73.060)	Acc@5 92.969 (94.786)
Epoch: [121][130/196]	Time 0.012 (0.021)	Data 0.010 (0.004)	Loss 2.0951 (1.9611)	Acc@1 70.703 (72.916)	Acc@5 92.188 (94.722)
Epoch: [121][140/196]	Time 0.014 (0.021)	Data 0.002 (0.004)	Loss 2.0782 (1.9696)	Acc@1 71.875 (72.723)	Acc@5 94.531 (94.601)
Epoch: [121][150/196]	Time 0.012 (0.020)	Data 0.007 (0.004)	Loss 1.9243 (1.9744)	Acc@1 75.000 (72.605)	Acc@5 94.141 (94.521)
Epoch: [121][160/196]	Time 0.012 (0.020)	Data 0.020 (0.004)	Loss 2.1375 (1.9833)	Acc@1 66.016 (72.363)	Acc@5 94.141 (94.446)
Epoch: [121][170/196]	Time 0.017 (0.020)	Data 0.000 (0.004)	Loss 2.0584 (1.9886)	Acc@1 71.875 (72.215)	Acc@5 92.188 (94.362)
Epoch: [121][180/196]	Time 0.011 (0.020)	Data 0.020 (0.004)	Loss 2.1331 (1.9949)	Acc@1 65.234 (72.043)	Acc@5 93.359 (94.292)
Epoch: [121][190/196]	Time 0.016 (0.019)	Data 0.000 (0.004)	Loss 2.2752 (1.9995)	Acc@1 64.453 (71.942)	Acc@5 92.969 (94.261)
num momentum params: 26
[0.1, 2.0009941790771486, 2.2443006408214567, 71.886, 47.44, tensor(0.5267, device='cuda:0', grad_fn=<DivBackward0>), 4.046719312667847, 0.501910924911499]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [122 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [122][0/196]	Time 0.057 (0.057)	Data 0.209 (0.209)	Loss 1.8444 (1.8444)	Acc@1 75.000 (75.000)	Acc@5 96.484 (96.484)
Epoch: [122][10/196]	Time 0.015 (0.020)	Data 0.002 (0.021)	Loss 1.9308 (1.9179)	Acc@1 76.562 (73.828)	Acc@5 95.312 (95.703)
Epoch: [122][20/196]	Time 0.014 (0.018)	Data 0.004 (0.012)	Loss 2.0144 (1.9173)	Acc@1 72.656 (73.438)	Acc@5 94.141 (95.517)
Epoch: [122][30/196]	Time 0.017 (0.017)	Data 0.002 (0.009)	Loss 1.9777 (1.9203)	Acc@1 71.875 (73.879)	Acc@5 94.141 (95.287)
Epoch: [122][40/196]	Time 0.014 (0.017)	Data 0.005 (0.007)	Loss 1.9519 (1.9104)	Acc@1 74.609 (74.352)	Acc@5 94.531 (95.389)
Epoch: [122][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9563 (1.9118)	Acc@1 73.438 (74.364)	Acc@5 92.969 (95.320)
Epoch: [122][60/196]	Time 0.011 (0.016)	Data 0.007 (0.006)	Loss 1.9421 (1.9232)	Acc@1 75.391 (73.860)	Acc@5 94.531 (95.357)
Epoch: [122][70/196]	Time 0.021 (0.016)	Data 0.001 (0.006)	Loss 1.9521 (1.9244)	Acc@1 70.703 (73.801)	Acc@5 94.531 (95.362)
Epoch: [122][80/196]	Time 0.012 (0.016)	Data 0.021 (0.006)	Loss 2.0279 (1.9278)	Acc@1 72.656 (73.703)	Acc@5 94.531 (95.269)
Epoch: [122][90/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8958 (1.9276)	Acc@1 73.047 (73.691)	Acc@5 94.531 (95.244)
Epoch: [122][100/196]	Time 0.012 (0.016)	Data 0.015 (0.006)	Loss 1.8353 (1.9226)	Acc@1 77.344 (73.971)	Acc@5 96.875 (95.196)
Epoch: [122][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9711 (1.9272)	Acc@1 74.609 (73.874)	Acc@5 96.875 (95.182)
Epoch: [122][120/196]	Time 0.012 (0.016)	Data 0.019 (0.005)	Loss 2.1503 (1.9377)	Acc@1 67.188 (73.573)	Acc@5 93.359 (95.035)
Epoch: [122][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0783 (1.9482)	Acc@1 68.750 (73.235)	Acc@5 94.922 (94.904)
Epoch: [122][140/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 2.0685 (1.9552)	Acc@1 69.141 (72.989)	Acc@5 93.359 (94.789)
Epoch: [122][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0061 (1.9623)	Acc@1 69.922 (72.822)	Acc@5 93.750 (94.705)
Epoch: [122][160/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.0593 (1.9696)	Acc@1 68.359 (72.688)	Acc@5 93.750 (94.611)
Epoch: [122][170/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9720 (1.9729)	Acc@1 69.531 (72.613)	Acc@5 94.531 (94.572)
Epoch: [122][180/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.1803 (1.9784)	Acc@1 64.453 (72.432)	Acc@5 94.922 (94.559)
Epoch: [122][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1637 (1.9853)	Acc@1 69.531 (72.239)	Acc@5 91.016 (94.505)
num momentum params: 26
[0.1, 1.9888182872772218, 1.9042064714431763, 72.162, 53.14, tensor(0.5294, device='cuda:0', grad_fn=<DivBackward0>), 3.0321290493011475, 0.4004857540130615]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [123 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [123][0/196]	Time 0.057 (0.057)	Data 0.210 (0.210)	Loss 1.8996 (1.8996)	Acc@1 74.219 (74.219)	Acc@5 94.531 (94.531)
Epoch: [123][10/196]	Time 0.015 (0.020)	Data 0.002 (0.021)	Loss 2.0151 (1.9479)	Acc@1 75.000 (73.260)	Acc@5 93.359 (95.099)
Epoch: [123][20/196]	Time 0.015 (0.017)	Data 0.003 (0.012)	Loss 1.9921 (1.9380)	Acc@1 73.047 (73.382)	Acc@5 94.531 (95.108)
Epoch: [123][30/196]	Time 0.013 (0.016)	Data 0.006 (0.009)	Loss 1.8455 (1.9273)	Acc@1 75.391 (73.589)	Acc@5 97.266 (95.149)
Epoch: [123][40/196]	Time 0.014 (0.016)	Data 0.002 (0.008)	Loss 1.9682 (1.9326)	Acc@1 75.000 (73.599)	Acc@5 90.625 (95.065)
Epoch: [123][50/196]	Time 0.012 (0.016)	Data 0.017 (0.008)	Loss 1.9089 (1.9250)	Acc@1 72.656 (73.736)	Acc@5 94.531 (95.320)
Epoch: [123][60/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 1.8182 (1.9225)	Acc@1 76.953 (73.803)	Acc@5 97.266 (95.402)
Epoch: [123][70/196]	Time 0.012 (0.016)	Data 0.011 (0.007)	Loss 1.9192 (1.9232)	Acc@1 71.875 (73.713)	Acc@5 95.703 (95.274)
Epoch: [123][80/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9047 (1.9249)	Acc@1 70.703 (73.635)	Acc@5 96.094 (95.274)
Epoch: [123][90/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 2.0016 (1.9296)	Acc@1 72.656 (73.472)	Acc@5 94.141 (95.244)
Epoch: [123][100/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0673 (1.9296)	Acc@1 67.969 (73.492)	Acc@5 94.922 (95.193)
Epoch: [123][110/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 1.9381 (1.9342)	Acc@1 70.703 (73.300)	Acc@5 96.484 (95.172)
Epoch: [123][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9663 (1.9405)	Acc@1 73.438 (73.111)	Acc@5 94.531 (95.099)
Epoch: [123][130/196]	Time 0.015 (0.015)	Data 0.012 (0.005)	Loss 2.0125 (1.9462)	Acc@1 72.656 (73.005)	Acc@5 93.750 (95.008)
Epoch: [123][140/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0056 (1.9537)	Acc@1 73.828 (72.836)	Acc@5 91.797 (94.886)
Epoch: [123][150/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.1174 (1.9597)	Acc@1 66.406 (72.716)	Acc@5 92.578 (94.813)
Epoch: [123][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1032 (1.9668)	Acc@1 68.359 (72.557)	Acc@5 91.406 (94.684)
Epoch: [123][170/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.0470 (1.9725)	Acc@1 72.656 (72.403)	Acc@5 93.359 (94.627)
Epoch: [123][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1237 (1.9805)	Acc@1 73.828 (72.238)	Acc@5 90.625 (94.518)
Epoch: [123][190/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.0796 (1.9900)	Acc@1 69.141 (72.018)	Acc@5 94.141 (94.382)
num momentum params: 26
[0.1, 1.9927961626434325, 2.090343414545059, 71.978, 50.07, tensor(0.5283, device='cuda:0', grad_fn=<DivBackward0>), 3.012218475341797, 0.40518450736999506]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [124 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [124][0/196]	Time 0.063 (0.063)	Data 0.222 (0.222)	Loss 1.9867 (1.9867)	Acc@1 71.875 (71.875)	Acc@5 94.141 (94.141)
Epoch: [124][10/196]	Time 0.017 (0.021)	Data 0.002 (0.022)	Loss 2.0285 (1.9618)	Acc@1 72.266 (73.047)	Acc@5 92.969 (94.780)
Epoch: [124][20/196]	Time 0.013 (0.018)	Data 0.005 (0.013)	Loss 1.8661 (1.9442)	Acc@1 76.562 (73.400)	Acc@5 94.531 (94.866)
Epoch: [124][30/196]	Time 0.013 (0.017)	Data 0.005 (0.009)	Loss 1.9569 (1.9295)	Acc@1 74.219 (73.866)	Acc@5 96.484 (95.048)
Epoch: [124][40/196]	Time 0.012 (0.016)	Data 0.005 (0.008)	Loss 2.0116 (1.9230)	Acc@1 70.312 (73.904)	Acc@5 92.969 (95.236)
Epoch: [124][50/196]	Time 0.012 (0.016)	Data 0.019 (0.007)	Loss 1.8411 (1.9263)	Acc@1 74.609 (73.706)	Acc@5 96.484 (95.182)
Epoch: [124][60/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 1.9387 (1.9310)	Acc@1 71.484 (73.591)	Acc@5 95.703 (95.076)
Epoch: [124][70/196]	Time 0.012 (0.016)	Data 0.019 (0.007)	Loss 1.9365 (1.9385)	Acc@1 72.656 (73.410)	Acc@5 95.703 (95.048)
Epoch: [124][80/196]	Time 0.020 (0.016)	Data 0.000 (0.007)	Loss 1.9521 (1.9437)	Acc@1 73.438 (73.312)	Acc@5 94.922 (94.941)
Epoch: [124][90/196]	Time 0.013 (0.016)	Data 0.019 (0.007)	Loss 1.9157 (1.9476)	Acc@1 72.266 (73.214)	Acc@5 94.141 (94.883)
Epoch: [124][100/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.8964 (1.9466)	Acc@1 75.391 (73.260)	Acc@5 95.312 (94.883)
Epoch: [124][110/196]	Time 0.011 (0.016)	Data 0.010 (0.006)	Loss 1.8559 (1.9526)	Acc@1 74.609 (73.082)	Acc@5 95.312 (94.795)
Epoch: [124][120/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9052 (1.9563)	Acc@1 73.047 (72.995)	Acc@5 95.312 (94.786)
Epoch: [124][130/196]	Time 0.011 (0.016)	Data 0.008 (0.006)	Loss 2.1616 (1.9650)	Acc@1 66.797 (72.767)	Acc@5 92.578 (94.686)
Epoch: [124][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9304 (1.9729)	Acc@1 75.391 (72.645)	Acc@5 93.359 (94.567)
Epoch: [124][150/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0417 (1.9772)	Acc@1 69.141 (72.501)	Acc@5 94.531 (94.557)
Epoch: [124][160/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0605 (1.9819)	Acc@1 68.359 (72.351)	Acc@5 94.141 (94.519)
Epoch: [124][170/196]	Time 0.012 (0.016)	Data 0.012 (0.005)	Loss 2.1397 (1.9851)	Acc@1 68.359 (72.268)	Acc@5 92.578 (94.465)
Epoch: [124][180/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0508 (1.9865)	Acc@1 67.578 (72.169)	Acc@5 93.359 (94.434)
Epoch: [124][190/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.2053 (1.9888)	Acc@1 67.188 (72.139)	Acc@5 92.188 (94.413)
num momentum params: 26
[0.1, 1.9896655257797242, 1.8072623085975648, 72.12, 55.02, tensor(0.5294, device='cuda:0', grad_fn=<DivBackward0>), 3.037520408630371, 0.4025704860687256]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [125 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [125][0/196]	Time 0.057 (0.057)	Data 0.226 (0.226)	Loss 1.7820 (1.7820)	Acc@1 78.516 (78.516)	Acc@5 95.703 (95.703)
Epoch: [125][10/196]	Time 0.016 (0.020)	Data 0.001 (0.022)	Loss 1.8928 (1.9065)	Acc@1 71.875 (73.580)	Acc@5 96.094 (95.526)
Epoch: [125][20/196]	Time 0.014 (0.018)	Data 0.003 (0.013)	Loss 1.7683 (1.8938)	Acc@1 78.125 (74.386)	Acc@5 96.875 (95.796)
Epoch: [125][30/196]	Time 0.016 (0.017)	Data 0.001 (0.010)	Loss 2.0123 (1.9013)	Acc@1 70.312 (74.257)	Acc@5 95.703 (95.766)
Epoch: [125][40/196]	Time 0.016 (0.016)	Data 0.001 (0.009)	Loss 2.1604 (1.9162)	Acc@1 69.922 (74.162)	Acc@5 92.969 (95.446)
Epoch: [125][50/196]	Time 0.016 (0.016)	Data 0.001 (0.008)	Loss 1.8599 (1.9150)	Acc@1 73.438 (74.127)	Acc@5 96.484 (95.412)
Epoch: [125][60/196]	Time 0.016 (0.016)	Data 0.001 (0.008)	Loss 2.0048 (1.9112)	Acc@1 72.266 (74.264)	Acc@5 94.922 (95.383)
Epoch: [125][70/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 2.0560 (1.9174)	Acc@1 71.875 (74.103)	Acc@5 92.969 (95.373)
Epoch: [125][80/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 2.1199 (1.9237)	Acc@1 68.750 (73.881)	Acc@5 94.141 (95.288)
Epoch: [125][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8430 (1.9296)	Acc@1 73.828 (73.674)	Acc@5 96.484 (95.244)
Epoch: [125][100/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9035 (1.9346)	Acc@1 74.609 (73.526)	Acc@5 94.141 (95.181)
Epoch: [125][110/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.8842 (1.9401)	Acc@1 74.609 (73.353)	Acc@5 97.266 (95.101)
Epoch: [125][120/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0016 (1.9456)	Acc@1 75.391 (73.170)	Acc@5 93.750 (95.064)
Epoch: [125][130/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.2170 (1.9535)	Acc@1 67.578 (72.969)	Acc@5 92.188 (94.958)
Epoch: [125][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1068 (1.9614)	Acc@1 69.531 (72.770)	Acc@5 92.969 (94.872)
Epoch: [125][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0492 (1.9698)	Acc@1 71.484 (72.522)	Acc@5 94.531 (94.798)
Epoch: [125][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0408 (1.9759)	Acc@1 70.312 (72.329)	Acc@5 92.969 (94.759)
Epoch: [125][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1940 (1.9823)	Acc@1 66.406 (72.222)	Acc@5 91.406 (94.657)
Epoch: [125][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2046 (1.9891)	Acc@1 66.406 (72.056)	Acc@5 91.797 (94.572)
Epoch: [125][190/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0162 (1.9973)	Acc@1 69.922 (71.820)	Acc@5 96.094 (94.464)
num momentum params: 26
[0.1, 2.001383427581787, 2.163615936040878, 71.738, 50.33, tensor(0.5264, device='cuda:0', grad_fn=<DivBackward0>), 3.007266044616699, 0.4136581420898438]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [126 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [126][0/196]	Time 0.060 (0.060)	Data 0.210 (0.210)	Loss 1.9331 (1.9331)	Acc@1 71.875 (71.875)	Acc@5 96.094 (96.094)
Epoch: [126][10/196]	Time 0.012 (0.019)	Data 0.005 (0.022)	Loss 2.0229 (1.9596)	Acc@1 72.266 (73.153)	Acc@5 96.484 (95.348)
Epoch: [126][20/196]	Time 0.013 (0.017)	Data 0.022 (0.013)	Loss 1.9699 (1.9464)	Acc@1 72.266 (73.382)	Acc@5 95.703 (95.387)
Epoch: [126][30/196]	Time 0.013 (0.017)	Data 0.019 (0.010)	Loss 1.8612 (1.9439)	Acc@1 78.125 (73.715)	Acc@5 94.922 (95.275)
Epoch: [126][40/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 1.9110 (1.9362)	Acc@1 72.266 (73.590)	Acc@5 94.531 (95.322)
Epoch: [126][50/196]	Time 0.012 (0.016)	Data 0.008 (0.007)	Loss 1.9524 (1.9289)	Acc@1 73.828 (73.920)	Acc@5 93.359 (95.305)
Epoch: [126][60/196]	Time 0.015 (0.016)	Data 0.001 (0.007)	Loss 1.8503 (1.9150)	Acc@1 76.562 (74.257)	Acc@5 95.312 (95.377)
Epoch: [126][70/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 1.9144 (1.9104)	Acc@1 73.438 (74.444)	Acc@5 96.094 (95.406)
Epoch: [126][80/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.0765 (1.9247)	Acc@1 66.797 (73.876)	Acc@5 93.359 (95.221)
Epoch: [126][90/196]	Time 0.013 (0.016)	Data 0.009 (0.006)	Loss 1.9157 (1.9377)	Acc@1 75.391 (73.601)	Acc@5 96.484 (95.072)
Epoch: [126][100/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.1217 (1.9487)	Acc@1 70.312 (73.283)	Acc@5 92.578 (95.011)
Epoch: [126][110/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.1018 (1.9550)	Acc@1 69.141 (73.096)	Acc@5 92.188 (94.911)
Epoch: [126][120/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0523 (1.9583)	Acc@1 71.484 (72.986)	Acc@5 93.359 (94.870)
Epoch: [126][130/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.1637 (1.9671)	Acc@1 67.578 (72.749)	Acc@5 92.578 (94.758)
Epoch: [126][140/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0629 (1.9717)	Acc@1 73.438 (72.568)	Acc@5 93.359 (94.681)
Epoch: [126][150/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 2.0820 (1.9797)	Acc@1 67.578 (72.346)	Acc@5 92.578 (94.614)
Epoch: [126][160/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.1802 (1.9872)	Acc@1 69.922 (72.244)	Acc@5 90.234 (94.541)
Epoch: [126][170/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.9485 (1.9900)	Acc@1 74.219 (72.147)	Acc@5 94.531 (94.515)
Epoch: [126][180/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.2432 (1.9938)	Acc@1 68.750 (72.050)	Acc@5 89.453 (94.479)
Epoch: [126][190/196]	Time 0.014 (0.015)	Data 0.017 (0.005)	Loss 1.9830 (1.9959)	Acc@1 74.219 (72.059)	Acc@5 95.703 (94.445)
num momentum params: 26
[0.1, 1.9984979386901855, 1.8867921507358552, 71.98, 52.99, tensor(0.5284, device='cuda:0', grad_fn=<DivBackward0>), 2.92505145072937, 0.4087309837341308]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [127 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [127][0/196]	Time 0.057 (0.057)	Data 0.202 (0.202)	Loss 1.9089 (1.9089)	Acc@1 73.438 (73.438)	Acc@5 94.141 (94.141)
Epoch: [127][10/196]	Time 0.017 (0.022)	Data 0.002 (0.020)	Loss 1.7926 (1.9316)	Acc@1 75.781 (73.473)	Acc@5 95.312 (95.490)
Epoch: [127][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 1.9744 (1.9357)	Acc@1 70.312 (73.568)	Acc@5 95.703 (95.238)
Epoch: [127][30/196]	Time 0.016 (0.018)	Data 0.004 (0.009)	Loss 1.9590 (1.9359)	Acc@1 71.875 (73.816)	Acc@5 94.141 (95.287)
Epoch: [127][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8265 (1.9160)	Acc@1 75.000 (74.285)	Acc@5 95.703 (95.427)
Epoch: [127][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.9734 (1.9126)	Acc@1 71.094 (74.219)	Acc@5 94.531 (95.450)
Epoch: [127][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9417 (1.9112)	Acc@1 74.609 (74.200)	Acc@5 95.703 (95.562)
Epoch: [127][70/196]	Time 0.013 (0.016)	Data 0.022 (0.006)	Loss 1.9921 (1.9125)	Acc@1 71.094 (74.202)	Acc@5 93.750 (95.434)
Epoch: [127][80/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0074 (1.9160)	Acc@1 69.922 (74.084)	Acc@5 94.531 (95.361)
Epoch: [127][90/196]	Time 0.012 (0.016)	Data 0.020 (0.006)	Loss 2.1793 (1.9238)	Acc@1 67.969 (73.854)	Acc@5 90.234 (95.227)
Epoch: [127][100/196]	Time 0.019 (0.016)	Data 0.000 (0.006)	Loss 1.9246 (1.9256)	Acc@1 74.609 (73.681)	Acc@5 94.922 (95.200)
Epoch: [127][110/196]	Time 0.012 (0.016)	Data 0.020 (0.006)	Loss 1.9694 (1.9286)	Acc@1 73.828 (73.691)	Acc@5 94.141 (95.112)
Epoch: [127][120/196]	Time 0.012 (0.016)	Data 0.001 (0.006)	Loss 1.9612 (1.9296)	Acc@1 73.828 (73.715)	Acc@5 95.312 (95.096)
Epoch: [127][130/196]	Time 0.011 (0.016)	Data 0.012 (0.005)	Loss 2.0089 (1.9395)	Acc@1 75.000 (73.479)	Acc@5 92.969 (94.949)
Epoch: [127][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.3195 (1.9472)	Acc@1 64.453 (73.302)	Acc@5 92.578 (94.919)
Epoch: [127][150/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 2.0208 (1.9558)	Acc@1 71.094 (73.073)	Acc@5 93.750 (94.837)
Epoch: [127][160/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9643 (1.9602)	Acc@1 73.047 (72.981)	Acc@5 94.141 (94.733)
Epoch: [127][170/196]	Time 0.012 (0.016)	Data 0.011 (0.005)	Loss 1.9843 (1.9659)	Acc@1 71.094 (72.830)	Acc@5 95.703 (94.666)
Epoch: [127][180/196]	Time 0.020 (0.016)	Data 0.000 (0.005)	Loss 2.1395 (1.9721)	Acc@1 66.797 (72.656)	Acc@5 92.188 (94.615)
Epoch: [127][190/196]	Time 0.012 (0.016)	Data 0.012 (0.005)	Loss 2.0865 (1.9775)	Acc@1 69.531 (72.489)	Acc@5 92.969 (94.550)
num momentum params: 26
[0.1, 1.9808140298461914, 1.778629525899887, 72.37, 54.96, tensor(0.5324, device='cuda:0', grad_fn=<DivBackward0>), 3.0803062915802006, 0.40504789352416987]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [128 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [128][0/196]	Time 0.060 (0.060)	Data 0.224 (0.224)	Loss 1.9172 (1.9172)	Acc@1 74.609 (74.609)	Acc@5 95.312 (95.312)
Epoch: [128][10/196]	Time 0.016 (0.021)	Data 0.001 (0.023)	Loss 2.0518 (1.9798)	Acc@1 69.531 (72.408)	Acc@5 94.922 (94.567)
Epoch: [128][20/196]	Time 0.013 (0.018)	Data 0.004 (0.013)	Loss 1.8970 (1.9502)	Acc@1 76.172 (73.605)	Acc@5 95.703 (95.033)
Epoch: [128][30/196]	Time 0.015 (0.017)	Data 0.002 (0.010)	Loss 1.7879 (1.9343)	Acc@1 77.344 (73.979)	Acc@5 97.656 (95.338)
Epoch: [128][40/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 2.0316 (1.9278)	Acc@1 75.781 (74.304)	Acc@5 91.406 (95.246)
Epoch: [128][50/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.9638 (1.9243)	Acc@1 71.484 (74.150)	Acc@5 95.703 (95.182)
Epoch: [128][60/196]	Time 0.018 (0.016)	Data 0.001 (0.007)	Loss 1.7829 (1.9186)	Acc@1 80.859 (74.353)	Acc@5 96.875 (95.223)
Epoch: [128][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8689 (1.9217)	Acc@1 76.953 (74.252)	Acc@5 94.141 (95.213)
Epoch: [128][80/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.0823 (1.9207)	Acc@1 71.484 (74.286)	Acc@5 95.312 (95.255)
Epoch: [128][90/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9174 (1.9227)	Acc@1 72.656 (74.266)	Acc@5 95.703 (95.149)
Epoch: [128][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9650 (1.9292)	Acc@1 70.703 (73.936)	Acc@5 96.875 (95.100)
Epoch: [128][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0358 (1.9352)	Acc@1 71.094 (73.747)	Acc@5 93.750 (95.045)
Epoch: [128][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1353 (1.9440)	Acc@1 67.969 (73.502)	Acc@5 92.188 (94.919)
Epoch: [128][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9232 (1.9531)	Acc@1 75.000 (73.297)	Acc@5 94.922 (94.853)
Epoch: [128][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1227 (1.9626)	Acc@1 67.188 (72.939)	Acc@5 94.141 (94.736)
Epoch: [128][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0607 (1.9713)	Acc@1 71.875 (72.718)	Acc@5 94.141 (94.663)
Epoch: [128][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8838 (1.9780)	Acc@1 75.391 (72.542)	Acc@5 96.094 (94.628)
Epoch: [128][170/196]	Time 0.011 (0.016)	Data 0.005 (0.004)	Loss 1.9576 (1.9806)	Acc@1 74.609 (72.485)	Acc@5 94.141 (94.604)
Epoch: [128][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0275 (1.9852)	Acc@1 69.141 (72.363)	Acc@5 94.922 (94.577)
Epoch: [128][190/196]	Time 0.011 (0.016)	Data 0.016 (0.004)	Loss 2.0090 (1.9888)	Acc@1 70.703 (72.298)	Acc@5 94.141 (94.517)
num momentum params: 26
[0.1, 1.9905909136962892, 1.8685564768314362, 72.246, 54.53, tensor(0.5297, device='cuda:0', grad_fn=<DivBackward0>), 3.0470263957977295, 0.3971419334411621]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [129 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [129][0/196]	Time 0.068 (0.068)	Data 0.193 (0.193)	Loss 1.7486 (1.7486)	Acc@1 78.516 (78.516)	Acc@5 98.047 (98.047)
Epoch: [129][10/196]	Time 0.015 (0.020)	Data 0.002 (0.020)	Loss 1.9103 (1.9554)	Acc@1 74.219 (72.266)	Acc@5 95.703 (95.632)
Epoch: [129][20/196]	Time 0.015 (0.018)	Data 0.003 (0.011)	Loss 1.7612 (1.9563)	Acc@1 75.391 (72.507)	Acc@5 96.875 (95.554)
Epoch: [129][30/196]	Time 0.012 (0.017)	Data 0.005 (0.009)	Loss 1.9488 (1.9487)	Acc@1 75.781 (73.148)	Acc@5 95.312 (95.287)
Epoch: [129][40/196]	Time 0.016 (0.017)	Data 0.001 (0.007)	Loss 1.7898 (1.9304)	Acc@1 78.516 (73.666)	Acc@5 94.922 (95.255)
Epoch: [129][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.9857 (1.9351)	Acc@1 70.312 (73.683)	Acc@5 96.094 (95.129)
Epoch: [129][60/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9432 (1.9338)	Acc@1 75.391 (73.809)	Acc@5 94.922 (95.076)
Epoch: [129][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.8473 (1.9271)	Acc@1 77.734 (73.977)	Acc@5 94.531 (95.131)
Epoch: [129][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9608 (1.9314)	Acc@1 71.875 (73.847)	Acc@5 94.922 (95.144)
Epoch: [129][90/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.9989 (1.9354)	Acc@1 69.141 (73.738)	Acc@5 95.312 (95.141)
Epoch: [129][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0178 (1.9360)	Acc@1 71.094 (73.666)	Acc@5 92.578 (95.131)
Epoch: [129][110/196]	Time 0.012 (0.015)	Data 0.016 (0.005)	Loss 2.0938 (1.9448)	Acc@1 68.750 (73.413)	Acc@5 93.359 (95.010)
Epoch: [129][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9482 (1.9535)	Acc@1 73.828 (73.170)	Acc@5 94.922 (94.886)
Epoch: [129][130/196]	Time 0.014 (0.015)	Data 0.016 (0.005)	Loss 2.0437 (1.9652)	Acc@1 75.000 (72.880)	Acc@5 94.141 (94.758)
Epoch: [129][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2046 (1.9752)	Acc@1 67.969 (72.659)	Acc@5 92.578 (94.678)
Epoch: [129][150/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0090 (1.9803)	Acc@1 71.094 (72.504)	Acc@5 94.141 (94.624)
Epoch: [129][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1671 (1.9879)	Acc@1 66.016 (72.312)	Acc@5 90.625 (94.524)
Epoch: [129][170/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.1278 (1.9968)	Acc@1 69.922 (72.090)	Acc@5 93.359 (94.465)
Epoch: [129][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0783 (2.0016)	Acc@1 73.047 (71.946)	Acc@5 94.531 (94.404)
Epoch: [129][190/196]	Time 0.013 (0.015)	Data 0.003 (0.004)	Loss 2.0020 (2.0048)	Acc@1 72.266 (71.846)	Acc@5 94.531 (94.337)
num momentum params: 26
[0.1, 2.0065766552734376, 1.7702152574062346, 71.804, 55.24, tensor(0.5262, device='cuda:0', grad_fn=<DivBackward0>), 3.007826566696167, 0.40006160736083984]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [509, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [474, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [474]
Non Pruning Epoch - module.bn7.bias: [474]
Non Pruning Epoch - module.conv8.weight: [306, 474, 3, 3]
Non Pruning Epoch - module.bn8.weight: [306]
Non Pruning Epoch - module.bn8.bias: [306]
Non Pruning Epoch - module.fc.weight: [100, 306]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [130 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [509, 509, 3, 3]
module.conv7.weight [474, 509, 3, 3]
module.conv8.weight [306, 474, 3, 3]
Epoch: [130][0/196]	Time 0.063 (0.063)	Data 0.210 (0.210)	Loss 2.0129 (2.0129)	Acc@1 74.609 (74.609)	Acc@5 95.703 (95.703)
Epoch: [130][10/196]	Time 0.016 (0.021)	Data 0.002 (0.021)	Loss 1.9654 (1.9964)	Acc@1 76.953 (72.976)	Acc@5 94.531 (94.780)
Epoch: [130][20/196]	Time 0.017 (0.018)	Data 0.006 (0.012)	Loss 1.9681 (1.9795)	Acc@1 73.047 (72.861)	Acc@5 96.875 (94.959)
Epoch: [130][30/196]	Time 0.017 (0.017)	Data 0.000 (0.009)	Loss 2.0134 (1.9666)	Acc@1 71.484 (73.311)	Acc@5 93.359 (95.023)
Epoch: [130][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 1.7488 (1.9454)	Acc@1 80.859 (73.704)	Acc@5 96.094 (95.217)
Epoch: [130][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.8658 (1.9383)	Acc@1 73.828 (73.790)	Acc@5 96.484 (95.244)
Epoch: [130][60/196]	Time 0.012 (0.016)	Data 0.006 (0.006)	Loss 1.8452 (1.9341)	Acc@1 76.953 (73.969)	Acc@5 95.703 (95.204)
Epoch: [130][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.7533 (1.9375)	Acc@1 76.562 (73.729)	Acc@5 98.047 (95.175)
Epoch: [130][80/196]	Time 0.016 (0.016)	Data 0.009 (0.005)	Loss 1.9802 (1.9379)	Acc@1 74.219 (73.688)	Acc@5 93.750 (95.158)
Epoch: [130][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9634 (1.9422)	Acc@1 72.656 (73.596)	Acc@5 94.531 (95.098)
Epoch: [130][100/196]	Time 0.013 (0.016)	Data 0.012 (0.005)	Loss 1.9841 (1.9434)	Acc@1 69.922 (73.515)	Acc@5 94.922 (95.115)
Epoch: [130][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1119 (1.9448)	Acc@1 69.141 (73.540)	Acc@5 93.750 (95.119)
Epoch: [130][120/196]	Time 0.012 (0.016)	Data 0.019 (0.004)	Loss 1.8863 (1.9527)	Acc@1 77.344 (73.273)	Acc@5 95.703 (94.996)
Epoch: [130][130/196]	Time 0.015 (0.016)	Data 0.001 (0.004)	Loss 1.8981 (1.9574)	Acc@1 75.781 (73.244)	Acc@5 96.484 (94.916)
Epoch: [130][140/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 2.0727 (1.9607)	Acc@1 72.266 (73.108)	Acc@5 95.703 (94.936)
Epoch: [130][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1213 (1.9636)	Acc@1 72.266 (73.057)	Acc@5 89.844 (94.834)
Epoch: [130][160/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.0420 (1.9674)	Acc@1 71.484 (72.943)	Acc@5 92.578 (94.820)
Epoch: [130][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1790 (1.9705)	Acc@1 65.234 (72.898)	Acc@5 91.016 (94.721)
Epoch: [130][180/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 1.9060 (1.9747)	Acc@1 76.172 (72.790)	Acc@5 96.484 (94.676)
Epoch: [130][190/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0791 (1.9785)	Acc@1 69.531 (72.699)	Acc@5 93.359 (94.603)
num momentum params: 26
[0.1, 1.9802902757263183, 1.8013851761817932, 72.608, 54.75, tensor(0.5324, device='cuda:0', grad_fn=<DivBackward0>), 3.0643603801727295, 0.39234375953674316]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [250, 128, 3, 3]
Before - module.bn3.weight: [250]
Before - module.bn3.bias: [250]
Before - module.conv4.weight: [256, 250, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [509, 256, 3, 3]
Before - module.bn5.weight: [509]
Before - module.bn5.bias: [509]
Before - module.conv6.weight: [509, 509, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [474, 509, 3, 3]
Before - module.bn7.weight: [474]
Before - module.bn7.bias: [474]
Before - module.conv8.weight: [306, 474, 3, 3]
Before - module.bn8.weight: [306]
Before - module.bn8.bias: [306]
Before - module.fc.weight: [100, 306]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [42, 3, 3, 3] >> [40, 3, 3, 3]
[module.bn1.weight]: 42 >> 40
running_mean [40]
running_var [40]
num_batches_tracked []
[module.conv2.weight]: [128, 42, 3, 3] >> [127, 40, 3, 3]
[module.bn2.weight]: 128 >> 127
running_mean [127]
running_var [127]
num_batches_tracked []
[module.conv3.weight]: [250, 128, 3, 3] >> [250, 127, 3, 3]
[module.conv6.weight]: [509, 509, 3, 3] >> [508, 509, 3, 3]
[module.bn6.weight]: 509 >> 508
running_mean [508]
running_var [508]
num_batches_tracked []
[module.conv7.weight]: [474, 509, 3, 3] >> [472, 508, 3, 3]
[module.bn7.weight]: 474 >> 472
running_mean [472]
running_var [472]
num_batches_tracked []
[module.conv8.weight]: [306, 474, 3, 3] >> [299, 472, 3, 3]
[module.bn8.weight]: 306 >> 299
running_mean [299]
running_var [299]
num_batches_tracked []
[module.fc.weight]: [100, 306] >> [100, 299]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [40, 3, 3, 3]
After - module.bn1.weight: [40]
After - module.bn1.bias: [40]
After - module.conv2.weight: [127, 40, 3, 3]
After - module.bn2.weight: [127]
After - module.bn2.bias: [127]
After - module.conv3.weight: [250, 127, 3, 3]
After - module.bn3.weight: [250]
After - module.bn3.bias: [250]
After - module.conv4.weight: [256, 250, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [509, 256, 3, 3]
After - module.bn5.weight: [509]
After - module.bn5.bias: [509]
After - module.conv6.weight: [508, 509, 3, 3]
After - module.bn6.weight: [508]
After - module.bn6.bias: [508]
After - module.conv7.weight: [472, 508, 3, 3]
After - module.bn7.weight: [472]
After - module.bn7.bias: [472]
After - module.conv8.weight: [299, 472, 3, 3]
After - module.bn8.weight: [299]
After - module.bn8.bias: [299]
After - module.fc.weight: [100, 299]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [40, 3, 3, 3]
conv2 --> [127, 40, 3, 3]
conv3 --> [250, 127, 3, 3]
conv4 --> [256, 250, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [508, 509, 3, 3]
conv7 --> [472, 508, 3, 3]
conv8 --> [299, 472, 3, 3]
fc --> [299, 100]
1, 442920960, 1105920, 40
2, 4892405760, 11704320, 127
3, 8339328000, 18288000, 250
4, 16809984000, 36864000, 256
5, 10207494144, 18763776, 509
6, 20255496192, 37234368, 508
7, 6629326848, 8631936, 472
8, 3901906944, 5080608, 299
fc, 11481600, 29900, 0
===================
FLOP REPORT: 27925915800000.0 50084800000.0 137702828 125212 2461 15.00936508178711
[INFO] Storing checkpoint...

Epoch: [131 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [131][0/196]	Time 0.568 (0.568)	Data 0.218 (0.218)	Loss 1.8710 (1.8710)	Acc@1 75.000 (75.000)	Acc@5 94.531 (94.531)
Epoch: [131][10/196]	Time 0.015 (0.066)	Data 0.002 (0.021)	Loss 2.0302 (1.9143)	Acc@1 70.312 (73.793)	Acc@5 94.141 (95.668)
Epoch: [131][20/196]	Time 0.016 (0.042)	Data 0.002 (0.012)	Loss 1.9662 (1.9386)	Acc@1 72.656 (73.047)	Acc@5 94.141 (95.480)
Epoch: [131][30/196]	Time 0.017 (0.033)	Data 0.000 (0.009)	Loss 1.8770 (1.9333)	Acc@1 75.781 (73.412)	Acc@5 95.703 (95.476)
Epoch: [131][40/196]	Time 0.017 (0.029)	Data 0.001 (0.008)	Loss 1.7780 (1.9248)	Acc@1 79.688 (73.714)	Acc@5 96.484 (95.675)
Epoch: [131][50/196]	Time 0.017 (0.026)	Data 0.000 (0.007)	Loss 1.9390 (1.9189)	Acc@1 73.828 (73.997)	Acc@5 95.312 (95.619)
Epoch: [131][60/196]	Time 0.016 (0.025)	Data 0.000 (0.006)	Loss 1.8562 (1.9162)	Acc@1 76.953 (74.193)	Acc@5 94.922 (95.524)
Epoch: [131][70/196]	Time 0.016 (0.023)	Data 0.001 (0.006)	Loss 2.0592 (1.9219)	Acc@1 70.703 (74.004)	Acc@5 94.531 (95.505)
Epoch: [131][80/196]	Time 0.016 (0.022)	Data 0.000 (0.006)	Loss 1.9760 (1.9252)	Acc@1 70.703 (73.790)	Acc@5 95.703 (95.433)
Epoch: [131][90/196]	Time 0.016 (0.021)	Data 0.001 (0.005)	Loss 2.1493 (1.9365)	Acc@1 68.750 (73.519)	Acc@5 92.969 (95.343)
Epoch: [131][100/196]	Time 0.017 (0.021)	Data 0.001 (0.005)	Loss 2.0729 (1.9451)	Acc@1 71.875 (73.240)	Acc@5 94.922 (95.289)
Epoch: [131][110/196]	Time 0.017 (0.020)	Data 0.001 (0.005)	Loss 1.9425 (1.9493)	Acc@1 73.438 (73.093)	Acc@5 96.484 (95.228)
Epoch: [131][120/196]	Time 0.018 (0.020)	Data 0.001 (0.005)	Loss 1.9643 (1.9555)	Acc@1 72.656 (72.937)	Acc@5 95.312 (95.128)
Epoch: [131][130/196]	Time 0.018 (0.020)	Data 0.001 (0.005)	Loss 2.0721 (1.9643)	Acc@1 69.531 (72.638)	Acc@5 92.188 (95.023)
Epoch: [131][140/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 2.0569 (1.9700)	Acc@1 71.484 (72.446)	Acc@5 95.312 (94.925)
Epoch: [131][150/196]	Time 0.017 (0.019)	Data 0.000 (0.004)	Loss 2.1350 (1.9761)	Acc@1 68.359 (72.315)	Acc@5 91.016 (94.842)
Epoch: [131][160/196]	Time 0.012 (0.019)	Data 0.006 (0.004)	Loss 2.1966 (1.9845)	Acc@1 67.578 (72.125)	Acc@5 91.016 (94.704)
Epoch: [131][170/196]	Time 0.018 (0.019)	Data 0.000 (0.004)	Loss 2.0919 (1.9905)	Acc@1 67.188 (71.957)	Acc@5 92.969 (94.609)
Epoch: [131][180/196]	Time 0.013 (0.018)	Data 0.005 (0.004)	Loss 2.0287 (1.9944)	Acc@1 71.875 (71.845)	Acc@5 96.094 (94.538)
Epoch: [131][190/196]	Time 0.018 (0.018)	Data 0.000 (0.004)	Loss 1.9592 (1.9967)	Acc@1 74.219 (71.867)	Acc@5 94.922 (94.501)
num momentum params: 26
[0.1, 1.9995518809509278, 1.8615964818000794, 71.77, 53.13, tensor(0.5275, device='cuda:0', grad_fn=<DivBackward0>), 3.767706394195557, 0.4783592224121094]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [132 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [132][0/196]	Time 0.053 (0.053)	Data 0.195 (0.195)	Loss 1.9414 (1.9414)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [132][10/196]	Time 0.016 (0.021)	Data 0.005 (0.020)	Loss 1.8611 (1.9403)	Acc@1 74.609 (73.011)	Acc@5 96.875 (95.384)
Epoch: [132][20/196]	Time 0.012 (0.018)	Data 0.005 (0.012)	Loss 1.6719 (1.9005)	Acc@1 80.078 (74.702)	Acc@5 96.484 (95.722)
Epoch: [132][30/196]	Time 0.013 (0.017)	Data 0.007 (0.009)	Loss 1.9518 (1.8982)	Acc@1 70.703 (74.446)	Acc@5 94.531 (95.728)
Epoch: [132][40/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 1.9272 (1.9023)	Acc@1 75.000 (74.543)	Acc@5 95.703 (95.665)
Epoch: [132][50/196]	Time 0.012 (0.016)	Data 0.028 (0.008)	Loss 1.8883 (1.8970)	Acc@1 73.047 (74.594)	Acc@5 96.875 (95.711)
Epoch: [132][60/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.8580 (1.8917)	Acc@1 74.609 (74.750)	Acc@5 94.531 (95.645)
Epoch: [132][70/196]	Time 0.012 (0.016)	Data 0.018 (0.007)	Loss 2.0008 (1.8918)	Acc@1 73.047 (74.725)	Acc@5 95.312 (95.615)
Epoch: [132][80/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.8657 (1.8952)	Acc@1 75.391 (74.643)	Acc@5 94.922 (95.573)
Epoch: [132][90/196]	Time 0.011 (0.016)	Data 0.021 (0.007)	Loss 1.9738 (1.9063)	Acc@1 72.656 (74.283)	Acc@5 95.312 (95.437)
Epoch: [132][100/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0284 (1.9164)	Acc@1 71.875 (73.991)	Acc@5 93.359 (95.312)
Epoch: [132][110/196]	Time 0.012 (0.016)	Data 0.020 (0.006)	Loss 2.0634 (1.9237)	Acc@1 73.047 (73.856)	Acc@5 92.578 (95.193)
Epoch: [132][120/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9347 (1.9331)	Acc@1 72.656 (73.622)	Acc@5 94.922 (95.103)
Epoch: [132][130/196]	Time 0.012 (0.016)	Data 0.019 (0.006)	Loss 2.0955 (1.9429)	Acc@1 69.141 (73.482)	Acc@5 94.922 (95.032)
Epoch: [132][140/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0964 (1.9520)	Acc@1 67.578 (73.199)	Acc@5 95.703 (94.933)
Epoch: [132][150/196]	Time 0.012 (0.016)	Data 0.020 (0.006)	Loss 2.0605 (1.9618)	Acc@1 68.750 (72.967)	Acc@5 94.922 (94.785)
Epoch: [132][160/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.1292 (1.9703)	Acc@1 66.016 (72.748)	Acc@5 94.531 (94.667)
Epoch: [132][170/196]	Time 0.012 (0.016)	Data 0.021 (0.006)	Loss 1.8930 (1.9760)	Acc@1 72.266 (72.606)	Acc@5 96.484 (94.570)
Epoch: [132][180/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.1751 (1.9839)	Acc@1 65.234 (72.356)	Acc@5 94.141 (94.525)
Epoch: [132][190/196]	Time 0.011 (0.016)	Data 0.010 (0.006)	Loss 2.0659 (1.9908)	Acc@1 71.094 (72.108)	Acc@5 93.750 (94.456)
num momentum params: 26
[0.1, 1.9935540657806397, 1.8220555210113525, 72.048, 54.07, tensor(0.5300, device='cuda:0', grad_fn=<DivBackward0>), 3.056009531021118, 0.40059351921081543]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [133 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [133][0/196]	Time 0.052 (0.052)	Data 0.210 (0.210)	Loss 2.0097 (2.0097)	Acc@1 71.094 (71.094)	Acc@5 95.703 (95.703)
Epoch: [133][10/196]	Time 0.012 (0.020)	Data 0.002 (0.021)	Loss 1.9847 (1.9615)	Acc@1 72.266 (73.757)	Acc@5 94.141 (94.780)
Epoch: [133][20/196]	Time 0.016 (0.018)	Data 0.002 (0.012)	Loss 1.9655 (1.9381)	Acc@1 70.312 (73.735)	Acc@5 94.141 (95.145)
Epoch: [133][30/196]	Time 0.014 (0.017)	Data 0.004 (0.009)	Loss 1.8573 (1.9221)	Acc@1 76.172 (74.194)	Acc@5 94.531 (95.376)
Epoch: [133][40/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 1.8578 (1.9099)	Acc@1 75.781 (74.447)	Acc@5 96.875 (95.627)
Epoch: [133][50/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.9503 (1.9114)	Acc@1 72.266 (74.303)	Acc@5 95.312 (95.565)
Epoch: [133][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 1.8380 (1.9023)	Acc@1 72.656 (74.411)	Acc@5 98.047 (95.729)
Epoch: [133][70/196]	Time 0.013 (0.016)	Data 0.007 (0.006)	Loss 2.0188 (1.9007)	Acc@1 70.703 (74.483)	Acc@5 95.312 (95.714)
Epoch: [133][80/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 2.1011 (1.9042)	Acc@1 70.703 (74.407)	Acc@5 92.188 (95.592)
Epoch: [133][90/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 2.0325 (1.9045)	Acc@1 69.141 (74.399)	Acc@5 95.703 (95.549)
Epoch: [133][100/196]	Time 0.011 (0.015)	Data 0.005 (0.005)	Loss 1.8958 (1.9112)	Acc@1 75.000 (74.308)	Acc@5 97.656 (95.421)
Epoch: [133][110/196]	Time 0.013 (0.015)	Data 0.009 (0.005)	Loss 1.8672 (1.9148)	Acc@1 76.172 (74.212)	Acc@5 96.484 (95.365)
Epoch: [133][120/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0309 (1.9225)	Acc@1 73.828 (73.996)	Acc@5 92.969 (95.290)
Epoch: [133][130/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.1634 (1.9357)	Acc@1 66.797 (73.596)	Acc@5 92.969 (95.131)
Epoch: [133][140/196]	Time 0.012 (0.015)	Data 0.021 (0.005)	Loss 2.0856 (1.9429)	Acc@1 67.578 (73.377)	Acc@5 93.359 (95.038)
Epoch: [133][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0384 (1.9472)	Acc@1 70.312 (73.287)	Acc@5 93.750 (94.981)
Epoch: [133][160/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.0333 (1.9541)	Acc@1 71.094 (73.144)	Acc@5 93.750 (94.871)
Epoch: [133][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0382 (1.9579)	Acc@1 69.531 (73.022)	Acc@5 93.750 (94.824)
Epoch: [133][180/196]	Time 0.013 (0.015)	Data 0.017 (0.005)	Loss 2.0246 (1.9643)	Acc@1 73.438 (72.900)	Acc@5 94.141 (94.764)
Epoch: [133][190/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1533 (1.9705)	Acc@1 69.922 (72.748)	Acc@5 92.188 (94.689)
num momentum params: 26
[0.1, 1.9728467527770996, 1.8665631592273713, 72.68, 53.91, tensor(0.5348, device='cuda:0', grad_fn=<DivBackward0>), 2.9954309463500977, 0.40315937995910645]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [134 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [134][0/196]	Time 0.059 (0.059)	Data 0.201 (0.201)	Loss 1.8313 (1.8313)	Acc@1 77.344 (77.344)	Acc@5 95.703 (95.703)
Epoch: [134][10/196]	Time 0.018 (0.020)	Data 0.001 (0.020)	Loss 1.8754 (1.9266)	Acc@1 73.828 (73.828)	Acc@5 95.312 (95.384)
Epoch: [134][20/196]	Time 0.014 (0.018)	Data 0.009 (0.012)	Loss 1.8361 (1.9436)	Acc@1 78.516 (73.456)	Acc@5 96.094 (95.182)
Epoch: [134][30/196]	Time 0.017 (0.018)	Data 0.001 (0.009)	Loss 1.8566 (1.9176)	Acc@1 73.438 (73.967)	Acc@5 96.094 (95.527)
Epoch: [134][40/196]	Time 0.011 (0.017)	Data 0.006 (0.007)	Loss 1.9964 (1.9266)	Acc@1 72.656 (73.800)	Acc@5 93.750 (95.446)
Epoch: [134][50/196]	Time 0.014 (0.017)	Data 0.002 (0.006)	Loss 1.8591 (1.9302)	Acc@1 75.000 (73.629)	Acc@5 96.484 (95.512)
Epoch: [134][60/196]	Time 0.012 (0.017)	Data 0.005 (0.006)	Loss 1.9926 (1.9336)	Acc@1 70.703 (73.585)	Acc@5 93.750 (95.460)
Epoch: [134][70/196]	Time 0.016 (0.017)	Data 0.001 (0.005)	Loss 1.8377 (1.9317)	Acc@1 76.562 (73.619)	Acc@5 96.484 (95.456)
Epoch: [134][80/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 1.9500 (1.9303)	Acc@1 74.219 (73.597)	Acc@5 93.359 (95.486)
Epoch: [134][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8168 (1.9335)	Acc@1 75.000 (73.605)	Acc@5 96.875 (95.364)
Epoch: [134][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0504 (1.9441)	Acc@1 70.703 (73.372)	Acc@5 93.750 (95.193)
Epoch: [134][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9904 (1.9493)	Acc@1 71.094 (73.328)	Acc@5 94.531 (95.080)
Epoch: [134][120/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0311 (1.9525)	Acc@1 69.531 (73.189)	Acc@5 96.484 (95.051)
Epoch: [134][130/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9301 (1.9551)	Acc@1 75.781 (73.175)	Acc@5 94.922 (95.020)
Epoch: [134][140/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 1.9805 (1.9598)	Acc@1 71.875 (73.041)	Acc@5 94.922 (94.977)
Epoch: [134][150/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.0864 (1.9665)	Acc@1 66.797 (72.757)	Acc@5 93.359 (94.914)
Epoch: [134][160/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0737 (1.9724)	Acc@1 70.312 (72.574)	Acc@5 94.141 (94.868)
Epoch: [134][170/196]	Time 0.015 (0.016)	Data 0.000 (0.004)	Loss 2.0311 (1.9762)	Acc@1 70.703 (72.467)	Acc@5 94.922 (94.856)
Epoch: [134][180/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0190 (1.9809)	Acc@1 71.875 (72.287)	Acc@5 93.750 (94.820)
Epoch: [134][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0945 (1.9861)	Acc@1 68.750 (72.214)	Acc@5 92.578 (94.740)
num momentum params: 26
[0.1, 1.9891317595672608, 1.7919711351394654, 72.112, 55.36, tensor(0.5302, device='cuda:0', grad_fn=<DivBackward0>), 3.101001501083374, 0.3968284130096435]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [135 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [135][0/196]	Time 0.059 (0.059)	Data 0.215 (0.215)	Loss 1.9091 (1.9091)	Acc@1 72.266 (72.266)	Acc@5 96.484 (96.484)
Epoch: [135][10/196]	Time 0.014 (0.021)	Data 0.003 (0.021)	Loss 1.9999 (1.9490)	Acc@1 74.609 (72.727)	Acc@5 92.188 (94.389)
Epoch: [135][20/196]	Time 0.016 (0.018)	Data 0.003 (0.012)	Loss 1.9460 (1.9309)	Acc@1 72.266 (73.493)	Acc@5 95.312 (95.126)
Epoch: [135][30/196]	Time 0.017 (0.018)	Data 0.002 (0.009)	Loss 1.9032 (1.9199)	Acc@1 73.047 (73.740)	Acc@5 97.266 (95.426)
Epoch: [135][40/196]	Time 0.012 (0.017)	Data 0.005 (0.008)	Loss 1.9308 (1.9205)	Acc@1 74.609 (74.066)	Acc@5 96.094 (95.255)
Epoch: [135][50/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.9312 (1.9153)	Acc@1 72.656 (74.357)	Acc@5 95.312 (95.328)
Epoch: [135][60/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9871 (1.9096)	Acc@1 74.609 (74.481)	Acc@5 94.531 (95.421)
Epoch: [135][70/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 2.0693 (1.9120)	Acc@1 70.703 (74.373)	Acc@5 94.531 (95.450)
Epoch: [135][80/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9068 (1.9228)	Acc@1 75.781 (74.084)	Acc@5 95.312 (95.312)
Epoch: [135][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9714 (1.9337)	Acc@1 72.656 (73.738)	Acc@5 92.969 (95.141)
Epoch: [135][100/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 2.0745 (1.9419)	Acc@1 69.141 (73.418)	Acc@5 94.531 (95.038)
Epoch: [135][110/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.9811 (1.9456)	Acc@1 70.703 (73.318)	Acc@5 97.266 (95.024)
Epoch: [135][120/196]	Time 0.018 (0.016)	Data 0.002 (0.005)	Loss 1.8990 (1.9476)	Acc@1 76.562 (73.263)	Acc@5 92.969 (94.993)
Epoch: [135][130/196]	Time 0.017 (0.016)	Data 0.003 (0.005)	Loss 2.0874 (1.9589)	Acc@1 66.797 (72.919)	Acc@5 95.312 (94.922)
Epoch: [135][140/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1326 (1.9661)	Acc@1 68.750 (72.770)	Acc@5 92.188 (94.814)
Epoch: [135][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0407 (1.9727)	Acc@1 71.094 (72.519)	Acc@5 96.094 (94.761)
Epoch: [135][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0405 (1.9776)	Acc@1 69.141 (72.418)	Acc@5 95.703 (94.713)
Epoch: [135][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0438 (1.9819)	Acc@1 69.531 (72.307)	Acc@5 94.141 (94.666)
Epoch: [135][180/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 1.9660 (1.9863)	Acc@1 72.656 (72.177)	Acc@5 93.750 (94.622)
Epoch: [135][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9999 (1.9899)	Acc@1 71.094 (72.090)	Acc@5 93.750 (94.564)
num momentum params: 26
[0.1, 1.9917130151367188, 1.9326620495319367, 72.032, 52.28, tensor(0.5299, device='cuda:0', grad_fn=<DivBackward0>), 3.0237228870391846, 0.3964531421661377]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [136 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [136][0/196]	Time 0.054 (0.054)	Data 0.216 (0.216)	Loss 1.8899 (1.8899)	Acc@1 76.953 (76.953)	Acc@5 94.531 (94.531)
Epoch: [136][10/196]	Time 0.016 (0.020)	Data 0.002 (0.021)	Loss 1.8909 (1.9235)	Acc@1 75.781 (74.041)	Acc@5 95.703 (95.916)
Epoch: [136][20/196]	Time 0.015 (0.017)	Data 0.002 (0.012)	Loss 1.8759 (1.8944)	Acc@1 75.391 (74.926)	Acc@5 95.703 (95.685)
Epoch: [136][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.7459 (1.8806)	Acc@1 76.953 (75.025)	Acc@5 97.266 (95.716)
Epoch: [136][40/196]	Time 0.014 (0.016)	Data 0.004 (0.008)	Loss 1.8496 (1.8892)	Acc@1 74.609 (74.752)	Acc@5 98.047 (95.789)
Epoch: [136][50/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 1.8437 (1.8973)	Acc@1 77.734 (74.594)	Acc@5 96.484 (95.718)
Epoch: [136][60/196]	Time 0.012 (0.016)	Data 0.019 (0.007)	Loss 1.8729 (1.8966)	Acc@1 74.609 (74.558)	Acc@5 95.312 (95.729)
Epoch: [136][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9250 (1.8997)	Acc@1 75.781 (74.604)	Acc@5 95.312 (95.703)
Epoch: [136][80/196]	Time 0.011 (0.016)	Data 0.008 (0.006)	Loss 1.8869 (1.9026)	Acc@1 75.391 (74.547)	Acc@5 95.312 (95.592)
Epoch: [136][90/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9823 (1.9093)	Acc@1 71.875 (74.283)	Acc@5 95.312 (95.634)
Epoch: [136][100/196]	Time 0.014 (0.016)	Data 0.009 (0.006)	Loss 1.9215 (1.9120)	Acc@1 73.438 (74.165)	Acc@5 95.703 (95.575)
Epoch: [136][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0293 (1.9188)	Acc@1 69.922 (73.944)	Acc@5 94.141 (95.474)
Epoch: [136][120/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 2.0724 (1.9263)	Acc@1 70.312 (73.712)	Acc@5 91.797 (95.377)
Epoch: [136][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1781 (1.9332)	Acc@1 67.578 (73.628)	Acc@5 91.797 (95.235)
Epoch: [136][140/196]	Time 0.013 (0.016)	Data 0.009 (0.005)	Loss 2.0638 (1.9419)	Acc@1 69.922 (73.365)	Acc@5 95.312 (95.119)
Epoch: [136][150/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0977 (1.9462)	Acc@1 70.312 (73.298)	Acc@5 95.703 (95.077)
Epoch: [136][160/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1919 (1.9556)	Acc@1 66.797 (73.059)	Acc@5 91.797 (94.951)
Epoch: [136][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9937 (1.9639)	Acc@1 70.703 (72.871)	Acc@5 94.531 (94.872)
Epoch: [136][180/196]	Time 0.013 (0.016)	Data 0.011 (0.004)	Loss 1.9638 (1.9704)	Acc@1 74.219 (72.721)	Acc@5 94.531 (94.786)
Epoch: [136][190/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0982 (1.9755)	Acc@1 69.531 (72.611)	Acc@5 92.578 (94.687)
num momentum params: 26
[0.1, 1.978046382598877, 1.9014263808727265, 72.532, 52.95, tensor(0.5342, device='cuda:0', grad_fn=<DivBackward0>), 3.053154706954956, 0.39954233169555664]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [137 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [137][0/196]	Time 0.062 (0.062)	Data 0.206 (0.206)	Loss 1.9064 (1.9064)	Acc@1 73.828 (73.828)	Acc@5 95.703 (95.703)
Epoch: [137][10/196]	Time 0.017 (0.021)	Data 0.002 (0.021)	Loss 2.0880 (1.9817)	Acc@1 72.656 (71.875)	Acc@5 93.359 (94.851)
Epoch: [137][20/196]	Time 0.014 (0.018)	Data 0.002 (0.012)	Loss 1.8228 (1.9530)	Acc@1 78.906 (73.103)	Acc@5 96.875 (94.940)
Epoch: [137][30/196]	Time 0.016 (0.017)	Data 0.000 (0.009)	Loss 2.0346 (1.9526)	Acc@1 71.094 (72.719)	Acc@5 92.578 (95.086)
Epoch: [137][40/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.9887 (1.9500)	Acc@1 73.828 (72.923)	Acc@5 94.141 (95.046)
Epoch: [137][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 2.1309 (1.9412)	Acc@1 72.266 (73.384)	Acc@5 92.188 (95.006)
Epoch: [137][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9349 (1.9358)	Acc@1 72.656 (73.514)	Acc@5 95.312 (95.082)
Epoch: [137][70/196]	Time 0.023 (0.016)	Data 0.000 (0.005)	Loss 2.1076 (1.9423)	Acc@1 68.359 (73.344)	Acc@5 92.969 (95.092)
Epoch: [137][80/196]	Time 0.014 (0.016)	Data 0.018 (0.005)	Loss 2.1086 (1.9547)	Acc@1 70.312 (72.907)	Acc@5 92.969 (95.009)
Epoch: [137][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9822 (1.9588)	Acc@1 73.047 (72.824)	Acc@5 94.922 (94.939)
Epoch: [137][100/196]	Time 0.013 (0.016)	Data 0.014 (0.005)	Loss 1.8118 (1.9620)	Acc@1 78.906 (72.753)	Acc@5 96.094 (94.949)
Epoch: [137][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.8786 (1.9650)	Acc@1 74.219 (72.639)	Acc@5 97.266 (94.947)
Epoch: [137][120/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 1.8919 (1.9700)	Acc@1 76.562 (72.521)	Acc@5 94.922 (94.909)
Epoch: [137][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0964 (1.9726)	Acc@1 71.875 (72.445)	Acc@5 93.750 (94.904)
Epoch: [137][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2112 (1.9789)	Acc@1 64.453 (72.257)	Acc@5 90.234 (94.806)
Epoch: [137][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9175 (1.9805)	Acc@1 76.172 (72.201)	Acc@5 92.969 (94.733)
Epoch: [137][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.9010 (1.9817)	Acc@1 74.219 (72.217)	Acc@5 96.484 (94.706)
Epoch: [137][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0248 (1.9826)	Acc@1 75.391 (72.268)	Acc@5 94.531 (94.693)
Epoch: [137][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1429 (1.9863)	Acc@1 67.188 (72.169)	Acc@5 92.969 (94.622)
Epoch: [137][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.2054 (1.9888)	Acc@1 68.750 (72.116)	Acc@5 90.625 (94.599)
num momentum params: 26
[0.1, 1.991451058807373, 1.9334450018405915, 72.056, 51.79, tensor(0.5309, device='cuda:0', grad_fn=<DivBackward0>), 2.9953176975250244, 0.40378856658935547]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [138 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [138][0/196]	Time 0.066 (0.066)	Data 0.193 (0.193)	Loss 1.8275 (1.8275)	Acc@1 76.172 (76.172)	Acc@5 96.484 (96.484)
Epoch: [138][10/196]	Time 0.015 (0.020)	Data 0.002 (0.020)	Loss 1.8691 (1.9287)	Acc@1 76.172 (73.722)	Acc@5 95.312 (95.348)
Epoch: [138][20/196]	Time 0.014 (0.018)	Data 0.002 (0.011)	Loss 1.8781 (1.9134)	Acc@1 75.000 (73.921)	Acc@5 95.703 (95.443)
Epoch: [138][30/196]	Time 0.012 (0.017)	Data 0.006 (0.009)	Loss 1.7807 (1.8989)	Acc@1 78.516 (74.471)	Acc@5 95.312 (95.489)
Epoch: [138][40/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 2.0654 (1.8960)	Acc@1 73.047 (74.590)	Acc@5 93.359 (95.617)
Epoch: [138][50/196]	Time 0.011 (0.016)	Data 0.016 (0.007)	Loss 1.9296 (1.9113)	Acc@1 71.094 (74.318)	Acc@5 97.266 (95.450)
Epoch: [138][60/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0551 (1.9087)	Acc@1 69.922 (74.443)	Acc@5 95.312 (95.607)
Epoch: [138][70/196]	Time 0.011 (0.016)	Data 0.019 (0.006)	Loss 2.1125 (1.9127)	Acc@1 71.484 (74.334)	Acc@5 93.750 (95.560)
Epoch: [138][80/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9321 (1.9250)	Acc@1 75.781 (73.963)	Acc@5 93.750 (95.399)
Epoch: [138][90/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 2.0887 (1.9347)	Acc@1 69.141 (73.665)	Acc@5 94.922 (95.304)
Epoch: [138][100/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1763 (1.9499)	Acc@1 68.750 (73.372)	Acc@5 92.188 (95.061)
Epoch: [138][110/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 1.8198 (1.9489)	Acc@1 78.125 (73.409)	Acc@5 96.875 (95.042)
Epoch: [138][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.2774 (1.9538)	Acc@1 65.625 (73.302)	Acc@5 91.406 (94.957)
Epoch: [138][130/196]	Time 0.012 (0.016)	Data 0.011 (0.005)	Loss 2.0273 (1.9584)	Acc@1 70.312 (73.139)	Acc@5 93.750 (94.865)
Epoch: [138][140/196]	Time 0.022 (0.016)	Data 0.001 (0.005)	Loss 1.7992 (1.9598)	Acc@1 79.688 (73.141)	Acc@5 95.703 (94.819)
Epoch: [138][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9132 (1.9610)	Acc@1 73.438 (73.044)	Acc@5 93.750 (94.759)
Epoch: [138][160/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0621 (1.9660)	Acc@1 70.312 (72.841)	Acc@5 92.188 (94.708)
Epoch: [138][170/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0136 (1.9706)	Acc@1 71.484 (72.688)	Acc@5 92.969 (94.661)
Epoch: [138][180/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 2.0262 (1.9767)	Acc@1 71.484 (72.512)	Acc@5 94.531 (94.594)
Epoch: [138][190/196]	Time 0.012 (0.016)	Data 0.004 (0.004)	Loss 1.9747 (1.9795)	Acc@1 74.219 (72.427)	Acc@5 96.875 (94.589)
num momentum params: 26
[0.1, 1.9807178968811034, 1.8796676969528199, 72.388, 53.38, tensor(0.5336, device='cuda:0', grad_fn=<DivBackward0>), 3.0394885540008545, 0.41005253791809076]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [139 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [139][0/196]	Time 0.065 (0.065)	Data 0.195 (0.195)	Loss 1.8416 (1.8416)	Acc@1 75.000 (75.000)	Acc@5 96.484 (96.484)
Epoch: [139][10/196]	Time 0.018 (0.020)	Data 0.002 (0.019)	Loss 1.7938 (1.9310)	Acc@1 80.469 (73.864)	Acc@5 96.484 (94.851)
Epoch: [139][20/196]	Time 0.012 (0.018)	Data 0.005 (0.012)	Loss 1.8960 (1.9204)	Acc@1 77.344 (74.349)	Acc@5 96.875 (95.201)
Epoch: [139][30/196]	Time 0.012 (0.017)	Data 0.005 (0.009)	Loss 1.8712 (1.9045)	Acc@1 74.219 (74.824)	Acc@5 97.266 (95.527)
Epoch: [139][40/196]	Time 0.016 (0.017)	Data 0.000 (0.007)	Loss 1.8301 (1.9064)	Acc@1 75.391 (74.657)	Acc@5 95.703 (95.598)
Epoch: [139][50/196]	Time 0.012 (0.017)	Data 0.017 (0.007)	Loss 1.7429 (1.8980)	Acc@1 78.906 (74.732)	Acc@5 98.438 (95.688)
Epoch: [139][60/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.9131 (1.8998)	Acc@1 74.219 (74.661)	Acc@5 96.094 (95.735)
Epoch: [139][70/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 1.8547 (1.8996)	Acc@1 77.734 (74.560)	Acc@5 94.141 (95.698)
Epoch: [139][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.8628 (1.8964)	Acc@1 74.219 (74.614)	Acc@5 96.875 (95.785)
Epoch: [139][90/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.0825 (1.9034)	Acc@1 70.703 (74.378)	Acc@5 94.531 (95.737)
Epoch: [139][100/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9463 (1.9094)	Acc@1 75.000 (74.199)	Acc@5 96.484 (95.676)
Epoch: [139][110/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.9173 (1.9164)	Acc@1 71.875 (74.015)	Acc@5 97.656 (95.650)
Epoch: [139][120/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0675 (1.9258)	Acc@1 71.094 (73.770)	Acc@5 92.578 (95.529)
Epoch: [139][130/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.0358 (1.9329)	Acc@1 72.656 (73.625)	Acc@5 94.531 (95.396)
Epoch: [139][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1198 (1.9460)	Acc@1 71.875 (73.324)	Acc@5 92.188 (95.144)
Epoch: [139][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1211 (1.9528)	Acc@1 68.359 (73.184)	Acc@5 94.141 (95.051)
Epoch: [139][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9984 (1.9579)	Acc@1 68.750 (73.088)	Acc@5 92.969 (94.963)
Epoch: [139][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0422 (1.9617)	Acc@1 66.797 (73.006)	Acc@5 94.922 (94.901)
Epoch: [139][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1994 (1.9680)	Acc@1 69.141 (72.853)	Acc@5 93.359 (94.823)
Epoch: [139][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9935 (1.9720)	Acc@1 72.266 (72.718)	Acc@5 93.750 (94.781)
num momentum params: 26
[0.1, 1.9741864111328125, 2.1022717964649202, 72.66, 51.47, tensor(0.5359, device='cuda:0', grad_fn=<DivBackward0>), 2.995675802230835, 0.4016494750976563]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [250, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [508, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [472, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [472]
Non Pruning Epoch - module.bn7.bias: [472]
Non Pruning Epoch - module.conv8.weight: [299, 472, 3, 3]
Non Pruning Epoch - module.bn8.weight: [299]
Non Pruning Epoch - module.bn8.bias: [299]
Non Pruning Epoch - module.fc.weight: [100, 299]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [140 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [250, 127, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [508, 509, 3, 3]
module.conv7.weight [472, 508, 3, 3]
module.conv8.weight [299, 472, 3, 3]
Epoch: [140][0/196]	Time 0.065 (0.065)	Data 0.207 (0.207)	Loss 2.0311 (2.0311)	Acc@1 69.141 (69.141)	Acc@5 96.875 (96.875)
Epoch: [140][10/196]	Time 0.015 (0.022)	Data 0.002 (0.021)	Loss 2.0044 (1.9807)	Acc@1 72.266 (71.839)	Acc@5 93.750 (95.455)
Epoch: [140][20/196]	Time 0.016 (0.019)	Data 0.003 (0.012)	Loss 1.8015 (1.9394)	Acc@1 78.125 (73.493)	Acc@5 95.312 (95.647)
Epoch: [140][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.8479 (1.9184)	Acc@1 77.734 (74.219)	Acc@5 96.875 (95.779)
Epoch: [140][40/196]	Time 0.014 (0.017)	Data 0.002 (0.007)	Loss 1.9101 (1.9069)	Acc@1 74.609 (74.333)	Acc@5 96.484 (95.846)
Epoch: [140][50/196]	Time 0.018 (0.017)	Data 0.000 (0.006)	Loss 1.9028 (1.9061)	Acc@1 75.781 (74.571)	Acc@5 94.922 (95.726)
Epoch: [140][60/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 1.9895 (1.9050)	Acc@1 72.656 (74.520)	Acc@5 94.922 (95.690)
Epoch: [140][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0141 (1.9029)	Acc@1 73.828 (74.626)	Acc@5 92.188 (95.588)
Epoch: [140][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9032 (1.9007)	Acc@1 73.047 (74.566)	Acc@5 96.094 (95.592)
Epoch: [140][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9434 (1.9038)	Acc@1 74.219 (74.596)	Acc@5 95.703 (95.497)
Epoch: [140][100/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.1507 (1.9169)	Acc@1 69.141 (74.296)	Acc@5 94.531 (95.363)
Epoch: [140][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9185 (1.9238)	Acc@1 73.047 (74.120)	Acc@5 96.484 (95.302)
Epoch: [140][120/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.1095 (1.9294)	Acc@1 69.531 (73.977)	Acc@5 94.141 (95.245)
Epoch: [140][130/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9702 (1.9381)	Acc@1 72.266 (73.736)	Acc@5 94.531 (95.172)
Epoch: [140][140/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 1.9761 (1.9439)	Acc@1 73.828 (73.554)	Acc@5 93.359 (95.105)
Epoch: [140][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9633 (1.9497)	Acc@1 72.266 (73.373)	Acc@5 95.703 (95.036)
Epoch: [140][160/196]	Time 0.012 (0.015)	Data 0.017 (0.005)	Loss 2.1002 (1.9556)	Acc@1 68.359 (73.166)	Acc@5 95.312 (94.973)
Epoch: [140][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1478 (1.9607)	Acc@1 68.750 (73.001)	Acc@5 92.578 (94.936)
Epoch: [140][180/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 1.9077 (1.9656)	Acc@1 75.391 (72.859)	Acc@5 92.969 (94.866)
Epoch: [140][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1519 (1.9692)	Acc@1 71.094 (72.812)	Acc@5 94.531 (94.830)
num momentum params: 26
[0.1, 1.969390407562256, 2.0251940488815308, 72.818, 51.75, tensor(0.5364, device='cuda:0', grad_fn=<DivBackward0>), 2.946847915649414, 0.3988687992095947]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [40, 3, 3, 3]
Before - module.bn1.weight: [40]
Before - module.bn1.bias: [40]
Before - module.conv2.weight: [127, 40, 3, 3]
Before - module.bn2.weight: [127]
Before - module.bn2.bias: [127]
Before - module.conv3.weight: [250, 127, 3, 3]
Before - module.bn3.weight: [250]
Before - module.bn3.bias: [250]
Before - module.conv4.weight: [256, 250, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [509, 256, 3, 3]
Before - module.bn5.weight: [509]
Before - module.bn5.bias: [509]
Before - module.conv6.weight: [508, 509, 3, 3]
Before - module.bn6.weight: [508]
Before - module.bn6.bias: [508]
Before - module.conv7.weight: [472, 508, 3, 3]
Before - module.bn7.weight: [472]
Before - module.bn7.bias: [472]
Before - module.conv8.weight: [299, 472, 3, 3]
Before - module.bn8.weight: [299]
Before - module.bn8.bias: [299]
Before - module.fc.weight: [100, 299]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv3.weight]: [250, 127, 3, 3] >> [249, 127, 3, 3]
[module.bn3.weight]: 250 >> 249
running_mean [249]
running_var [249]
num_batches_tracked []
[module.conv4.weight]: [256, 250, 3, 3] >> [256, 249, 3, 3]
[module.conv6.weight]: [508, 509, 3, 3] >> [507, 509, 3, 3]
[module.bn6.weight]: 508 >> 507
running_mean [507]
running_var [507]
num_batches_tracked []
[module.conv7.weight]: [472, 508, 3, 3] >> [470, 507, 3, 3]
[module.bn7.weight]: 472 >> 470
running_mean [470]
running_var [470]
num_batches_tracked []
[module.conv8.weight]: [299, 472, 3, 3] >> [289, 470, 3, 3]
[module.bn8.weight]: 299 >> 289
running_mean [289]
running_var [289]
num_batches_tracked []
[module.fc.weight]: [100, 299] >> [100, 289]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [40, 3, 3, 3]
After - module.bn1.weight: [40]
After - module.bn1.bias: [40]
After - module.conv2.weight: [127, 40, 3, 3]
After - module.bn2.weight: [127]
After - module.bn2.bias: [127]
After - module.conv3.weight: [249, 127, 3, 3]
After - module.bn3.weight: [249]
After - module.bn3.bias: [249]
After - module.conv4.weight: [256, 249, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [509, 256, 3, 3]
After - module.bn5.weight: [509]
After - module.bn5.bias: [509]
After - module.conv6.weight: [507, 509, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [470, 507, 3, 3]
After - module.bn7.weight: [470]
After - module.bn7.bias: [470]
After - module.conv8.weight: [289, 470, 3, 3]
After - module.bn8.weight: [289]
After - module.bn8.bias: [289]
After - module.fc.weight: [100, 289]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [40, 3, 3, 3]
conv2 --> [127, 40, 3, 3]
conv3 --> [249, 127, 3, 3]
conv4 --> [256, 249, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [507, 509, 3, 3]
conv7 --> [470, 507, 3, 3]
conv8 --> [289, 470, 3, 3]
fc --> [289, 100]
1, 442920960, 1105920, 40
2, 4892405760, 11704320, 127
3, 8305970688, 18214848, 249
4, 16742744064, 36716544, 256
5, 10207494144, 18763776, 509
6, 20215623168, 37161072, 507
7, 6588241920, 8578440, 470
8, 3755427840, 4889880, 289
fc, 11097600, 28900, 0
===================
FLOP REPORT: 27797627400000.0 50033600000.0 137163700 125084 2447 14.87564468383789
[INFO] Storing checkpoint...

Epoch: [141 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [141][0/196]	Time 0.684 (0.684)	Data 0.214 (0.214)	Loss 1.7603 (1.7603)	Acc@1 79.688 (79.688)	Acc@5 98.047 (98.047)
Epoch: [141][10/196]	Time 0.014 (0.077)	Data 0.002 (0.021)	Loss 1.9788 (1.9266)	Acc@1 75.391 (74.680)	Acc@5 95.312 (95.277)
Epoch: [141][20/196]	Time 0.016 (0.047)	Data 0.003 (0.012)	Loss 2.0512 (1.9162)	Acc@1 71.875 (74.981)	Acc@5 94.922 (95.833)
Epoch: [141][30/196]	Time 0.018 (0.037)	Data 0.000 (0.009)	Loss 1.8923 (1.9037)	Acc@1 77.344 (75.063)	Acc@5 94.922 (95.716)
Epoch: [141][40/196]	Time 0.014 (0.032)	Data 0.002 (0.007)	Loss 1.9325 (1.8904)	Acc@1 73.438 (75.343)	Acc@5 97.656 (95.770)
Epoch: [141][50/196]	Time 0.016 (0.028)	Data 0.001 (0.006)	Loss 1.9445 (1.8848)	Acc@1 74.609 (75.437)	Acc@5 94.141 (95.795)
Epoch: [141][60/196]	Time 0.018 (0.026)	Data 0.000 (0.006)	Loss 1.7274 (1.8908)	Acc@1 76.172 (75.186)	Acc@5 97.656 (95.806)
Epoch: [141][70/196]	Time 0.017 (0.024)	Data 0.001 (0.006)	Loss 2.0166 (1.9027)	Acc@1 71.094 (74.884)	Acc@5 92.969 (95.643)
Epoch: [141][80/196]	Time 0.016 (0.023)	Data 0.001 (0.006)	Loss 1.9367 (1.9060)	Acc@1 69.531 (74.658)	Acc@5 93.750 (95.558)
Epoch: [141][90/196]	Time 0.016 (0.022)	Data 0.000 (0.005)	Loss 1.9125 (1.9088)	Acc@1 76.172 (74.566)	Acc@5 94.531 (95.514)
Epoch: [141][100/196]	Time 0.017 (0.021)	Data 0.000 (0.006)	Loss 2.1369 (1.9153)	Acc@1 70.703 (74.412)	Acc@5 92.188 (95.417)
Epoch: [141][110/196]	Time 0.016 (0.021)	Data 0.000 (0.005)	Loss 1.8268 (1.9195)	Acc@1 76.562 (74.286)	Acc@5 95.312 (95.327)
Epoch: [141][120/196]	Time 0.016 (0.020)	Data 0.000 (0.005)	Loss 2.0757 (1.9266)	Acc@1 69.141 (74.083)	Acc@5 92.188 (95.206)
Epoch: [141][130/196]	Time 0.013 (0.020)	Data 0.003 (0.005)	Loss 1.9991 (1.9358)	Acc@1 73.047 (73.795)	Acc@5 94.922 (95.104)
Epoch: [141][140/196]	Time 0.017 (0.020)	Data 0.000 (0.005)	Loss 1.9647 (1.9400)	Acc@1 74.219 (73.712)	Acc@5 96.094 (95.016)
Epoch: [141][150/196]	Time 0.017 (0.019)	Data 0.001 (0.005)	Loss 1.8948 (1.9459)	Acc@1 77.344 (73.588)	Acc@5 94.922 (94.976)
Epoch: [141][160/196]	Time 0.025 (0.019)	Data 0.001 (0.005)	Loss 1.9728 (1.9529)	Acc@1 71.875 (73.406)	Acc@5 96.094 (94.883)
Epoch: [141][170/196]	Time 0.016 (0.019)	Data 0.001 (0.005)	Loss 1.9220 (1.9589)	Acc@1 74.219 (73.246)	Acc@5 97.266 (94.821)
Epoch: [141][180/196]	Time 0.016 (0.019)	Data 0.000 (0.005)	Loss 2.1794 (1.9667)	Acc@1 68.750 (73.045)	Acc@5 92.578 (94.710)
Epoch: [141][190/196]	Time 0.016 (0.018)	Data 0.000 (0.005)	Loss 2.1549 (1.9772)	Acc@1 67.188 (72.787)	Acc@5 93.359 (94.578)
num momentum params: 26
[0.1, 1.9805964344024658, 1.9549215018749238, 72.714, 53.04, tensor(0.5338, device='cuda:0', grad_fn=<DivBackward0>), 3.8353891372680664, 0.49263525009155273]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [142 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [142][0/196]	Time 0.060 (0.060)	Data 0.211 (0.211)	Loss 1.8480 (1.8480)	Acc@1 76.172 (76.172)	Acc@5 96.875 (96.875)
Epoch: [142][10/196]	Time 0.015 (0.020)	Data 0.002 (0.021)	Loss 1.9299 (1.8970)	Acc@1 75.391 (74.858)	Acc@5 95.312 (95.668)
Epoch: [142][20/196]	Time 0.017 (0.018)	Data 0.008 (0.012)	Loss 1.8395 (1.8842)	Acc@1 77.344 (75.093)	Acc@5 95.312 (95.778)
Epoch: [142][30/196]	Time 0.017 (0.018)	Data 0.001 (0.009)	Loss 1.9317 (1.8893)	Acc@1 73.828 (74.811)	Acc@5 94.531 (95.766)
Epoch: [142][40/196]	Time 0.013 (0.017)	Data 0.007 (0.007)	Loss 1.9390 (1.8909)	Acc@1 74.219 (74.857)	Acc@5 94.922 (95.770)
Epoch: [142][50/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.9324 (1.8984)	Acc@1 74.219 (74.510)	Acc@5 93.750 (95.604)
Epoch: [142][60/196]	Time 0.013 (0.016)	Data 0.020 (0.006)	Loss 1.8629 (1.9062)	Acc@1 75.000 (74.404)	Acc@5 95.312 (95.485)
Epoch: [142][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9401 (1.9080)	Acc@1 71.875 (74.246)	Acc@5 94.141 (95.494)
Epoch: [142][80/196]	Time 0.013 (0.016)	Data 0.020 (0.006)	Loss 1.9506 (1.9215)	Acc@1 69.922 (73.852)	Acc@5 95.703 (95.409)
Epoch: [142][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0897 (1.9395)	Acc@1 68.750 (73.356)	Acc@5 92.969 (95.240)
Epoch: [142][100/196]	Time 0.012 (0.016)	Data 0.023 (0.006)	Loss 2.0530 (1.9439)	Acc@1 68.750 (73.271)	Acc@5 94.531 (95.189)
Epoch: [142][110/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9413 (1.9477)	Acc@1 72.656 (73.181)	Acc@5 93.359 (95.151)
Epoch: [142][120/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 2.0425 (1.9531)	Acc@1 69.922 (72.950)	Acc@5 93.359 (95.125)
Epoch: [142][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1296 (1.9657)	Acc@1 69.141 (72.641)	Acc@5 92.969 (94.987)
Epoch: [142][140/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.1179 (1.9738)	Acc@1 69.531 (72.487)	Acc@5 95.312 (94.878)
Epoch: [142][150/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.1708 (1.9812)	Acc@1 71.484 (72.325)	Acc@5 89.844 (94.790)
Epoch: [142][160/196]	Time 0.013 (0.016)	Data 0.020 (0.005)	Loss 2.0344 (1.9846)	Acc@1 71.094 (72.287)	Acc@5 95.703 (94.750)
Epoch: [142][170/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0818 (1.9856)	Acc@1 71.484 (72.288)	Acc@5 92.578 (94.703)
Epoch: [142][180/196]	Time 0.015 (0.016)	Data 0.008 (0.005)	Loss 2.0617 (1.9895)	Acc@1 73.047 (72.261)	Acc@5 93.359 (94.659)
Epoch: [142][190/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 2.0226 (1.9927)	Acc@1 70.312 (72.231)	Acc@5 95.312 (94.611)
num momentum params: 26
[0.1, 1.9953362652587892, 1.7909713804721832, 72.192, 54.39, tensor(0.5308, device='cuda:0', grad_fn=<DivBackward0>), 3.042065382003784, 0.40007781982421875]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [143 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [143][0/196]	Time 0.062 (0.062)	Data 0.198 (0.198)	Loss 1.9195 (1.9195)	Acc@1 76.172 (76.172)	Acc@5 94.141 (94.141)
Epoch: [143][10/196]	Time 0.017 (0.021)	Data 0.002 (0.020)	Loss 1.8371 (1.8712)	Acc@1 76.172 (76.030)	Acc@5 95.312 (95.845)
Epoch: [143][20/196]	Time 0.014 (0.018)	Data 0.002 (0.012)	Loss 1.9946 (1.8749)	Acc@1 75.000 (75.595)	Acc@5 96.484 (96.001)
Epoch: [143][30/196]	Time 0.016 (0.017)	Data 0.001 (0.009)	Loss 1.7578 (1.8646)	Acc@1 77.344 (75.806)	Acc@5 96.875 (95.980)
Epoch: [143][40/196]	Time 0.013 (0.016)	Data 0.003 (0.007)	Loss 1.7770 (1.8611)	Acc@1 78.125 (75.953)	Acc@5 98.047 (95.979)
Epoch: [143][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.8609 (1.8652)	Acc@1 77.344 (75.804)	Acc@5 97.266 (95.956)
Epoch: [143][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0039 (1.8726)	Acc@1 74.219 (75.717)	Acc@5 93.750 (95.895)
Epoch: [143][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.7342 (1.8741)	Acc@1 76.562 (75.644)	Acc@5 98.828 (95.863)
Epoch: [143][80/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0370 (1.8863)	Acc@1 72.656 (75.270)	Acc@5 93.750 (95.742)
Epoch: [143][90/196]	Time 0.015 (0.015)	Data 0.001 (0.006)	Loss 1.9775 (1.8955)	Acc@1 73.438 (75.052)	Acc@5 92.969 (95.604)
Epoch: [143][100/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.1325 (1.9072)	Acc@1 67.578 (74.710)	Acc@5 92.578 (95.463)
Epoch: [143][110/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0187 (1.9173)	Acc@1 73.047 (74.377)	Acc@5 95.703 (95.337)
Epoch: [143][120/196]	Time 0.015 (0.015)	Data 0.000 (0.005)	Loss 2.0635 (1.9270)	Acc@1 67.578 (74.048)	Acc@5 96.094 (95.296)
Epoch: [143][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0282 (1.9340)	Acc@1 69.141 (73.834)	Acc@5 94.141 (95.175)
Epoch: [143][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0676 (1.9443)	Acc@1 69.531 (73.568)	Acc@5 93.359 (95.058)
Epoch: [143][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0573 (1.9508)	Acc@1 70.312 (73.339)	Acc@5 92.578 (94.992)
Epoch: [143][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1277 (1.9584)	Acc@1 70.703 (73.236)	Acc@5 92.188 (94.871)
Epoch: [143][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1567 (1.9659)	Acc@1 66.406 (73.024)	Acc@5 93.359 (94.812)
Epoch: [143][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0588 (1.9705)	Acc@1 73.438 (72.907)	Acc@5 92.578 (94.771)
Epoch: [143][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1536 (1.9786)	Acc@1 66.406 (72.709)	Acc@5 90.234 (94.681)
num momentum params: 26
[0.1, 1.9806755770874023, 1.8646122217178345, 72.658, 54.39, tensor(0.5351, device='cuda:0', grad_fn=<DivBackward0>), 2.9683480262756348, 0.4023337364196777]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [144 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [144][0/196]	Time 0.060 (0.060)	Data 0.196 (0.196)	Loss 1.8826 (1.8826)	Acc@1 74.219 (74.219)	Acc@5 96.484 (96.484)
Epoch: [144][10/196]	Time 0.017 (0.021)	Data 0.003 (0.019)	Loss 1.9602 (1.8967)	Acc@1 72.266 (74.077)	Acc@5 96.094 (95.597)
Epoch: [144][20/196]	Time 0.013 (0.019)	Data 0.003 (0.011)	Loss 2.0426 (1.9197)	Acc@1 70.703 (73.717)	Acc@5 92.969 (95.164)
Epoch: [144][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9135 (1.9149)	Acc@1 73.438 (74.093)	Acc@5 96.484 (95.212)
Epoch: [144][40/196]	Time 0.012 (0.017)	Data 0.013 (0.007)	Loss 1.9457 (1.9143)	Acc@1 74.219 (74.209)	Acc@5 92.578 (95.208)
Epoch: [144][50/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 1.8185 (1.9085)	Acc@1 75.391 (74.242)	Acc@5 98.047 (95.259)
Epoch: [144][60/196]	Time 0.012 (0.016)	Data 0.013 (0.006)	Loss 1.9300 (1.9104)	Acc@1 75.781 (74.328)	Acc@5 94.141 (95.236)
Epoch: [144][70/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 2.1313 (1.9114)	Acc@1 71.484 (74.329)	Acc@5 93.750 (95.335)
Epoch: [144][80/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 2.0148 (1.9133)	Acc@1 69.531 (74.209)	Acc@5 93.750 (95.390)
Epoch: [144][90/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.0091 (1.9198)	Acc@1 71.484 (74.013)	Acc@5 94.922 (95.300)
Epoch: [144][100/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 2.0909 (1.9304)	Acc@1 66.406 (73.766)	Acc@5 94.531 (95.189)
Epoch: [144][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0878 (1.9410)	Acc@1 70.312 (73.522)	Acc@5 92.969 (95.034)
Epoch: [144][120/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 2.0782 (1.9499)	Acc@1 72.656 (73.321)	Acc@5 93.750 (94.993)
Epoch: [144][130/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.2515 (1.9625)	Acc@1 64.844 (72.889)	Acc@5 91.406 (94.910)
Epoch: [144][140/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 1.9679 (1.9678)	Acc@1 69.922 (72.756)	Acc@5 96.094 (94.864)
Epoch: [144][150/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9760 (1.9704)	Acc@1 73.047 (72.721)	Acc@5 93.359 (94.795)
Epoch: [144][160/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0719 (1.9773)	Acc@1 68.750 (72.574)	Acc@5 94.922 (94.728)
Epoch: [144][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0749 (1.9824)	Acc@1 70.703 (72.416)	Acc@5 93.359 (94.691)
Epoch: [144][180/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0237 (1.9860)	Acc@1 72.656 (72.300)	Acc@5 93.359 (94.631)
Epoch: [144][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1445 (1.9896)	Acc@1 67.578 (72.190)	Acc@5 92.969 (94.589)
num momentum params: 26
[0.1, 1.9919680236816406, 1.8693610644340515, 72.146, 53.23, tensor(0.5319, device='cuda:0', grad_fn=<DivBackward0>), 2.904707431793213, 0.40040111541748047]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [145 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [145][0/196]	Time 0.057 (0.057)	Data 0.207 (0.207)	Loss 1.9436 (1.9436)	Acc@1 73.047 (73.047)	Acc@5 94.141 (94.141)
Epoch: [145][10/196]	Time 0.013 (0.020)	Data 0.004 (0.021)	Loss 1.8454 (1.9363)	Acc@1 76.172 (74.006)	Acc@5 96.484 (94.993)
Epoch: [145][20/196]	Time 0.015 (0.017)	Data 0.003 (0.012)	Loss 1.9123 (1.9252)	Acc@1 76.172 (73.754)	Acc@5 94.922 (95.368)
Epoch: [145][30/196]	Time 0.013 (0.016)	Data 0.007 (0.009)	Loss 2.0367 (1.9241)	Acc@1 71.484 (73.967)	Acc@5 94.531 (95.237)
Epoch: [145][40/196]	Time 0.014 (0.016)	Data 0.002 (0.008)	Loss 1.9607 (1.9301)	Acc@1 74.219 (73.742)	Acc@5 95.703 (95.303)
Epoch: [145][50/196]	Time 0.013 (0.015)	Data 0.020 (0.008)	Loss 1.8774 (1.9366)	Acc@1 76.172 (73.652)	Acc@5 95.703 (95.282)
Epoch: [145][60/196]	Time 0.016 (0.015)	Data 0.001 (0.008)	Loss 1.8564 (1.9413)	Acc@1 75.391 (73.546)	Acc@5 96.094 (95.242)
Epoch: [145][70/196]	Time 0.011 (0.015)	Data 0.019 (0.007)	Loss 1.8768 (1.9444)	Acc@1 72.656 (73.520)	Acc@5 95.703 (95.241)
Epoch: [145][80/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 1.9372 (1.9440)	Acc@1 75.000 (73.553)	Acc@5 96.484 (95.332)
Epoch: [145][90/196]	Time 0.013 (0.015)	Data 0.009 (0.007)	Loss 1.9812 (1.9468)	Acc@1 73.438 (73.459)	Acc@5 92.188 (95.205)
Epoch: [145][100/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 1.8461 (1.9495)	Acc@1 76.953 (73.445)	Acc@5 94.922 (95.154)
Epoch: [145][110/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 2.0459 (1.9547)	Acc@1 70.703 (73.247)	Acc@5 94.141 (95.077)
Epoch: [145][120/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.9432 (1.9580)	Acc@1 73.828 (73.173)	Acc@5 95.312 (95.041)
Epoch: [145][130/196]	Time 0.011 (0.015)	Data 0.016 (0.006)	Loss 2.1227 (1.9666)	Acc@1 67.969 (72.871)	Acc@5 92.188 (94.907)
Epoch: [145][140/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0402 (1.9723)	Acc@1 73.047 (72.784)	Acc@5 92.969 (94.808)
Epoch: [145][150/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 1.9444 (1.9765)	Acc@1 73.047 (72.651)	Acc@5 96.094 (94.730)
Epoch: [145][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9775 (1.9796)	Acc@1 73.828 (72.576)	Acc@5 94.531 (94.696)
Epoch: [145][170/196]	Time 0.013 (0.015)	Data 0.014 (0.005)	Loss 2.0417 (1.9826)	Acc@1 72.266 (72.517)	Acc@5 94.531 (94.671)
Epoch: [145][180/196]	Time 0.015 (0.015)	Data 0.001 (0.005)	Loss 1.9863 (1.9871)	Acc@1 73.438 (72.447)	Acc@5 95.703 (94.609)
Epoch: [145][190/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 2.0660 (1.9917)	Acc@1 69.531 (72.366)	Acc@5 94.531 (94.546)
num momentum params: 26
[0.1, 1.9964106434631348, 1.843949592113495, 72.252, 53.61, tensor(0.5309, device='cuda:0', grad_fn=<DivBackward0>), 2.9458224773406982, 0.4102339744567871]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [146 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [146][0/196]	Time 0.069 (0.069)	Data 0.215 (0.215)	Loss 1.9532 (1.9532)	Acc@1 72.656 (72.656)	Acc@5 94.141 (94.141)
Epoch: [146][10/196]	Time 0.015 (0.021)	Data 0.002 (0.021)	Loss 1.7594 (1.9301)	Acc@1 81.250 (75.071)	Acc@5 97.266 (95.526)
Epoch: [146][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 1.9078 (1.9126)	Acc@1 73.047 (75.167)	Acc@5 95.703 (95.499)
Epoch: [146][30/196]	Time 0.016 (0.017)	Data 0.003 (0.009)	Loss 1.7676 (1.9068)	Acc@1 76.172 (75.076)	Acc@5 98.047 (95.640)
Epoch: [146][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 2.0459 (1.8932)	Acc@1 69.141 (75.238)	Acc@5 94.922 (95.760)
Epoch: [146][50/196]	Time 0.014 (0.016)	Data 0.002 (0.007)	Loss 1.8923 (1.8931)	Acc@1 76.953 (75.100)	Acc@5 93.750 (95.672)
Epoch: [146][60/196]	Time 0.011 (0.016)	Data 0.012 (0.006)	Loss 2.0248 (1.9001)	Acc@1 67.969 (74.923)	Acc@5 94.922 (95.485)
Epoch: [146][70/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9169 (1.9039)	Acc@1 76.953 (74.780)	Acc@5 96.094 (95.489)
Epoch: [146][80/196]	Time 0.012 (0.015)	Data 0.008 (0.006)	Loss 1.9415 (1.9101)	Acc@1 73.438 (74.431)	Acc@5 95.312 (95.467)
Epoch: [146][90/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 1.9004 (1.9180)	Acc@1 76.172 (74.197)	Acc@5 96.094 (95.360)
Epoch: [146][100/196]	Time 0.012 (0.015)	Data 0.003 (0.005)	Loss 2.0638 (1.9303)	Acc@1 70.312 (73.855)	Acc@5 92.969 (95.204)
Epoch: [146][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0475 (1.9378)	Acc@1 69.922 (73.708)	Acc@5 92.969 (95.094)
Epoch: [146][120/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.1524 (1.9474)	Acc@1 67.188 (73.447)	Acc@5 91.797 (94.967)
Epoch: [146][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 1.9877 (1.9556)	Acc@1 71.484 (73.187)	Acc@5 95.312 (94.901)
Epoch: [146][140/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 1.9369 (1.9654)	Acc@1 73.047 (72.944)	Acc@5 95.703 (94.828)
Epoch: [146][150/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0784 (1.9691)	Acc@1 69.141 (72.791)	Acc@5 93.750 (94.793)
Epoch: [146][160/196]	Time 0.011 (0.014)	Data 0.011 (0.005)	Loss 1.9649 (1.9710)	Acc@1 73.828 (72.773)	Acc@5 94.531 (94.735)
Epoch: [146][170/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 2.0499 (1.9741)	Acc@1 67.969 (72.688)	Acc@5 94.922 (94.700)
Epoch: [146][180/196]	Time 0.012 (0.014)	Data 0.020 (0.005)	Loss 2.2119 (1.9816)	Acc@1 68.750 (72.514)	Acc@5 92.188 (94.624)
Epoch: [146][190/196]	Time 0.017 (0.014)	Data 0.000 (0.005)	Loss 1.9835 (1.9864)	Acc@1 73.047 (72.380)	Acc@5 96.484 (94.589)
num momentum params: 26
[0.1, 1.9860131666564942, 1.7814970815181732, 72.366, 54.59, tensor(0.5343, device='cuda:0', grad_fn=<DivBackward0>), 2.823988676071167, 0.41200780868530273]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [147 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [147][0/196]	Time 0.075 (0.075)	Data 0.212 (0.212)	Loss 2.0010 (2.0010)	Acc@1 72.266 (72.266)	Acc@5 92.969 (92.969)
Epoch: [147][10/196]	Time 0.019 (0.021)	Data 0.002 (0.022)	Loss 1.9355 (1.9583)	Acc@1 73.047 (73.402)	Acc@5 96.094 (95.028)
Epoch: [147][20/196]	Time 0.014 (0.018)	Data 0.003 (0.013)	Loss 1.8080 (1.9283)	Acc@1 77.344 (74.684)	Acc@5 95.312 (95.145)
Epoch: [147][30/196]	Time 0.016 (0.017)	Data 0.001 (0.010)	Loss 1.8555 (1.9091)	Acc@1 75.781 (74.546)	Acc@5 96.094 (95.476)
Epoch: [147][40/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 1.8549 (1.8980)	Acc@1 74.609 (74.914)	Acc@5 96.484 (95.665)
Epoch: [147][50/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.7613 (1.8878)	Acc@1 77.344 (75.253)	Acc@5 96.875 (95.688)
Epoch: [147][60/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 2.0584 (1.8974)	Acc@1 69.531 (74.949)	Acc@5 94.531 (95.690)
Epoch: [147][70/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9061 (1.9035)	Acc@1 75.391 (74.714)	Acc@5 96.484 (95.654)
Epoch: [147][80/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9150 (1.9075)	Acc@1 75.000 (74.576)	Acc@5 94.531 (95.621)
Epoch: [147][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8664 (1.9089)	Acc@1 75.391 (74.515)	Acc@5 95.703 (95.613)
Epoch: [147][100/196]	Time 0.015 (0.016)	Data 0.000 (0.006)	Loss 1.9858 (1.9111)	Acc@1 73.047 (74.505)	Acc@5 94.531 (95.572)
Epoch: [147][110/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.8251 (1.9172)	Acc@1 79.297 (74.282)	Acc@5 96.875 (95.534)
Epoch: [147][120/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.8463 (1.9258)	Acc@1 75.391 (73.990)	Acc@5 95.312 (95.422)
Epoch: [147][130/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9971 (1.9339)	Acc@1 71.484 (73.745)	Acc@5 94.531 (95.312)
Epoch: [147][140/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.1840 (1.9407)	Acc@1 71.484 (73.643)	Acc@5 91.797 (95.232)
Epoch: [147][150/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0536 (1.9456)	Acc@1 70.312 (73.466)	Acc@5 94.922 (95.199)
Epoch: [147][160/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1317 (1.9508)	Acc@1 69.922 (73.319)	Acc@5 90.625 (95.082)
Epoch: [147][170/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0247 (1.9566)	Acc@1 73.438 (73.173)	Acc@5 93.750 (94.974)
Epoch: [147][180/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0326 (1.9665)	Acc@1 70.703 (72.896)	Acc@5 95.703 (94.857)
Epoch: [147][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1320 (1.9706)	Acc@1 66.406 (72.804)	Acc@5 93.359 (94.816)
num momentum params: 26
[0.1, 1.9726623518371582, 1.8622472715377807, 72.71, 53.11, tensor(0.5371, device='cuda:0', grad_fn=<DivBackward0>), 3.034247398376465, 0.40724086761474604]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [148 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [148][0/196]	Time 0.079 (0.079)	Data 0.205 (0.205)	Loss 1.8080 (1.8080)	Acc@1 76.953 (76.953)	Acc@5 97.266 (97.266)
Epoch: [148][10/196]	Time 0.015 (0.022)	Data 0.002 (0.021)	Loss 1.7365 (1.8946)	Acc@1 78.906 (75.071)	Acc@5 97.266 (96.165)
Epoch: [148][20/196]	Time 0.012 (0.018)	Data 0.005 (0.012)	Loss 1.9884 (1.9267)	Acc@1 72.266 (74.126)	Acc@5 94.922 (95.350)
Epoch: [148][30/196]	Time 0.014 (0.017)	Data 0.002 (0.009)	Loss 1.9423 (1.8953)	Acc@1 74.219 (75.050)	Acc@5 95.312 (95.615)
Epoch: [148][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.8849 (1.8849)	Acc@1 78.125 (75.114)	Acc@5 94.922 (95.827)
Epoch: [148][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.7993 (1.8798)	Acc@1 78.516 (75.314)	Acc@5 95.703 (95.864)
Epoch: [148][60/196]	Time 0.012 (0.016)	Data 0.010 (0.006)	Loss 1.8073 (1.8830)	Acc@1 81.250 (75.237)	Acc@5 97.656 (95.857)
Epoch: [148][70/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 1.8437 (1.8910)	Acc@1 75.391 (74.967)	Acc@5 96.484 (95.764)
Epoch: [148][80/196]	Time 0.012 (0.015)	Data 0.018 (0.006)	Loss 1.9246 (1.8987)	Acc@1 74.219 (74.735)	Acc@5 96.484 (95.636)
Epoch: [148][90/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 2.0032 (1.9091)	Acc@1 76.172 (74.511)	Acc@5 95.312 (95.527)
Epoch: [148][100/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.9953 (1.9147)	Acc@1 72.266 (74.327)	Acc@5 94.922 (95.494)
Epoch: [148][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.1721 (1.9244)	Acc@1 68.359 (74.064)	Acc@5 92.969 (95.358)
Epoch: [148][120/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 1.8951 (1.9303)	Acc@1 75.000 (73.909)	Acc@5 95.703 (95.267)
Epoch: [148][130/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.1614 (1.9402)	Acc@1 66.406 (73.578)	Acc@5 94.922 (95.208)
Epoch: [148][140/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.0318 (1.9496)	Acc@1 70.703 (73.263)	Acc@5 94.531 (95.113)
Epoch: [148][150/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0011 (1.9575)	Acc@1 73.828 (73.065)	Acc@5 92.969 (95.007)
Epoch: [148][160/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 1.9458 (1.9628)	Acc@1 73.438 (72.882)	Acc@5 94.922 (94.970)
Epoch: [148][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1792 (1.9696)	Acc@1 66.797 (72.768)	Acc@5 91.797 (94.858)
Epoch: [148][180/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 2.1232 (1.9769)	Acc@1 69.922 (72.598)	Acc@5 92.969 (94.777)
Epoch: [148][190/196]	Time 0.015 (0.015)	Data 0.000 (0.005)	Loss 1.9401 (1.9783)	Acc@1 71.875 (72.595)	Acc@5 94.922 (94.742)
num momentum params: 26
[0.1, 1.9793028545379638, 1.7209878408908843, 72.592, 56.25, tensor(0.5355, device='cuda:0', grad_fn=<DivBackward0>), 2.8934695720672607, 0.40826845169067383]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [149 | 180] LR: 0.100000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [149][0/196]	Time 0.071 (0.071)	Data 0.202 (0.202)	Loss 1.8178 (1.8178)	Acc@1 77.344 (77.344)	Acc@5 96.094 (96.094)
Epoch: [149][10/196]	Time 0.015 (0.021)	Data 0.002 (0.020)	Loss 1.9028 (1.9057)	Acc@1 75.391 (74.219)	Acc@5 96.094 (95.881)
Epoch: [149][20/196]	Time 0.019 (0.018)	Data 0.002 (0.012)	Loss 1.8481 (1.8891)	Acc@1 76.172 (74.963)	Acc@5 96.875 (96.038)
Epoch: [149][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.7068 (1.8903)	Acc@1 78.516 (74.975)	Acc@5 98.438 (96.043)
Epoch: [149][40/196]	Time 0.013 (0.017)	Data 0.002 (0.007)	Loss 1.8892 (1.8850)	Acc@1 75.781 (75.010)	Acc@5 95.312 (96.037)
Epoch: [149][50/196]	Time 0.011 (0.016)	Data 0.004 (0.006)	Loss 1.9306 (1.8880)	Acc@1 72.656 (74.862)	Acc@5 95.312 (95.971)
Epoch: [149][60/196]	Time 0.021 (0.016)	Data 0.001 (0.006)	Loss 2.0500 (1.8919)	Acc@1 72.656 (74.725)	Acc@5 93.750 (95.914)
Epoch: [149][70/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.8559 (1.8936)	Acc@1 75.391 (74.703)	Acc@5 95.703 (95.835)
Epoch: [149][80/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9680 (1.8968)	Acc@1 73.438 (74.711)	Acc@5 92.969 (95.732)
Epoch: [149][90/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 1.9862 (1.8977)	Acc@1 74.609 (74.772)	Acc@5 93.359 (95.699)
Epoch: [149][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9687 (1.9000)	Acc@1 75.000 (74.698)	Acc@5 94.531 (95.661)
Epoch: [149][110/196]	Time 0.014 (0.016)	Data 0.006 (0.004)	Loss 2.0659 (1.9039)	Acc@1 69.922 (74.578)	Acc@5 92.969 (95.598)
Epoch: [149][120/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0510 (1.9121)	Acc@1 72.656 (74.361)	Acc@5 93.750 (95.529)
Epoch: [149][130/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9931 (1.9195)	Acc@1 73.047 (74.177)	Acc@5 94.141 (95.420)
Epoch: [149][140/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0410 (1.9298)	Acc@1 70.703 (73.917)	Acc@5 94.141 (95.312)
Epoch: [149][150/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0022 (1.9412)	Acc@1 70.703 (73.611)	Acc@5 95.312 (95.175)
Epoch: [149][160/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0411 (1.9476)	Acc@1 69.141 (73.399)	Acc@5 94.531 (95.089)
Epoch: [149][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9682 (1.9507)	Acc@1 71.094 (73.248)	Acc@5 96.484 (95.066)
Epoch: [149][180/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1782 (1.9569)	Acc@1 64.453 (73.079)	Acc@5 93.359 (94.969)
Epoch: [149][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9686 (1.9615)	Acc@1 71.484 (72.924)	Acc@5 93.750 (94.908)
num momentum params: 26
[0.1, 1.963117319946289, 2.0013289308547972, 72.872, 50.99, tensor(0.5389, device='cuda:0', grad_fn=<DivBackward0>), 3.053541421890259, 0.40800118446350103]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [249, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [249]
Non Pruning Epoch - module.bn3.bias: [249]
Non Pruning Epoch - module.conv4.weight: [256, 249, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [470, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [470]
Non Pruning Epoch - module.bn7.bias: [470]
Non Pruning Epoch - module.conv8.weight: [289, 470, 3, 3]
Non Pruning Epoch - module.bn8.weight: [289]
Non Pruning Epoch - module.bn8.bias: [289]
Non Pruning Epoch - module.fc.weight: [100, 289]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [150 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [249, 127, 3, 3]
module.conv4.weight [256, 249, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [470, 507, 3, 3]
module.conv8.weight [289, 470, 3, 3]
Epoch: [150][0/196]	Time 0.064 (0.064)	Data 0.207 (0.207)	Loss 2.0518 (2.0518)	Acc@1 69.141 (69.141)	Acc@5 93.359 (93.359)
Epoch: [150][10/196]	Time 0.015 (0.021)	Data 0.002 (0.021)	Loss 1.8625 (1.9033)	Acc@1 78.906 (75.178)	Acc@5 96.875 (95.739)
Epoch: [150][20/196]	Time 0.014 (0.018)	Data 0.002 (0.012)	Loss 1.6975 (1.8218)	Acc@1 82.422 (77.604)	Acc@5 98.047 (96.540)
Epoch: [150][30/196]	Time 0.015 (0.017)	Data 0.001 (0.009)	Loss 1.6626 (1.7647)	Acc@1 82.812 (79.322)	Acc@5 98.047 (96.812)
Epoch: [150][40/196]	Time 0.012 (0.016)	Data 0.003 (0.007)	Loss 1.5458 (1.7268)	Acc@1 85.938 (80.488)	Acc@5 98.828 (97.104)
Epoch: [150][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.5982 (1.6983)	Acc@1 84.375 (81.181)	Acc@5 97.266 (97.373)
Epoch: [150][60/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.6666 (1.6769)	Acc@1 82.422 (81.679)	Acc@5 97.656 (97.509)
Epoch: [150][70/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 1.5090 (1.6544)	Acc@1 87.500 (82.449)	Acc@5 97.656 (97.623)
Epoch: [150][80/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.4914 (1.6348)	Acc@1 86.328 (82.957)	Acc@5 98.047 (97.738)
Epoch: [150][90/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.4886 (1.6191)	Acc@1 88.281 (83.426)	Acc@5 99.219 (97.871)
Epoch: [150][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.4289 (1.6041)	Acc@1 89.453 (83.907)	Acc@5 98.047 (97.993)
Epoch: [150][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.4341 (1.5940)	Acc@1 89.453 (84.150)	Acc@5 99.609 (98.061)
Epoch: [150][120/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.4316 (1.5845)	Acc@1 89.453 (84.407)	Acc@5 98.438 (98.124)
Epoch: [150][130/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.4081 (1.5746)	Acc@1 87.891 (84.661)	Acc@5 99.219 (98.190)
Epoch: [150][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.4453 (1.5651)	Acc@1 88.281 (84.876)	Acc@5 97.656 (98.249)
Epoch: [150][150/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.4606 (1.5558)	Acc@1 88.672 (85.159)	Acc@5 99.219 (98.282)
Epoch: [150][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.4929 (1.5493)	Acc@1 84.375 (85.319)	Acc@5 99.609 (98.348)
Epoch: [150][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.5028 (1.5454)	Acc@1 86.719 (85.405)	Acc@5 98.047 (98.360)
Epoch: [150][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.4614 (1.5391)	Acc@1 87.500 (85.571)	Acc@5 99.219 (98.386)
Epoch: [150][190/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 1.4852 (1.5334)	Acc@1 87.891 (85.745)	Acc@5 98.047 (98.411)
num momentum params: 26
[0.010000000000000002, 1.530484148979187, 1.1836565440893174, 85.826, 68.54, tensor(0.6902, device='cuda:0', grad_fn=<DivBackward0>), 3.0513014793395996, 0.4012916088104248]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [40, 3, 3, 3]
Before - module.bn1.weight: [40]
Before - module.bn1.bias: [40]
Before - module.conv2.weight: [127, 40, 3, 3]
Before - module.bn2.weight: [127]
Before - module.bn2.bias: [127]
Before - module.conv3.weight: [249, 127, 3, 3]
Before - module.bn3.weight: [249]
Before - module.bn3.bias: [249]
Before - module.conv4.weight: [256, 249, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [509, 256, 3, 3]
Before - module.bn5.weight: [509]
Before - module.bn5.bias: [509]
Before - module.conv6.weight: [507, 509, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [470, 507, 3, 3]
Before - module.bn7.weight: [470]
Before - module.bn7.bias: [470]
Before - module.conv8.weight: [289, 470, 3, 3]
Before - module.bn8.weight: [289]
Before - module.bn8.bias: [289]
Before - module.fc.weight: [100, 289]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv3.weight]: [249, 127, 3, 3] >> [248, 127, 3, 3]
[module.bn3.weight]: 249 >> 248
running_mean [248]
running_var [248]
num_batches_tracked []
[module.conv4.weight]: [256, 249, 3, 3] >> [256, 248, 3, 3]
[module.conv5.weight]: [509, 256, 3, 3] >> [508, 256, 3, 3]
[module.bn5.weight]: 509 >> 508
running_mean [508]
running_var [508]
num_batches_tracked []
[module.conv6.weight]: [507, 509, 3, 3] >> [507, 508, 3, 3]
[module.conv7.weight]: [470, 507, 3, 3] >> [469, 507, 3, 3]
[module.bn7.weight]: 470 >> 469
running_mean [469]
running_var [469]
num_batches_tracked []
[module.conv8.weight]: [289, 470, 3, 3] >> [286, 469, 3, 3]
[module.bn8.weight]: 289 >> 286
running_mean [286]
running_var [286]
num_batches_tracked []
[module.fc.weight]: [100, 289] >> [100, 286]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [40, 3, 3, 3]
After - module.bn1.weight: [40]
After - module.bn1.bias: [40]
After - module.conv2.weight: [127, 40, 3, 3]
After - module.bn2.weight: [127]
After - module.bn2.bias: [127]
After - module.conv3.weight: [248, 127, 3, 3]
After - module.bn3.weight: [248]
After - module.bn3.bias: [248]
After - module.conv4.weight: [256, 248, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [508, 256, 3, 3]
After - module.bn5.weight: [508]
After - module.bn5.bias: [508]
After - module.conv6.weight: [507, 508, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [469, 507, 3, 3]
After - module.bn7.weight: [469]
After - module.bn7.bias: [469]
After - module.conv8.weight: [286, 469, 3, 3]
After - module.bn8.weight: [286]
After - module.bn8.bias: [286]
After - module.fc.weight: [100, 286]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [40, 3, 3, 3]
conv2 --> [127, 40, 3, 3]
conv3 --> [248, 127, 3, 3]
conv4 --> [256, 248, 3, 3]
conv5 --> [508, 256, 3, 3]
conv6 --> [507, 508, 3, 3]
conv7 --> [469, 507, 3, 3]
conv8 --> [286, 469, 3, 3]
fc --> [286, 100]
1, 442920960, 1105920, 40
2, 4892405760, 11704320, 127
3, 8272613376, 18141696, 248
4, 16675504128, 36569088, 256
5, 10187440128, 18726912, 508
6, 20175906816, 37088064, 507
7, 6574224384, 8560188, 469
8, 3708536832, 4828824, 286
fc, 10982400, 28600, 0
===================
FLOP REPORT: 27711146400000.0 49995200000.0 136753612 124988 2441 14.81756591796875
[INFO] Storing checkpoint...

Epoch: [151 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [151][0/196]	Time 0.644 (0.644)	Data 0.210 (0.210)	Loss 1.4178 (1.4178)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [151][10/196]	Time 0.016 (0.073)	Data 0.002 (0.021)	Loss 1.3893 (1.3692)	Acc@1 89.453 (90.057)	Acc@5 99.219 (99.290)
Epoch: [151][20/196]	Time 0.014 (0.045)	Data 0.003 (0.012)	Loss 1.3555 (1.3591)	Acc@1 90.625 (90.737)	Acc@5 99.609 (99.368)
Epoch: [151][30/196]	Time 0.023 (0.036)	Data 0.004 (0.009)	Loss 1.3070 (1.3636)	Acc@1 92.969 (90.512)	Acc@5 100.000 (99.383)
Epoch: [151][40/196]	Time 0.013 (0.031)	Data 0.003 (0.008)	Loss 1.3043 (1.3580)	Acc@1 93.359 (90.816)	Acc@5 99.219 (99.381)
Epoch: [151][50/196]	Time 0.015 (0.027)	Data 0.002 (0.007)	Loss 1.3680 (1.3559)	Acc@1 91.406 (90.970)	Acc@5 99.609 (99.418)
Epoch: [151][60/196]	Time 0.014 (0.025)	Data 0.003 (0.006)	Loss 1.3800 (1.3598)	Acc@1 90.625 (90.926)	Acc@5 99.219 (99.392)
Epoch: [151][70/196]	Time 0.018 (0.024)	Data 0.003 (0.006)	Loss 1.3624 (1.3603)	Acc@1 89.453 (90.851)	Acc@5 99.219 (99.417)
Epoch: [151][80/196]	Time 0.015 (0.022)	Data 0.003 (0.006)	Loss 1.4719 (1.3613)	Acc@1 83.984 (90.784)	Acc@5 99.219 (99.392)
Epoch: [151][90/196]	Time 0.014 (0.022)	Data 0.008 (0.005)	Loss 1.2819 (1.3598)	Acc@1 93.750 (90.840)	Acc@5 99.609 (99.373)
Epoch: [151][100/196]	Time 0.013 (0.021)	Data 0.003 (0.005)	Loss 1.3288 (1.3601)	Acc@1 93.750 (90.826)	Acc@5 99.609 (99.377)
Epoch: [151][110/196]	Time 0.012 (0.020)	Data 0.016 (0.005)	Loss 1.3994 (1.3592)	Acc@1 90.625 (90.847)	Acc@5 98.828 (99.377)
Epoch: [151][120/196]	Time 0.015 (0.020)	Data 0.002 (0.005)	Loss 1.3331 (1.3589)	Acc@1 91.016 (90.822)	Acc@5 98.828 (99.377)
Epoch: [151][130/196]	Time 0.012 (0.019)	Data 0.013 (0.005)	Loss 1.3582 (1.3568)	Acc@1 91.016 (90.896)	Acc@5 99.219 (99.374)
Epoch: [151][140/196]	Time 0.014 (0.019)	Data 0.002 (0.005)	Loss 1.2837 (1.3539)	Acc@1 91.797 (90.996)	Acc@5 100.000 (99.385)
Epoch: [151][150/196]	Time 0.011 (0.019)	Data 0.014 (0.005)	Loss 1.3521 (1.3529)	Acc@1 90.625 (91.005)	Acc@5 99.219 (99.371)
Epoch: [151][160/196]	Time 0.014 (0.018)	Data 0.003 (0.005)	Loss 1.2584 (1.3509)	Acc@1 95.703 (91.076)	Acc@5 100.000 (99.367)
Epoch: [151][170/196]	Time 0.013 (0.018)	Data 0.008 (0.005)	Loss 1.3477 (1.3489)	Acc@1 90.234 (91.116)	Acc@5 98.828 (99.360)
Epoch: [151][180/196]	Time 0.014 (0.018)	Data 0.003 (0.005)	Loss 1.3573 (1.3487)	Acc@1 89.453 (91.065)	Acc@5 99.219 (99.355)
Epoch: [151][190/196]	Time 0.011 (0.018)	Data 0.005 (0.005)	Loss 1.3214 (1.3484)	Acc@1 92.969 (91.059)	Acc@5 99.609 (99.362)
num momentum params: 26
[0.010000000000000002, 1.3478487675476074, 1.1754760181903838, 91.072, 69.4, tensor(0.7687, device='cuda:0', grad_fn=<DivBackward0>), 3.695429563522339, 0.49562621116638184]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [152 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [152][0/196]	Time 0.058 (0.058)	Data 0.203 (0.203)	Loss 1.3163 (1.3163)	Acc@1 91.016 (91.016)	Acc@5 99.219 (99.219)
Epoch: [152][10/196]	Time 0.016 (0.020)	Data 0.001 (0.020)	Loss 1.2734 (1.2832)	Acc@1 92.578 (92.898)	Acc@5 100.000 (99.645)
Epoch: [152][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 1.3403 (1.2846)	Acc@1 89.453 (92.690)	Acc@5 99.609 (99.647)
Epoch: [152][30/196]	Time 0.013 (0.017)	Data 0.002 (0.009)	Loss 1.2704 (1.2770)	Acc@1 93.359 (93.044)	Acc@5 100.000 (99.672)
Epoch: [152][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.2567 (1.2797)	Acc@1 93.750 (92.959)	Acc@5 99.219 (99.657)
Epoch: [152][50/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 1.2924 (1.2793)	Acc@1 92.188 (92.999)	Acc@5 100.000 (99.640)
Epoch: [152][60/196]	Time 0.013 (0.016)	Data 0.003 (0.006)	Loss 1.2838 (1.2757)	Acc@1 92.969 (93.033)	Acc@5 99.609 (99.641)
Epoch: [152][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.2402 (1.2742)	Acc@1 92.578 (92.952)	Acc@5 100.000 (99.642)
Epoch: [152][80/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 1.2658 (1.2740)	Acc@1 93.359 (92.867)	Acc@5 99.219 (99.648)
Epoch: [152][90/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 1.2915 (1.2731)	Acc@1 91.797 (92.990)	Acc@5 98.438 (99.622)
Epoch: [152][100/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.2508 (1.2739)	Acc@1 94.141 (92.926)	Acc@5 99.609 (99.629)
Epoch: [152][110/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.2099 (1.2742)	Acc@1 95.312 (92.930)	Acc@5 100.000 (99.613)
Epoch: [152][120/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 1.2692 (1.2755)	Acc@1 91.797 (92.907)	Acc@5 99.609 (99.603)
Epoch: [152][130/196]	Time 0.012 (0.014)	Data 0.011 (0.005)	Loss 1.2469 (1.2735)	Acc@1 94.922 (93.013)	Acc@5 99.609 (99.591)
Epoch: [152][140/196]	Time 0.012 (0.014)	Data 0.004 (0.005)	Loss 1.2496 (1.2731)	Acc@1 92.969 (92.977)	Acc@5 99.609 (99.601)
Epoch: [152][150/196]	Time 0.011 (0.014)	Data 0.014 (0.005)	Loss 1.2789 (1.2728)	Acc@1 90.625 (93.000)	Acc@5 99.219 (99.599)
Epoch: [152][160/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 1.2548 (1.2721)	Acc@1 94.531 (93.020)	Acc@5 99.219 (99.592)
Epoch: [152][170/196]	Time 0.014 (0.014)	Data 0.010 (0.005)	Loss 1.2254 (1.2714)	Acc@1 94.531 (93.049)	Acc@5 100.000 (99.587)
Epoch: [152][180/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.2280 (1.2699)	Acc@1 92.188 (93.077)	Acc@5 100.000 (99.588)
Epoch: [152][190/196]	Time 0.011 (0.015)	Data 0.005 (0.005)	Loss 1.2681 (1.2703)	Acc@1 91.016 (93.053)	Acc@5 99.609 (99.591)
num momentum params: 26
[0.010000000000000002, 1.2699431075286864, 1.172471556663513, 93.054, 69.46, tensor(0.8041, device='cuda:0', grad_fn=<DivBackward0>), 2.8443026542663574, 0.4046649932861328]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [153 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [153][0/196]	Time 0.066 (0.066)	Data 0.197 (0.197)	Loss 1.2152 (1.2152)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [153][10/196]	Time 0.017 (0.021)	Data 0.001 (0.020)	Loss 1.2175 (1.2252)	Acc@1 94.141 (93.999)	Acc@5 99.609 (99.787)
Epoch: [153][20/196]	Time 0.014 (0.018)	Data 0.004 (0.012)	Loss 1.2838 (1.2262)	Acc@1 92.188 (94.308)	Acc@5 99.609 (99.758)
Epoch: [153][30/196]	Time 0.017 (0.017)	Data 0.000 (0.009)	Loss 1.2091 (1.2268)	Acc@1 93.750 (94.229)	Acc@5 100.000 (99.761)
Epoch: [153][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.2423 (1.2265)	Acc@1 92.969 (94.188)	Acc@5 100.000 (99.733)
Epoch: [153][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.1986 (1.2272)	Acc@1 96.484 (94.141)	Acc@5 99.609 (99.747)
Epoch: [153][60/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 1.2213 (1.2262)	Acc@1 93.359 (94.141)	Acc@5 99.609 (99.725)
Epoch: [153][70/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.1858 (1.2251)	Acc@1 95.703 (94.196)	Acc@5 100.000 (99.719)
Epoch: [153][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.1675 (1.2244)	Acc@1 95.312 (94.203)	Acc@5 100.000 (99.701)
Epoch: [153][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.2351 (1.2216)	Acc@1 94.922 (94.274)	Acc@5 98.438 (99.704)
Epoch: [153][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.2253 (1.2221)	Acc@1 94.141 (94.183)	Acc@5 99.609 (99.710)
Epoch: [153][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.2695 (1.2211)	Acc@1 91.016 (94.193)	Acc@5 98.828 (99.701)
Epoch: [153][120/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.1952 (1.2214)	Acc@1 95.703 (94.170)	Acc@5 99.609 (99.703)
Epoch: [153][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.2304 (1.2208)	Acc@1 94.141 (94.203)	Acc@5 98.828 (99.705)
Epoch: [153][140/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.1795 (1.2199)	Acc@1 96.484 (94.204)	Acc@5 100.000 (99.706)
Epoch: [153][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.2311 (1.2195)	Acc@1 91.797 (94.174)	Acc@5 100.000 (99.708)
Epoch: [153][160/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.2406 (1.2185)	Acc@1 94.531 (94.201)	Acc@5 99.609 (99.719)
Epoch: [153][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.2370 (1.2185)	Acc@1 92.969 (94.189)	Acc@5 100.000 (99.730)
Epoch: [153][180/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.2531 (1.2180)	Acc@1 92.578 (94.195)	Acc@5 99.609 (99.722)
Epoch: [153][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.1959 (1.2174)	Acc@1 94.922 (94.194)	Acc@5 99.609 (99.728)
num momentum params: 26
[0.010000000000000002, 1.2165263424682617, 1.1825462204217911, 94.216, 69.43, tensor(0.8276, device='cuda:0', grad_fn=<DivBackward0>), 3.0780885219573975, 0.39222812652587896]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [154 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [154][0/196]	Time 0.063 (0.063)	Data 0.211 (0.211)	Loss 1.1894 (1.1894)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [154][10/196]	Time 0.015 (0.020)	Data 0.003 (0.022)	Loss 1.2398 (1.1816)	Acc@1 93.359 (95.455)	Acc@5 99.609 (99.858)
Epoch: [154][20/196]	Time 0.014 (0.018)	Data 0.003 (0.012)	Loss 1.2185 (1.1687)	Acc@1 93.750 (95.796)	Acc@5 99.609 (99.814)
Epoch: [154][30/196]	Time 0.015 (0.017)	Data 0.004 (0.009)	Loss 1.1373 (1.1699)	Acc@1 97.266 (95.691)	Acc@5 100.000 (99.836)
Epoch: [154][40/196]	Time 0.012 (0.016)	Data 0.005 (0.008)	Loss 1.2080 (1.1682)	Acc@1 93.750 (95.684)	Acc@5 100.000 (99.838)
Epoch: [154][50/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 1.1931 (1.1690)	Acc@1 94.141 (95.680)	Acc@5 100.000 (99.809)
Epoch: [154][60/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 1.1630 (1.1663)	Acc@1 95.312 (95.678)	Acc@5 100.000 (99.840)
Epoch: [154][70/196]	Time 0.012 (0.015)	Data 0.004 (0.006)	Loss 1.1603 (1.1685)	Acc@1 96.094 (95.582)	Acc@5 99.609 (99.824)
Epoch: [154][80/196]	Time 0.012 (0.015)	Data 0.016 (0.006)	Loss 1.1572 (1.1686)	Acc@1 96.094 (95.534)	Acc@5 100.000 (99.822)
Epoch: [154][90/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 1.1516 (1.1686)	Acc@1 94.922 (95.510)	Acc@5 100.000 (99.833)
Epoch: [154][100/196]	Time 0.012 (0.015)	Data 0.019 (0.006)	Loss 1.2167 (1.1689)	Acc@1 94.531 (95.487)	Acc@5 99.609 (99.826)
Epoch: [154][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.1924 (1.1704)	Acc@1 94.922 (95.415)	Acc@5 99.609 (99.824)
Epoch: [154][120/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.1995 (1.1712)	Acc@1 94.141 (95.374)	Acc@5 99.609 (99.813)
Epoch: [154][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.1335 (1.1701)	Acc@1 96.094 (95.378)	Acc@5 99.609 (99.803)
Epoch: [154][140/196]	Time 0.012 (0.015)	Data 0.021 (0.005)	Loss 1.2015 (1.1699)	Acc@1 92.969 (95.387)	Acc@5 99.609 (99.806)
Epoch: [154][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.1754 (1.1699)	Acc@1 95.312 (95.354)	Acc@5 99.219 (99.806)
Epoch: [154][160/196]	Time 0.016 (0.015)	Data 0.025 (0.005)	Loss 1.1462 (1.1701)	Acc@1 93.359 (95.322)	Acc@5 100.000 (99.806)
Epoch: [154][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.1282 (1.1693)	Acc@1 98.047 (95.351)	Acc@5 100.000 (99.806)
Epoch: [154][180/196]	Time 0.011 (0.015)	Data 0.020 (0.005)	Loss 1.1711 (1.1684)	Acc@1 94.922 (95.375)	Acc@5 99.219 (99.810)
Epoch: [154][190/196]	Time 0.015 (0.015)	Data 0.000 (0.005)	Loss 1.1724 (1.1684)	Acc@1 94.531 (95.353)	Acc@5 99.609 (99.808)
num momentum params: 26
[0.010000000000000002, 1.168257591819763, 1.1957216715812684, 95.334, 69.28, tensor(0.8492, device='cuda:0', grad_fn=<DivBackward0>), 2.9825987815856934, 0.3938279151916504]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [155 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [155][0/196]	Time 0.065 (0.065)	Data 0.217 (0.217)	Loss 1.1416 (1.1416)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [155][10/196]	Time 0.012 (0.020)	Data 0.004 (0.022)	Loss 1.1393 (1.1401)	Acc@1 97.266 (95.881)	Acc@5 100.000 (99.822)
Epoch: [155][20/196]	Time 0.012 (0.018)	Data 0.005 (0.013)	Loss 1.1073 (1.1400)	Acc@1 97.656 (95.982)	Acc@5 100.000 (99.870)
Epoch: [155][30/196]	Time 0.014 (0.017)	Data 0.002 (0.009)	Loss 1.1325 (1.1362)	Acc@1 94.531 (95.993)	Acc@5 100.000 (99.899)
Epoch: [155][40/196]	Time 0.012 (0.016)	Data 0.004 (0.008)	Loss 1.1387 (1.1354)	Acc@1 94.922 (96.094)	Acc@5 100.000 (99.876)
Epoch: [155][50/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.1353 (1.1336)	Acc@1 96.094 (96.094)	Acc@5 100.000 (99.900)
Epoch: [155][60/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 1.1184 (1.1342)	Acc@1 95.703 (95.998)	Acc@5 100.000 (99.904)
Epoch: [155][70/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 1.1575 (1.1339)	Acc@1 94.922 (96.000)	Acc@5 100.000 (99.901)
Epoch: [155][80/196]	Time 0.012 (0.015)	Data 0.004 (0.006)	Loss 1.1485 (1.1340)	Acc@1 94.531 (96.007)	Acc@5 100.000 (99.904)
Epoch: [155][90/196]	Time 0.013 (0.015)	Data 0.014 (0.006)	Loss 1.1099 (1.1325)	Acc@1 97.656 (96.102)	Acc@5 100.000 (99.910)
Epoch: [155][100/196]	Time 0.012 (0.015)	Data 0.004 (0.006)	Loss 1.1740 (1.1327)	Acc@1 94.141 (96.086)	Acc@5 99.609 (99.915)
Epoch: [155][110/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.1386 (1.1335)	Acc@1 97.266 (96.059)	Acc@5 100.000 (99.901)
Epoch: [155][120/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.1173 (1.1326)	Acc@1 96.875 (96.100)	Acc@5 99.609 (99.890)
Epoch: [155][130/196]	Time 0.013 (0.015)	Data 0.011 (0.005)	Loss 1.0859 (1.1321)	Acc@1 96.875 (96.112)	Acc@5 100.000 (99.890)
Epoch: [155][140/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.0825 (1.1323)	Acc@1 96.875 (96.063)	Acc@5 100.000 (99.884)
Epoch: [155][150/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 1.1560 (1.1318)	Acc@1 94.141 (96.065)	Acc@5 100.000 (99.891)
Epoch: [155][160/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.1214 (1.1315)	Acc@1 94.922 (96.067)	Acc@5 100.000 (99.886)
Epoch: [155][170/196]	Time 0.014 (0.015)	Data 0.005 (0.005)	Loss 1.1136 (1.1308)	Acc@1 96.484 (96.071)	Acc@5 99.609 (99.890)
Epoch: [155][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0836 (1.1296)	Acc@1 97.656 (96.089)	Acc@5 100.000 (99.894)
Epoch: [155][190/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 1.1117 (1.1285)	Acc@1 96.875 (96.098)	Acc@5 100.000 (99.896)
num momentum params: 26
[0.010000000000000002, 1.1283437606811524, 1.203034737110138, 96.102, 69.35, tensor(0.8664, device='cuda:0', grad_fn=<DivBackward0>), 2.8857133388519287, 0.39675283432006836]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [156 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [156][0/196]	Time 0.065 (0.065)	Data 0.212 (0.212)	Loss 1.1059 (1.1059)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [156][10/196]	Time 0.018 (0.021)	Data 0.002 (0.021)	Loss 1.0901 (1.1071)	Acc@1 97.656 (96.378)	Acc@5 100.000 (99.929)
Epoch: [156][20/196]	Time 0.012 (0.018)	Data 0.004 (0.012)	Loss 1.1202 (1.1090)	Acc@1 95.703 (96.447)	Acc@5 99.609 (99.907)
Epoch: [156][30/196]	Time 0.014 (0.017)	Data 0.002 (0.009)	Loss 1.1368 (1.1081)	Acc@1 94.141 (96.497)	Acc@5 100.000 (99.899)
Epoch: [156][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.0907 (1.1045)	Acc@1 96.875 (96.723)	Acc@5 100.000 (99.924)
Epoch: [156][50/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 1.0856 (1.1036)	Acc@1 97.266 (96.714)	Acc@5 100.000 (99.939)
Epoch: [156][60/196]	Time 0.013 (0.016)	Data 0.007 (0.006)	Loss 1.0906 (1.1009)	Acc@1 97.266 (96.792)	Acc@5 99.609 (99.930)
Epoch: [156][70/196]	Time 0.016 (0.015)	Data 0.003 (0.006)	Loss 1.1018 (1.0996)	Acc@1 96.094 (96.825)	Acc@5 99.219 (99.917)
Epoch: [156][80/196]	Time 0.012 (0.015)	Data 0.021 (0.006)	Loss 1.0561 (1.0999)	Acc@1 98.047 (96.774)	Acc@5 100.000 (99.928)
Epoch: [156][90/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 1.1168 (1.0994)	Acc@1 94.922 (96.720)	Acc@5 100.000 (99.927)
Epoch: [156][100/196]	Time 0.012 (0.015)	Data 0.015 (0.006)	Loss 1.1348 (1.0992)	Acc@1 94.922 (96.713)	Acc@5 100.000 (99.927)
Epoch: [156][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.1268 (1.0988)	Acc@1 95.312 (96.713)	Acc@5 100.000 (99.923)
Epoch: [156][120/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 1.1053 (1.0986)	Acc@1 96.094 (96.704)	Acc@5 100.000 (99.929)
Epoch: [156][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.1100 (1.0981)	Acc@1 96.094 (96.726)	Acc@5 99.609 (99.928)
Epoch: [156][140/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 1.0885 (1.0980)	Acc@1 96.875 (96.703)	Acc@5 100.000 (99.920)
Epoch: [156][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.1128 (1.0979)	Acc@1 94.531 (96.678)	Acc@5 100.000 (99.912)
Epoch: [156][160/196]	Time 0.013 (0.015)	Data 0.019 (0.005)	Loss 1.1058 (1.0977)	Acc@1 95.703 (96.632)	Acc@5 100.000 (99.913)
Epoch: [156][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.0596 (1.0970)	Acc@1 98.828 (96.649)	Acc@5 100.000 (99.913)
Epoch: [156][180/196]	Time 0.011 (0.015)	Data 0.017 (0.005)	Loss 1.1061 (1.0972)	Acc@1 96.094 (96.640)	Acc@5 100.000 (99.918)
Epoch: [156][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.1085 (1.0970)	Acc@1 95.312 (96.623)	Acc@5 100.000 (99.914)
num momentum params: 26
[0.010000000000000002, 1.0965654307174684, 1.2143534457683562, 96.62, 69.58, tensor(0.8785, device='cuda:0', grad_fn=<DivBackward0>), 2.9688916206359863, 0.40038347244262695]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [157 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [157][0/196]	Time 0.061 (0.061)	Data 0.213 (0.213)	Loss 1.0598 (1.0598)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [157][10/196]	Time 0.017 (0.020)	Data 0.003 (0.021)	Loss 1.0891 (1.0898)	Acc@1 96.484 (96.662)	Acc@5 100.000 (99.893)
Epoch: [157][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 1.0874 (1.0788)	Acc@1 97.656 (96.838)	Acc@5 100.000 (99.926)
Epoch: [157][30/196]	Time 0.014 (0.017)	Data 0.003 (0.009)	Loss 1.0520 (1.0735)	Acc@1 98.047 (97.077)	Acc@5 100.000 (99.937)
Epoch: [157][40/196]	Time 0.014 (0.016)	Data 0.002 (0.007)	Loss 1.0842 (1.0721)	Acc@1 97.266 (97.208)	Acc@5 100.000 (99.943)
Epoch: [157][50/196]	Time 0.012 (0.016)	Data 0.011 (0.007)	Loss 1.0510 (1.0709)	Acc@1 96.875 (97.235)	Acc@5 100.000 (99.939)
Epoch: [157][60/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 1.0704 (1.0695)	Acc@1 97.656 (97.317)	Acc@5 100.000 (99.942)
Epoch: [157][70/196]	Time 0.013 (0.015)	Data 0.011 (0.006)	Loss 1.0858 (1.0701)	Acc@1 97.266 (97.266)	Acc@5 100.000 (99.950)
Epoch: [157][80/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.0671 (1.0696)	Acc@1 96.875 (97.266)	Acc@5 99.609 (99.942)
Epoch: [157][90/196]	Time 0.012 (0.015)	Data 0.006 (0.006)	Loss 1.0840 (1.0696)	Acc@1 95.703 (97.231)	Acc@5 100.000 (99.931)
Epoch: [157][100/196]	Time 0.015 (0.015)	Data 0.000 (0.006)	Loss 1.0516 (1.0691)	Acc@1 96.875 (97.188)	Acc@5 99.609 (99.934)
Epoch: [157][110/196]	Time 0.014 (0.015)	Data 0.012 (0.006)	Loss 1.0826 (1.0690)	Acc@1 97.266 (97.188)	Acc@5 100.000 (99.940)
Epoch: [157][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.0753 (1.0686)	Acc@1 97.266 (97.201)	Acc@5 100.000 (99.932)
Epoch: [157][130/196]	Time 0.014 (0.015)	Data 0.007 (0.005)	Loss 1.0865 (1.0689)	Acc@1 96.484 (97.170)	Acc@5 99.609 (99.928)
Epoch: [157][140/196]	Time 0.019 (0.015)	Data 0.000 (0.005)	Loss 1.0473 (1.0676)	Acc@1 97.266 (97.221)	Acc@5 100.000 (99.931)
Epoch: [157][150/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 1.0337 (1.0670)	Acc@1 99.219 (97.240)	Acc@5 100.000 (99.930)
Epoch: [157][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0619 (1.0662)	Acc@1 98.047 (97.246)	Acc@5 100.000 (99.927)
Epoch: [157][170/196]	Time 0.012 (0.015)	Data 0.014 (0.005)	Loss 1.0363 (1.0653)	Acc@1 98.047 (97.247)	Acc@5 100.000 (99.925)
Epoch: [157][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0145 (1.0645)	Acc@1 99.609 (97.261)	Acc@5 100.000 (99.929)
Epoch: [157][190/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 1.0360 (1.0638)	Acc@1 97.656 (97.268)	Acc@5 100.000 (99.933)
num momentum params: 26
[0.010000000000000002, 1.0633023777770996, 1.2295304095745088, 97.278, 69.54, tensor(0.8926, device='cuda:0', grad_fn=<DivBackward0>), 2.988480806350708, 0.4019994735717774]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [158 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [158][0/196]	Time 0.065 (0.065)	Data 0.189 (0.189)	Loss 1.0418 (1.0418)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [158][10/196]	Time 0.014 (0.021)	Data 0.003 (0.019)	Loss 1.0251 (1.0373)	Acc@1 98.047 (98.118)	Acc@5 100.000 (99.964)
Epoch: [158][20/196]	Time 0.011 (0.018)	Data 0.007 (0.011)	Loss 1.0391 (1.0371)	Acc@1 98.438 (98.065)	Acc@5 100.000 (99.981)
Epoch: [158][30/196]	Time 0.016 (0.017)	Data 0.002 (0.009)	Loss 1.0427 (1.0394)	Acc@1 96.094 (97.833)	Acc@5 100.000 (99.987)
Epoch: [158][40/196]	Time 0.012 (0.016)	Data 0.011 (0.007)	Loss 1.0470 (1.0368)	Acc@1 96.875 (97.904)	Acc@5 100.000 (99.990)
Epoch: [158][50/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.0251 (1.0358)	Acc@1 98.047 (97.917)	Acc@5 100.000 (99.992)
Epoch: [158][60/196]	Time 0.012 (0.015)	Data 0.016 (0.006)	Loss 1.0168 (1.0354)	Acc@1 98.828 (97.906)	Acc@5 100.000 (99.994)
Epoch: [158][70/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 1.0601 (1.0355)	Acc@1 96.484 (97.915)	Acc@5 99.609 (99.989)
Epoch: [158][80/196]	Time 0.012 (0.015)	Data 0.013 (0.006)	Loss 1.0637 (1.0367)	Acc@1 96.875 (97.825)	Acc@5 100.000 (99.986)
Epoch: [158][90/196]	Time 0.017 (0.015)	Data 0.002 (0.006)	Loss 1.0216 (1.0363)	Acc@1 98.828 (97.819)	Acc@5 100.000 (99.983)
Epoch: [158][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.0275 (1.0368)	Acc@1 98.828 (97.784)	Acc@5 100.000 (99.981)
Epoch: [158][110/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.0193 (1.0364)	Acc@1 98.047 (97.797)	Acc@5 100.000 (99.979)
Epoch: [158][120/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 1.0260 (1.0359)	Acc@1 98.438 (97.814)	Acc@5 100.000 (99.974)
Epoch: [158][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0631 (1.0362)	Acc@1 95.703 (97.770)	Acc@5 100.000 (99.970)
Epoch: [158][140/196]	Time 0.013 (0.015)	Data 0.008 (0.005)	Loss 1.0155 (1.0360)	Acc@1 98.047 (97.720)	Acc@5 100.000 (99.972)
Epoch: [158][150/196]	Time 0.017 (0.015)	Data 0.002 (0.005)	Loss 1.0701 (1.0362)	Acc@1 95.312 (97.705)	Acc@5 99.609 (99.972)
Epoch: [158][160/196]	Time 0.014 (0.015)	Data 0.011 (0.005)	Loss 1.0306 (1.0366)	Acc@1 98.047 (97.681)	Acc@5 100.000 (99.973)
Epoch: [158][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0346 (1.0367)	Acc@1 96.484 (97.647)	Acc@5 100.000 (99.973)
Epoch: [158][180/196]	Time 0.012 (0.015)	Data 0.021 (0.005)	Loss 1.0670 (1.0368)	Acc@1 96.875 (97.617)	Acc@5 99.609 (99.970)
Epoch: [158][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0017 (1.0359)	Acc@1 98.047 (97.619)	Acc@5 100.000 (99.967)
num momentum params: 26
[0.010000000000000002, 1.0354803550338745, 1.2363114613294601, 97.63, 69.21, tensor(0.9030, device='cuda:0', grad_fn=<DivBackward0>), 2.8737258911132812, 0.3969764709472657]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [159 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [159][0/196]	Time 0.066 (0.066)	Data 0.206 (0.206)	Loss 1.0015 (1.0015)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [159][10/196]	Time 0.014 (0.021)	Data 0.002 (0.020)	Loss 1.0071 (1.0127)	Acc@1 98.438 (98.011)	Acc@5 100.000 (100.000)
Epoch: [159][20/196]	Time 0.014 (0.018)	Data 0.003 (0.012)	Loss 1.0431 (1.0161)	Acc@1 97.266 (97.898)	Acc@5 100.000 (99.963)
Epoch: [159][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.0091 (1.0135)	Acc@1 98.828 (98.097)	Acc@5 100.000 (99.962)
Epoch: [159][40/196]	Time 0.017 (0.016)	Data 0.003 (0.007)	Loss 1.0008 (1.0115)	Acc@1 98.828 (98.180)	Acc@5 100.000 (99.952)
Epoch: [159][50/196]	Time 0.015 (0.016)	Data 0.000 (0.006)	Loss 1.0251 (1.0119)	Acc@1 96.875 (98.154)	Acc@5 100.000 (99.962)
Epoch: [159][60/196]	Time 0.012 (0.016)	Data 0.011 (0.006)	Loss 1.0244 (1.0117)	Acc@1 98.047 (98.105)	Acc@5 100.000 (99.968)
Epoch: [159][70/196]	Time 0.013 (0.015)	Data 0.002 (0.006)	Loss 1.0145 (1.0109)	Acc@1 97.266 (98.135)	Acc@5 100.000 (99.972)
Epoch: [159][80/196]	Time 0.011 (0.015)	Data 0.005 (0.006)	Loss 1.0008 (1.0114)	Acc@1 97.656 (98.085)	Acc@5 100.000 (99.976)
Epoch: [159][90/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.0326 (1.0121)	Acc@1 97.656 (98.008)	Acc@5 100.000 (99.970)
Epoch: [159][100/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.0343 (1.0117)	Acc@1 97.656 (98.024)	Acc@5 100.000 (99.973)
Epoch: [159][110/196]	Time 0.015 (0.015)	Data 0.001 (0.006)	Loss 1.0035 (1.0117)	Acc@1 97.656 (97.994)	Acc@5 100.000 (99.972)
Epoch: [159][120/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 1.0134 (1.0115)	Acc@1 98.828 (97.995)	Acc@5 100.000 (99.971)
Epoch: [159][130/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 1.0133 (1.0110)	Acc@1 97.656 (97.987)	Acc@5 100.000 (99.973)
Epoch: [159][140/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.9986 (1.0106)	Acc@1 98.438 (98.005)	Acc@5 100.000 (99.970)
Epoch: [159][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.9954 (1.0104)	Acc@1 99.219 (98.005)	Acc@5 100.000 (99.969)
Epoch: [159][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.0296 (1.0103)	Acc@1 97.656 (98.001)	Acc@5 99.609 (99.966)
Epoch: [159][170/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.0125 (1.0106)	Acc@1 98.047 (97.990)	Acc@5 100.000 (99.963)
Epoch: [159][180/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.0393 (1.0106)	Acc@1 96.484 (97.984)	Acc@5 100.000 (99.965)
Epoch: [159][190/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.0183 (1.0103)	Acc@1 97.656 (97.988)	Acc@5 100.000 (99.967)
num momentum params: 26
[0.010000000000000002, 1.009940453262329, 1.2462224423885346, 97.998, 69.19, tensor(0.9119, device='cuda:0', grad_fn=<DivBackward0>), 2.9124374389648438, 0.4093499183654785]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [248, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [160 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [248, 127, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [160][0/196]	Time 0.074 (0.074)	Data 0.196 (0.196)	Loss 1.0028 (1.0028)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [160][10/196]	Time 0.016 (0.021)	Data 0.002 (0.020)	Loss 1.0149 (0.9950)	Acc@1 97.656 (98.366)	Acc@5 100.000 (99.929)
Epoch: [160][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 0.9872 (0.9925)	Acc@1 98.438 (98.344)	Acc@5 100.000 (99.944)
Epoch: [160][30/196]	Time 0.015 (0.018)	Data 0.003 (0.009)	Loss 0.9696 (0.9884)	Acc@1 99.219 (98.450)	Acc@5 100.000 (99.962)
Epoch: [160][40/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 0.9671 (0.9890)	Acc@1 99.219 (98.447)	Acc@5 100.000 (99.971)
Epoch: [160][50/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 0.9812 (0.9876)	Acc@1 98.828 (98.522)	Acc@5 100.000 (99.977)
Epoch: [160][60/196]	Time 0.011 (0.016)	Data 0.010 (0.006)	Loss 0.9771 (0.9875)	Acc@1 98.828 (98.502)	Acc@5 100.000 (99.981)
Epoch: [160][70/196]	Time 0.012 (0.016)	Data 0.015 (0.006)	Loss 0.9691 (0.9865)	Acc@1 98.828 (98.498)	Acc@5 100.000 (99.983)
Epoch: [160][80/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 0.9597 (0.9867)	Acc@1 100.000 (98.481)	Acc@5 100.000 (99.981)
Epoch: [160][90/196]	Time 0.012 (0.016)	Data 0.019 (0.006)	Loss 0.9963 (0.9872)	Acc@1 98.438 (98.463)	Acc@5 100.000 (99.979)
Epoch: [160][100/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.9748 (0.9871)	Acc@1 97.656 (98.426)	Acc@5 100.000 (99.981)
Epoch: [160][110/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 0.9882 (0.9870)	Acc@1 98.438 (98.413)	Acc@5 100.000 (99.982)
Epoch: [160][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.9803 (0.9871)	Acc@1 98.047 (98.402)	Acc@5 100.000 (99.984)
Epoch: [160][130/196]	Time 0.014 (0.016)	Data 0.010 (0.005)	Loss 1.0005 (0.9870)	Acc@1 97.656 (98.372)	Acc@5 100.000 (99.985)
Epoch: [160][140/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.9896 (0.9876)	Acc@1 97.266 (98.324)	Acc@5 100.000 (99.986)
Epoch: [160][150/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 0.9918 (0.9870)	Acc@1 97.266 (98.337)	Acc@5 100.000 (99.984)
Epoch: [160][160/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 0.9840 (0.9867)	Acc@1 99.219 (98.343)	Acc@5 100.000 (99.981)
Epoch: [160][170/196]	Time 0.013 (0.015)	Data 0.008 (0.005)	Loss 0.9698 (0.9868)	Acc@1 98.828 (98.316)	Acc@5 100.000 (99.979)
Epoch: [160][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 0.9843 (0.9869)	Acc@1 98.047 (98.282)	Acc@5 100.000 (99.981)
Epoch: [160][190/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 0.9750 (0.9866)	Acc@1 98.828 (98.286)	Acc@5 100.000 (99.982)
num momentum params: 26
[0.010000000000000002, 0.9864822293281555, 1.2594892305135728, 98.272, 69.16, tensor(0.9194, device='cuda:0', grad_fn=<DivBackward0>), 3.0171968936920166, 0.3975532054901123]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [40, 3, 3, 3]
Before - module.bn1.weight: [40]
Before - module.bn1.bias: [40]
Before - module.conv2.weight: [127, 40, 3, 3]
Before - module.bn2.weight: [127]
Before - module.bn2.bias: [127]
Before - module.conv3.weight: [248, 127, 3, 3]
Before - module.bn3.weight: [248]
Before - module.bn3.bias: [248]
Before - module.conv4.weight: [256, 248, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [508, 256, 3, 3]
Before - module.bn5.weight: [508]
Before - module.bn5.bias: [508]
Before - module.conv6.weight: [507, 508, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [469, 507, 3, 3]
Before - module.bn7.weight: [469]
Before - module.bn7.bias: [469]
Before - module.conv8.weight: [286, 469, 3, 3]
Before - module.bn8.weight: [286]
Before - module.bn8.bias: [286]
Before - module.fc.weight: [100, 286]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv3.weight]: [248, 127, 3, 3] >> [246, 127, 3, 3]
[module.bn3.weight]: 248 >> 246
running_mean [246]
running_var [246]
num_batches_tracked []
[module.conv4.weight]: [256, 248, 3, 3] >> [256, 246, 3, 3]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [40, 3, 3, 3]
After - module.bn1.weight: [40]
After - module.bn1.bias: [40]
After - module.conv2.weight: [127, 40, 3, 3]
After - module.bn2.weight: [127]
After - module.bn2.bias: [127]
After - module.conv3.weight: [246, 127, 3, 3]
After - module.bn3.weight: [246]
After - module.bn3.bias: [246]
After - module.conv4.weight: [256, 246, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [508, 256, 3, 3]
After - module.bn5.weight: [508]
After - module.bn5.bias: [508]
After - module.conv6.weight: [507, 508, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [469, 507, 3, 3]
After - module.bn7.weight: [469]
After - module.bn7.bias: [469]
After - module.conv8.weight: [286, 469, 3, 3]
After - module.bn8.weight: [286]
After - module.bn8.bias: [286]
After - module.fc.weight: [100, 286]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [40, 3, 3, 3]
conv2 --> [127, 40, 3, 3]
conv3 --> [246, 127, 3, 3]
conv4 --> [256, 246, 3, 3]
conv5 --> [508, 256, 3, 3]
conv6 --> [507, 508, 3, 3]
conv7 --> [469, 507, 3, 3]
conv8 --> [286, 469, 3, 3]
fc --> [286, 100]
1, 442920960, 1105920, 40
2, 4892405760, 11704320, 127
3, 8205898752, 17995392, 246
4, 16541024256, 36274176, 256
5, 10187440128, 18726912, 508
6, 20175906816, 37088064, 507
7, 6574224384, 8560188, 469
8, 3708536832, 4828824, 286
fc, 10982400, 28600, 0
===================
FLOP REPORT: 27632554800000.0 49944000000.0 136312396 124860 2439 14.804412841796875
[INFO] Storing checkpoint...

Epoch: [161 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [161][0/196]	Time 0.208 (0.208)	Data 0.202 (0.202)	Loss 0.9544 (0.9544)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [161][10/196]	Time 0.017 (0.034)	Data 0.002 (0.020)	Loss 0.9585 (0.9696)	Acc@1 98.438 (98.615)	Acc@5 100.000 (100.000)
Epoch: [161][20/196]	Time 0.012 (0.025)	Data 0.004 (0.012)	Loss 0.9860 (0.9716)	Acc@1 97.266 (98.586)	Acc@5 100.000 (100.000)
Epoch: [161][30/196]	Time 0.014 (0.022)	Data 0.003 (0.009)	Loss 0.9812 (0.9706)	Acc@1 98.047 (98.702)	Acc@5 100.000 (100.000)
Epoch: [161][40/196]	Time 0.014 (0.021)	Data 0.004 (0.007)	Loss 0.9533 (0.9697)	Acc@1 99.219 (98.714)	Acc@5 100.000 (100.000)
Epoch: [161][50/196]	Time 0.013 (0.020)	Data 0.004 (0.006)	Loss 0.9706 (0.9677)	Acc@1 98.438 (98.721)	Acc@5 100.000 (100.000)
Epoch: [161][60/196]	Time 0.014 (0.019)	Data 0.004 (0.006)	Loss 0.9961 (0.9673)	Acc@1 97.656 (98.726)	Acc@5 100.000 (100.000)
Epoch: [161][70/196]	Time 0.013 (0.019)	Data 0.004 (0.005)	Loss 0.9649 (0.9664)	Acc@1 99.219 (98.746)	Acc@5 100.000 (99.994)
Epoch: [161][80/196]	Time 0.014 (0.018)	Data 0.002 (0.005)	Loss 0.9745 (0.9660)	Acc@1 98.438 (98.732)	Acc@5 100.000 (99.995)
Epoch: [161][90/196]	Time 0.012 (0.018)	Data 0.004 (0.005)	Loss 0.9604 (0.9655)	Acc@1 98.047 (98.751)	Acc@5 100.000 (99.996)
Epoch: [161][100/196]	Time 0.016 (0.017)	Data 0.001 (0.004)	Loss 0.9349 (0.9653)	Acc@1 99.609 (98.747)	Acc@5 100.000 (99.996)
Epoch: [161][110/196]	Time 0.013 (0.017)	Data 0.008 (0.004)	Loss 0.9505 (0.9653)	Acc@1 99.219 (98.744)	Acc@5 100.000 (99.996)
Epoch: [161][120/196]	Time 0.016 (0.017)	Data 0.000 (0.004)	Loss 0.9583 (0.9649)	Acc@1 98.438 (98.735)	Acc@5 100.000 (99.997)
Epoch: [161][130/196]	Time 0.012 (0.017)	Data 0.009 (0.004)	Loss 0.9467 (0.9647)	Acc@1 98.047 (98.703)	Acc@5 100.000 (99.997)
Epoch: [161][140/196]	Time 0.017 (0.017)	Data 0.000 (0.004)	Loss 0.9467 (0.9646)	Acc@1 99.609 (98.695)	Acc@5 100.000 (99.992)
Epoch: [161][150/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9708 (0.9644)	Acc@1 98.438 (98.683)	Acc@5 100.000 (99.992)
Epoch: [161][160/196]	Time 0.016 (0.017)	Data 0.001 (0.004)	Loss 0.9871 (0.9643)	Acc@1 98.047 (98.668)	Acc@5 100.000 (99.993)
Epoch: [161][170/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 0.9540 (0.9641)	Acc@1 98.438 (98.641)	Acc@5 100.000 (99.993)
Epoch: [161][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.9621 (0.9639)	Acc@1 98.438 (98.619)	Acc@5 100.000 (99.989)
Epoch: [161][190/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.9824 (0.9640)	Acc@1 97.656 (98.603)	Acc@5 100.000 (99.986)
num momentum params: 26
[0.010000000000000002, 0.9639629559326172, 1.26352900326252, 98.592, 69.21, tensor(0.9266, device='cuda:0', grad_fn=<DivBackward0>), 3.242726802825928, 0.4291560649871826]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [162 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [162][0/196]	Time 0.066 (0.066)	Data 0.212 (0.212)	Loss 0.9392 (0.9392)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [162][10/196]	Time 0.015 (0.021)	Data 0.002 (0.021)	Loss 0.9631 (0.9503)	Acc@1 97.656 (98.509)	Acc@5 99.609 (99.964)
Epoch: [162][20/196]	Time 0.023 (0.019)	Data 0.003 (0.012)	Loss 0.9659 (0.9521)	Acc@1 97.656 (98.531)	Acc@5 100.000 (99.963)
Epoch: [162][30/196]	Time 0.016 (0.017)	Data 0.002 (0.009)	Loss 0.9332 (0.9501)	Acc@1 99.219 (98.664)	Acc@5 100.000 (99.962)
Epoch: [162][40/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 0.9415 (0.9485)	Acc@1 98.438 (98.685)	Acc@5 100.000 (99.971)
Epoch: [162][50/196]	Time 0.018 (0.016)	Data 0.001 (0.007)	Loss 0.9438 (0.9492)	Acc@1 99.609 (98.644)	Acc@5 100.000 (99.969)
Epoch: [162][60/196]	Time 0.017 (0.016)	Data 0.002 (0.006)	Loss 0.9336 (0.9486)	Acc@1 99.219 (98.674)	Acc@5 100.000 (99.974)
Epoch: [162][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.9251 (0.9486)	Acc@1 99.219 (98.652)	Acc@5 100.000 (99.972)
Epoch: [162][80/196]	Time 0.013 (0.016)	Data 0.021 (0.006)	Loss 0.9626 (0.9485)	Acc@1 98.047 (98.659)	Acc@5 100.000 (99.971)
Epoch: [162][90/196]	Time 0.012 (0.016)	Data 0.004 (0.005)	Loss 0.9405 (0.9474)	Acc@1 99.219 (98.699)	Acc@5 100.000 (99.974)
Epoch: [162][100/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.9496 (0.9466)	Acc@1 98.438 (98.731)	Acc@5 100.000 (99.977)
Epoch: [162][110/196]	Time 0.012 (0.015)	Data 0.022 (0.005)	Loss 0.9542 (0.9460)	Acc@1 98.438 (98.772)	Acc@5 99.609 (99.975)
Epoch: [162][120/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.9509 (0.9460)	Acc@1 98.438 (98.747)	Acc@5 100.000 (99.977)
Epoch: [162][130/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 0.9252 (0.9456)	Acc@1 99.219 (98.768)	Acc@5 100.000 (99.979)
Epoch: [162][140/196]	Time 0.014 (0.015)	Data 0.008 (0.005)	Loss 0.9433 (0.9453)	Acc@1 98.047 (98.745)	Acc@5 100.000 (99.981)
Epoch: [162][150/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.9322 (0.9449)	Acc@1 98.828 (98.732)	Acc@5 100.000 (99.982)
Epoch: [162][160/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 0.9720 (0.9450)	Acc@1 98.047 (98.729)	Acc@5 100.000 (99.981)
Epoch: [162][170/196]	Time 0.013 (0.015)	Data 0.008 (0.005)	Loss 0.9441 (0.9448)	Acc@1 98.438 (98.712)	Acc@5 100.000 (99.982)
Epoch: [162][180/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.9319 (0.9445)	Acc@1 98.438 (98.709)	Acc@5 100.000 (99.981)
Epoch: [162][190/196]	Time 0.012 (0.015)	Data 0.012 (0.005)	Loss 0.9360 (0.9446)	Acc@1 99.219 (98.705)	Acc@5 100.000 (99.982)
num momentum params: 26
[0.010000000000000002, 0.9445406151771545, 1.2711258202791214, 98.704, 69.33, tensor(0.9312, device='cuda:0', grad_fn=<DivBackward0>), 2.9720869064331055, 0.3996086120605469]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [163 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [163][0/196]	Time 0.065 (0.065)	Data 0.204 (0.204)	Loss 0.9450 (0.9450)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [163][10/196]	Time 0.014 (0.021)	Data 0.003 (0.021)	Loss 0.9239 (0.9337)	Acc@1 99.219 (98.793)	Acc@5 100.000 (100.000)
Epoch: [163][20/196]	Time 0.013 (0.018)	Data 0.006 (0.012)	Loss 0.9511 (0.9317)	Acc@1 98.438 (98.865)	Acc@5 100.000 (100.000)
Epoch: [163][30/196]	Time 0.016 (0.017)	Data 0.002 (0.009)	Loss 0.9327 (0.9294)	Acc@1 98.828 (98.954)	Acc@5 100.000 (100.000)
Epoch: [163][40/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 0.9295 (0.9299)	Acc@1 98.828 (98.876)	Acc@5 100.000 (100.000)
Epoch: [163][50/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 0.9265 (0.9310)	Acc@1 98.828 (98.851)	Acc@5 100.000 (100.000)
Epoch: [163][60/196]	Time 0.016 (0.016)	Data 0.004 (0.006)	Loss 0.9222 (0.9299)	Acc@1 98.438 (98.847)	Acc@5 100.000 (100.000)
Epoch: [163][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.9307 (0.9300)	Acc@1 99.609 (98.828)	Acc@5 100.000 (100.000)
Epoch: [163][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.9232 (0.9292)	Acc@1 98.438 (98.843)	Acc@5 100.000 (100.000)
Epoch: [163][90/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 0.9346 (0.9291)	Acc@1 99.219 (98.884)	Acc@5 100.000 (100.000)
Epoch: [163][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 0.9291 (0.9284)	Acc@1 98.828 (98.909)	Acc@5 100.000 (100.000)
Epoch: [163][110/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 0.9264 (0.9285)	Acc@1 97.656 (98.902)	Acc@5 100.000 (100.000)
Epoch: [163][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 0.9369 (0.9284)	Acc@1 99.219 (98.883)	Acc@5 100.000 (100.000)
Epoch: [163][130/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 0.9125 (0.9291)	Acc@1 99.219 (98.828)	Acc@5 100.000 (100.000)
Epoch: [163][140/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 0.9373 (0.9287)	Acc@1 98.438 (98.831)	Acc@5 100.000 (100.000)
Epoch: [163][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.9137 (0.9285)	Acc@1 99.219 (98.823)	Acc@5 100.000 (100.000)
Epoch: [163][160/196]	Time 0.018 (0.015)	Data 0.002 (0.004)	Loss 0.8998 (0.9278)	Acc@1 99.609 (98.838)	Acc@5 100.000 (100.000)
Epoch: [163][170/196]	Time 0.012 (0.015)	Data 0.015 (0.004)	Loss 0.9239 (0.9273)	Acc@1 99.219 (98.851)	Acc@5 100.000 (100.000)
Epoch: [163][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.9140 (0.9270)	Acc@1 98.438 (98.835)	Acc@5 100.000 (100.000)
Epoch: [163][190/196]	Time 0.012 (0.015)	Data 0.018 (0.004)	Loss 0.9123 (0.9265)	Acc@1 99.219 (98.836)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.926357885055542, 1.2738159066438675, 98.836, 69.7, tensor(0.9348, device='cuda:0', grad_fn=<DivBackward0>), 2.9755454063415527, 0.4145607948303222]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [164 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [164][0/196]	Time 0.072 (0.072)	Data 0.195 (0.195)	Loss 0.9044 (0.9044)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [164][10/196]	Time 0.016 (0.022)	Data 0.002 (0.020)	Loss 0.9267 (0.9121)	Acc@1 98.828 (99.148)	Acc@5 100.000 (100.000)
Epoch: [164][20/196]	Time 0.014 (0.018)	Data 0.002 (0.011)	Loss 0.9054 (0.9149)	Acc@1 99.609 (99.014)	Acc@5 100.000 (100.000)
Epoch: [164][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 0.8954 (0.9123)	Acc@1 99.219 (99.042)	Acc@5 100.000 (100.000)
Epoch: [164][40/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 0.9099 (0.9110)	Acc@1 99.609 (99.114)	Acc@5 100.000 (100.000)
Epoch: [164][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.8937 (0.9101)	Acc@1 99.609 (99.081)	Acc@5 100.000 (100.000)
Epoch: [164][60/196]	Time 0.011 (0.016)	Data 0.017 (0.006)	Loss 0.9052 (0.9106)	Acc@1 99.219 (99.059)	Acc@5 100.000 (100.000)
Epoch: [164][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.9337 (0.9101)	Acc@1 98.047 (99.065)	Acc@5 100.000 (99.994)
Epoch: [164][80/196]	Time 0.012 (0.016)	Data 0.020 (0.006)	Loss 0.8971 (0.9096)	Acc@1 100.000 (99.074)	Acc@5 100.000 (99.995)
Epoch: [164][90/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.8928 (0.9092)	Acc@1 100.000 (99.077)	Acc@5 100.000 (99.996)
Epoch: [164][100/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 0.8968 (0.9089)	Acc@1 100.000 (99.049)	Acc@5 100.000 (99.996)
Epoch: [164][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.9344 (0.9094)	Acc@1 97.656 (98.997)	Acc@5 100.000 (99.996)
Epoch: [164][120/196]	Time 0.011 (0.016)	Data 0.015 (0.005)	Loss 0.8805 (0.9088)	Acc@1 100.000 (99.009)	Acc@5 100.000 (99.997)
Epoch: [164][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.9093 (0.9083)	Acc@1 98.828 (99.013)	Acc@5 100.000 (99.997)
Epoch: [164][140/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 0.9199 (0.9085)	Acc@1 98.047 (98.994)	Acc@5 100.000 (99.994)
Epoch: [164][150/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.9278 (0.9084)	Acc@1 98.047 (98.965)	Acc@5 100.000 (99.995)
Epoch: [164][160/196]	Time 0.011 (0.016)	Data 0.012 (0.005)	Loss 0.8882 (0.9081)	Acc@1 99.219 (98.957)	Acc@5 100.000 (99.995)
Epoch: [164][170/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.9138 (0.9080)	Acc@1 98.828 (98.958)	Acc@5 100.000 (99.995)
Epoch: [164][180/196]	Time 0.012 (0.016)	Data 0.017 (0.005)	Loss 0.9101 (0.9078)	Acc@1 98.828 (98.958)	Acc@5 100.000 (99.996)
Epoch: [164][190/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.9107 (0.9077)	Acc@1 98.047 (98.945)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.9077274191474914, 1.2773497170209884, 98.94, 69.63, tensor(0.9392, device='cuda:0', grad_fn=<DivBackward0>), 3.0454180240631104, 0.4107804298400879]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [165 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [165][0/196]	Time 0.068 (0.068)	Data 0.201 (0.201)	Loss 0.8811 (0.8811)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [165][10/196]	Time 0.017 (0.021)	Data 0.002 (0.021)	Loss 0.8896 (0.9008)	Acc@1 99.609 (98.899)	Acc@5 100.000 (100.000)
Epoch: [165][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 0.8735 (0.8958)	Acc@1 100.000 (99.126)	Acc@5 100.000 (100.000)
Epoch: [165][30/196]	Time 0.016 (0.017)	Data 0.001 (0.009)	Loss 0.8935 (0.8944)	Acc@1 98.828 (99.206)	Acc@5 100.000 (100.000)
Epoch: [165][40/196]	Time 0.012 (0.017)	Data 0.019 (0.008)	Loss 0.8934 (0.8944)	Acc@1 99.219 (99.181)	Acc@5 100.000 (100.000)
Epoch: [165][50/196]	Time 0.018 (0.017)	Data 0.001 (0.007)	Loss 0.8904 (0.8932)	Acc@1 98.828 (99.180)	Acc@5 100.000 (100.000)
Epoch: [165][60/196]	Time 0.012 (0.017)	Data 0.007 (0.007)	Loss 0.8853 (0.8923)	Acc@1 99.609 (99.206)	Acc@5 100.000 (100.000)
Epoch: [165][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.8804 (0.8909)	Acc@1 99.609 (99.219)	Acc@5 100.000 (100.000)
Epoch: [165][80/196]	Time 0.012 (0.016)	Data 0.005 (0.006)	Loss 0.8774 (0.8898)	Acc@1 99.219 (99.233)	Acc@5 100.000 (100.000)
Epoch: [165][90/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 0.8747 (0.8894)	Acc@1 99.609 (99.240)	Acc@5 100.000 (100.000)
Epoch: [165][100/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 0.8845 (0.8891)	Acc@1 100.000 (99.230)	Acc@5 100.000 (100.000)
Epoch: [165][110/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 0.8963 (0.8891)	Acc@1 98.828 (99.226)	Acc@5 100.000 (100.000)
Epoch: [165][120/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 0.8866 (0.8890)	Acc@1 99.219 (99.209)	Acc@5 100.000 (100.000)
Epoch: [165][130/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 0.8750 (0.8890)	Acc@1 99.609 (99.183)	Acc@5 100.000 (100.000)
Epoch: [165][140/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 0.8950 (0.8888)	Acc@1 98.828 (99.194)	Acc@5 99.609 (99.997)
Epoch: [165][150/196]	Time 0.013 (0.016)	Data 0.006 (0.004)	Loss 0.8968 (0.8889)	Acc@1 98.438 (99.188)	Acc@5 100.000 (99.997)
Epoch: [165][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8794 (0.8887)	Acc@1 99.609 (99.187)	Acc@5 100.000 (99.998)
Epoch: [165][170/196]	Time 0.013 (0.016)	Data 0.010 (0.004)	Loss 0.8813 (0.8888)	Acc@1 99.609 (99.166)	Acc@5 100.000 (99.995)
Epoch: [165][180/196]	Time 0.013 (0.016)	Data 0.002 (0.004)	Loss 0.9001 (0.8884)	Acc@1 98.828 (99.158)	Acc@5 100.000 (99.996)
Epoch: [165][190/196]	Time 0.011 (0.016)	Data 0.010 (0.004)	Loss 0.8781 (0.8878)	Acc@1 100.000 (99.170)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.8877481051254272, 1.286358430981636, 99.166, 69.08, tensor(0.9452, device='cuda:0', grad_fn=<DivBackward0>), 3.041179895401001, 0.3968188762664795]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [166 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [166][0/196]	Time 0.071 (0.071)	Data 0.202 (0.202)	Loss 0.8673 (0.8673)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [166][10/196]	Time 0.017 (0.022)	Data 0.002 (0.021)	Loss 0.8899 (0.8741)	Acc@1 98.828 (99.396)	Acc@5 100.000 (100.000)
Epoch: [166][20/196]	Time 0.015 (0.019)	Data 0.002 (0.012)	Loss 0.8683 (0.8738)	Acc@1 99.609 (99.349)	Acc@5 100.000 (100.000)
Epoch: [166][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 0.8718 (0.8732)	Acc@1 99.609 (99.357)	Acc@5 100.000 (100.000)
Epoch: [166][40/196]	Time 0.017 (0.018)	Data 0.002 (0.007)	Loss 0.8815 (0.8748)	Acc@1 98.828 (99.247)	Acc@5 100.000 (99.990)
Epoch: [166][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 0.8772 (0.8745)	Acc@1 99.219 (99.265)	Acc@5 100.000 (99.992)
Epoch: [166][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.8874 (0.8742)	Acc@1 98.828 (99.296)	Acc@5 100.000 (99.994)
Epoch: [166][70/196]	Time 0.011 (0.017)	Data 0.002 (0.005)	Loss 0.8772 (0.8744)	Acc@1 98.828 (99.274)	Acc@5 100.000 (99.994)
Epoch: [166][80/196]	Time 0.016 (0.017)	Data 0.000 (0.005)	Loss 0.8667 (0.8741)	Acc@1 99.609 (99.286)	Acc@5 100.000 (99.995)
Epoch: [166][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.8597 (0.8737)	Acc@1 100.000 (99.287)	Acc@5 100.000 (99.996)
Epoch: [166][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8721 (0.8736)	Acc@1 98.828 (99.273)	Acc@5 100.000 (99.996)
Epoch: [166][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.8781 (0.8736)	Acc@1 98.828 (99.254)	Acc@5 100.000 (99.996)
Epoch: [166][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8698 (0.8730)	Acc@1 99.219 (99.261)	Acc@5 100.000 (99.997)
Epoch: [166][130/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.8732 (0.8726)	Acc@1 99.219 (99.263)	Acc@5 100.000 (99.997)
Epoch: [166][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8671 (0.8724)	Acc@1 99.609 (99.274)	Acc@5 100.000 (99.997)
Epoch: [166][150/196]	Time 0.013 (0.016)	Data 0.009 (0.004)	Loss 0.8792 (0.8722)	Acc@1 98.828 (99.278)	Acc@5 100.000 (99.997)
Epoch: [166][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.8562 (0.8720)	Acc@1 100.000 (99.275)	Acc@5 100.000 (99.998)
Epoch: [166][170/196]	Time 0.013 (0.016)	Data 0.022 (0.004)	Loss 0.8611 (0.8717)	Acc@1 99.219 (99.274)	Acc@5 100.000 (99.998)
Epoch: [166][180/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 0.8707 (0.8716)	Acc@1 98.438 (99.266)	Acc@5 100.000 (99.998)
Epoch: [166][190/196]	Time 0.013 (0.016)	Data 0.007 (0.004)	Loss 0.8672 (0.8712)	Acc@1 98.828 (99.272)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8710208133125306, 1.2776863914728165, 99.282, 69.58, tensor(0.9481, device='cuda:0', grad_fn=<DivBackward0>), 3.084956169128418, 0.40566062927246094]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [167 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [167][0/196]	Time 0.064 (0.064)	Data 0.202 (0.202)	Loss 0.8671 (0.8671)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [167][10/196]	Time 0.016 (0.022)	Data 0.002 (0.020)	Loss 0.8485 (0.8541)	Acc@1 100.000 (99.396)	Acc@5 100.000 (100.000)
Epoch: [167][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.8669 (0.8537)	Acc@1 99.219 (99.479)	Acc@5 100.000 (100.000)
Epoch: [167][30/196]	Time 0.015 (0.019)	Data 0.002 (0.008)	Loss 0.8579 (0.8549)	Acc@1 100.000 (99.446)	Acc@5 100.000 (100.000)
Epoch: [167][40/196]	Time 0.015 (0.018)	Data 0.002 (0.007)	Loss 0.8590 (0.8561)	Acc@1 99.609 (99.371)	Acc@5 100.000 (100.000)
Epoch: [167][50/196]	Time 0.014 (0.017)	Data 0.002 (0.006)	Loss 0.8963 (0.8576)	Acc@1 97.656 (99.349)	Acc@5 100.000 (100.000)
Epoch: [167][60/196]	Time 0.014 (0.017)	Data 0.002 (0.006)	Loss 0.8573 (0.8578)	Acc@1 99.219 (99.340)	Acc@5 100.000 (100.000)
Epoch: [167][70/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 0.8482 (0.8576)	Acc@1 100.000 (99.356)	Acc@5 100.000 (100.000)
Epoch: [167][80/196]	Time 0.013 (0.017)	Data 0.003 (0.005)	Loss 0.8457 (0.8571)	Acc@1 99.609 (99.359)	Acc@5 100.000 (100.000)
Epoch: [167][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.8478 (0.8571)	Acc@1 99.609 (99.365)	Acc@5 100.000 (100.000)
Epoch: [167][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8529 (0.8569)	Acc@1 99.609 (99.370)	Acc@5 100.000 (100.000)
Epoch: [167][110/196]	Time 0.022 (0.016)	Data 0.001 (0.004)	Loss 0.8481 (0.8565)	Acc@1 99.609 (99.377)	Acc@5 100.000 (100.000)
Epoch: [167][120/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 0.8786 (0.8565)	Acc@1 98.047 (99.361)	Acc@5 100.000 (100.000)
Epoch: [167][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 0.8532 (0.8560)	Acc@1 99.219 (99.371)	Acc@5 100.000 (100.000)
Epoch: [167][140/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 0.8583 (0.8559)	Acc@1 99.219 (99.368)	Acc@5 100.000 (100.000)
Epoch: [167][150/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.8462 (0.8553)	Acc@1 98.828 (99.358)	Acc@5 100.000 (100.000)
Epoch: [167][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.8569 (0.8552)	Acc@1 98.438 (99.342)	Acc@5 100.000 (100.000)
Epoch: [167][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8486 (0.8550)	Acc@1 99.609 (99.333)	Acc@5 100.000 (100.000)
Epoch: [167][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8695 (0.8553)	Acc@1 98.438 (99.309)	Acc@5 100.000 (100.000)
Epoch: [167][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8710 (0.8550)	Acc@1 98.438 (99.313)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.8550826577568055, 1.2808502948284148, 99.304, 69.32, tensor(0.9504, device='cuda:0', grad_fn=<DivBackward0>), 3.0099427700042725, 0.4011244773864746]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [168 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [168][0/196]	Time 0.064 (0.064)	Data 0.200 (0.200)	Loss 0.8352 (0.8352)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [168][10/196]	Time 0.014 (0.020)	Data 0.002 (0.020)	Loss 0.8484 (0.8502)	Acc@1 99.219 (99.041)	Acc@5 100.000 (100.000)
Epoch: [168][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 0.8486 (0.8488)	Acc@1 99.219 (99.126)	Acc@5 100.000 (100.000)
Epoch: [168][30/196]	Time 0.014 (0.017)	Data 0.005 (0.009)	Loss 0.8551 (0.8483)	Acc@1 99.609 (99.156)	Acc@5 100.000 (100.000)
Epoch: [168][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 0.8555 (0.8470)	Acc@1 98.828 (99.190)	Acc@5 100.000 (100.000)
Epoch: [168][50/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 0.8534 (0.8459)	Acc@1 98.828 (99.211)	Acc@5 100.000 (100.000)
Epoch: [168][60/196]	Time 0.013 (0.016)	Data 0.012 (0.006)	Loss 0.8374 (0.8453)	Acc@1 99.219 (99.238)	Acc@5 100.000 (100.000)
Epoch: [168][70/196]	Time 0.015 (0.015)	Data 0.007 (0.006)	Loss 0.8574 (0.8452)	Acc@1 98.828 (99.230)	Acc@5 100.000 (100.000)
Epoch: [168][80/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 0.8597 (0.8452)	Acc@1 98.438 (99.209)	Acc@5 100.000 (100.000)
Epoch: [168][90/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.8580 (0.8451)	Acc@1 98.828 (99.197)	Acc@5 100.000 (100.000)
Epoch: [168][100/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.8298 (0.8449)	Acc@1 99.219 (99.196)	Acc@5 100.000 (100.000)
Epoch: [168][110/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.8239 (0.8440)	Acc@1 100.000 (99.222)	Acc@5 100.000 (100.000)
Epoch: [168][120/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.8264 (0.8435)	Acc@1 100.000 (99.245)	Acc@5 100.000 (100.000)
Epoch: [168][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.8476 (0.8431)	Acc@1 99.219 (99.255)	Acc@5 100.000 (100.000)
Epoch: [168][140/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.8296 (0.8427)	Acc@1 99.609 (99.274)	Acc@5 100.000 (100.000)
Epoch: [168][150/196]	Time 0.015 (0.015)	Data 0.001 (0.005)	Loss 0.8347 (0.8423)	Acc@1 99.219 (99.296)	Acc@5 100.000 (100.000)
Epoch: [168][160/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.8381 (0.8422)	Acc@1 99.219 (99.289)	Acc@5 100.000 (100.000)
Epoch: [168][170/196]	Time 0.015 (0.015)	Data 0.001 (0.005)	Loss 0.8447 (0.8421)	Acc@1 98.828 (99.278)	Acc@5 100.000 (100.000)
Epoch: [168][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8526 (0.8421)	Acc@1 98.828 (99.271)	Acc@5 100.000 (100.000)
Epoch: [168][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8382 (0.8421)	Acc@1 99.609 (99.266)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.8419283478546142, 1.2926352715492249, 99.274, 69.3, tensor(0.9498, device='cuda:0', grad_fn=<DivBackward0>), 2.874342679977417, 0.40361666679382324]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [169 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [169][0/196]	Time 0.074 (0.074)	Data 0.198 (0.198)	Loss 0.8185 (0.8185)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [169][10/196]	Time 0.015 (0.022)	Data 0.002 (0.020)	Loss 0.8285 (0.8256)	Acc@1 99.609 (99.538)	Acc@5 100.000 (100.000)
Epoch: [169][20/196]	Time 0.015 (0.019)	Data 0.002 (0.012)	Loss 0.8238 (0.8273)	Acc@1 99.219 (99.479)	Acc@5 100.000 (100.000)
Epoch: [169][30/196]	Time 0.014 (0.018)	Data 0.003 (0.009)	Loss 0.8229 (0.8280)	Acc@1 100.000 (99.521)	Acc@5 100.000 (100.000)
Epoch: [169][40/196]	Time 0.013 (0.017)	Data 0.005 (0.007)	Loss 0.8275 (0.8273)	Acc@1 99.609 (99.476)	Acc@5 100.000 (100.000)
Epoch: [169][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 0.8154 (0.8271)	Acc@1 100.000 (99.479)	Acc@5 100.000 (100.000)
Epoch: [169][60/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 0.8142 (0.8274)	Acc@1 100.000 (99.468)	Acc@5 100.000 (100.000)
Epoch: [169][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.8177 (0.8265)	Acc@1 99.609 (99.499)	Acc@5 100.000 (100.000)
Epoch: [169][80/196]	Time 0.013 (0.016)	Data 0.012 (0.006)	Loss 0.8296 (0.8266)	Acc@1 99.219 (99.484)	Acc@5 100.000 (100.000)
Epoch: [169][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.8195 (0.8264)	Acc@1 99.609 (99.476)	Acc@5 100.000 (100.000)
Epoch: [169][100/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 0.8253 (0.8268)	Acc@1 99.609 (99.455)	Acc@5 100.000 (100.000)
Epoch: [169][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.8376 (0.8268)	Acc@1 98.047 (99.430)	Acc@5 100.000 (100.000)
Epoch: [169][120/196]	Time 0.013 (0.015)	Data 0.018 (0.005)	Loss 0.8254 (0.8266)	Acc@1 98.828 (99.432)	Acc@5 100.000 (99.997)
Epoch: [169][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.8236 (0.8266)	Acc@1 99.219 (99.419)	Acc@5 100.000 (99.997)
Epoch: [169][140/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.8250 (0.8268)	Acc@1 98.828 (99.402)	Acc@5 100.000 (99.997)
Epoch: [169][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.8537 (0.8268)	Acc@1 98.047 (99.387)	Acc@5 100.000 (99.997)
Epoch: [169][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.8636 (0.8271)	Acc@1 99.219 (99.369)	Acc@5 100.000 (99.998)
Epoch: [169][170/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 0.8295 (0.8269)	Acc@1 99.219 (99.367)	Acc@5 100.000 (99.998)
Epoch: [169][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8248 (0.8265)	Acc@1 98.828 (99.361)	Acc@5 100.000 (99.998)
Epoch: [169][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8190 (0.8263)	Acc@1 99.219 (99.350)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8262196483039856, 1.298180064558983, 99.346, 69.16, tensor(0.9523, device='cuda:0', grad_fn=<DivBackward0>), 2.991062879562378, 0.4221043586730957]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [170 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [170][0/196]	Time 0.073 (0.073)	Data 0.191 (0.191)	Loss 0.8244 (0.8244)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [170][10/196]	Time 0.017 (0.021)	Data 0.002 (0.019)	Loss 0.8086 (0.8113)	Acc@1 99.609 (99.574)	Acc@5 100.000 (100.000)
Epoch: [170][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 0.8227 (0.8139)	Acc@1 98.828 (99.442)	Acc@5 100.000 (100.000)
Epoch: [170][30/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 0.7972 (0.8130)	Acc@1 100.000 (99.496)	Acc@5 100.000 (100.000)
Epoch: [170][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 0.8380 (0.8145)	Acc@1 99.609 (99.447)	Acc@5 100.000 (100.000)
Epoch: [170][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.8091 (0.8139)	Acc@1 99.609 (99.456)	Acc@5 100.000 (100.000)
Epoch: [170][60/196]	Time 0.011 (0.016)	Data 0.004 (0.006)	Loss 0.8128 (0.8131)	Acc@1 100.000 (99.468)	Acc@5 100.000 (100.000)
Epoch: [170][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.8085 (0.8129)	Acc@1 100.000 (99.488)	Acc@5 100.000 (100.000)
Epoch: [170][80/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.8151 (0.8126)	Acc@1 100.000 (99.484)	Acc@5 100.000 (100.000)
Epoch: [170][90/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.8004 (0.8125)	Acc@1 100.000 (99.485)	Acc@5 100.000 (99.996)
Epoch: [170][100/196]	Time 0.013 (0.015)	Data 0.011 (0.005)	Loss 0.8009 (0.8120)	Acc@1 100.000 (99.505)	Acc@5 100.000 (99.996)
Epoch: [170][110/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 0.8098 (0.8119)	Acc@1 99.609 (99.493)	Acc@5 100.000 (99.996)
Epoch: [170][120/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.8058 (0.8115)	Acc@1 100.000 (99.500)	Acc@5 100.000 (99.997)
Epoch: [170][130/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.8053 (0.8108)	Acc@1 99.609 (99.517)	Acc@5 100.000 (99.997)
Epoch: [170][140/196]	Time 0.011 (0.014)	Data 0.006 (0.005)	Loss 0.8123 (0.8108)	Acc@1 98.438 (99.487)	Acc@5 100.000 (99.997)
Epoch: [170][150/196]	Time 0.012 (0.014)	Data 0.006 (0.005)	Loss 0.8042 (0.8105)	Acc@1 100.000 (99.490)	Acc@5 100.000 (99.997)
Epoch: [170][160/196]	Time 0.012 (0.014)	Data 0.013 (0.005)	Loss 0.8330 (0.8104)	Acc@1 98.438 (99.483)	Acc@5 100.000 (99.998)
Epoch: [170][170/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 0.8083 (0.8105)	Acc@1 99.219 (99.470)	Acc@5 100.000 (99.998)
Epoch: [170][180/196]	Time 0.013 (0.014)	Data 0.011 (0.005)	Loss 0.8173 (0.8104)	Acc@1 99.219 (99.463)	Acc@5 100.000 (99.998)
Epoch: [170][190/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.8123 (0.8100)	Acc@1 99.219 (99.468)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8099373247528077, 1.2951295101642608, 99.462, 69.39, tensor(0.9556, device='cuda:0', grad_fn=<DivBackward0>), 2.812098741531372, 0.41469144821166987]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [40, 3, 3, 3]
Before - module.bn1.weight: [40]
Before - module.bn1.bias: [40]
Before - module.conv2.weight: [127, 40, 3, 3]
Before - module.bn2.weight: [127]
Before - module.bn2.bias: [127]
Before - module.conv3.weight: [246, 127, 3, 3]
Before - module.bn3.weight: [246]
Before - module.bn3.bias: [246]
Before - module.conv4.weight: [256, 246, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [508, 256, 3, 3]
Before - module.bn5.weight: [508]
Before - module.bn5.bias: [508]
Before - module.conv6.weight: [507, 508, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [469, 507, 3, 3]
Before - module.bn7.weight: [469]
Before - module.bn7.bias: [469]
Before - module.conv8.weight: [286, 469, 3, 3]
Before - module.bn8.weight: [286]
Before - module.bn8.bias: [286]
Before - module.fc.weight: [100, 286]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [40, 3, 3, 3]
After - module.bn1.weight: [40]
After - module.bn1.bias: [40]
After - module.conv2.weight: [127, 40, 3, 3]
After - module.bn2.weight: [127]
After - module.bn2.bias: [127]
After - module.conv3.weight: [246, 127, 3, 3]
After - module.bn3.weight: [246]
After - module.bn3.bias: [246]
After - module.conv4.weight: [256, 246, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [508, 256, 3, 3]
After - module.bn5.weight: [508]
After - module.bn5.bias: [508]
After - module.conv6.weight: [507, 508, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [469, 507, 3, 3]
After - module.bn7.weight: [469]
After - module.bn7.bias: [469]
After - module.conv8.weight: [286, 469, 3, 3]
After - module.bn8.weight: [286]
After - module.bn8.bias: [286]
After - module.fc.weight: [100, 286]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [40, 3, 3, 3]
conv2 --> [127, 40, 3, 3]
conv3 --> [246, 127, 3, 3]
conv4 --> [256, 246, 3, 3]
conv5 --> [508, 256, 3, 3]
conv6 --> [507, 508, 3, 3]
conv7 --> [469, 507, 3, 3]
conv8 --> [286, 469, 3, 3]
fc --> [286, 100]
1, 442920960, 1105920, 40
2, 4892405760, 11704320, 127
3, 8205898752, 17995392, 246
4, 16541024256, 36274176, 256
5, 10187440128, 18726912, 508
6, 20175906816, 37088064, 507
7, 6574224384, 8560188, 469
8, 3708536832, 4828824, 286
fc, 10982400, 28600, 0
===================
FLOP REPORT: 27632554800000.0 49944000000.0 136312396 124860 2439 14.804412841796875
[INFO] Storing checkpoint...

Epoch: [171 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [171][0/196]	Time 0.072 (0.072)	Data 0.192 (0.192)	Loss 0.8026 (0.8026)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [171][10/196]	Time 0.018 (0.022)	Data 0.001 (0.019)	Loss 0.7949 (0.8001)	Acc@1 99.609 (99.325)	Acc@5 100.000 (100.000)
Epoch: [171][20/196]	Time 0.014 (0.018)	Data 0.003 (0.011)	Loss 0.8053 (0.7995)	Acc@1 99.219 (99.386)	Acc@5 100.000 (100.000)
Epoch: [171][30/196]	Time 0.017 (0.017)	Data 0.000 (0.009)	Loss 0.7971 (0.7996)	Acc@1 99.609 (99.420)	Acc@5 100.000 (100.000)
Epoch: [171][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 0.7936 (0.7994)	Acc@1 99.609 (99.400)	Acc@5 100.000 (100.000)
Epoch: [171][50/196]	Time 0.019 (0.016)	Data 0.000 (0.006)	Loss 0.8118 (0.7994)	Acc@1 98.828 (99.403)	Acc@5 100.000 (100.000)
Epoch: [171][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 0.7985 (0.7982)	Acc@1 99.609 (99.449)	Acc@5 100.000 (100.000)
Epoch: [171][70/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.8131 (0.7979)	Acc@1 98.828 (99.483)	Acc@5 100.000 (100.000)
Epoch: [171][80/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 0.7983 (0.7978)	Acc@1 98.828 (99.479)	Acc@5 100.000 (100.000)
Epoch: [171][90/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 0.7972 (0.7978)	Acc@1 99.609 (99.485)	Acc@5 100.000 (100.000)
Epoch: [171][100/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.8056 (0.7980)	Acc@1 98.828 (99.466)	Acc@5 100.000 (99.996)
Epoch: [171][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.7952 (0.7979)	Acc@1 98.828 (99.444)	Acc@5 100.000 (99.996)
Epoch: [171][120/196]	Time 0.024 (0.016)	Data 0.001 (0.005)	Loss 0.7862 (0.7978)	Acc@1 100.000 (99.451)	Acc@5 100.000 (99.997)
Epoch: [171][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8133 (0.7978)	Acc@1 98.438 (99.442)	Acc@5 100.000 (99.997)
Epoch: [171][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7849 (0.7971)	Acc@1 100.000 (99.460)	Acc@5 100.000 (99.997)
Epoch: [171][150/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.8027 (0.7969)	Acc@1 100.000 (99.465)	Acc@5 100.000 (99.997)
Epoch: [171][160/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.7973 (0.7966)	Acc@1 99.609 (99.459)	Acc@5 100.000 (99.998)
Epoch: [171][170/196]	Time 0.011 (0.015)	Data 0.015 (0.005)	Loss 0.7832 (0.7964)	Acc@1 100.000 (99.461)	Acc@5 100.000 (99.998)
Epoch: [171][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7904 (0.7962)	Acc@1 100.000 (99.465)	Acc@5 100.000 (99.998)
Epoch: [171][190/196]	Time 0.011 (0.015)	Data 0.018 (0.005)	Loss 0.7811 (0.7961)	Acc@1 99.609 (99.458)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.795950646057129, 1.2981736713647842, 99.456, 69.65, tensor(0.9565, device='cuda:0', grad_fn=<DivBackward0>), 3.009199380874634, 0.40950512886047363]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [172 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [172][0/196]	Time 0.073 (0.073)	Data 0.192 (0.192)	Loss 0.7905 (0.7905)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [172][10/196]	Time 0.016 (0.022)	Data 0.002 (0.019)	Loss 0.7855 (0.7896)	Acc@1 99.219 (99.396)	Acc@5 100.000 (100.000)
Epoch: [172][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 0.7823 (0.7871)	Acc@1 99.609 (99.516)	Acc@5 100.000 (100.000)
Epoch: [172][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 0.7916 (0.7865)	Acc@1 99.219 (99.534)	Acc@5 100.000 (100.000)
Epoch: [172][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 0.7792 (0.7857)	Acc@1 99.609 (99.581)	Acc@5 100.000 (100.000)
Epoch: [172][50/196]	Time 0.013 (0.016)	Data 0.003 (0.006)	Loss 0.7754 (0.7851)	Acc@1 100.000 (99.609)	Acc@5 100.000 (100.000)
Epoch: [172][60/196]	Time 0.012 (0.016)	Data 0.019 (0.006)	Loss 0.7817 (0.7853)	Acc@1 99.609 (99.603)	Acc@5 100.000 (100.000)
Epoch: [172][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.7764 (0.7856)	Acc@1 100.000 (99.582)	Acc@5 100.000 (100.000)
Epoch: [172][80/196]	Time 0.011 (0.016)	Data 0.021 (0.006)	Loss 0.7849 (0.7850)	Acc@1 99.219 (99.580)	Acc@5 100.000 (100.000)
Epoch: [172][90/196]	Time 0.026 (0.016)	Data 0.000 (0.005)	Loss 0.7819 (0.7850)	Acc@1 99.609 (99.549)	Acc@5 100.000 (100.000)
Epoch: [172][100/196]	Time 0.014 (0.016)	Data 0.009 (0.005)	Loss 0.7830 (0.7847)	Acc@1 99.609 (99.559)	Acc@5 100.000 (100.000)
Epoch: [172][110/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 0.7766 (0.7842)	Acc@1 100.000 (99.564)	Acc@5 100.000 (100.000)
Epoch: [172][120/196]	Time 0.012 (0.016)	Data 0.016 (0.005)	Loss 0.7688 (0.7836)	Acc@1 99.609 (99.567)	Acc@5 100.000 (100.000)
Epoch: [172][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7865 (0.7834)	Acc@1 99.219 (99.553)	Acc@5 100.000 (100.000)
Epoch: [172][140/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 0.7802 (0.7832)	Acc@1 99.219 (99.551)	Acc@5 100.000 (100.000)
Epoch: [172][150/196]	Time 0.015 (0.015)	Data 0.000 (0.005)	Loss 0.7974 (0.7829)	Acc@1 98.438 (99.545)	Acc@5 100.000 (100.000)
Epoch: [172][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7723 (0.7828)	Acc@1 99.609 (99.544)	Acc@5 100.000 (100.000)
Epoch: [172][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7760 (0.7827)	Acc@1 99.609 (99.552)	Acc@5 100.000 (100.000)
Epoch: [172][180/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7832 (0.7822)	Acc@1 99.609 (99.551)	Acc@5 100.000 (100.000)
Epoch: [172][190/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.7750 (0.7820)	Acc@1 100.000 (99.546)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.782161157875061, 1.2952400982379912, 99.534, 69.64, tensor(0.9573, device='cuda:0', grad_fn=<DivBackward0>), 3.0189990997314453, 0.4033920764923096]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [173 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [173][0/196]	Time 0.072 (0.072)	Data 0.199 (0.199)	Loss 0.7759 (0.7759)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [173][10/196]	Time 0.017 (0.022)	Data 0.002 (0.020)	Loss 0.7793 (0.7750)	Acc@1 99.609 (99.432)	Acc@5 100.000 (100.000)
Epoch: [173][20/196]	Time 0.017 (0.019)	Data 0.001 (0.012)	Loss 0.7899 (0.7767)	Acc@1 98.438 (99.330)	Acc@5 100.000 (100.000)
Epoch: [173][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 0.7642 (0.7745)	Acc@1 100.000 (99.420)	Acc@5 100.000 (100.000)
Epoch: [173][40/196]	Time 0.017 (0.017)	Data 0.001 (0.007)	Loss 0.7729 (0.7728)	Acc@1 99.219 (99.476)	Acc@5 100.000 (100.000)
Epoch: [173][50/196]	Time 0.012 (0.017)	Data 0.004 (0.006)	Loss 0.7652 (0.7720)	Acc@1 99.219 (99.502)	Acc@5 100.000 (100.000)
Epoch: [173][60/196]	Time 0.018 (0.017)	Data 0.000 (0.006)	Loss 0.7731 (0.7712)	Acc@1 99.609 (99.539)	Acc@5 100.000 (100.000)
Epoch: [173][70/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7686 (0.7706)	Acc@1 99.609 (99.576)	Acc@5 100.000 (100.000)
Epoch: [173][80/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7633 (0.7701)	Acc@1 99.219 (99.585)	Acc@5 100.000 (100.000)
Epoch: [173][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7677 (0.7701)	Acc@1 99.219 (99.588)	Acc@5 100.000 (100.000)
Epoch: [173][100/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 0.7662 (0.7694)	Acc@1 99.219 (99.590)	Acc@5 100.000 (99.996)
Epoch: [173][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.7621 (0.7692)	Acc@1 99.219 (99.602)	Acc@5 100.000 (99.996)
Epoch: [173][120/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.7656 (0.7690)	Acc@1 100.000 (99.606)	Acc@5 100.000 (99.997)
Epoch: [173][130/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.7570 (0.7689)	Acc@1 100.000 (99.615)	Acc@5 100.000 (99.997)
Epoch: [173][140/196]	Time 0.012 (0.016)	Data 0.011 (0.005)	Loss 0.7630 (0.7689)	Acc@1 99.609 (99.601)	Acc@5 100.000 (99.997)
Epoch: [173][150/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7665 (0.7688)	Acc@1 99.219 (99.584)	Acc@5 100.000 (99.997)
Epoch: [173][160/196]	Time 0.012 (0.015)	Data 0.022 (0.005)	Loss 0.7722 (0.7687)	Acc@1 99.219 (99.566)	Acc@5 100.000 (99.998)
Epoch: [173][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7672 (0.7686)	Acc@1 98.828 (99.555)	Acc@5 100.000 (99.998)
Epoch: [173][180/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 0.8138 (0.7684)	Acc@1 98.047 (99.558)	Acc@5 100.000 (99.998)
Epoch: [173][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7601 (0.7683)	Acc@1 99.609 (99.548)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.7682340327262879, 1.2924661087989806, 99.542, 69.86, tensor(0.9585, device='cuda:0', grad_fn=<DivBackward0>), 3.021728515625, 0.4056525230407715]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [174 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [174][0/196]	Time 0.086 (0.086)	Data 0.206 (0.206)	Loss 0.7574 (0.7574)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [174][10/196]	Time 0.015 (0.023)	Data 0.003 (0.020)	Loss 0.7715 (0.7623)	Acc@1 99.219 (99.432)	Acc@5 100.000 (100.000)
Epoch: [174][20/196]	Time 0.012 (0.019)	Data 0.006 (0.012)	Loss 0.7615 (0.7611)	Acc@1 98.828 (99.442)	Acc@5 100.000 (100.000)
Epoch: [174][30/196]	Time 0.016 (0.018)	Data 0.001 (0.009)	Loss 0.7778 (0.7608)	Acc@1 98.828 (99.458)	Acc@5 100.000 (100.000)
Epoch: [174][40/196]	Time 0.012 (0.017)	Data 0.020 (0.008)	Loss 0.7531 (0.7601)	Acc@1 100.000 (99.495)	Acc@5 100.000 (100.000)
Epoch: [174][50/196]	Time 0.016 (0.017)	Data 0.000 (0.008)	Loss 0.7537 (0.7591)	Acc@1 100.000 (99.517)	Acc@5 100.000 (100.000)
Epoch: [174][60/196]	Time 0.013 (0.017)	Data 0.018 (0.007)	Loss 0.7585 (0.7592)	Acc@1 99.219 (99.501)	Acc@5 100.000 (100.000)
Epoch: [174][70/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 0.7555 (0.7588)	Acc@1 100.000 (99.516)	Acc@5 100.000 (100.000)
Epoch: [174][80/196]	Time 0.013 (0.016)	Data 0.013 (0.006)	Loss 0.7497 (0.7587)	Acc@1 99.609 (99.513)	Acc@5 100.000 (100.000)
Epoch: [174][90/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 0.7464 (0.7585)	Acc@1 100.000 (99.485)	Acc@5 100.000 (100.000)
Epoch: [174][100/196]	Time 0.011 (0.016)	Data 0.011 (0.006)	Loss 0.7526 (0.7582)	Acc@1 99.609 (99.509)	Acc@5 100.000 (100.000)
Epoch: [174][110/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7519 (0.7577)	Acc@1 99.609 (99.514)	Acc@5 100.000 (100.000)
Epoch: [174][120/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7625 (0.7576)	Acc@1 99.609 (99.519)	Acc@5 100.000 (100.000)
Epoch: [174][130/196]	Time 0.015 (0.016)	Data 0.001 (0.006)	Loss 0.7572 (0.7571)	Acc@1 100.000 (99.526)	Acc@5 100.000 (100.000)
Epoch: [174][140/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 0.7526 (0.7572)	Acc@1 99.609 (99.515)	Acc@5 100.000 (100.000)
Epoch: [174][150/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7498 (0.7567)	Acc@1 100.000 (99.527)	Acc@5 100.000 (100.000)
Epoch: [174][160/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7501 (0.7564)	Acc@1 100.000 (99.529)	Acc@5 100.000 (100.000)
Epoch: [174][170/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.7482 (0.7559)	Acc@1 99.609 (99.539)	Acc@5 100.000 (100.000)
Epoch: [174][180/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.7499 (0.7556)	Acc@1 99.219 (99.536)	Acc@5 100.000 (100.000)
Epoch: [174][190/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7470 (0.7553)	Acc@1 99.609 (99.534)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.755279586982727, 1.2865708142518997, 99.538, 69.89, tensor(0.9587, device='cuda:0', grad_fn=<DivBackward0>), 3.0488836765289307, 0.4079315662384033]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [175 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [175][0/196]	Time 0.060 (0.060)	Data 0.198 (0.198)	Loss 0.7473 (0.7473)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [175][10/196]	Time 0.014 (0.021)	Data 0.002 (0.020)	Loss 0.7445 (0.7433)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [175][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 0.7513 (0.7442)	Acc@1 99.609 (99.628)	Acc@5 100.000 (100.000)
Epoch: [175][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 0.7480 (0.7446)	Acc@1 99.609 (99.622)	Acc@5 100.000 (100.000)
Epoch: [175][40/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 0.7406 (0.7444)	Acc@1 100.000 (99.628)	Acc@5 100.000 (100.000)
Epoch: [175][50/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 0.7394 (0.7440)	Acc@1 100.000 (99.632)	Acc@5 100.000 (100.000)
Epoch: [175][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 0.7484 (0.7434)	Acc@1 98.828 (99.629)	Acc@5 100.000 (100.000)
Epoch: [175][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.7443 (0.7435)	Acc@1 99.609 (99.620)	Acc@5 100.000 (100.000)
Epoch: [175][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.7433 (0.7429)	Acc@1 98.828 (99.629)	Acc@5 100.000 (100.000)
Epoch: [175][90/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 0.7393 (0.7434)	Acc@1 100.000 (99.614)	Acc@5 100.000 (100.000)
Epoch: [175][100/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7371 (0.7432)	Acc@1 100.000 (99.629)	Acc@5 100.000 (100.000)
Epoch: [175][110/196]	Time 0.011 (0.015)	Data 0.012 (0.005)	Loss 0.7546 (0.7429)	Acc@1 99.219 (99.630)	Acc@5 100.000 (100.000)
Epoch: [175][120/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7450 (0.7428)	Acc@1 99.219 (99.606)	Acc@5 100.000 (100.000)
Epoch: [175][130/196]	Time 0.014 (0.015)	Data 0.009 (0.005)	Loss 0.7421 (0.7424)	Acc@1 100.000 (99.615)	Acc@5 100.000 (100.000)
Epoch: [175][140/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 0.7553 (0.7424)	Acc@1 99.219 (99.596)	Acc@5 100.000 (100.000)
Epoch: [175][150/196]	Time 0.014 (0.015)	Data 0.010 (0.005)	Loss 0.7377 (0.7422)	Acc@1 99.609 (99.596)	Acc@5 100.000 (100.000)
Epoch: [175][160/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 0.7367 (0.7418)	Acc@1 99.609 (99.600)	Acc@5 100.000 (100.000)
Epoch: [175][170/196]	Time 0.014 (0.015)	Data 0.010 (0.005)	Loss 0.7301 (0.7416)	Acc@1 99.609 (99.593)	Acc@5 100.000 (100.000)
Epoch: [175][180/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 0.7359 (0.7413)	Acc@1 99.609 (99.590)	Acc@5 100.000 (100.000)
Epoch: [175][190/196]	Time 0.012 (0.015)	Data 0.003 (0.005)	Loss 0.7472 (0.7413)	Acc@1 99.219 (99.583)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7414241097640991, 1.2877725672721863, 99.566, 69.51, tensor(0.9602, device='cuda:0', grad_fn=<DivBackward0>), 2.9006917476654053, 0.40386438369750977]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [176 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [176][0/196]	Time 0.068 (0.068)	Data 0.213 (0.213)	Loss 0.7428 (0.7428)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [176][10/196]	Time 0.016 (0.021)	Data 0.003 (0.022)	Loss 0.7274 (0.7332)	Acc@1 100.000 (99.680)	Acc@5 100.000 (100.000)
Epoch: [176][20/196]	Time 0.015 (0.018)	Data 0.003 (0.012)	Loss 0.7420 (0.7342)	Acc@1 99.609 (99.665)	Acc@5 100.000 (100.000)
Epoch: [176][30/196]	Time 0.013 (0.017)	Data 0.003 (0.009)	Loss 0.7376 (0.7331)	Acc@1 98.828 (99.685)	Acc@5 100.000 (100.000)
Epoch: [176][40/196]	Time 0.021 (0.017)	Data 0.002 (0.008)	Loss 0.7429 (0.7341)	Acc@1 99.219 (99.628)	Acc@5 100.000 (100.000)
Epoch: [176][50/196]	Time 0.016 (0.017)	Data 0.004 (0.007)	Loss 0.7299 (0.7332)	Acc@1 99.609 (99.640)	Acc@5 100.000 (100.000)
Epoch: [176][60/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.7260 (0.7323)	Acc@1 99.609 (99.654)	Acc@5 100.000 (100.000)
Epoch: [176][70/196]	Time 0.012 (0.016)	Data 0.019 (0.006)	Loss 0.7295 (0.7318)	Acc@1 100.000 (99.664)	Acc@5 100.000 (100.000)
Epoch: [176][80/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.7269 (0.7319)	Acc@1 100.000 (99.658)	Acc@5 100.000 (100.000)
Epoch: [176][90/196]	Time 0.012 (0.016)	Data 0.027 (0.006)	Loss 0.7249 (0.7316)	Acc@1 100.000 (99.652)	Acc@5 100.000 (100.000)
Epoch: [176][100/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.7300 (0.7313)	Acc@1 99.609 (99.640)	Acc@5 100.000 (100.000)
Epoch: [176][110/196]	Time 0.013 (0.016)	Data 0.021 (0.006)	Loss 0.7312 (0.7307)	Acc@1 99.219 (99.634)	Acc@5 100.000 (100.000)
Epoch: [176][120/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.7197 (0.7302)	Acc@1 100.000 (99.648)	Acc@5 100.000 (100.000)
Epoch: [176][130/196]	Time 0.012 (0.016)	Data 0.021 (0.006)	Loss 0.7176 (0.7301)	Acc@1 100.000 (99.654)	Acc@5 100.000 (100.000)
Epoch: [176][140/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7370 (0.7300)	Acc@1 98.828 (99.643)	Acc@5 100.000 (100.000)
Epoch: [176][150/196]	Time 0.013 (0.016)	Data 0.009 (0.005)	Loss 0.7189 (0.7297)	Acc@1 100.000 (99.630)	Acc@5 100.000 (100.000)
Epoch: [176][160/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7320 (0.7295)	Acc@1 99.219 (99.609)	Acc@5 100.000 (100.000)
Epoch: [176][170/196]	Time 0.014 (0.016)	Data 0.006 (0.005)	Loss 0.7221 (0.7292)	Acc@1 99.609 (99.607)	Acc@5 100.000 (100.000)
Epoch: [176][180/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.7228 (0.7290)	Acc@1 100.000 (99.605)	Acc@5 100.000 (100.000)
Epoch: [176][190/196]	Time 0.012 (0.016)	Data 0.012 (0.005)	Loss 0.7251 (0.7287)	Acc@1 99.609 (99.599)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7286344522094726, 1.2901290792226792, 99.6, 69.6, tensor(0.9606, device='cuda:0', grad_fn=<DivBackward0>), 3.0570130348205566, 0.41058778762817383]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [177 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [177][0/196]	Time 0.072 (0.072)	Data 0.195 (0.195)	Loss 0.7208 (0.7208)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [177][10/196]	Time 0.016 (0.021)	Data 0.002 (0.020)	Loss 0.7394 (0.7286)	Acc@1 99.219 (99.467)	Acc@5 100.000 (100.000)
Epoch: [177][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 0.7115 (0.7238)	Acc@1 100.000 (99.591)	Acc@5 100.000 (100.000)
Epoch: [177][30/196]	Time 0.022 (0.018)	Data 0.003 (0.009)	Loss 0.7219 (0.7220)	Acc@1 99.609 (99.635)	Acc@5 100.000 (100.000)
Epoch: [177][40/196]	Time 0.022 (0.017)	Data 0.002 (0.007)	Loss 0.7199 (0.7205)	Acc@1 99.609 (99.647)	Acc@5 100.000 (100.000)
Epoch: [177][50/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 0.7220 (0.7198)	Acc@1 99.219 (99.648)	Acc@5 100.000 (100.000)
Epoch: [177][60/196]	Time 0.011 (0.016)	Data 0.006 (0.006)	Loss 0.7184 (0.7191)	Acc@1 99.609 (99.654)	Acc@5 100.000 (100.000)
Epoch: [177][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7195 (0.7191)	Acc@1 99.219 (99.648)	Acc@5 100.000 (100.000)
Epoch: [177][80/196]	Time 0.014 (0.016)	Data 0.012 (0.005)	Loss 0.7148 (0.7185)	Acc@1 99.609 (99.658)	Acc@5 100.000 (100.000)
Epoch: [177][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.7037 (0.7179)	Acc@1 100.000 (99.669)	Acc@5 100.000 (100.000)
Epoch: [177][100/196]	Time 0.013 (0.016)	Data 0.010 (0.005)	Loss 0.7169 (0.7179)	Acc@1 99.219 (99.652)	Acc@5 100.000 (100.000)
Epoch: [177][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7085 (0.7179)	Acc@1 100.000 (99.634)	Acc@5 100.000 (100.000)
Epoch: [177][120/196]	Time 0.014 (0.016)	Data 0.007 (0.005)	Loss 0.7214 (0.7178)	Acc@1 99.609 (99.629)	Acc@5 100.000 (100.000)
Epoch: [177][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 0.7129 (0.7177)	Acc@1 99.609 (99.627)	Acc@5 100.000 (100.000)
Epoch: [177][140/196]	Time 0.012 (0.016)	Data 0.012 (0.004)	Loss 0.7185 (0.7174)	Acc@1 99.219 (99.637)	Acc@5 100.000 (100.000)
Epoch: [177][150/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 0.7098 (0.7171)	Acc@1 99.609 (99.638)	Acc@5 100.000 (100.000)
Epoch: [177][160/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 0.7062 (0.7166)	Acc@1 99.609 (99.638)	Acc@5 100.000 (100.000)
Epoch: [177][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 0.7121 (0.7165)	Acc@1 100.000 (99.625)	Acc@5 100.000 (100.000)
Epoch: [177][180/196]	Time 0.012 (0.016)	Data 0.010 (0.004)	Loss 0.7130 (0.7162)	Acc@1 99.609 (99.633)	Acc@5 100.000 (100.000)
Epoch: [177][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.7074 (0.7158)	Acc@1 99.609 (99.632)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7156895524215698, 1.2875668978691102, 99.634, 69.97, tensor(0.9613, device='cuda:0', grad_fn=<DivBackward0>), 3.0537898540496826, 0.40775108337402344]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [178 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [178][0/196]	Time 0.076 (0.076)	Data 0.197 (0.197)	Loss 0.7038 (0.7038)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [178][10/196]	Time 0.015 (0.021)	Data 0.003 (0.020)	Loss 0.7020 (0.7064)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [178][20/196]	Time 0.013 (0.019)	Data 0.004 (0.012)	Loss 0.7115 (0.7074)	Acc@1 99.609 (99.554)	Acc@5 100.000 (100.000)
Epoch: [178][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 0.6998 (0.7062)	Acc@1 99.609 (99.622)	Acc@5 100.000 (100.000)
Epoch: [178][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 0.6951 (0.7053)	Acc@1 100.000 (99.619)	Acc@5 100.000 (100.000)
Epoch: [178][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 0.7064 (0.7049)	Acc@1 99.609 (99.648)	Acc@5 100.000 (100.000)
Epoch: [178][60/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 0.7049 (0.7047)	Acc@1 100.000 (99.661)	Acc@5 100.000 (100.000)
Epoch: [178][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.7004 (0.7047)	Acc@1 99.609 (99.670)	Acc@5 100.000 (100.000)
Epoch: [178][80/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7058 (0.7048)	Acc@1 100.000 (99.672)	Acc@5 100.000 (100.000)
Epoch: [178][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.7039 (0.7044)	Acc@1 99.609 (99.687)	Acc@5 100.000 (100.000)
Epoch: [178][100/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.6996 (0.7043)	Acc@1 100.000 (99.679)	Acc@5 100.000 (100.000)
Epoch: [178][110/196]	Time 0.013 (0.016)	Data 0.010 (0.005)	Loss 0.7068 (0.7040)	Acc@1 99.219 (99.690)	Acc@5 100.000 (100.000)
Epoch: [178][120/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 0.7003 (0.7037)	Acc@1 99.609 (99.697)	Acc@5 100.000 (100.000)
Epoch: [178][130/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 0.7058 (0.7034)	Acc@1 99.609 (99.696)	Acc@5 100.000 (100.000)
Epoch: [178][140/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.6946 (0.7031)	Acc@1 100.000 (99.704)	Acc@5 100.000 (100.000)
Epoch: [178][150/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.6895 (0.7026)	Acc@1 100.000 (99.708)	Acc@5 100.000 (100.000)
Epoch: [178][160/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7099 (0.7023)	Acc@1 98.828 (99.716)	Acc@5 100.000 (100.000)
Epoch: [178][170/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 0.6969 (0.7021)	Acc@1 99.609 (99.714)	Acc@5 100.000 (100.000)
Epoch: [178][180/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7012 (0.7019)	Acc@1 99.609 (99.713)	Acc@5 100.000 (100.000)
Epoch: [178][190/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 0.7071 (0.7018)	Acc@1 99.609 (99.710)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7017374988555908, 1.2894811069965362, 99.704, 69.81, tensor(0.9635, device='cuda:0', grad_fn=<DivBackward0>), 2.931368350982666, 0.41123318672180176]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [179 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [179][0/196]	Time 0.069 (0.069)	Data 0.193 (0.193)	Loss 0.6968 (0.6968)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [179][10/196]	Time 0.015 (0.021)	Data 0.003 (0.020)	Loss 0.6998 (0.6960)	Acc@1 99.609 (99.716)	Acc@5 100.000 (100.000)
Epoch: [179][20/196]	Time 0.016 (0.018)	Data 0.002 (0.011)	Loss 0.7051 (0.6984)	Acc@1 99.219 (99.665)	Acc@5 100.000 (99.981)
Epoch: [179][30/196]	Time 0.014 (0.017)	Data 0.002 (0.009)	Loss 0.6873 (0.6966)	Acc@1 99.609 (99.672)	Acc@5 100.000 (99.987)
Epoch: [179][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 0.6877 (0.6964)	Acc@1 100.000 (99.647)	Acc@5 100.000 (99.990)
Epoch: [179][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 0.6956 (0.6956)	Acc@1 99.609 (99.663)	Acc@5 100.000 (99.992)
Epoch: [179][60/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 0.6854 (0.6948)	Acc@1 100.000 (99.686)	Acc@5 100.000 (99.994)
Epoch: [179][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.6872 (0.6953)	Acc@1 100.000 (99.659)	Acc@5 100.000 (99.994)
Epoch: [179][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.6884 (0.6950)	Acc@1 99.609 (99.662)	Acc@5 100.000 (99.995)
Epoch: [179][90/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.6873 (0.6945)	Acc@1 99.219 (99.665)	Acc@5 100.000 (99.996)
Epoch: [179][100/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6907 (0.6940)	Acc@1 99.609 (99.679)	Acc@5 100.000 (99.996)
Epoch: [179][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6924 (0.6939)	Acc@1 99.609 (99.662)	Acc@5 100.000 (99.996)
Epoch: [179][120/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7029 (0.6940)	Acc@1 99.219 (99.648)	Acc@5 100.000 (99.997)
Epoch: [179][130/196]	Time 0.015 (0.015)	Data 0.004 (0.005)	Loss 0.6931 (0.6940)	Acc@1 99.609 (99.648)	Acc@5 100.000 (99.997)
Epoch: [179][140/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.6973 (0.6933)	Acc@1 99.219 (99.656)	Acc@5 100.000 (99.997)
Epoch: [179][150/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 0.6807 (0.6930)	Acc@1 99.609 (99.653)	Acc@5 100.000 (99.997)
Epoch: [179][160/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.6783 (0.6927)	Acc@1 99.609 (99.648)	Acc@5 100.000 (99.998)
Epoch: [179][170/196]	Time 0.015 (0.015)	Data 0.000 (0.005)	Loss 0.7033 (0.6924)	Acc@1 98.828 (99.639)	Acc@5 100.000 (99.998)
Epoch: [179][180/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7121 (0.6923)	Acc@1 99.219 (99.648)	Acc@5 100.000 (99.998)
Epoch: [179][190/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.6771 (0.6919)	Acc@1 100.000 (99.652)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.6916438332366943, 1.294634594321251, 99.658, 69.8, tensor(0.9609, device='cuda:0', grad_fn=<DivBackward0>), 2.9380414485931396, 0.4072024822235108]
Non Pruning Epoch - module.conv1.weight: [40, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [40]
Non Pruning Epoch - module.bn1.bias: [40]
Non Pruning Epoch - module.conv2.weight: [127, 40, 3, 3]
Non Pruning Epoch - module.bn2.weight: [127]
Non Pruning Epoch - module.bn2.bias: [127]
Non Pruning Epoch - module.conv3.weight: [246, 127, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [508, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [508]
Non Pruning Epoch - module.bn5.bias: [508]
Non Pruning Epoch - module.conv6.weight: [507, 508, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [469, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [469]
Non Pruning Epoch - module.bn7.bias: [469]
Non Pruning Epoch - module.conv8.weight: [286, 469, 3, 3]
Non Pruning Epoch - module.bn8.weight: [286]
Non Pruning Epoch - module.bn8.bias: [286]
Non Pruning Epoch - module.fc.weight: [100, 286]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [180 | 180] LR: 0.010000
module.conv1.weight [40, 3, 3, 3]
module.conv2.weight [127, 40, 3, 3]
module.conv3.weight [246, 127, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [508, 256, 3, 3]
module.conv6.weight [507, 508, 3, 3]
module.conv7.weight [469, 507, 3, 3]
module.conv8.weight [286, 469, 3, 3]
Epoch: [180][0/196]	Time 0.071 (0.071)	Data 0.211 (0.211)	Loss 0.6922 (0.6922)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [180][10/196]	Time 0.015 (0.022)	Data 0.002 (0.021)	Loss 0.6791 (0.6816)	Acc@1 99.609 (99.680)	Acc@5 100.000 (100.000)
Epoch: [180][20/196]	Time 0.014 (0.019)	Data 0.002 (0.012)	Loss 0.6821 (0.6815)	Acc@1 100.000 (99.702)	Acc@5 100.000 (100.000)
Epoch: [180][30/196]	Time 0.014 (0.018)	Data 0.002 (0.009)	Loss 0.6802 (0.6812)	Acc@1 99.609 (99.748)	Acc@5 100.000 (100.000)
Epoch: [180][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 0.6764 (0.6814)	Acc@1 100.000 (99.762)	Acc@5 100.000 (100.000)
Epoch: [180][50/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 0.6727 (0.6810)	Acc@1 100.000 (99.755)	Acc@5 100.000 (100.000)
Epoch: [180][60/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 0.6749 (0.6810)	Acc@1 99.609 (99.718)	Acc@5 100.000 (100.000)
Epoch: [180][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 0.6760 (0.6805)	Acc@1 100.000 (99.736)	Acc@5 100.000 (100.000)
Epoch: [180][80/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 0.6765 (0.6799)	Acc@1 99.609 (99.730)	Acc@5 100.000 (100.000)
Epoch: [180][90/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 0.6750 (0.6797)	Acc@1 99.609 (99.730)	Acc@5 100.000 (100.000)
Epoch: [180][100/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.6739 (0.6791)	Acc@1 100.000 (99.729)	Acc@5 100.000 (100.000)
Epoch: [180][110/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 0.6675 (0.6787)	Acc@1 100.000 (99.740)	Acc@5 100.000 (100.000)
Epoch: [180][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.6814 (0.6784)	Acc@1 99.609 (99.748)	Acc@5 100.000 (100.000)
Epoch: [180][130/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 0.6723 (0.6783)	Acc@1 100.000 (99.741)	Acc@5 100.000 (100.000)
Epoch: [180][140/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.6764 (0.6783)	Acc@1 99.609 (99.734)	Acc@5 100.000 (100.000)
Epoch: [180][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.6720 (0.6782)	Acc@1 100.000 (99.731)	Acc@5 100.000 (100.000)
Epoch: [180][160/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.6710 (0.6781)	Acc@1 99.609 (99.709)	Acc@5 100.000 (100.000)
Epoch: [180][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6833 (0.6780)	Acc@1 99.219 (99.712)	Acc@5 100.000 (100.000)
Epoch: [180][180/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6727 (0.6779)	Acc@1 99.609 (99.711)	Acc@5 100.000 (100.000)
Epoch: [180][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6703 (0.6777)	Acc@1 99.219 (99.708)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.6777208646011352, 1.293142956495285, 99.704, 69.57, tensor(0.9635, device='cuda:0', grad_fn=<DivBackward0>), 3.0176990032196045, 0.4082012176513672]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [40, 3, 3, 3]
Before - module.bn1.weight: [40]
Before - module.bn1.bias: [40]
Before - module.conv2.weight: [127, 40, 3, 3]
Before - module.bn2.weight: [127]
Before - module.bn2.bias: [127]
Before - module.conv3.weight: [246, 127, 3, 3]
Before - module.bn3.weight: [246]
Before - module.bn3.bias: [246]
Before - module.conv4.weight: [256, 246, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [508, 256, 3, 3]
Before - module.bn5.weight: [508]
Before - module.bn5.bias: [508]
Before - module.conv6.weight: [507, 508, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [469, 507, 3, 3]
Before - module.bn7.weight: [469]
Before - module.bn7.bias: [469]
Before - module.conv8.weight: [286, 469, 3, 3]
Before - module.bn8.weight: [286]
Before - module.bn8.bias: [286]
Before - module.fc.weight: [100, 286]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv3.weight]: [246, 127, 3, 3] >> [245, 127, 3, 3]
[module.bn3.weight]: 246 >> 245
running_mean [245]
running_var [245]
num_batches_tracked []
[module.conv4.weight]: [256, 246, 3, 3] >> [256, 245, 3, 3]
[module.conv8.weight]: [286, 469, 3, 3] >> [285, 469, 3, 3]
[module.bn8.weight]: 286 >> 285
running_mean [285]
running_var [285]
num_batches_tracked []
[module.fc.weight]: [100, 286] >> [100, 285]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [40, 3, 3, 3]
After - module.bn1.weight: [40]
After - module.bn1.bias: [40]
After - module.conv2.weight: [127, 40, 3, 3]
After - module.bn2.weight: [127]
After - module.bn2.bias: [127]
After - module.conv3.weight: [245, 127, 3, 3]
After - module.bn3.weight: [245]
After - module.bn3.bias: [245]
After - module.conv4.weight: [256, 245, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [508, 256, 3, 3]
After - module.bn5.weight: [508]
After - module.bn5.bias: [508]
After - module.conv6.weight: [507, 508, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [469, 507, 3, 3]
After - module.bn7.weight: [469]
After - module.bn7.bias: [469]
After - module.conv8.weight: [285, 469, 3, 3]
After - module.bn8.weight: [285]
After - module.bn8.bias: [285]
After - module.fc.weight: [100, 285]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [40, 3, 3, 3]
conv2 --> [127, 40, 3, 3]
conv3 --> [245, 127, 3, 3]
conv4 --> [256, 245, 3, 3]
conv5 --> [508, 256, 3, 3]
conv6 --> [507, 508, 3, 3]
conv7 --> [469, 507, 3, 3]
conv8 --> [285, 469, 3, 3]
fc --> [285, 100]
1, 442920960, 1105920, 40
2, 4892405760, 11704320, 127
3, 8172541440, 17922240, 245
4, 16473784320, 36126720, 256
5, 10187440128, 18726912, 508
6, 20175906816, 37088064, 507
7, 6574224384, 8560188, 469
8, 3695569920, 4811940, 285
fc, 10944000, 28500, 0
===================
FLOP REPORT: 27588178800000.0 49916800000.0 136074804 124792 2437 14.789590835571289
[INFO] Storing checkpoint...
Best acc:
69.97

Total time:
1355.103498
[2021-06-21T01:56:07.499448] Command finished with return code 0


[2021-06-21T01:56:07.499715] The experiment completed successfully. Finalizing run...
Cleaning up all outstanding Run operations, waiting 900.0 seconds
1 items cleaning up...
Cleanup took 0.08198380470275879 seconds
[2021-06-21T01:56:07.732874] Finished context manager injector.
2021/06/21 01:56:08 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status
2021/06/21 01:56:08 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 2
FilteredData: 0.
2021/06/21 01:56:08 Process Exiting with Code:  0
2021/06/21 01:56:09 All App Insights Logs was send successfully
