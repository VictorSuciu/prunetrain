bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 03:13:54 Starting App Insight Logger for task:  runTaskLet
2021/06/20 03:13:54 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612
2021/06/20 03:13:54 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info
bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 03:13:54 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status
[2021-06-20T03:13:54.823627] Entering context manager injector.
[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 200 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15'])
Script type = COMMAND
[2021-06-20T03:13:55.426859] Command=python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 200 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
[2021-06-20T03:13:55.427184] Entering Run History Context Manager.
[2021-06-20T03:13:56.902026] Command Working Directory=/mnt/batch/tasks/shared/LS_root/jobs/prunetrain/azureml/tensor/wd/azureml/Tensor
[2021-06-20T03:13:56.902351] Starting Linux command : python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 200 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
2021/06/20 03:13:59 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 1
FilteredData: 0.
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/data/torch/cifar-100-python.tar.gz
0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%Extracting ./dataset/data/torch/cifar-100-python.tar.gz to ./dataset/data/torch
==> creating model 'vgg11_bn_flat'
    Total params: 9.27M
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375

Epoch: [1 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
cifar.py:337: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [1][0/196]	Time 1.107 (1.107)	Data 0.206 (0.206)	Loss 6.0768 (6.0768)	Acc@1 0.781 (0.781)	Acc@5 2.734 (2.734)
Epoch: [1][10/196]	Time 0.017 (0.115)	Data 0.002 (0.021)	Loss 6.6075 (6.3977)	Acc@1 4.688 (1.918)	Acc@5 15.625 (9.908)
Epoch: [1][20/196]	Time 0.016 (0.068)	Data 0.002 (0.012)	Loss 7.2669 (6.7543)	Acc@1 0.781 (2.028)	Acc@5 8.594 (9.375)
Epoch: [1][30/196]	Time 0.017 (0.051)	Data 0.002 (0.009)	Loss 6.1235 (6.6436)	Acc@1 1.172 (1.978)	Acc@5 9.375 (9.098)
Epoch: [1][40/196]	Time 0.015 (0.042)	Data 0.003 (0.007)	Loss 5.9955 (6.5009)	Acc@1 0.000 (1.820)	Acc@5 6.641 (8.794)
Epoch: [1][50/196]	Time 0.015 (0.037)	Data 0.003 (0.006)	Loss 5.8560 (6.3866)	Acc@1 2.344 (1.892)	Acc@5 8.594 (8.747)
Epoch: [1][60/196]	Time 0.015 (0.034)	Data 0.002 (0.006)	Loss 5.7451 (6.2894)	Acc@1 1.562 (1.960)	Acc@5 10.156 (8.933)
Epoch: [1][70/196]	Time 0.027 (0.031)	Data 0.001 (0.005)	Loss 5.7046 (6.2026)	Acc@1 2.734 (2.036)	Acc@5 9.375 (9.215)
Epoch: [1][80/196]	Time 0.015 (0.029)	Data 0.003 (0.005)	Loss 5.6264 (6.1339)	Acc@1 2.734 (2.160)	Acc@5 13.281 (9.563)
Epoch: [1][90/196]	Time 0.018 (0.028)	Data 0.000 (0.005)	Loss 5.4877 (6.0650)	Acc@1 3.125 (2.412)	Acc@5 14.844 (10.294)
Epoch: [1][100/196]	Time 0.018 (0.027)	Data 0.001 (0.004)	Loss 5.2750 (5.9978)	Acc@1 8.203 (2.792)	Acc@5 23.828 (11.266)
Epoch: [1][110/196]	Time 0.017 (0.026)	Data 0.000 (0.004)	Loss 5.4624 (5.9392)	Acc@1 3.906 (2.995)	Acc@5 18.359 (12.088)
Epoch: [1][120/196]	Time 0.019 (0.025)	Data 0.001 (0.004)	Loss 5.3411 (5.8861)	Acc@1 3.906 (3.270)	Acc@5 18.750 (12.910)
Epoch: [1][130/196]	Time 0.018 (0.024)	Data 0.001 (0.004)	Loss 5.2602 (5.8403)	Acc@1 5.469 (3.516)	Acc@5 21.875 (13.630)
Epoch: [1][140/196]	Time 0.017 (0.023)	Data 0.001 (0.004)	Loss 5.1420 (5.7939)	Acc@1 6.641 (3.732)	Acc@5 29.297 (14.464)
Epoch: [1][150/196]	Time 0.017 (0.023)	Data 0.000 (0.004)	Loss 5.1586 (5.7552)	Acc@1 6.641 (3.893)	Acc@5 28.125 (15.133)
Epoch: [1][160/196]	Time 0.016 (0.023)	Data 0.001 (0.004)	Loss 5.0252 (5.7116)	Acc@1 10.156 (4.149)	Acc@5 30.859 (15.991)
Epoch: [1][170/196]	Time 0.017 (0.022)	Data 0.001 (0.004)	Loss 5.0638 (5.6736)	Acc@1 7.031 (4.388)	Acc@5 27.344 (16.660)
Epoch: [1][180/196]	Time 0.017 (0.022)	Data 0.001 (0.004)	Loss 4.9522 (5.6403)	Acc@1 10.547 (4.575)	Acc@5 33.203 (17.216)
Epoch: [1][190/196]	Time 0.017 (0.021)	Data 0.000 (0.004)	Loss 5.0352 (5.6082)	Acc@1 7.422 (4.767)	Acc@5 26.953 (17.783)
cifar.py:424: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
num momentum params: 26
[0.1, 5.592877301025391, 3.917550091743469, 4.88, 9.16, tensor(0.2124, device='cuda:0', grad_fn=<DivBackward0>), 4.401887655258179, 0.49914312362670904]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [2 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [2][0/196]	Time 0.022 (0.022)	Data 0.147 (0.147)	Loss 5.0739 (5.0739)	Acc@1 7.812 (7.812)	Acc@5 28.906 (28.906)
Epoch: [2][10/196]	Time 0.017 (0.016)	Data 0.000 (0.016)	Loss 4.9710 (4.9476)	Acc@1 8.984 (8.629)	Acc@5 31.250 (30.362)
Epoch: [2][20/196]	Time 0.012 (0.015)	Data 0.005 (0.009)	Loss 4.8653 (4.9308)	Acc@1 10.938 (9.152)	Acc@5 32.422 (30.636)
Epoch: [2][30/196]	Time 0.013 (0.015)	Data 0.004 (0.007)	Loss 4.9600 (4.9200)	Acc@1 7.812 (9.236)	Acc@5 28.906 (30.708)
Epoch: [2][40/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 4.7357 (4.8981)	Acc@1 10.156 (9.585)	Acc@5 35.938 (31.469)
Epoch: [2][50/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 4.8271 (4.8824)	Acc@1 8.984 (9.835)	Acc@5 33.984 (31.740)
Epoch: [2][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 4.8824 (4.8659)	Acc@1 8.594 (10.079)	Acc@5 32.812 (32.134)
Epoch: [2][70/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 4.7426 (4.8517)	Acc@1 11.719 (10.145)	Acc@5 36.719 (32.554)
Epoch: [2][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 4.6707 (4.8372)	Acc@1 14.453 (10.291)	Acc@5 39.453 (32.870)
Epoch: [2][90/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 4.7685 (4.8257)	Acc@1 11.328 (10.538)	Acc@5 36.328 (33.207)
Epoch: [2][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 4.6239 (4.8131)	Acc@1 12.109 (10.705)	Acc@5 34.375 (33.478)
Epoch: [2][110/196]	Time 0.012 (0.015)	Data 0.014 (0.004)	Loss 4.6709 (4.8008)	Acc@1 12.500 (10.818)	Acc@5 37.891 (33.717)
Epoch: [2][120/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 4.5679 (4.7876)	Acc@1 13.672 (11.009)	Acc@5 42.188 (34.023)
Epoch: [2][130/196]	Time 0.011 (0.015)	Data 0.015 (0.004)	Loss 4.6097 (4.7728)	Acc@1 14.062 (11.215)	Acc@5 39.453 (34.318)
Epoch: [2][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 4.5486 (4.7563)	Acc@1 15.234 (11.467)	Acc@5 42.578 (34.727)
Epoch: [2][150/196]	Time 0.011 (0.015)	Data 0.015 (0.004)	Loss 4.5405 (4.7436)	Acc@1 19.922 (11.589)	Acc@5 40.234 (34.991)
Epoch: [2][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 4.5376 (4.7311)	Acc@1 9.766 (11.724)	Acc@5 41.406 (35.333)
Epoch: [2][170/196]	Time 0.011 (0.015)	Data 0.015 (0.004)	Loss 4.3889 (4.7177)	Acc@1 18.750 (11.888)	Acc@5 43.359 (35.657)
Epoch: [2][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 4.3838 (4.7002)	Acc@1 17.188 (12.124)	Acc@5 46.875 (36.050)
Epoch: [2][190/196]	Time 0.011 (0.015)	Data 0.013 (0.004)	Loss 4.3854 (4.6837)	Acc@1 18.359 (12.314)	Acc@5 42.969 (36.498)
num momentum params: 26
[0.1, 4.674600709228516, 3.4668705916404723, 12.418, 15.54, tensor(0.2208, device='cuda:0', grad_fn=<DivBackward0>), 2.946437120437622, 0.372575044631958]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [3 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [3][0/196]	Time 0.022 (0.022)	Data 0.153 (0.153)	Loss 4.3347 (4.3347)	Acc@1 15.234 (15.234)	Acc@5 43.750 (43.750)
Epoch: [3][10/196]	Time 0.013 (0.015)	Data 0.004 (0.016)	Loss 4.1156 (4.3991)	Acc@1 18.359 (15.057)	Acc@5 52.734 (43.572)
Epoch: [3][20/196]	Time 0.011 (0.015)	Data 0.007 (0.010)	Loss 4.1780 (4.3099)	Acc@1 19.531 (16.871)	Acc@5 50.000 (45.517)
Epoch: [3][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 4.2154 (4.2987)	Acc@1 16.797 (17.389)	Acc@5 50.000 (45.854)
Epoch: [3][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 4.2842 (4.2974)	Acc@1 16.797 (17.302)	Acc@5 43.750 (45.665)
Epoch: [3][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 4.3375 (4.2855)	Acc@1 20.703 (17.563)	Acc@5 43.750 (45.964)
Epoch: [3][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 4.2200 (4.2712)	Acc@1 17.578 (17.655)	Acc@5 46.484 (46.318)
Epoch: [3][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 4.2260 (4.2591)	Acc@1 15.625 (17.864)	Acc@5 48.047 (46.803)
Epoch: [3][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 4.0679 (4.2399)	Acc@1 24.219 (18.109)	Acc@5 50.781 (47.377)
Epoch: [3][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 4.0632 (4.2193)	Acc@1 19.141 (18.394)	Acc@5 53.906 (47.789)
Epoch: [3][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 3.9275 (4.2144)	Acc@1 22.656 (18.429)	Acc@5 56.250 (47.826)
Epoch: [3][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 4.2210 (4.2011)	Acc@1 17.188 (18.651)	Acc@5 47.656 (48.086)
Epoch: [3][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 4.1013 (4.1941)	Acc@1 22.266 (18.792)	Acc@5 48.828 (48.234)
Epoch: [3][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 4.1040 (4.1855)	Acc@1 20.703 (18.947)	Acc@5 44.922 (48.312)
Epoch: [3][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 3.9458 (4.1698)	Acc@1 24.219 (19.199)	Acc@5 52.344 (48.648)
Epoch: [3][150/196]	Time 0.020 (0.015)	Data 0.001 (0.004)	Loss 3.9452 (4.1544)	Acc@1 18.359 (19.394)	Acc@5 53.125 (48.952)
Epoch: [3][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 3.9094 (4.1395)	Acc@1 25.781 (19.667)	Acc@5 53.516 (49.275)
Epoch: [3][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.8349 (4.1281)	Acc@1 25.391 (19.858)	Acc@5 55.469 (49.479)
Epoch: [3][180/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 3.8596 (4.1119)	Acc@1 25.781 (20.086)	Acc@5 55.078 (49.836)
Epoch: [3][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.7746 (4.0980)	Acc@1 25.391 (20.308)	Acc@5 59.766 (50.145)
num momentum params: 26
[0.1, 4.091514786834717, 3.1440924167633058, 20.408, 21.85, tensor(0.2223, device='cuda:0', grad_fn=<DivBackward0>), 2.9525904655456543, 0.3669722080230713]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [4 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [4][0/196]	Time 0.022 (0.022)	Data 0.163 (0.163)	Loss 3.8305 (3.8305)	Acc@1 25.391 (25.391)	Acc@5 54.688 (54.688)
Epoch: [4][10/196]	Time 0.017 (0.016)	Data 0.000 (0.017)	Loss 3.8591 (3.7837)	Acc@1 22.656 (24.964)	Acc@5 55.859 (57.280)
Epoch: [4][20/196]	Time 0.012 (0.016)	Data 0.005 (0.010)	Loss 3.7352 (3.7862)	Acc@1 25.000 (25.019)	Acc@5 57.422 (57.292)
Epoch: [4][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 3.7988 (3.7981)	Acc@1 22.656 (24.635)	Acc@5 56.641 (56.956)
Epoch: [4][40/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 3.7602 (3.7785)	Acc@1 26.562 (25.057)	Acc@5 56.641 (57.260)
Epoch: [4][50/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 3.8741 (3.7793)	Acc@1 25.391 (25.337)	Acc@5 53.125 (57.246)
Epoch: [4][60/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 3.5819 (3.7710)	Acc@1 30.859 (25.448)	Acc@5 64.062 (57.236)
Epoch: [4][70/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 3.6542 (3.7561)	Acc@1 24.219 (25.666)	Acc@5 63.672 (57.658)
Epoch: [4][80/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 3.7226 (3.7445)	Acc@1 24.219 (25.801)	Acc@5 56.250 (57.885)
Epoch: [4][90/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 3.7329 (3.7367)	Acc@1 24.219 (25.897)	Acc@5 54.297 (57.941)
Epoch: [4][100/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 3.4675 (3.7223)	Acc@1 26.953 (26.118)	Acc@5 65.234 (58.222)
Epoch: [4][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.6408 (3.7105)	Acc@1 25.391 (26.260)	Acc@5 64.844 (58.471)
Epoch: [4][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.8058 (3.6990)	Acc@1 21.875 (26.479)	Acc@5 56.250 (58.736)
Epoch: [4][130/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 3.5419 (3.6889)	Acc@1 26.953 (26.589)	Acc@5 61.328 (58.949)
Epoch: [4][140/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 3.5897 (3.6786)	Acc@1 28.125 (26.729)	Acc@5 58.984 (59.137)
Epoch: [4][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 3.5640 (3.6712)	Acc@1 29.688 (26.857)	Acc@5 64.062 (59.318)
Epoch: [4][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 3.3630 (3.6610)	Acc@1 33.203 (26.980)	Acc@5 64.844 (59.523)
Epoch: [4][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.2851 (3.6520)	Acc@1 35.547 (27.081)	Acc@5 70.703 (59.702)
Epoch: [4][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 3.4504 (3.6413)	Acc@1 30.078 (27.262)	Acc@5 61.719 (59.884)
Epoch: [4][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 3.5543 (3.6326)	Acc@1 27.344 (27.379)	Acc@5 61.719 (60.034)
num momentum params: 26
[0.1, 3.6306686374664308, 2.9077647280693055, 27.446, 26.84, tensor(0.2250, device='cuda:0', grad_fn=<DivBackward0>), 2.9542696475982666, 0.36910390853881836]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [5 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [5][0/196]	Time 0.024 (0.024)	Data 0.175 (0.175)	Loss 3.5017 (3.5017)	Acc@1 28.906 (28.906)	Acc@5 58.984 (58.984)
Epoch: [5][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 3.3262 (3.4435)	Acc@1 35.547 (30.398)	Acc@5 66.797 (63.246)
Epoch: [5][20/196]	Time 0.011 (0.016)	Data 0.013 (0.011)	Loss 3.2887 (3.4246)	Acc@1 35.156 (30.376)	Acc@5 65.625 (63.876)
Epoch: [5][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 3.3864 (3.4112)	Acc@1 32.031 (30.834)	Acc@5 62.500 (64.352)
Epoch: [5][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 3.4435 (3.4135)	Acc@1 28.906 (30.697)	Acc@5 65.234 (64.272)
Epoch: [5][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 3.3566 (3.4022)	Acc@1 32.031 (30.859)	Acc@5 63.281 (64.514)
Epoch: [5][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 3.3777 (3.3982)	Acc@1 31.641 (30.866)	Acc@5 64.453 (64.575)
Epoch: [5][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 3.1772 (3.3856)	Acc@1 36.328 (31.118)	Acc@5 69.141 (64.706)
Epoch: [5][80/196]	Time 0.010 (0.015)	Data 0.008 (0.005)	Loss 3.3541 (3.3711)	Acc@1 33.203 (31.433)	Acc@5 62.891 (64.892)
Epoch: [5][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 3.1577 (3.3644)	Acc@1 32.812 (31.525)	Acc@5 69.141 (65.015)
Epoch: [5][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.2265 (3.3551)	Acc@1 35.938 (31.753)	Acc@5 71.484 (65.091)
Epoch: [5][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 3.2944 (3.3501)	Acc@1 33.203 (31.971)	Acc@5 64.844 (65.111)
Epoch: [5][120/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 3.4504 (3.3408)	Acc@1 28.516 (32.112)	Acc@5 63.672 (65.380)
Epoch: [5][130/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 3.1591 (3.3325)	Acc@1 36.719 (32.270)	Acc@5 70.312 (65.574)
Epoch: [5][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.1612 (3.3242)	Acc@1 33.984 (32.416)	Acc@5 69.141 (65.736)
Epoch: [5][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 3.1773 (3.3138)	Acc@1 35.547 (32.603)	Acc@5 69.141 (65.902)
Epoch: [5][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.3226 (3.3074)	Acc@1 35.156 (32.783)	Acc@5 65.234 (66.008)
Epoch: [5][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 3.2968 (3.2977)	Acc@1 34.375 (33.030)	Acc@5 68.750 (66.219)
Epoch: [5][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 3.1663 (3.2899)	Acc@1 36.328 (33.182)	Acc@5 67.188 (66.361)
Epoch: [5][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.9890 (3.2817)	Acc@1 38.672 (33.350)	Acc@5 70.703 (66.556)
num momentum params: 26
[0.1, 3.279727486114502, 2.667681169509888, 33.364, 31.4, tensor(0.2282, device='cuda:0', grad_fn=<DivBackward0>), 3.0034730434417725, 0.3668835163116455]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [6 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [6][0/196]	Time 0.022 (0.022)	Data 0.164 (0.164)	Loss 3.1849 (3.1849)	Acc@1 39.062 (39.062)	Acc@5 64.062 (64.062)
Epoch: [6][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 2.9509 (3.1189)	Acc@1 39.062 (36.151)	Acc@5 74.609 (69.354)
Epoch: [6][20/196]	Time 0.014 (0.015)	Data 0.004 (0.010)	Loss 3.1239 (3.0943)	Acc@1 35.938 (36.217)	Acc@5 71.484 (70.071)
Epoch: [6][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.9127 (3.0849)	Acc@1 42.188 (36.656)	Acc@5 71.484 (70.073)
Epoch: [6][40/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.9682 (3.0697)	Acc@1 37.109 (37.052)	Acc@5 71.094 (70.436)
Epoch: [6][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 3.0282 (3.0622)	Acc@1 34.375 (37.209)	Acc@5 71.484 (70.588)
Epoch: [6][60/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 3.0980 (3.0644)	Acc@1 34.375 (36.981)	Acc@5 70.312 (70.678)
Epoch: [6][70/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.9812 (3.0641)	Acc@1 41.016 (37.071)	Acc@5 71.875 (70.698)
Epoch: [6][80/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.9157 (3.0583)	Acc@1 40.234 (37.244)	Acc@5 73.438 (70.804)
Epoch: [6][90/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.9997 (3.0504)	Acc@1 37.109 (37.397)	Acc@5 72.656 (71.004)
Epoch: [6][100/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.9099 (3.0436)	Acc@1 42.188 (37.550)	Acc@5 70.703 (71.094)
Epoch: [6][110/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 3.2023 (3.0402)	Acc@1 34.766 (37.577)	Acc@5 66.406 (71.139)
Epoch: [6][120/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.9957 (3.0358)	Acc@1 37.109 (37.710)	Acc@5 69.141 (71.139)
Epoch: [6][130/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 3.0543 (3.0356)	Acc@1 36.328 (37.715)	Acc@5 69.531 (71.055)
Epoch: [6][140/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 3.1230 (3.0340)	Acc@1 35.547 (37.727)	Acc@5 67.969 (71.052)
Epoch: [6][150/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 3.0614 (3.0365)	Acc@1 38.281 (37.805)	Acc@5 70.312 (70.983)
Epoch: [6][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.9396 (3.0317)	Acc@1 37.891 (37.883)	Acc@5 73.438 (71.106)
Epoch: [6][170/196]	Time 0.017 (0.015)	Data 0.001 (0.003)	Loss 2.7745 (3.0245)	Acc@1 45.703 (38.039)	Acc@5 74.219 (71.249)
Epoch: [6][180/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 3.0013 (3.0201)	Acc@1 37.500 (38.117)	Acc@5 71.094 (71.340)
Epoch: [6][190/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 2.7870 (3.0133)	Acc@1 39.844 (38.193)	Acc@5 74.609 (71.464)
num momentum params: 26
[0.1, 3.0113158837890626, 2.4113324594497683, 38.21, 37.08, tensor(0.2326, device='cuda:0', grad_fn=<DivBackward0>), 2.9617502689361572, 0.39609503746032715]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [7 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [7][0/196]	Time 0.024 (0.024)	Data 0.160 (0.160)	Loss 2.8185 (2.8185)	Acc@1 37.891 (37.891)	Acc@5 76.562 (76.562)
Epoch: [7][10/196]	Time 0.013 (0.015)	Data 0.004 (0.017)	Loss 2.9493 (2.9397)	Acc@1 38.672 (38.388)	Acc@5 72.266 (72.727)
Epoch: [7][20/196]	Time 0.015 (0.015)	Data 0.003 (0.010)	Loss 2.9404 (2.8811)	Acc@1 41.797 (39.844)	Acc@5 70.703 (73.605)
Epoch: [7][30/196]	Time 0.015 (0.015)	Data 0.002 (0.008)	Loss 2.8080 (2.8508)	Acc@1 41.016 (40.789)	Acc@5 74.219 (74.370)
Epoch: [7][40/196]	Time 0.014 (0.015)	Data 0.004 (0.006)	Loss 2.9072 (2.8448)	Acc@1 39.844 (41.073)	Acc@5 70.312 (74.505)
Epoch: [7][50/196]	Time 0.013 (0.015)	Data 0.005 (0.006)	Loss 2.8084 (2.8285)	Acc@1 43.359 (41.299)	Acc@5 78.906 (74.939)
Epoch: [7][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.9100 (2.8386)	Acc@1 40.625 (41.227)	Acc@5 74.609 (74.789)
Epoch: [7][70/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.7545 (2.8400)	Acc@1 39.453 (41.302)	Acc@5 79.688 (74.758)
Epoch: [7][80/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.7900 (2.8367)	Acc@1 44.141 (41.315)	Acc@5 73.047 (74.725)
Epoch: [7][90/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.8342 (2.8307)	Acc@1 42.188 (41.466)	Acc@5 76.172 (74.828)
Epoch: [7][100/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.8198 (2.8259)	Acc@1 41.797 (41.569)	Acc@5 76.172 (74.857)
Epoch: [7][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.8812 (2.8273)	Acc@1 37.500 (41.565)	Acc@5 71.875 (74.740)
Epoch: [7][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.8267 (2.8275)	Acc@1 45.703 (41.545)	Acc@5 71.875 (74.690)
Epoch: [7][130/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.8475 (2.8238)	Acc@1 40.625 (41.695)	Acc@5 75.000 (74.714)
Epoch: [7][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.7714 (2.8201)	Acc@1 41.797 (41.772)	Acc@5 76.953 (74.823)
Epoch: [7][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.5783 (2.8154)	Acc@1 48.047 (41.906)	Acc@5 80.078 (74.876)
Epoch: [7][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.6760 (2.8128)	Acc@1 42.578 (41.950)	Acc@5 78.516 (74.888)
Epoch: [7][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.6415 (2.8090)	Acc@1 48.438 (42.046)	Acc@5 76.953 (74.929)
Epoch: [7][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.8565 (2.8076)	Acc@1 40.234 (42.116)	Acc@5 74.609 (74.948)
Epoch: [7][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.0276 (2.8086)	Acc@1 37.500 (42.050)	Acc@5 67.969 (75.008)
num momentum params: 26
[0.1, 2.808650187149048, 2.387734286785126, 42.022, 37.97, tensor(0.2383, device='cuda:0', grad_fn=<DivBackward0>), 2.953011989593506, 0.37068915367126465]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [8 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [8][0/196]	Time 0.027 (0.027)	Data 0.169 (0.169)	Loss 2.7116 (2.7116)	Acc@1 45.703 (45.703)	Acc@5 77.734 (77.734)
Epoch: [8][10/196]	Time 0.018 (0.017)	Data 0.000 (0.018)	Loss 2.8720 (2.7297)	Acc@1 43.750 (43.999)	Acc@5 74.609 (76.705)
Epoch: [8][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 2.7592 (2.7194)	Acc@1 40.234 (43.713)	Acc@5 75.781 (76.842)
Epoch: [8][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.6252 (2.6965)	Acc@1 45.703 (43.977)	Acc@5 79.297 (77.281)
Epoch: [8][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.5436 (2.6939)	Acc@1 46.094 (43.941)	Acc@5 79.297 (77.201)
Epoch: [8][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.4684 (2.6821)	Acc@1 49.219 (44.217)	Acc@5 83.203 (77.367)
Epoch: [8][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.5518 (2.6799)	Acc@1 47.266 (44.422)	Acc@5 78.906 (77.318)
Epoch: [8][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.7460 (2.6820)	Acc@1 42.969 (44.229)	Acc@5 75.781 (77.333)
Epoch: [8][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.6138 (2.6929)	Acc@1 48.438 (44.097)	Acc@5 75.391 (77.001)
Epoch: [8][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.5537 (2.6940)	Acc@1 43.359 (44.012)	Acc@5 80.859 (77.013)
Epoch: [8][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.6348 (2.6884)	Acc@1 46.875 (44.110)	Acc@5 79.688 (77.158)
Epoch: [8][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.5573 (2.6836)	Acc@1 49.609 (44.310)	Acc@5 77.344 (77.157)
Epoch: [8][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.8373 (2.6799)	Acc@1 39.844 (44.380)	Acc@5 73.047 (77.179)
Epoch: [8][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.7679 (2.6807)	Acc@1 39.062 (44.311)	Acc@5 76.172 (77.192)
Epoch: [8][140/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.7333 (2.6829)	Acc@1 47.266 (44.335)	Acc@5 76.562 (77.136)
Epoch: [8][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.8055 (2.6830)	Acc@1 41.016 (44.324)	Acc@5 73.828 (77.155)
Epoch: [8][160/196]	Time 0.015 (0.015)	Data 0.003 (0.003)	Loss 2.5345 (2.6844)	Acc@1 47.656 (44.296)	Acc@5 81.641 (77.135)
Epoch: [8][170/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.5422 (2.6838)	Acc@1 52.344 (44.406)	Acc@5 80.469 (77.102)
Epoch: [8][180/196]	Time 0.015 (0.015)	Data 0.003 (0.003)	Loss 2.6287 (2.6797)	Acc@1 46.484 (44.501)	Acc@5 76.562 (77.175)
Epoch: [8][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.7319 (2.6762)	Acc@1 41.406 (44.566)	Acc@5 76.953 (77.295)
num momentum params: 26
[0.1, 2.67455270690918, 2.1497078740596773, 44.634, 42.3, tensor(0.2437, device='cuda:0', grad_fn=<DivBackward0>), 3.0242152214050293, 0.3846414089202881]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [9 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [9][0/196]	Time 0.027 (0.027)	Data 0.194 (0.194)	Loss 2.5123 (2.5123)	Acc@1 47.656 (47.656)	Acc@5 80.859 (80.859)
Epoch: [9][10/196]	Time 0.012 (0.016)	Data 0.005 (0.020)	Loss 2.5251 (2.5631)	Acc@1 48.438 (47.479)	Acc@5 83.203 (79.865)
Epoch: [9][20/196]	Time 0.011 (0.016)	Data 0.007 (0.012)	Loss 2.5038 (2.5528)	Acc@1 48.438 (47.693)	Acc@5 80.859 (79.892)
Epoch: [9][30/196]	Time 0.013 (0.015)	Data 0.005 (0.009)	Loss 2.7457 (2.5684)	Acc@1 45.312 (47.114)	Acc@5 78.516 (79.675)
Epoch: [9][40/196]	Time 0.013 (0.015)	Data 0.005 (0.007)	Loss 2.6537 (2.5690)	Acc@1 42.578 (46.837)	Acc@5 78.125 (79.535)
Epoch: [9][50/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 2.4057 (2.5496)	Acc@1 53.516 (47.534)	Acc@5 81.250 (79.756)
Epoch: [9][60/196]	Time 0.013 (0.015)	Data 0.005 (0.006)	Loss 2.5895 (2.5611)	Acc@1 48.828 (47.310)	Acc@5 75.391 (79.540)
Epoch: [9][70/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.3767 (2.5570)	Acc@1 49.219 (47.392)	Acc@5 85.156 (79.605)
Epoch: [9][80/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.6407 (2.5595)	Acc@1 46.484 (47.261)	Acc@5 75.391 (79.499)
Epoch: [9][90/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.5388 (2.5627)	Acc@1 45.312 (47.218)	Acc@5 80.469 (79.447)
Epoch: [9][100/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.4114 (2.5576)	Acc@1 51.172 (47.362)	Acc@5 82.422 (79.471)
Epoch: [9][110/196]	Time 0.012 (0.015)	Data 0.018 (0.005)	Loss 2.3144 (2.5602)	Acc@1 54.297 (47.336)	Acc@5 84.375 (79.329)
Epoch: [9][120/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.7987 (2.5608)	Acc@1 41.016 (47.295)	Acc@5 75.391 (79.287)
Epoch: [9][130/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.6639 (2.5594)	Acc@1 41.406 (47.301)	Acc@5 78.125 (79.327)
Epoch: [9][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.6468 (2.5615)	Acc@1 46.484 (47.304)	Acc@5 75.000 (79.258)
Epoch: [9][150/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.5571 (2.5639)	Acc@1 48.438 (47.281)	Acc@5 78.516 (79.191)
Epoch: [9][160/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.6199 (2.5651)	Acc@1 47.656 (47.239)	Acc@5 77.734 (79.163)
Epoch: [9][170/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.5916 (2.5618)	Acc@1 48.047 (47.323)	Acc@5 79.297 (79.203)
Epoch: [9][180/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.3921 (2.5606)	Acc@1 54.297 (47.397)	Acc@5 82.031 (79.228)
Epoch: [9][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.7569 (2.5632)	Acc@1 41.797 (47.315)	Acc@5 75.781 (79.229)
num momentum params: 26
[0.1, 2.563467431182861, 2.1456868767738344, 47.328, 43.23, tensor(0.2515, device='cuda:0', grad_fn=<DivBackward0>), 2.9490582942962646, 0.36598706245422363]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [10 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [10][0/196]	Time 0.022 (0.022)	Data 0.182 (0.182)	Loss 2.4773 (2.4773)	Acc@1 48.047 (48.047)	Acc@5 80.469 (80.469)
Epoch: [10][10/196]	Time 0.012 (0.015)	Data 0.005 (0.019)	Loss 2.3929 (2.4734)	Acc@1 50.781 (48.544)	Acc@5 85.938 (81.499)
Epoch: [10][20/196]	Time 0.012 (0.015)	Data 0.008 (0.012)	Loss 2.3735 (2.4836)	Acc@1 50.391 (48.251)	Acc@5 82.031 (80.729)
Epoch: [10][30/196]	Time 0.017 (0.015)	Data 0.000 (0.009)	Loss 2.3516 (2.4728)	Acc@1 51.953 (48.664)	Acc@5 82.031 (81.036)
Epoch: [10][40/196]	Time 0.012 (0.015)	Data 0.008 (0.008)	Loss 2.2734 (2.4680)	Acc@1 52.344 (48.714)	Acc@5 83.594 (81.069)
Epoch: [10][50/196]	Time 0.016 (0.015)	Data 0.001 (0.007)	Loss 2.4179 (2.4616)	Acc@1 48.047 (49.066)	Acc@5 85.156 (81.334)
Epoch: [10][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.5453 (2.4650)	Acc@1 48.828 (49.071)	Acc@5 77.734 (81.045)
Epoch: [10][70/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.3418 (2.4673)	Acc@1 50.391 (49.120)	Acc@5 83.203 (80.991)
Epoch: [10][80/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 2.4653 (2.4651)	Acc@1 51.172 (49.233)	Acc@5 82.031 (81.004)
Epoch: [10][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.4818 (2.4640)	Acc@1 53.516 (49.390)	Acc@5 80.859 (81.010)
Epoch: [10][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.6856 (2.4663)	Acc@1 40.625 (49.366)	Acc@5 79.297 (80.948)
Epoch: [10][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.5015 (2.4731)	Acc@1 48.438 (49.257)	Acc@5 80.469 (80.838)
Epoch: [10][120/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.5120 (2.4702)	Acc@1 48.438 (49.422)	Acc@5 80.078 (80.895)
Epoch: [10][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.3518 (2.4677)	Acc@1 51.562 (49.508)	Acc@5 83.203 (80.949)
Epoch: [10][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.3723 (2.4669)	Acc@1 54.688 (49.571)	Acc@5 80.859 (81.001)
Epoch: [10][150/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.5735 (2.4705)	Acc@1 49.609 (49.568)	Acc@5 78.125 (80.898)
Epoch: [10][160/196]	Time 0.012 (0.015)	Data 0.011 (0.004)	Loss 2.5066 (2.4671)	Acc@1 52.734 (49.680)	Acc@5 79.297 (80.935)
Epoch: [10][170/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.4587 (2.4669)	Acc@1 51.172 (49.703)	Acc@5 80.859 (80.921)
Epoch: [10][180/196]	Time 0.013 (0.015)	Data 0.014 (0.004)	Loss 2.5270 (2.4691)	Acc@1 46.094 (49.624)	Acc@5 78.516 (80.900)
Epoch: [10][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4666 (2.4737)	Acc@1 50.781 (49.526)	Acc@5 82.812 (80.833)
num momentum params: 26
[0.1, 2.47541660697937, 2.0408053278923033, 49.5, 45.0, tensor(0.2615, device='cuda:0', grad_fn=<DivBackward0>), 2.999804973602295, 0.3790853023529053]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [11 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [11][0/196]	Time 0.022 (0.022)	Data 0.201 (0.201)	Loss 2.3496 (2.3496)	Acc@1 56.641 (56.641)	Acc@5 80.469 (80.469)
Epoch: [11][10/196]	Time 0.017 (0.017)	Data 0.000 (0.020)	Loss 2.4260 (2.4398)	Acc@1 51.953 (51.278)	Acc@5 81.250 (81.286)
Epoch: [11][20/196]	Time 0.011 (0.016)	Data 0.008 (0.012)	Loss 2.2214 (2.4000)	Acc@1 54.297 (51.562)	Acc@5 86.328 (82.217)
Epoch: [11][30/196]	Time 0.016 (0.016)	Data 0.001 (0.009)	Loss 2.3113 (2.3914)	Acc@1 53.906 (51.336)	Acc@5 83.594 (82.686)
Epoch: [11][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.4206 (2.3936)	Acc@1 50.000 (51.381)	Acc@5 82.812 (82.527)
Epoch: [11][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.2784 (2.3761)	Acc@1 55.859 (51.869)	Acc@5 84.375 (82.675)
Epoch: [11][60/196]	Time 0.011 (0.016)	Data 0.007 (0.006)	Loss 2.3800 (2.3761)	Acc@1 51.562 (51.857)	Acc@5 81.250 (82.492)
Epoch: [11][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.3789 (2.3821)	Acc@1 50.000 (51.678)	Acc@5 82.422 (82.339)
Epoch: [11][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.4909 (2.3814)	Acc@1 48.438 (51.746)	Acc@5 83.594 (82.398)
Epoch: [11][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3130 (2.3784)	Acc@1 53.125 (51.842)	Acc@5 81.641 (82.495)
Epoch: [11][100/196]	Time 0.013 (0.015)	Data 0.006 (0.004)	Loss 2.4624 (2.3810)	Acc@1 50.000 (51.709)	Acc@5 78.125 (82.457)
Epoch: [11][110/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.3697 (2.3894)	Acc@1 52.734 (51.629)	Acc@5 82.812 (82.362)
Epoch: [11][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.3909 (2.3905)	Acc@1 52.734 (51.663)	Acc@5 81.250 (82.351)
Epoch: [11][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4653 (2.3971)	Acc@1 50.781 (51.458)	Acc@5 84.375 (82.312)
Epoch: [11][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.4821 (2.4009)	Acc@1 48.828 (51.324)	Acc@5 80.859 (82.281)
Epoch: [11][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4217 (2.4038)	Acc@1 46.875 (51.265)	Acc@5 86.328 (82.215)
Epoch: [11][160/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3763 (2.4074)	Acc@1 52.344 (51.186)	Acc@5 83.984 (82.167)
Epoch: [11][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.3179 (2.4077)	Acc@1 54.688 (51.215)	Acc@5 80.859 (82.111)
Epoch: [11][180/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.4884 (2.4091)	Acc@1 47.656 (51.176)	Acc@5 81.641 (82.090)
Epoch: [11][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.5924 (2.4141)	Acc@1 48.438 (51.115)	Acc@5 77.734 (82.021)
num momentum params: 26
[0.1, 2.4152155137634277, 2.1871807050704954, 51.064, 41.9, tensor(0.2718, device='cuda:0', grad_fn=<DivBackward0>), 2.9801442623138428, 0.36635518074035645]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [12 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [12][0/196]	Time 0.025 (0.025)	Data 0.194 (0.194)	Loss 2.3312 (2.3312)	Acc@1 52.734 (52.734)	Acc@5 85.156 (85.156)
Epoch: [12][10/196]	Time 0.012 (0.016)	Data 0.005 (0.020)	Loss 2.1432 (2.3886)	Acc@1 59.766 (51.172)	Acc@5 86.328 (82.457)
Epoch: [12][20/196]	Time 0.013 (0.016)	Data 0.005 (0.012)	Loss 2.4121 (2.3617)	Acc@1 51.562 (52.176)	Acc@5 83.594 (83.129)
Epoch: [12][30/196]	Time 0.015 (0.015)	Data 0.002 (0.009)	Loss 2.2015 (2.3577)	Acc@1 55.469 (52.092)	Acc@5 86.328 (83.216)
Epoch: [12][40/196]	Time 0.013 (0.015)	Data 0.005 (0.008)	Loss 2.4520 (2.3582)	Acc@1 49.609 (52.106)	Acc@5 80.469 (83.089)
Epoch: [12][50/196]	Time 0.017 (0.015)	Data 0.001 (0.007)	Loss 2.3459 (2.3813)	Acc@1 52.734 (51.754)	Acc@5 83.594 (82.667)
Epoch: [12][60/196]	Time 0.017 (0.015)	Data 0.007 (0.007)	Loss 2.4091 (2.3812)	Acc@1 53.516 (51.735)	Acc@5 82.812 (82.640)
Epoch: [12][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.3291 (2.3819)	Acc@1 54.688 (51.871)	Acc@5 84.375 (82.680)
Epoch: [12][80/196]	Time 0.012 (0.015)	Data 0.006 (0.006)	Loss 2.3099 (2.3828)	Acc@1 52.734 (51.828)	Acc@5 84.375 (82.793)
Epoch: [12][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3489 (2.3801)	Acc@1 51.172 (51.932)	Acc@5 81.641 (82.791)
Epoch: [12][100/196]	Time 0.012 (0.015)	Data 0.032 (0.006)	Loss 2.3039 (2.3826)	Acc@1 51.953 (51.926)	Acc@5 84.766 (82.700)
Epoch: [12][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3941 (2.3821)	Acc@1 50.391 (52.101)	Acc@5 84.766 (82.689)
Epoch: [12][120/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.3633 (2.3806)	Acc@1 51.172 (52.202)	Acc@5 83.594 (82.754)
Epoch: [12][130/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.2719 (2.3802)	Acc@1 56.641 (52.296)	Acc@5 86.328 (82.818)
Epoch: [12][140/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.4217 (2.3799)	Acc@1 50.391 (52.421)	Acc@5 82.812 (82.826)
Epoch: [12][150/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.3788 (2.3803)	Acc@1 57.031 (52.499)	Acc@5 83.203 (82.836)
Epoch: [12][160/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.3829 (2.3791)	Acc@1 50.781 (52.480)	Acc@5 83.203 (82.873)
Epoch: [12][170/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.4502 (2.3783)	Acc@1 50.000 (52.499)	Acc@5 82.422 (82.892)
Epoch: [12][180/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.4446 (2.3796)	Acc@1 54.688 (52.493)	Acc@5 79.297 (82.879)
Epoch: [12][190/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.3343 (2.3816)	Acc@1 53.906 (52.440)	Acc@5 82.812 (82.845)
num momentum params: 26
[0.1, 2.3831966175842285, 2.083646537065506, 52.432, 45.3, tensor(0.2815, device='cuda:0', grad_fn=<DivBackward0>), 2.9421229362487793, 0.3704073429107666]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [13 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [13][0/196]	Time 0.031 (0.031)	Data 0.188 (0.188)	Loss 2.3668 (2.3668)	Acc@1 52.734 (52.734)	Acc@5 82.422 (82.422)
Epoch: [13][10/196]	Time 0.016 (0.017)	Data 0.000 (0.019)	Loss 2.3734 (2.3259)	Acc@1 53.516 (53.232)	Acc@5 82.812 (84.588)
Epoch: [13][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 2.2483 (2.3045)	Acc@1 53.516 (53.702)	Acc@5 84.375 (84.561)
Epoch: [13][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 2.3766 (2.3084)	Acc@1 51.953 (53.931)	Acc@5 81.250 (84.325)
Epoch: [13][40/196]	Time 0.011 (0.016)	Data 0.009 (0.007)	Loss 2.3930 (2.3147)	Acc@1 50.000 (53.487)	Acc@5 87.891 (84.432)
Epoch: [13][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.4155 (2.2987)	Acc@1 54.688 (54.052)	Acc@5 83.203 (84.758)
Epoch: [13][60/196]	Time 0.011 (0.016)	Data 0.010 (0.006)	Loss 2.2806 (2.3000)	Acc@1 56.250 (53.945)	Acc@5 82.422 (84.740)
Epoch: [13][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.4086 (2.3013)	Acc@1 51.562 (54.071)	Acc@5 81.641 (84.672)
Epoch: [13][80/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.3724 (2.3030)	Acc@1 51.953 (53.988)	Acc@5 81.250 (84.770)
Epoch: [13][90/196]	Time 0.012 (0.015)	Data 0.020 (0.005)	Loss 2.3504 (2.3139)	Acc@1 57.422 (53.906)	Acc@5 81.250 (84.482)
Epoch: [13][100/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.3141 (2.3170)	Acc@1 54.297 (53.899)	Acc@5 83.984 (84.356)
Epoch: [13][110/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 2.2655 (2.3183)	Acc@1 54.688 (53.938)	Acc@5 84.375 (84.210)
Epoch: [13][120/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1875 (2.3210)	Acc@1 57.812 (53.780)	Acc@5 87.891 (84.278)
Epoch: [13][130/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.4459 (2.3199)	Acc@1 53.516 (53.775)	Acc@5 81.250 (84.315)
Epoch: [13][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.5449 (2.3217)	Acc@1 47.656 (53.804)	Acc@5 80.469 (84.209)
Epoch: [13][150/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.2724 (2.3273)	Acc@1 59.766 (53.720)	Acc@5 86.719 (84.103)
Epoch: [13][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.5844 (2.3328)	Acc@1 47.266 (53.637)	Acc@5 80.078 (84.040)
Epoch: [13][170/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.5670 (2.3378)	Acc@1 48.047 (53.577)	Acc@5 81.250 (83.962)
Epoch: [13][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3903 (2.3372)	Acc@1 50.391 (53.630)	Acc@5 83.203 (83.954)
Epoch: [13][190/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.5632 (2.3367)	Acc@1 46.875 (53.630)	Acc@5 80.859 (83.954)
num momentum params: 26
[0.1, 2.340049179611206, 1.952812259197235, 53.56, 48.16, tensor(0.2940, device='cuda:0', grad_fn=<DivBackward0>), 2.978867530822754, 0.36826038360595703]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [14 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [14][0/196]	Time 0.025 (0.025)	Data 0.187 (0.187)	Loss 2.1543 (2.1543)	Acc@1 58.203 (58.203)	Acc@5 87.891 (87.891)
Epoch: [14][10/196]	Time 0.017 (0.016)	Data 0.001 (0.020)	Loss 2.2746 (2.2575)	Acc@1 53.906 (55.682)	Acc@5 87.109 (86.293)
Epoch: [14][20/196]	Time 0.017 (0.015)	Data 0.000 (0.012)	Loss 2.3045 (2.2554)	Acc@1 56.641 (55.822)	Acc@5 86.719 (86.049)
Epoch: [14][30/196]	Time 0.017 (0.015)	Data 0.000 (0.009)	Loss 2.2717 (2.2481)	Acc@1 56.250 (55.973)	Acc@5 85.547 (85.975)
Epoch: [14][40/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.3382 (2.2671)	Acc@1 54.297 (55.707)	Acc@5 84.375 (85.690)
Epoch: [14][50/196]	Time 0.017 (0.015)	Data 0.000 (0.007)	Loss 2.3347 (2.2754)	Acc@1 51.172 (55.438)	Acc@5 85.938 (85.463)
Epoch: [14][60/196]	Time 0.018 (0.015)	Data 0.001 (0.006)	Loss 2.2629 (2.2759)	Acc@1 57.422 (55.642)	Acc@5 85.938 (85.374)
Epoch: [14][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.3479 (2.2876)	Acc@1 55.469 (55.436)	Acc@5 82.031 (84.997)
Epoch: [14][80/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.3068 (2.2953)	Acc@1 52.344 (55.122)	Acc@5 85.547 (84.915)
Epoch: [14][90/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.2776 (2.2986)	Acc@1 58.594 (55.203)	Acc@5 83.594 (84.787)
Epoch: [14][100/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.2490 (2.2966)	Acc@1 58.984 (55.275)	Acc@5 85.938 (84.847)
Epoch: [14][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1725 (2.2954)	Acc@1 58.203 (55.335)	Acc@5 85.547 (84.843)
Epoch: [14][120/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 2.4325 (2.3014)	Acc@1 53.516 (55.282)	Acc@5 81.641 (84.743)
Epoch: [14][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2528 (2.3043)	Acc@1 54.297 (55.203)	Acc@5 87.500 (84.715)
Epoch: [14][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.3840 (2.3064)	Acc@1 53.906 (55.228)	Acc@5 84.766 (84.730)
Epoch: [14][150/196]	Time 0.011 (0.015)	Data 0.023 (0.005)	Loss 2.0149 (2.3060)	Acc@1 61.719 (55.267)	Acc@5 89.844 (84.750)
Epoch: [14][160/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1716 (2.3056)	Acc@1 59.375 (55.280)	Acc@5 85.156 (84.761)
Epoch: [14][170/196]	Time 0.012 (0.015)	Data 0.020 (0.005)	Loss 2.3066 (2.3096)	Acc@1 53.125 (55.151)	Acc@5 86.719 (84.713)
Epoch: [14][180/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.4736 (2.3160)	Acc@1 53.125 (54.962)	Acc@5 83.203 (84.662)
Epoch: [14][190/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 2.4398 (2.3176)	Acc@1 53.906 (54.966)	Acc@5 84.375 (84.645)
num momentum params: 26
[0.1, 2.318880336456299, 2.3441429460048675, 54.926, 41.17, tensor(0.3050, device='cuda:0', grad_fn=<DivBackward0>), 3.0214436054229736, 0.3703022003173828]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [15 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [15][0/196]	Time 0.027 (0.027)	Data 0.174 (0.174)	Loss 2.2714 (2.2714)	Acc@1 56.250 (56.250)	Acc@5 85.938 (85.938)
Epoch: [15][10/196]	Time 0.013 (0.016)	Data 0.005 (0.018)	Loss 2.2985 (2.3065)	Acc@1 58.594 (55.433)	Acc@5 85.547 (85.156)
Epoch: [15][20/196]	Time 0.015 (0.016)	Data 0.003 (0.011)	Loss 2.2449 (2.2901)	Acc@1 56.250 (55.673)	Acc@5 86.719 (85.733)
Epoch: [15][30/196]	Time 0.015 (0.015)	Data 0.003 (0.008)	Loss 2.1622 (2.2815)	Acc@1 57.812 (55.910)	Acc@5 88.281 (86.001)
Epoch: [15][40/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 2.4167 (2.2644)	Acc@1 53.906 (56.564)	Acc@5 83.984 (86.052)
Epoch: [15][50/196]	Time 0.017 (0.015)	Data 0.000 (0.007)	Loss 2.4755 (2.2784)	Acc@1 47.656 (56.028)	Acc@5 84.766 (85.892)
Epoch: [15][60/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.3439 (2.2776)	Acc@1 54.688 (56.103)	Acc@5 84.375 (85.861)
Epoch: [15][70/196]	Time 0.018 (0.015)	Data 0.001 (0.006)	Loss 2.3650 (2.2794)	Acc@1 54.297 (56.024)	Acc@5 82.812 (85.871)
Epoch: [15][80/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3460 (2.2871)	Acc@1 51.562 (55.782)	Acc@5 86.328 (85.701)
Epoch: [15][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.4008 (2.2919)	Acc@1 48.828 (55.696)	Acc@5 85.547 (85.525)
Epoch: [15][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2143 (2.2921)	Acc@1 60.547 (55.724)	Acc@5 84.766 (85.462)
Epoch: [15][110/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.4477 (2.2929)	Acc@1 55.859 (55.828)	Acc@5 79.297 (85.381)
Epoch: [15][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.2436 (2.2930)	Acc@1 58.594 (55.876)	Acc@5 85.156 (85.327)
Epoch: [15][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.3895 (2.2959)	Acc@1 51.562 (55.758)	Acc@5 83.203 (85.243)
Epoch: [15][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1709 (2.2983)	Acc@1 60.938 (55.746)	Acc@5 86.719 (85.228)
Epoch: [15][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.3635 (2.2983)	Acc@1 59.375 (55.802)	Acc@5 83.594 (85.226)
Epoch: [15][160/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.3653 (2.3049)	Acc@1 55.469 (55.731)	Acc@5 85.547 (85.144)
Epoch: [15][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2117 (2.3047)	Acc@1 58.594 (55.770)	Acc@5 88.672 (85.161)
Epoch: [15][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2141 (2.3070)	Acc@1 58.984 (55.751)	Acc@5 86.719 (85.141)
Epoch: [15][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3653 (2.3112)	Acc@1 56.250 (55.712)	Acc@5 82.812 (85.042)
num momentum params: 26
[0.1, 2.3124805310821532, 2.1533385038375856, 55.71, 45.16, tensor(0.3141, device='cuda:0', grad_fn=<DivBackward0>), 3.0970394611358647, 0.37514734268188477]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [16 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [16][0/196]	Time 0.025 (0.025)	Data 0.176 (0.176)	Loss 2.2717 (2.2717)	Acc@1 57.812 (57.812)	Acc@5 85.547 (85.547)
Epoch: [16][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 2.1727 (2.2514)	Acc@1 54.688 (56.641)	Acc@5 87.500 (86.186)
Epoch: [16][20/196]	Time 0.013 (0.016)	Data 0.026 (0.012)	Loss 2.2110 (2.2130)	Acc@1 58.594 (57.924)	Acc@5 85.938 (86.663)
Epoch: [16][30/196]	Time 0.017 (0.016)	Data 0.000 (0.009)	Loss 2.1934 (2.2205)	Acc@1 60.156 (57.876)	Acc@5 86.719 (86.316)
Epoch: [16][40/196]	Time 0.013 (0.016)	Data 0.009 (0.008)	Loss 2.0677 (2.2232)	Acc@1 59.375 (57.841)	Acc@5 87.109 (86.147)
Epoch: [16][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 2.3913 (2.2380)	Acc@1 55.078 (57.330)	Acc@5 84.375 (86.098)
Epoch: [16][60/196]	Time 0.011 (0.016)	Data 0.009 (0.006)	Loss 2.2363 (2.2539)	Acc@1 58.203 (57.031)	Acc@5 85.938 (85.918)
Epoch: [16][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2744 (2.2560)	Acc@1 53.516 (56.943)	Acc@5 87.109 (85.822)
Epoch: [16][80/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.3680 (2.2579)	Acc@1 51.562 (56.877)	Acc@5 84.375 (85.783)
Epoch: [16][90/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.1994 (2.2574)	Acc@1 59.766 (56.928)	Acc@5 86.328 (85.925)
Epoch: [16][100/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 2.3730 (2.2622)	Acc@1 54.688 (56.842)	Acc@5 85.156 (85.891)
Epoch: [16][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0566 (2.2605)	Acc@1 61.328 (56.901)	Acc@5 89.453 (85.941)
Epoch: [16][120/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.3245 (2.2649)	Acc@1 56.641 (56.834)	Acc@5 82.422 (85.915)
Epoch: [16][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3236 (2.2657)	Acc@1 56.641 (56.805)	Acc@5 84.766 (85.905)
Epoch: [16][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.3755 (2.2664)	Acc@1 54.688 (56.815)	Acc@5 83.203 (85.929)
Epoch: [16][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1970 (2.2661)	Acc@1 57.812 (56.785)	Acc@5 86.328 (85.981)
Epoch: [16][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1990 (2.2678)	Acc@1 61.719 (56.762)	Acc@5 86.328 (85.933)
Epoch: [16][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.3330 (2.2680)	Acc@1 53.516 (56.791)	Acc@5 83.594 (85.924)
Epoch: [16][180/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.2050 (2.2678)	Acc@1 58.203 (56.831)	Acc@5 87.500 (85.925)
Epoch: [16][190/196]	Time 0.018 (0.016)	Data 0.003 (0.004)	Loss 2.1351 (2.2715)	Acc@1 60.938 (56.735)	Acc@5 88.672 (85.903)
num momentum params: 26
[0.1, 2.2709391610717775, 1.8850413715839387, 56.8, 50.07, tensor(0.3278, device='cuda:0', grad_fn=<DivBackward0>), 3.0555529594421387, 0.3680381774902344]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [17 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [17][0/196]	Time 0.027 (0.027)	Data 0.166 (0.166)	Loss 2.2413 (2.2413)	Acc@1 61.328 (61.328)	Acc@5 82.031 (82.031)
Epoch: [17][10/196]	Time 0.017 (0.016)	Data 0.001 (0.018)	Loss 2.2128 (2.2208)	Acc@1 58.203 (58.878)	Acc@5 88.672 (86.967)
Epoch: [17][20/196]	Time 0.011 (0.016)	Data 0.006 (0.010)	Loss 2.1511 (2.2207)	Acc@1 60.156 (58.929)	Acc@5 87.500 (86.700)
Epoch: [17][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.1685 (2.2224)	Acc@1 57.031 (58.795)	Acc@5 90.234 (86.744)
Epoch: [17][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 2.2292 (2.2069)	Acc@1 59.375 (59.232)	Acc@5 87.109 (87.119)
Epoch: [17][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.0941 (2.1999)	Acc@1 63.672 (59.367)	Acc@5 87.500 (87.178)
Epoch: [17][60/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 2.3579 (2.2095)	Acc@1 57.422 (59.106)	Acc@5 85.547 (87.116)
Epoch: [17][70/196]	Time 0.014 (0.015)	Data 0.000 (0.005)	Loss 2.3061 (2.2141)	Acc@1 58.594 (58.984)	Acc@5 85.938 (87.049)
Epoch: [17][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1663 (2.2202)	Acc@1 59.766 (58.835)	Acc@5 86.328 (86.945)
Epoch: [17][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0916 (2.2238)	Acc@1 59.375 (58.684)	Acc@5 91.406 (86.878)
Epoch: [17][100/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.4088 (2.2299)	Acc@1 53.516 (58.520)	Acc@5 85.938 (86.757)
Epoch: [17][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2676 (2.2345)	Acc@1 58.203 (58.435)	Acc@5 85.938 (86.684)
Epoch: [17][120/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.3115 (2.2367)	Acc@1 56.250 (58.384)	Acc@5 86.328 (86.661)
Epoch: [17][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.3436 (2.2387)	Acc@1 54.297 (58.275)	Acc@5 86.719 (86.647)
Epoch: [17][140/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.3555 (2.2420)	Acc@1 54.297 (58.187)	Acc@5 84.766 (86.577)
Epoch: [17][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3497 (2.2443)	Acc@1 57.031 (58.154)	Acc@5 84.766 (86.571)
Epoch: [17][160/196]	Time 0.017 (0.015)	Data 0.003 (0.004)	Loss 2.2201 (2.2469)	Acc@1 59.375 (58.089)	Acc@5 83.594 (86.539)
Epoch: [17][170/196]	Time 0.024 (0.015)	Data 0.000 (0.004)	Loss 2.1801 (2.2507)	Acc@1 60.156 (58.023)	Acc@5 85.938 (86.470)
Epoch: [17][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3707 (2.2558)	Acc@1 55.078 (57.873)	Acc@5 83.203 (86.360)
Epoch: [17][190/196]	Time 0.013 (0.015)	Data 0.001 (0.004)	Loss 2.2305 (2.2634)	Acc@1 57.422 (57.655)	Acc@5 88.672 (86.257)
num momentum params: 26
[0.1, 2.262577592468262, 2.300832812786102, 57.68, 43.16, tensor(0.3365, device='cuda:0', grad_fn=<DivBackward0>), 2.9471585750579834, 0.3688242435455322]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [18 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [18][0/196]	Time 0.027 (0.027)	Data 0.172 (0.172)	Loss 2.2518 (2.2518)	Acc@1 57.422 (57.422)	Acc@5 85.547 (85.547)
Epoch: [18][10/196]	Time 0.014 (0.016)	Data 0.004 (0.018)	Loss 2.1842 (2.2086)	Acc@1 64.062 (59.979)	Acc@5 86.328 (87.216)
Epoch: [18][20/196]	Time 0.012 (0.015)	Data 0.011 (0.011)	Loss 2.1790 (2.1998)	Acc@1 60.547 (59.524)	Acc@5 88.281 (87.277)
Epoch: [18][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.2819 (2.1995)	Acc@1 56.641 (59.803)	Acc@5 89.453 (87.399)
Epoch: [18][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 2.0776 (2.1981)	Acc@1 63.672 (59.842)	Acc@5 89.453 (87.367)
Epoch: [18][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.3978 (2.2033)	Acc@1 55.078 (59.643)	Acc@5 81.641 (87.232)
Epoch: [18][60/196]	Time 0.014 (0.016)	Data 0.008 (0.006)	Loss 2.2261 (2.2024)	Acc@1 60.156 (59.503)	Acc@5 86.328 (87.436)
Epoch: [18][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.3541 (2.2170)	Acc@1 54.688 (58.990)	Acc@5 83.984 (87.274)
Epoch: [18][80/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 2.2510 (2.2237)	Acc@1 59.375 (58.806)	Acc@5 86.719 (87.105)
Epoch: [18][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.2697 (2.2267)	Acc@1 59.375 (58.873)	Acc@5 85.938 (87.045)
Epoch: [18][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.1677 (2.2266)	Acc@1 57.812 (58.783)	Acc@5 89.062 (87.136)
Epoch: [18][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2348 (2.2253)	Acc@1 56.250 (58.703)	Acc@5 87.891 (87.215)
Epoch: [18][120/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.4661 (2.2325)	Acc@1 52.734 (58.603)	Acc@5 81.641 (87.087)
Epoch: [18][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1040 (2.2343)	Acc@1 61.719 (58.489)	Acc@5 90.234 (87.080)
Epoch: [18][140/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.1844 (2.2351)	Acc@1 59.375 (58.461)	Acc@5 90.234 (87.057)
Epoch: [18][150/196]	Time 0.014 (0.016)	Data 0.001 (0.004)	Loss 2.2109 (2.2390)	Acc@1 59.375 (58.382)	Acc@5 86.328 (87.003)
Epoch: [18][160/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.1511 (2.2409)	Acc@1 64.062 (58.397)	Acc@5 87.109 (86.964)
Epoch: [18][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.2288 (2.2391)	Acc@1 59.766 (58.489)	Acc@5 85.938 (87.000)
Epoch: [18][180/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.2630 (2.2402)	Acc@1 57.422 (58.443)	Acc@5 84.375 (86.952)
Epoch: [18][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3785 (2.2426)	Acc@1 56.641 (58.463)	Acc@5 85.156 (86.917)
num momentum params: 26
[0.1, 2.2459195484924317, 2.3168455028533934, 58.412, 43.65, tensor(0.3464, device='cuda:0', grad_fn=<DivBackward0>), 3.05106782913208, 0.38746023178100586]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [19 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [19][0/196]	Time 0.026 (0.026)	Data 0.170 (0.170)	Loss 2.1627 (2.1627)	Acc@1 61.719 (61.719)	Acc@5 88.672 (88.672)
Epoch: [19][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 2.2377 (2.2378)	Acc@1 55.469 (58.594)	Acc@5 87.109 (87.358)
Epoch: [19][20/196]	Time 0.012 (0.016)	Data 0.005 (0.011)	Loss 2.1108 (2.2496)	Acc@1 62.891 (58.352)	Acc@5 89.062 (87.091)
Epoch: [19][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.1831 (2.2251)	Acc@1 58.203 (59.035)	Acc@5 87.891 (87.185)
Epoch: [19][40/196]	Time 0.012 (0.016)	Data 0.008 (0.007)	Loss 2.1352 (2.1977)	Acc@1 59.375 (59.718)	Acc@5 89.062 (87.700)
Epoch: [19][50/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 2.1658 (2.2036)	Acc@1 60.938 (59.628)	Acc@5 89.453 (87.768)
Epoch: [19][60/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 2.2235 (2.1998)	Acc@1 60.547 (59.503)	Acc@5 88.281 (87.839)
Epoch: [19][70/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 2.2432 (2.2017)	Acc@1 58.984 (59.524)	Acc@5 86.719 (87.797)
Epoch: [19][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.3236 (2.2025)	Acc@1 55.859 (59.481)	Acc@5 85.938 (87.862)
Epoch: [19][90/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.3531 (2.2146)	Acc@1 58.594 (59.165)	Acc@5 85.938 (87.667)
Epoch: [19][100/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1763 (2.2162)	Acc@1 59.375 (59.220)	Acc@5 85.938 (87.554)
Epoch: [19][110/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.1974 (2.2173)	Acc@1 59.766 (59.269)	Acc@5 91.406 (87.542)
Epoch: [19][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2398 (2.2200)	Acc@1 57.422 (59.233)	Acc@5 87.500 (87.584)
Epoch: [19][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3088 (2.2215)	Acc@1 57.422 (59.229)	Acc@5 86.328 (87.494)
Epoch: [19][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.3492 (2.2253)	Acc@1 56.641 (59.134)	Acc@5 87.500 (87.431)
Epoch: [19][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1621 (2.2299)	Acc@1 60.938 (59.057)	Acc@5 86.328 (87.306)
Epoch: [19][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1823 (2.2295)	Acc@1 60.156 (59.055)	Acc@5 87.891 (87.286)
Epoch: [19][170/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 2.2240 (2.2317)	Acc@1 58.594 (58.987)	Acc@5 87.500 (87.228)
Epoch: [19][180/196]	Time 0.011 (0.015)	Data 0.008 (0.003)	Loss 2.3254 (2.2337)	Acc@1 56.641 (58.961)	Acc@5 83.984 (87.217)
Epoch: [19][190/196]	Time 0.018 (0.015)	Data 0.000 (0.003)	Loss 2.1186 (2.2355)	Acc@1 61.328 (58.931)	Acc@5 90.234 (87.197)
num momentum params: 26
[0.1, 2.237909722900391, 2.1651708924770356, 58.894, 45.38, tensor(0.3538, device='cuda:0', grad_fn=<DivBackward0>), 3.032552480697632, 0.36818456649780273]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [20 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [20][0/196]	Time 0.026 (0.026)	Data 0.176 (0.176)	Loss 2.1326 (2.1326)	Acc@1 61.328 (61.328)	Acc@5 89.062 (89.062)
Epoch: [20][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 2.2423 (2.1518)	Acc@1 58.984 (61.825)	Acc@5 87.109 (89.524)
Epoch: [20][20/196]	Time 0.011 (0.016)	Data 0.007 (0.011)	Loss 2.1588 (2.1269)	Acc@1 58.594 (61.570)	Acc@5 86.328 (89.509)
Epoch: [20][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.1572 (2.1302)	Acc@1 60.156 (61.303)	Acc@5 88.281 (89.365)
Epoch: [20][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 2.0029 (2.1354)	Acc@1 65.234 (61.280)	Acc@5 91.406 (89.158)
Epoch: [20][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1339 (2.1250)	Acc@1 60.938 (61.680)	Acc@5 89.844 (89.292)
Epoch: [20][60/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.3705 (2.1353)	Acc@1 52.734 (61.392)	Acc@5 87.891 (89.114)
Epoch: [20][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1650 (2.1429)	Acc@1 60.938 (61.224)	Acc@5 87.109 (88.903)
Epoch: [20][80/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.2678 (2.1489)	Acc@1 56.641 (61.135)	Acc@5 88.672 (88.831)
Epoch: [20][90/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0319 (2.1529)	Acc@1 66.016 (61.139)	Acc@5 91.406 (88.745)
Epoch: [20][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.5671 (2.1610)	Acc@1 53.125 (60.972)	Acc@5 80.469 (88.595)
Epoch: [20][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2673 (2.1682)	Acc@1 60.156 (60.804)	Acc@5 87.500 (88.503)
Epoch: [20][120/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2292 (2.1785)	Acc@1 61.719 (60.650)	Acc@5 86.719 (88.385)
Epoch: [20][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1764 (2.1865)	Acc@1 58.594 (60.422)	Acc@5 87.109 (88.278)
Epoch: [20][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2702 (2.1899)	Acc@1 57.031 (60.331)	Acc@5 87.500 (88.215)
Epoch: [20][150/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.2896 (2.1937)	Acc@1 57.812 (60.205)	Acc@5 85.156 (88.123)
Epoch: [20][160/196]	Time 0.012 (0.015)	Data 0.006 (0.003)	Loss 2.3138 (2.1996)	Acc@1 56.250 (60.040)	Acc@5 84.375 (87.993)
Epoch: [20][170/196]	Time 0.018 (0.015)	Data 0.000 (0.003)	Loss 2.2708 (2.2053)	Acc@1 55.859 (59.942)	Acc@5 88.281 (87.918)
Epoch: [20][180/196]	Time 0.011 (0.015)	Data 0.006 (0.003)	Loss 2.3759 (2.2096)	Acc@1 55.469 (59.817)	Acc@5 85.156 (87.835)
Epoch: [20][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.3112 (2.2146)	Acc@1 57.422 (59.741)	Acc@5 87.891 (87.809)
num momentum params: 26
[0.1, 2.2156424794006346, 1.8889204061031342, 59.738, 50.1, tensor(0.3633, device='cuda:0', grad_fn=<DivBackward0>), 3.0129921436309814, 0.3698606491088867]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [21 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [21][0/196]	Time 0.029 (0.029)	Data 0.165 (0.165)	Loss 2.0709 (2.0709)	Acc@1 64.453 (64.453)	Acc@5 87.109 (87.109)
Epoch: [21][10/196]	Time 0.015 (0.017)	Data 0.002 (0.017)	Loss 2.0222 (2.1470)	Acc@1 66.016 (61.115)	Acc@5 90.625 (89.737)
Epoch: [21][20/196]	Time 0.017 (0.016)	Data 0.002 (0.010)	Loss 2.0676 (2.1250)	Acc@1 62.891 (62.054)	Acc@5 87.891 (89.323)
Epoch: [21][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 2.0855 (2.1071)	Acc@1 63.281 (62.815)	Acc@5 89.453 (89.239)
Epoch: [21][40/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0696 (2.0965)	Acc@1 65.234 (63.148)	Acc@5 89.453 (89.405)
Epoch: [21][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.2214 (2.1098)	Acc@1 64.453 (62.921)	Acc@5 85.938 (89.331)
Epoch: [21][60/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.4031 (2.1277)	Acc@1 57.031 (62.494)	Acc@5 83.984 (89.043)
Epoch: [21][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.2867 (2.1373)	Acc@1 60.938 (62.181)	Acc@5 87.500 (89.013)
Epoch: [21][80/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0547 (2.1389)	Acc@1 64.062 (62.124)	Acc@5 91.406 (89.048)
Epoch: [21][90/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.5115 (2.1503)	Acc@1 50.781 (61.809)	Acc@5 84.766 (88.801)
Epoch: [21][100/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.1701 (2.1600)	Acc@1 60.156 (61.564)	Acc@5 88.281 (88.622)
Epoch: [21][110/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2448 (2.1669)	Acc@1 61.719 (61.377)	Acc@5 85.156 (88.531)
Epoch: [21][120/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.3117 (2.1730)	Acc@1 55.469 (61.189)	Acc@5 87.500 (88.485)
Epoch: [21][130/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.2405 (2.1770)	Acc@1 58.594 (61.096)	Acc@5 87.500 (88.406)
Epoch: [21][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.3018 (2.1843)	Acc@1 60.938 (60.987)	Acc@5 85.156 (88.362)
Epoch: [21][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1527 (2.1902)	Acc@1 65.234 (60.901)	Acc@5 88.281 (88.281)
Epoch: [21][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.3985 (2.1947)	Acc@1 53.125 (60.777)	Acc@5 85.547 (88.204)
Epoch: [21][170/196]	Time 0.015 (0.015)	Data 0.003 (0.003)	Loss 2.1952 (2.1987)	Acc@1 61.719 (60.666)	Acc@5 89.453 (88.160)
Epoch: [21][180/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.1958 (2.1990)	Acc@1 62.500 (60.694)	Acc@5 85.156 (88.128)
Epoch: [21][190/196]	Time 0.015 (0.015)	Data 0.003 (0.003)	Loss 2.1721 (2.2001)	Acc@1 62.500 (60.729)	Acc@5 89.844 (88.107)
num momentum params: 26
[0.1, 2.2026132590484617, 1.7874005842208862, 60.668, 52.89, tensor(0.3712, device='cuda:0', grad_fn=<DivBackward0>), 2.967841148376465, 0.37468528747558594]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [22 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [22][0/196]	Time 0.029 (0.029)	Data 0.172 (0.172)	Loss 2.1871 (2.1871)	Acc@1 57.031 (57.031)	Acc@5 91.406 (91.406)
Epoch: [22][10/196]	Time 0.017 (0.017)	Data 0.000 (0.017)	Loss 2.2188 (2.1474)	Acc@1 60.938 (61.044)	Acc@5 89.844 (90.092)
Epoch: [22][20/196]	Time 0.013 (0.016)	Data 0.005 (0.010)	Loss 2.1419 (2.1292)	Acc@1 65.625 (62.016)	Acc@5 89.844 (89.807)
Epoch: [22][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 2.1492 (2.1161)	Acc@1 66.016 (62.702)	Acc@5 87.891 (89.856)
Epoch: [22][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 2.3321 (2.1289)	Acc@1 57.422 (62.500)	Acc@5 86.328 (89.758)
Epoch: [22][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1619 (2.1184)	Acc@1 62.891 (62.699)	Acc@5 89.453 (89.874)
Epoch: [22][60/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.2967 (2.1342)	Acc@1 57.812 (62.410)	Acc@5 84.766 (89.562)
Epoch: [22][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2458 (2.1440)	Acc@1 61.328 (62.340)	Acc@5 87.109 (89.321)
Epoch: [22][80/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.2503 (2.1488)	Acc@1 56.641 (61.994)	Acc@5 90.234 (89.304)
Epoch: [22][90/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.0601 (2.1477)	Acc@1 64.844 (61.916)	Acc@5 91.797 (89.367)
Epoch: [22][100/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 1.9967 (2.1490)	Acc@1 66.797 (61.889)	Acc@5 91.406 (89.360)
Epoch: [22][110/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.1640 (2.1563)	Acc@1 62.500 (61.687)	Acc@5 87.500 (89.165)
Epoch: [22][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2120 (2.1632)	Acc@1 63.281 (61.557)	Acc@5 87.891 (88.998)
Epoch: [22][130/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.2094 (2.1693)	Acc@1 58.594 (61.388)	Acc@5 88.281 (88.937)
Epoch: [22][140/196]	Time 0.017 (0.015)	Data 0.002 (0.004)	Loss 2.4658 (2.1750)	Acc@1 55.469 (61.345)	Acc@5 83.594 (88.785)
Epoch: [22][150/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 2.2878 (2.1820)	Acc@1 58.984 (61.163)	Acc@5 87.891 (88.711)
Epoch: [22][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2722 (2.1884)	Acc@1 56.250 (60.996)	Acc@5 86.328 (88.621)
Epoch: [22][170/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0762 (2.1909)	Acc@1 64.844 (60.985)	Acc@5 90.234 (88.617)
Epoch: [22][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2508 (2.1919)	Acc@1 62.500 (60.963)	Acc@5 88.672 (88.624)
Epoch: [22][190/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0733 (2.1949)	Acc@1 64.844 (60.882)	Acc@5 90.625 (88.568)
num momentum params: 26
[0.1, 2.1964256578063965, 2.160226037502289, 60.88, 45.99, tensor(0.3775, device='cuda:0', grad_fn=<DivBackward0>), 2.958111047744751, 0.37353992462158203]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [23 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [23][0/196]	Time 0.028 (0.028)	Data 0.170 (0.170)	Loss 2.0380 (2.0380)	Acc@1 65.234 (65.234)	Acc@5 91.797 (91.797)
Epoch: [23][10/196]	Time 0.015 (0.017)	Data 0.002 (0.018)	Loss 2.2022 (2.1436)	Acc@1 60.547 (62.322)	Acc@5 88.281 (90.057)
Epoch: [23][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 2.2037 (2.1362)	Acc@1 61.719 (62.946)	Acc@5 88.672 (89.472)
Epoch: [23][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 2.2282 (2.1439)	Acc@1 60.156 (62.765)	Acc@5 88.281 (89.126)
Epoch: [23][40/196]	Time 0.011 (0.015)	Data 0.009 (0.007)	Loss 2.2318 (2.1343)	Acc@1 59.375 (62.843)	Acc@5 88.672 (89.282)
Epoch: [23][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1276 (2.1289)	Acc@1 64.062 (63.074)	Acc@5 86.719 (89.285)
Epoch: [23][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.2107 (2.1279)	Acc@1 63.672 (63.147)	Acc@5 88.672 (89.293)
Epoch: [23][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2511 (2.1306)	Acc@1 58.203 (63.023)	Acc@5 89.453 (89.277)
Epoch: [23][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.1232 (2.1327)	Acc@1 62.500 (63.040)	Acc@5 88.672 (89.299)
Epoch: [23][90/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.2620 (2.1375)	Acc@1 60.938 (62.852)	Acc@5 90.625 (89.324)
Epoch: [23][100/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.0748 (2.1399)	Acc@1 65.625 (62.821)	Acc@5 91.406 (89.322)
Epoch: [23][110/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0579 (2.1408)	Acc@1 66.797 (62.715)	Acc@5 91.406 (89.362)
Epoch: [23][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3467 (2.1466)	Acc@1 58.203 (62.623)	Acc@5 85.938 (89.266)
Epoch: [23][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3352 (2.1506)	Acc@1 58.594 (62.581)	Acc@5 86.719 (89.209)
Epoch: [23][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.4216 (2.1585)	Acc@1 56.641 (62.392)	Acc@5 83.203 (89.115)
Epoch: [23][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2059 (2.1620)	Acc@1 62.109 (62.259)	Acc@5 87.500 (89.039)
Epoch: [23][160/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2672 (2.1686)	Acc@1 58.984 (62.044)	Acc@5 87.500 (88.980)
Epoch: [23][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2323 (2.1758)	Acc@1 58.984 (61.883)	Acc@5 90.234 (88.845)
Epoch: [23][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3615 (2.1791)	Acc@1 57.812 (61.799)	Acc@5 86.719 (88.838)
Epoch: [23][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2813 (2.1840)	Acc@1 57.812 (61.661)	Acc@5 86.719 (88.770)
num momentum params: 26
[0.1, 2.1864012002563475, 1.9194976305961609, 61.628, 50.43, tensor(0.3840, device='cuda:0', grad_fn=<DivBackward0>), 2.9794561862945557, 0.3740203380584717]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [24 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [24][0/196]	Time 0.028 (0.028)	Data 0.168 (0.168)	Loss 2.1205 (2.1205)	Acc@1 62.109 (62.109)	Acc@5 90.625 (90.625)
Epoch: [24][10/196]	Time 0.017 (0.016)	Data 0.001 (0.018)	Loss 2.3188 (2.1932)	Acc@1 58.984 (60.973)	Acc@5 87.500 (89.062)
Epoch: [24][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 2.2737 (2.1743)	Acc@1 61.719 (62.054)	Acc@5 87.500 (89.007)
Epoch: [24][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.0543 (2.1429)	Acc@1 66.016 (62.979)	Acc@5 89.453 (89.252)
Epoch: [24][40/196]	Time 0.015 (0.015)	Data 0.005 (0.007)	Loss 2.2065 (2.1235)	Acc@1 60.547 (63.176)	Acc@5 91.406 (89.596)
Epoch: [24][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.0326 (2.1119)	Acc@1 64.062 (63.411)	Acc@5 92.188 (89.936)
Epoch: [24][60/196]	Time 0.012 (0.015)	Data 0.007 (0.006)	Loss 2.1139 (2.1090)	Acc@1 61.719 (63.505)	Acc@5 89.062 (89.959)
Epoch: [24][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1588 (2.1121)	Acc@1 60.547 (63.232)	Acc@5 91.016 (90.047)
Epoch: [24][80/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.1094 (2.1139)	Acc@1 67.188 (63.257)	Acc@5 88.672 (89.950)
Epoch: [24][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2318 (2.1188)	Acc@1 62.891 (63.170)	Acc@5 85.938 (89.857)
Epoch: [24][100/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.0840 (2.1241)	Acc@1 66.016 (63.011)	Acc@5 90.234 (89.848)
Epoch: [24][110/196]	Time 0.019 (0.015)	Data 0.001 (0.004)	Loss 2.2218 (2.1300)	Acc@1 58.203 (62.817)	Acc@5 87.891 (89.756)
Epoch: [24][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.3136 (2.1392)	Acc@1 60.938 (62.581)	Acc@5 85.938 (89.634)
Epoch: [24][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2180 (2.1457)	Acc@1 60.156 (62.345)	Acc@5 89.844 (89.507)
Epoch: [24][140/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.2580 (2.1513)	Acc@1 58.594 (62.289)	Acc@5 89.453 (89.437)
Epoch: [24][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.4282 (2.1557)	Acc@1 55.469 (62.156)	Acc@5 86.328 (89.399)
Epoch: [24][160/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.0081 (2.1603)	Acc@1 67.969 (62.078)	Acc@5 91.406 (89.346)
Epoch: [24][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1464 (2.1668)	Acc@1 63.281 (61.933)	Acc@5 89.062 (89.245)
Epoch: [24][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2386 (2.1674)	Acc@1 62.109 (61.939)	Acc@5 87.891 (89.235)
Epoch: [24][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3130 (2.1738)	Acc@1 58.984 (61.835)	Acc@5 87.500 (89.161)
num momentum params: 26
[0.1, 2.175640404586792, 1.9899841499328614, 61.81, 49.45, tensor(0.3901, device='cuda:0', grad_fn=<DivBackward0>), 3.0175814628601074, 0.3751034736633301]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [25 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [25][0/196]	Time 0.030 (0.030)	Data 0.170 (0.170)	Loss 2.0476 (2.0476)	Acc@1 64.844 (64.844)	Acc@5 89.062 (89.062)
Epoch: [25][10/196]	Time 0.018 (0.017)	Data 0.000 (0.017)	Loss 2.1303 (2.0690)	Acc@1 64.062 (65.305)	Acc@5 91.016 (91.087)
Epoch: [25][20/196]	Time 0.012 (0.016)	Data 0.013 (0.011)	Loss 2.2326 (2.1011)	Acc@1 62.891 (64.807)	Acc@5 90.625 (90.439)
Epoch: [25][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.0390 (2.1128)	Acc@1 65.625 (64.478)	Acc@5 91.797 (90.171)
Epoch: [25][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.2072 (2.1175)	Acc@1 63.672 (64.215)	Acc@5 86.719 (90.034)
Epoch: [25][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1453 (2.1210)	Acc@1 62.500 (63.994)	Acc@5 88.281 (89.943)
Epoch: [25][60/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.1567 (2.1315)	Acc@1 58.984 (63.531)	Acc@5 90.625 (89.869)
Epoch: [25][70/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1435 (2.1347)	Acc@1 63.672 (63.375)	Acc@5 91.016 (89.717)
Epoch: [25][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1822 (2.1412)	Acc@1 64.062 (63.291)	Acc@5 88.281 (89.511)
Epoch: [25][90/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1358 (2.1493)	Acc@1 62.891 (63.041)	Acc@5 90.625 (89.342)
Epoch: [25][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2445 (2.1524)	Acc@1 61.328 (62.925)	Acc@5 85.547 (89.325)
Epoch: [25][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3281 (2.1564)	Acc@1 58.984 (62.782)	Acc@5 89.062 (89.330)
Epoch: [25][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0969 (2.1593)	Acc@1 65.625 (62.700)	Acc@5 91.797 (89.318)
Epoch: [25][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0983 (2.1630)	Acc@1 61.719 (62.557)	Acc@5 89.844 (89.259)
Epoch: [25][140/196]	Time 0.011 (0.015)	Data 0.014 (0.004)	Loss 2.0660 (2.1661)	Acc@1 68.359 (62.547)	Acc@5 91.406 (89.259)
Epoch: [25][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4329 (2.1705)	Acc@1 58.203 (62.492)	Acc@5 86.328 (89.215)
Epoch: [25][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.2727 (2.1741)	Acc@1 61.719 (62.464)	Acc@5 87.500 (89.155)
Epoch: [25][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1272 (2.1750)	Acc@1 59.766 (62.406)	Acc@5 91.797 (89.158)
Epoch: [25][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.2505 (2.1752)	Acc@1 58.594 (62.431)	Acc@5 89.062 (89.162)
Epoch: [25][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2651 (2.1788)	Acc@1 59.766 (62.300)	Acc@5 87.109 (89.110)
num momentum params: 26
[0.1, 2.1801264798736573, 1.8272962498664855, 62.234, 51.56, tensor(0.3936, device='cuda:0', grad_fn=<DivBackward0>), 2.9795680046081543, 0.3764369487762451]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [26 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [26][0/196]	Time 0.031 (0.031)	Data 0.169 (0.169)	Loss 2.1732 (2.1732)	Acc@1 61.719 (61.719)	Acc@5 89.844 (89.844)
Epoch: [26][10/196]	Time 0.013 (0.016)	Data 0.004 (0.018)	Loss 2.0436 (2.1452)	Acc@1 63.672 (63.104)	Acc@5 92.188 (90.412)
Epoch: [26][20/196]	Time 0.014 (0.016)	Data 0.004 (0.010)	Loss 2.0866 (2.1367)	Acc@1 65.234 (63.337)	Acc@5 90.625 (90.402)
Epoch: [26][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 2.2008 (2.1312)	Acc@1 60.547 (63.508)	Acc@5 89.062 (90.348)
Epoch: [26][40/196]	Time 0.012 (0.015)	Data 0.004 (0.007)	Loss 2.1985 (2.1340)	Acc@1 61.719 (63.310)	Acc@5 87.500 (90.225)
Epoch: [26][50/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1915 (2.1321)	Acc@1 62.891 (63.220)	Acc@5 87.500 (90.035)
Epoch: [26][60/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.0756 (2.1292)	Acc@1 64.453 (63.275)	Acc@5 90.234 (90.004)
Epoch: [26][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0927 (2.1316)	Acc@1 62.891 (63.446)	Acc@5 90.625 (89.910)
Epoch: [26][80/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2844 (2.1269)	Acc@1 58.984 (63.643)	Acc@5 88.672 (89.979)
Epoch: [26][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0377 (2.1237)	Acc@1 64.844 (63.711)	Acc@5 91.406 (90.088)
Epoch: [26][100/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1090 (2.1293)	Acc@1 62.500 (63.633)	Acc@5 91.797 (90.006)
Epoch: [26][110/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1445 (2.1325)	Acc@1 65.234 (63.573)	Acc@5 88.672 (89.977)
Epoch: [26][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2330 (2.1386)	Acc@1 62.500 (63.430)	Acc@5 92.188 (89.918)
Epoch: [26][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2206 (2.1435)	Acc@1 65.625 (63.418)	Acc@5 87.500 (89.719)
Epoch: [26][140/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2854 (2.1474)	Acc@1 60.938 (63.292)	Acc@5 87.109 (89.700)
Epoch: [26][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0402 (2.1507)	Acc@1 67.578 (63.227)	Acc@5 89.844 (89.665)
Epoch: [26][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2086 (2.1537)	Acc@1 59.375 (63.124)	Acc@5 89.062 (89.611)
Epoch: [26][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0241 (2.1553)	Acc@1 65.234 (63.089)	Acc@5 90.625 (89.627)
Epoch: [26][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1069 (2.1615)	Acc@1 60.156 (62.914)	Acc@5 90.234 (89.494)
Epoch: [26][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.3467 (2.1638)	Acc@1 57.031 (62.858)	Acc@5 88.281 (89.443)
num momentum params: 26
[0.1, 2.1649599508666992, 1.9891715550422668, 62.838, 48.94, tensor(0.4005, device='cuda:0', grad_fn=<DivBackward0>), 2.996513843536377, 0.39072585105895996]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [27 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [27][0/196]	Time 0.031 (0.031)	Data 0.182 (0.182)	Loss 2.0081 (2.0081)	Acc@1 69.141 (69.141)	Acc@5 89.844 (89.844)
Epoch: [27][10/196]	Time 0.018 (0.019)	Data 0.002 (0.018)	Loss 1.9801 (2.0584)	Acc@1 67.578 (65.589)	Acc@5 92.578 (91.371)
Epoch: [27][20/196]	Time 0.012 (0.017)	Data 0.005 (0.011)	Loss 2.1950 (2.0876)	Acc@1 64.844 (65.067)	Acc@5 87.891 (91.090)
Epoch: [27][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9627 (2.0922)	Acc@1 69.922 (65.134)	Acc@5 92.188 (90.965)
Epoch: [27][40/196]	Time 0.012 (0.016)	Data 0.007 (0.007)	Loss 2.0531 (2.0887)	Acc@1 62.891 (65.101)	Acc@5 91.016 (90.796)
Epoch: [27][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.1801 (2.0931)	Acc@1 62.891 (64.920)	Acc@5 91.016 (90.809)
Epoch: [27][60/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.2107 (2.0916)	Acc@1 59.766 (65.004)	Acc@5 87.891 (90.702)
Epoch: [27][70/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.1716 (2.1004)	Acc@1 63.281 (64.904)	Acc@5 88.672 (90.421)
Epoch: [27][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0355 (2.1045)	Acc@1 64.062 (64.675)	Acc@5 94.141 (90.355)
Epoch: [27][90/196]	Time 0.020 (0.016)	Data 0.001 (0.004)	Loss 2.1448 (2.1065)	Acc@1 65.234 (64.681)	Acc@5 89.844 (90.406)
Epoch: [27][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.2901 (2.1134)	Acc@1 64.062 (64.453)	Acc@5 88.281 (90.304)
Epoch: [27][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2176 (2.1207)	Acc@1 58.594 (64.150)	Acc@5 91.016 (90.333)
Epoch: [27][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1786 (2.1228)	Acc@1 60.156 (64.166)	Acc@5 88.672 (90.280)
Epoch: [27][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2949 (2.1289)	Acc@1 59.766 (64.012)	Acc@5 85.547 (90.160)
Epoch: [27][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.3425 (2.1354)	Acc@1 59.766 (63.844)	Acc@5 88.281 (90.068)
Epoch: [27][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3714 (2.1420)	Acc@1 58.984 (63.623)	Acc@5 87.109 (89.973)
Epoch: [27][160/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2146 (2.1502)	Acc@1 60.547 (63.417)	Acc@5 90.625 (89.846)
Epoch: [27][170/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.2023 (2.1584)	Acc@1 64.453 (63.274)	Acc@5 88.281 (89.720)
Epoch: [27][180/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.0937 (2.1601)	Acc@1 62.891 (63.247)	Acc@5 90.234 (89.678)
Epoch: [27][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.0863 (2.1633)	Acc@1 69.141 (63.222)	Acc@5 90.234 (89.631)
num momentum params: 26
[0.1, 2.16416405090332, 1.8376014089584352, 63.198, 52.15, tensor(0.4049, device='cuda:0', grad_fn=<DivBackward0>), 3.0244174003601074, 0.3741123676300049]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [28 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [28][0/196]	Time 0.029 (0.029)	Data 0.170 (0.170)	Loss 2.0014 (2.0014)	Acc@1 64.844 (64.844)	Acc@5 94.531 (94.531)
Epoch: [28][10/196]	Time 0.018 (0.017)	Data 0.001 (0.018)	Loss 2.1675 (2.0772)	Acc@1 62.500 (65.305)	Acc@5 89.453 (91.229)
Epoch: [28][20/196]	Time 0.011 (0.016)	Data 0.007 (0.011)	Loss 2.0820 (2.0777)	Acc@1 68.359 (65.737)	Acc@5 92.188 (91.090)
Epoch: [28][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.0301 (2.0758)	Acc@1 64.062 (65.852)	Acc@5 94.141 (90.915)
Epoch: [28][40/196]	Time 0.013 (0.016)	Data 0.009 (0.007)	Loss 2.1227 (2.0706)	Acc@1 66.016 (65.825)	Acc@5 89.844 (90.958)
Epoch: [28][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0089 (2.0738)	Acc@1 71.094 (65.862)	Acc@5 90.625 (90.878)
Epoch: [28][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.1578 (2.0819)	Acc@1 61.328 (65.465)	Acc@5 89.453 (90.695)
Epoch: [28][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0549 (2.0889)	Acc@1 65.234 (65.273)	Acc@5 91.797 (90.608)
Epoch: [28][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0809 (2.0931)	Acc@1 66.406 (65.181)	Acc@5 89.453 (90.529)
Epoch: [28][90/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0835 (2.0997)	Acc@1 64.844 (64.912)	Acc@5 92.969 (90.432)
Epoch: [28][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.0846 (2.1032)	Acc@1 65.234 (64.933)	Acc@5 89.844 (90.393)
Epoch: [28][110/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0490 (2.1099)	Acc@1 64.453 (64.819)	Acc@5 92.188 (90.340)
Epoch: [28][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.2218 (2.1132)	Acc@1 64.844 (64.689)	Acc@5 87.891 (90.296)
Epoch: [28][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1445 (2.1209)	Acc@1 64.062 (64.492)	Acc@5 87.891 (90.169)
Epoch: [28][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3066 (2.1302)	Acc@1 61.719 (64.290)	Acc@5 84.375 (90.054)
Epoch: [28][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.4204 (2.1374)	Acc@1 55.078 (64.109)	Acc@5 85.156 (89.934)
Epoch: [28][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2755 (2.1452)	Acc@1 59.375 (63.832)	Acc@5 89.062 (89.836)
Epoch: [28][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0722 (2.1476)	Acc@1 66.797 (63.850)	Acc@5 89.844 (89.755)
Epoch: [28][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2279 (2.1514)	Acc@1 61.719 (63.752)	Acc@5 89.062 (89.732)
Epoch: [28][190/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 1.9621 (2.1516)	Acc@1 67.188 (63.727)	Acc@5 91.797 (89.752)
num momentum params: 26
[0.1, 2.1537892372894287, 1.8452119576930999, 63.696, 51.35, tensor(0.4107, device='cuda:0', grad_fn=<DivBackward0>), 2.9732255935668945, 0.3742177486419678]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [29 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [29][0/196]	Time 0.030 (0.030)	Data 0.180 (0.180)	Loss 2.0227 (2.0227)	Acc@1 67.578 (67.578)	Acc@5 92.188 (92.188)
Epoch: [29][10/196]	Time 0.015 (0.017)	Data 0.003 (0.019)	Loss 2.0734 (2.0855)	Acc@1 66.016 (64.453)	Acc@5 91.406 (91.726)
Epoch: [29][20/196]	Time 0.015 (0.016)	Data 0.003 (0.011)	Loss 2.0205 (2.0778)	Acc@1 67.969 (65.123)	Acc@5 91.797 (91.406)
Epoch: [29][30/196]	Time 0.016 (0.016)	Data 0.003 (0.008)	Loss 2.0385 (2.0618)	Acc@1 68.359 (65.990)	Acc@5 90.234 (91.494)
Epoch: [29][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 1.9604 (2.0681)	Acc@1 67.578 (65.882)	Acc@5 92.578 (91.406)
Epoch: [29][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1280 (2.0706)	Acc@1 65.234 (65.755)	Acc@5 91.016 (91.368)
Epoch: [29][60/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 2.0629 (2.0651)	Acc@1 62.500 (65.804)	Acc@5 90.625 (91.272)
Epoch: [29][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0904 (2.0655)	Acc@1 65.625 (65.856)	Acc@5 89.453 (91.203)
Epoch: [29][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.1915 (2.0814)	Acc@1 59.766 (65.432)	Acc@5 88.672 (90.972)
Epoch: [29][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1848 (2.0815)	Acc@1 63.672 (65.458)	Acc@5 87.500 (90.900)
Epoch: [29][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0042 (2.0878)	Acc@1 67.188 (65.285)	Acc@5 91.016 (90.803)
Epoch: [29][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0755 (2.0994)	Acc@1 66.016 (65.023)	Acc@5 89.453 (90.565)
Epoch: [29][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0992 (2.1063)	Acc@1 64.844 (64.899)	Acc@5 90.625 (90.457)
Epoch: [29][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0786 (2.1113)	Acc@1 64.844 (64.748)	Acc@5 93.750 (90.366)
Epoch: [29][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.3464 (2.1192)	Acc@1 61.719 (64.542)	Acc@5 84.375 (90.232)
Epoch: [29][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1106 (2.1231)	Acc@1 65.625 (64.476)	Acc@5 89.844 (90.180)
Epoch: [29][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1555 (2.1274)	Acc@1 66.406 (64.361)	Acc@5 89.453 (90.084)
Epoch: [29][170/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.0778 (2.1313)	Acc@1 67.969 (64.327)	Acc@5 92.188 (90.079)
Epoch: [29][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2810 (2.1345)	Acc@1 60.938 (64.270)	Acc@5 89.453 (90.038)
Epoch: [29][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2974 (2.1380)	Acc@1 58.984 (64.208)	Acc@5 89.062 (90.024)
num momentum params: 26
[0.1, 2.138525873794556, 1.7415443086624145, 64.18, 54.66, tensor(0.4168, device='cuda:0', grad_fn=<DivBackward0>), 2.9566948413848877, 0.37256860733032227]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [30 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [30][0/196]	Time 0.030 (0.030)	Data 0.170 (0.170)	Loss 1.7996 (1.7996)	Acc@1 73.828 (73.828)	Acc@5 95.703 (95.703)
Epoch: [30][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 2.0029 (2.0623)	Acc@1 66.406 (65.838)	Acc@5 93.359 (91.903)
Epoch: [30][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 2.1524 (2.0587)	Acc@1 61.328 (66.016)	Acc@5 90.625 (91.741)
Epoch: [30][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0785 (2.0504)	Acc@1 65.625 (66.482)	Acc@5 90.625 (91.809)
Epoch: [30][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 2.0440 (2.0409)	Acc@1 64.844 (66.530)	Acc@5 92.578 (91.825)
Epoch: [30][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 1.8301 (2.0535)	Acc@1 71.875 (66.023)	Acc@5 94.531 (91.613)
Epoch: [30][60/196]	Time 0.012 (0.015)	Data 0.018 (0.005)	Loss 2.0932 (2.0588)	Acc@1 66.406 (65.862)	Acc@5 90.234 (91.496)
Epoch: [30][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0572 (2.0683)	Acc@1 67.188 (65.691)	Acc@5 92.188 (91.329)
Epoch: [30][80/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.1191 (2.0719)	Acc@1 68.750 (65.726)	Acc@5 91.797 (91.339)
Epoch: [30][90/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0366 (2.0843)	Acc@1 71.094 (65.522)	Acc@5 93.359 (91.226)
Epoch: [30][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.4058 (2.0964)	Acc@1 56.641 (65.142)	Acc@5 88.672 (91.089)
Epoch: [30][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1084 (2.1022)	Acc@1 61.328 (64.882)	Acc@5 91.797 (91.072)
Epoch: [30][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1586 (2.1077)	Acc@1 64.453 (64.824)	Acc@5 91.797 (90.996)
Epoch: [30][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3271 (2.1132)	Acc@1 58.984 (64.751)	Acc@5 84.766 (90.852)
Epoch: [30][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.3686 (2.1215)	Acc@1 56.250 (64.520)	Acc@5 90.234 (90.755)
Epoch: [30][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3335 (2.1265)	Acc@1 60.938 (64.451)	Acc@5 87.891 (90.687)
Epoch: [30][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1029 (2.1339)	Acc@1 66.016 (64.278)	Acc@5 90.625 (90.596)
Epoch: [30][170/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.1610 (2.1346)	Acc@1 66.797 (64.261)	Acc@5 88.281 (90.570)
Epoch: [30][180/196]	Time 0.011 (0.015)	Data 0.009 (0.003)	Loss 2.1990 (2.1396)	Acc@1 64.453 (64.138)	Acc@5 88.281 (90.472)
Epoch: [30][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.3142 (2.1465)	Acc@1 59.766 (63.985)	Acc@5 87.891 (90.353)
num momentum params: 26
[0.1, 2.147142088165283, 1.817153742313385, 64.014, 52.81, tensor(0.4188, device='cuda:0', grad_fn=<DivBackward0>), 3.0217390060424805, 0.3818352222442627]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [31 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [31][0/196]	Time 0.033 (0.033)	Data 0.178 (0.178)	Loss 2.0694 (2.0694)	Acc@1 68.359 (68.359)	Acc@5 91.016 (91.016)
Epoch: [31][10/196]	Time 0.016 (0.017)	Data 0.002 (0.018)	Loss 1.9785 (2.0479)	Acc@1 71.484 (66.406)	Acc@5 91.797 (91.726)
Epoch: [31][20/196]	Time 0.013 (0.016)	Data 0.004 (0.010)	Loss 2.0861 (2.0700)	Acc@1 63.281 (65.402)	Acc@5 90.625 (91.518)
Epoch: [31][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.9940 (2.0466)	Acc@1 67.969 (66.331)	Acc@5 91.406 (91.583)
Epoch: [31][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.0141 (2.0465)	Acc@1 68.359 (66.321)	Acc@5 91.016 (91.597)
Epoch: [31][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.3057 (2.0480)	Acc@1 60.547 (66.506)	Acc@5 86.719 (91.590)
Epoch: [31][60/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0931 (2.0475)	Acc@1 67.969 (66.425)	Acc@5 90.625 (91.650)
Epoch: [31][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1128 (2.0462)	Acc@1 65.234 (66.467)	Acc@5 89.844 (91.610)
Epoch: [31][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0326 (2.0550)	Acc@1 66.016 (66.073)	Acc@5 92.188 (91.585)
Epoch: [31][90/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0700 (2.0648)	Acc@1 65.234 (65.956)	Acc@5 91.797 (91.419)
Epoch: [31][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0089 (2.0754)	Acc@1 69.531 (65.845)	Acc@5 92.188 (91.286)
Epoch: [31][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0913 (2.0813)	Acc@1 63.281 (65.709)	Acc@5 90.625 (91.178)
Epoch: [31][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0872 (2.0862)	Acc@1 66.797 (65.570)	Acc@5 91.797 (91.090)
Epoch: [31][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1549 (2.0904)	Acc@1 62.500 (65.407)	Acc@5 90.234 (91.090)
Epoch: [31][140/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 2.1807 (2.0964)	Acc@1 63.672 (65.331)	Acc@5 88.672 (90.985)
Epoch: [31][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1266 (2.1017)	Acc@1 65.234 (65.234)	Acc@5 91.406 (90.910)
Epoch: [31][160/196]	Time 0.011 (0.015)	Data 0.009 (0.003)	Loss 2.2194 (2.1081)	Acc@1 64.062 (65.079)	Acc@5 89.844 (90.860)
Epoch: [31][170/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.1893 (2.1130)	Acc@1 63.672 (64.988)	Acc@5 89.062 (90.767)
Epoch: [31][180/196]	Time 0.011 (0.015)	Data 0.009 (0.003)	Loss 2.1809 (2.1190)	Acc@1 60.938 (64.814)	Acc@5 91.797 (90.685)
Epoch: [31][190/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 2.3093 (2.1235)	Acc@1 57.031 (64.717)	Acc@5 86.719 (90.588)
num momentum params: 26
[0.1, 2.1272050826263427, 1.9081421160697938, 64.642, 51.13, tensor(0.4256, device='cuda:0', grad_fn=<DivBackward0>), 3.0055065155029297, 0.37667083740234375]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [32 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [32][0/196]	Time 0.036 (0.036)	Data 0.182 (0.182)	Loss 2.0293 (2.0293)	Acc@1 68.750 (68.750)	Acc@5 92.188 (92.188)
Epoch: [32][10/196]	Time 0.017 (0.018)	Data 0.000 (0.018)	Loss 2.0661 (2.1057)	Acc@1 66.406 (65.767)	Acc@5 93.750 (91.193)
Epoch: [32][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 2.3246 (2.1088)	Acc@1 58.984 (65.644)	Acc@5 88.672 (90.978)
Epoch: [32][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.2073 (2.0852)	Acc@1 64.844 (66.142)	Acc@5 88.672 (91.268)
Epoch: [32][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.1160 (2.0703)	Acc@1 65.625 (66.454)	Acc@5 91.406 (91.549)
Epoch: [32][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.1663 (2.0779)	Acc@1 65.625 (66.222)	Acc@5 88.672 (91.368)
Epoch: [32][60/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0865 (2.0880)	Acc@1 69.531 (65.830)	Acc@5 89.062 (91.246)
Epoch: [32][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0662 (2.1008)	Acc@1 66.797 (65.509)	Acc@5 91.406 (91.098)
Epoch: [32][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1964 (2.1075)	Acc@1 63.672 (65.408)	Acc@5 89.844 (91.045)
Epoch: [32][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9282 (2.1134)	Acc@1 72.266 (65.325)	Acc@5 92.969 (90.951)
Epoch: [32][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2470 (2.1159)	Acc@1 62.109 (65.196)	Acc@5 87.109 (90.900)
Epoch: [32][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1544 (2.1188)	Acc@1 64.062 (65.115)	Acc@5 89.062 (90.878)
Epoch: [32][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2320 (2.1215)	Acc@1 62.891 (65.070)	Acc@5 90.234 (90.857)
Epoch: [32][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1842 (2.1229)	Acc@1 64.844 (65.005)	Acc@5 87.891 (90.783)
Epoch: [32][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.9987 (2.1206)	Acc@1 69.531 (65.063)	Acc@5 92.188 (90.874)
Epoch: [32][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0961 (2.1227)	Acc@1 66.406 (65.017)	Acc@5 90.625 (90.858)
Epoch: [32][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2962 (2.1263)	Acc@1 60.547 (64.924)	Acc@5 89.062 (90.778)
Epoch: [32][170/196]	Time 0.018 (0.015)	Data 0.001 (0.003)	Loss 2.4266 (2.1280)	Acc@1 55.469 (64.910)	Acc@5 87.109 (90.753)
Epoch: [32][180/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 2.1095 (2.1287)	Acc@1 65.234 (64.893)	Acc@5 90.625 (90.698)
Epoch: [32][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.0844 (2.1303)	Acc@1 63.281 (64.846)	Acc@5 91.016 (90.697)
num momentum params: 26
[0.1, 2.135337011871338, 1.7903356862068176, 64.762, 53.55, tensor(0.4274, device='cuda:0', grad_fn=<DivBackward0>), 2.975234031677246, 0.38289809226989746]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [33 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [33][0/196]	Time 0.037 (0.037)	Data 0.177 (0.177)	Loss 2.0479 (2.0479)	Acc@1 67.578 (67.578)	Acc@5 91.016 (91.016)
Epoch: [33][10/196]	Time 0.018 (0.018)	Data 0.001 (0.018)	Loss 2.0668 (2.0557)	Acc@1 66.406 (67.507)	Acc@5 91.016 (92.259)
Epoch: [33][20/196]	Time 0.012 (0.016)	Data 0.006 (0.011)	Loss 2.0599 (2.0436)	Acc@1 68.359 (67.374)	Acc@5 90.234 (92.057)
Epoch: [33][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.9852 (2.0509)	Acc@1 69.531 (67.263)	Acc@5 92.188 (91.860)
Epoch: [33][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 2.1435 (2.0635)	Acc@1 64.453 (66.587)	Acc@5 88.672 (91.730)
Epoch: [33][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 2.0955 (2.0617)	Acc@1 64.062 (66.498)	Acc@5 92.578 (91.759)
Epoch: [33][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0203 (2.0557)	Acc@1 67.969 (66.784)	Acc@5 91.406 (91.726)
Epoch: [33][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1743 (2.0654)	Acc@1 66.406 (66.621)	Acc@5 89.844 (91.483)
Epoch: [33][80/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0486 (2.0732)	Acc@1 66.406 (66.435)	Acc@5 91.797 (91.315)
Epoch: [33][90/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2663 (2.0813)	Acc@1 59.375 (66.132)	Acc@5 89.844 (91.256)
Epoch: [33][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2553 (2.0858)	Acc@1 62.500 (66.097)	Acc@5 87.500 (91.190)
Epoch: [33][110/196]	Time 0.023 (0.015)	Data 0.000 (0.004)	Loss 2.2038 (2.0918)	Acc@1 61.719 (65.892)	Acc@5 89.453 (91.139)
Epoch: [33][120/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 1.9989 (2.0950)	Acc@1 68.359 (65.803)	Acc@5 91.797 (91.112)
Epoch: [33][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2748 (2.0993)	Acc@1 59.375 (65.738)	Acc@5 86.328 (91.034)
Epoch: [33][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1444 (2.1048)	Acc@1 63.672 (65.628)	Acc@5 91.797 (90.938)
Epoch: [33][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1932 (2.1094)	Acc@1 62.891 (65.532)	Acc@5 87.500 (90.902)
Epoch: [33][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3333 (2.1129)	Acc@1 58.594 (65.472)	Acc@5 89.062 (90.865)
Epoch: [33][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2774 (2.1137)	Acc@1 62.109 (65.433)	Acc@5 89.062 (90.824)
Epoch: [33][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.4488 (2.1198)	Acc@1 58.594 (65.219)	Acc@5 84.375 (90.746)
Epoch: [33][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2172 (2.1237)	Acc@1 64.844 (65.191)	Acc@5 90.625 (90.717)
num momentum params: 26
[0.1, 2.1237463356018065, 2.0172890532016754, 65.2, 48.93, tensor(0.4323, device='cuda:0', grad_fn=<DivBackward0>), 2.9740238189697266, 0.37735962867736816]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [34 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [34][0/196]	Time 0.045 (0.045)	Data 0.175 (0.175)	Loss 2.0538 (2.0538)	Acc@1 67.578 (67.578)	Acc@5 90.234 (90.234)
Epoch: [34][10/196]	Time 0.018 (0.019)	Data 0.001 (0.018)	Loss 1.8399 (2.0438)	Acc@1 73.438 (67.543)	Acc@5 94.922 (91.939)
Epoch: [34][20/196]	Time 0.011 (0.017)	Data 0.006 (0.010)	Loss 2.1433 (2.0510)	Acc@1 64.453 (67.262)	Acc@5 89.844 (91.983)
Epoch: [34][30/196]	Time 0.017 (0.017)	Data 0.001 (0.008)	Loss 2.1499 (2.0520)	Acc@1 61.328 (66.872)	Acc@5 90.625 (91.961)
Epoch: [34][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.9516 (2.0433)	Acc@1 69.531 (67.311)	Acc@5 92.578 (91.987)
Epoch: [34][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.1298 (2.0529)	Acc@1 68.359 (67.226)	Acc@5 89.062 (91.919)
Epoch: [34][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.0022 (2.0544)	Acc@1 65.625 (67.149)	Acc@5 94.141 (91.893)
Epoch: [34][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.0487 (2.0578)	Acc@1 66.016 (67.088)	Acc@5 91.406 (91.808)
Epoch: [34][80/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1551 (2.0668)	Acc@1 64.062 (66.768)	Acc@5 90.625 (91.686)
Epoch: [34][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2209 (2.0713)	Acc@1 61.719 (66.621)	Acc@5 89.453 (91.539)
Epoch: [34][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1480 (2.0749)	Acc@1 62.891 (66.627)	Acc@5 90.234 (91.410)
Epoch: [34][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0285 (2.0869)	Acc@1 68.359 (66.294)	Acc@5 92.969 (91.290)
Epoch: [34][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0312 (2.0896)	Acc@1 70.312 (66.177)	Acc@5 89.453 (91.264)
Epoch: [34][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1673 (2.0953)	Acc@1 64.062 (66.001)	Acc@5 90.234 (91.195)
Epoch: [34][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1968 (2.0981)	Acc@1 66.016 (65.941)	Acc@5 88.281 (91.149)
Epoch: [34][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0804 (2.1026)	Acc@1 67.969 (65.904)	Acc@5 92.578 (91.078)
Epoch: [34][160/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.1395 (2.1044)	Acc@1 66.016 (65.870)	Acc@5 90.625 (91.057)
Epoch: [34][170/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.2391 (2.1090)	Acc@1 60.547 (65.776)	Acc@5 91.406 (90.963)
Epoch: [34][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2690 (2.1118)	Acc@1 60.938 (65.675)	Acc@5 88.672 (90.908)
Epoch: [34][190/196]	Time 0.016 (0.015)	Data 0.000 (0.003)	Loss 1.9937 (2.1143)	Acc@1 69.141 (65.637)	Acc@5 92.188 (90.932)
num momentum params: 26
[0.1, 2.1163045890045167, 1.801086015701294, 65.556, 52.62, tensor(0.4366, device='cuda:0', grad_fn=<DivBackward0>), 3.0335726737976074, 0.37706542015075684]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [35 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [35][0/196]	Time 0.038 (0.038)	Data 0.168 (0.168)	Loss 1.9227 (1.9227)	Acc@1 68.359 (68.359)	Acc@5 94.531 (94.531)
Epoch: [35][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 1.9330 (2.0057)	Acc@1 72.656 (68.324)	Acc@5 92.969 (92.472)
Epoch: [35][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 1.9723 (1.9951)	Acc@1 70.312 (68.452)	Acc@5 92.969 (92.578)
Epoch: [35][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 2.0351 (2.0080)	Acc@1 68.750 (68.095)	Acc@5 91.406 (92.452)
Epoch: [35][40/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 2.0502 (1.9986)	Acc@1 67.969 (68.588)	Acc@5 93.750 (92.607)
Epoch: [35][50/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 2.1527 (2.0094)	Acc@1 62.891 (68.375)	Acc@5 91.016 (92.486)
Epoch: [35][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9699 (2.0104)	Acc@1 68.359 (68.110)	Acc@5 93.359 (92.482)
Epoch: [35][70/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1016 (2.0193)	Acc@1 64.844 (67.848)	Acc@5 88.672 (92.309)
Epoch: [35][80/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0691 (2.0369)	Acc@1 67.578 (67.289)	Acc@5 91.016 (92.144)
Epoch: [35][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0356 (2.0489)	Acc@1 71.875 (67.093)	Acc@5 92.578 (91.964)
Epoch: [35][100/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.0710 (2.0546)	Acc@1 70.703 (67.122)	Acc@5 92.969 (91.778)
Epoch: [35][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0212 (2.0587)	Acc@1 69.531 (66.906)	Acc@5 91.016 (91.712)
Epoch: [35][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1786 (2.0638)	Acc@1 65.234 (66.832)	Acc@5 90.625 (91.671)
Epoch: [35][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1165 (2.0656)	Acc@1 64.062 (66.731)	Acc@5 92.969 (91.716)
Epoch: [35][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0072 (2.0675)	Acc@1 71.484 (66.714)	Acc@5 92.188 (91.741)
Epoch: [35][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3794 (2.0745)	Acc@1 58.984 (66.543)	Acc@5 89.062 (91.639)
Epoch: [35][160/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.0854 (2.0791)	Acc@1 66.797 (66.460)	Acc@5 92.969 (91.595)
Epoch: [35][170/196]	Time 0.018 (0.015)	Data 0.000 (0.003)	Loss 2.2305 (2.0840)	Acc@1 62.109 (66.383)	Acc@5 87.500 (91.468)
Epoch: [35][180/196]	Time 0.017 (0.015)	Data 0.001 (0.003)	Loss 1.9660 (2.0897)	Acc@1 69.922 (66.208)	Acc@5 93.359 (91.413)
Epoch: [35][190/196]	Time 0.018 (0.015)	Data 0.000 (0.003)	Loss 2.1284 (2.0960)	Acc@1 64.453 (66.032)	Acc@5 90.625 (91.333)
num momentum params: 26
[0.1, 2.0986645652008056, 2.0436660552024843, 65.934, 49.68, tensor(0.4428, device='cuda:0', grad_fn=<DivBackward0>), 2.9842381477355957, 0.3795149326324463]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [36 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [36][0/196]	Time 0.038 (0.038)	Data 0.182 (0.182)	Loss 2.0123 (2.0123)	Acc@1 69.922 (69.922)	Acc@5 90.625 (90.625)
Epoch: [36][10/196]	Time 0.017 (0.017)	Data 0.001 (0.018)	Loss 2.0656 (2.0736)	Acc@1 66.797 (67.791)	Acc@5 89.453 (90.732)
Epoch: [36][20/196]	Time 0.011 (0.016)	Data 0.007 (0.011)	Loss 2.0657 (2.0589)	Acc@1 65.625 (67.541)	Acc@5 94.531 (91.685)
Epoch: [36][30/196]	Time 0.018 (0.016)	Data 0.000 (0.008)	Loss 1.9247 (2.0461)	Acc@1 73.828 (67.692)	Acc@5 92.969 (92.011)
Epoch: [36][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 2.0214 (2.0297)	Acc@1 68.359 (68.121)	Acc@5 92.969 (92.159)
Epoch: [36][50/196]	Time 0.021 (0.016)	Data 0.001 (0.006)	Loss 2.0714 (2.0271)	Acc@1 64.453 (68.183)	Acc@5 91.406 (92.180)
Epoch: [36][60/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9270 (2.0284)	Acc@1 66.797 (68.154)	Acc@5 95.312 (92.168)
Epoch: [36][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0654 (2.0419)	Acc@1 66.406 (67.804)	Acc@5 91.016 (91.945)
Epoch: [36][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.2250 (2.0524)	Acc@1 60.156 (67.448)	Acc@5 92.578 (91.869)
Epoch: [36][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0560 (2.0597)	Acc@1 67.188 (67.175)	Acc@5 92.969 (91.861)
Epoch: [36][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 1.9945 (2.0664)	Acc@1 69.141 (66.913)	Acc@5 91.406 (91.816)
Epoch: [36][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2683 (2.0778)	Acc@1 62.891 (66.656)	Acc@5 88.672 (91.646)
Epoch: [36][120/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.0547 (2.0825)	Acc@1 68.359 (66.529)	Acc@5 91.406 (91.597)
Epoch: [36][130/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.3148 (2.0846)	Acc@1 63.281 (66.490)	Acc@5 87.500 (91.529)
Epoch: [36][140/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.2994 (2.0936)	Acc@1 60.547 (66.226)	Acc@5 88.281 (91.423)
Epoch: [36][150/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.1111 (2.0967)	Acc@1 62.500 (66.142)	Acc@5 91.016 (91.388)
Epoch: [36][160/196]	Time 0.011 (0.015)	Data 0.020 (0.004)	Loss 2.1814 (2.1011)	Acc@1 64.062 (66.037)	Acc@5 89.453 (91.331)
Epoch: [36][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2325 (2.1053)	Acc@1 60.547 (65.979)	Acc@5 89.453 (91.271)
Epoch: [36][180/196]	Time 0.011 (0.015)	Data 0.017 (0.004)	Loss 2.2525 (2.1086)	Acc@1 63.672 (65.916)	Acc@5 91.016 (91.225)
Epoch: [36][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3252 (2.1130)	Acc@1 58.594 (65.795)	Acc@5 90.625 (91.179)
num momentum params: 26
[0.1, 2.1150291400146486, 1.9696595299243926, 65.788, 50.32, tensor(0.4419, device='cuda:0', grad_fn=<DivBackward0>), 2.9877071380615234, 0.3883543014526367]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [37 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [37][0/196]	Time 0.036 (0.036)	Data 0.185 (0.185)	Loss 1.9167 (1.9167)	Acc@1 71.094 (71.094)	Acc@5 93.359 (93.359)
Epoch: [37][10/196]	Time 0.015 (0.017)	Data 0.002 (0.019)	Loss 2.1691 (2.0495)	Acc@1 62.891 (66.903)	Acc@5 91.406 (92.507)
Epoch: [37][20/196]	Time 0.011 (0.016)	Data 0.009 (0.011)	Loss 1.8931 (2.0161)	Acc@1 74.219 (68.527)	Acc@5 93.359 (92.764)
Epoch: [37][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.1137 (2.0086)	Acc@1 67.188 (68.548)	Acc@5 91.406 (92.767)
Epoch: [37][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 2.0052 (2.0212)	Acc@1 67.969 (68.150)	Acc@5 93.750 (92.578)
Epoch: [37][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0434 (2.0312)	Acc@1 67.969 (67.785)	Acc@5 92.969 (92.417)
Epoch: [37][60/196]	Time 0.011 (0.016)	Data 0.008 (0.006)	Loss 2.0554 (2.0335)	Acc@1 69.531 (67.853)	Acc@5 91.406 (92.386)
Epoch: [37][70/196]	Time 0.015 (0.016)	Data 0.000 (0.005)	Loss 2.0735 (2.0442)	Acc@1 66.406 (67.595)	Acc@5 93.359 (92.380)
Epoch: [37][80/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.2514 (2.0570)	Acc@1 58.594 (67.197)	Acc@5 91.406 (92.318)
Epoch: [37][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2803 (2.0625)	Acc@1 63.672 (67.046)	Acc@5 86.719 (92.153)
Epoch: [37][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0184 (2.0663)	Acc@1 70.312 (66.990)	Acc@5 91.797 (92.075)
Epoch: [37][110/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1009 (2.0683)	Acc@1 67.969 (66.959)	Acc@5 94.531 (92.047)
Epoch: [37][120/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.2597 (2.0787)	Acc@1 64.453 (66.761)	Acc@5 87.500 (91.887)
Epoch: [37][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2482 (2.0827)	Acc@1 61.328 (66.672)	Acc@5 90.625 (91.803)
Epoch: [37][140/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1922 (2.0868)	Acc@1 65.625 (66.631)	Acc@5 88.672 (91.694)
Epoch: [37][150/196]	Time 0.021 (0.016)	Data 0.000 (0.004)	Loss 2.3227 (2.0909)	Acc@1 62.500 (66.569)	Acc@5 89.062 (91.593)
Epoch: [37][160/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0628 (2.0961)	Acc@1 69.141 (66.411)	Acc@5 93.359 (91.559)
Epoch: [37][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1203 (2.0963)	Acc@1 66.406 (66.361)	Acc@5 89.453 (91.559)
Epoch: [37][180/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.2881 (2.0981)	Acc@1 62.891 (66.281)	Acc@5 89.062 (91.534)
Epoch: [37][190/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.1916 (2.1026)	Acc@1 62.500 (66.187)	Acc@5 91.406 (91.492)
num momentum params: 26
[0.1, 2.10423254447937, 1.8448877131938934, 66.14, 52.68, tensor(0.4462, device='cuda:0', grad_fn=<DivBackward0>), 3.0317437648773193, 0.39939689636230463]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [38 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [38][0/196]	Time 0.039 (0.039)	Data 0.171 (0.171)	Loss 1.9299 (1.9299)	Acc@1 73.828 (73.828)	Acc@5 94.922 (94.922)
Epoch: [38][10/196]	Time 0.018 (0.017)	Data 0.001 (0.018)	Loss 2.2357 (2.0667)	Acc@1 62.500 (67.045)	Acc@5 89.844 (92.188)
Epoch: [38][20/196]	Time 0.011 (0.016)	Data 0.007 (0.011)	Loss 1.9827 (2.0757)	Acc@1 70.703 (67.057)	Acc@5 93.750 (92.057)
Epoch: [38][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0625 (2.0669)	Acc@1 69.141 (67.855)	Acc@5 90.234 (92.061)
Epoch: [38][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 2.0429 (2.0577)	Acc@1 68.359 (68.102)	Acc@5 90.625 (92.092)
Epoch: [38][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.0282 (2.0451)	Acc@1 66.797 (68.313)	Acc@5 93.359 (92.264)
Epoch: [38][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0778 (2.0434)	Acc@1 66.406 (68.206)	Acc@5 91.797 (92.360)
Epoch: [38][70/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.0036 (2.0429)	Acc@1 73.438 (68.167)	Acc@5 92.969 (92.402)
Epoch: [38][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0504 (2.0480)	Acc@1 65.234 (67.940)	Acc@5 94.922 (92.405)
Epoch: [38][90/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 1.8922 (2.0505)	Acc@1 74.219 (67.853)	Acc@5 95.703 (92.406)
Epoch: [38][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1035 (2.0556)	Acc@1 67.578 (67.752)	Acc@5 92.188 (92.311)
Epoch: [38][110/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0897 (2.0607)	Acc@1 66.797 (67.596)	Acc@5 91.406 (92.205)
Epoch: [38][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.4172 (2.0698)	Acc@1 57.812 (67.291)	Acc@5 89.062 (92.062)
Epoch: [38][130/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0750 (2.0763)	Acc@1 69.922 (67.134)	Acc@5 90.234 (91.946)
Epoch: [38][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 1.9900 (2.0787)	Acc@1 69.141 (67.052)	Acc@5 91.406 (91.883)
Epoch: [38][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2164 (2.0881)	Acc@1 63.281 (66.789)	Acc@5 91.406 (91.755)
Epoch: [38][160/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2213 (2.0934)	Acc@1 61.719 (66.608)	Acc@5 89.453 (91.676)
Epoch: [38][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2581 (2.0978)	Acc@1 61.328 (66.450)	Acc@5 89.844 (91.642)
Epoch: [38][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 1.9900 (2.0991)	Acc@1 67.969 (66.447)	Acc@5 94.141 (91.590)
Epoch: [38][190/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1579 (2.1019)	Acc@1 67.578 (66.400)	Acc@5 88.281 (91.533)
num momentum params: 26
[0.1, 2.1008409541320803, 1.8147332310676574, 66.442, 53.48, tensor(0.4495, device='cuda:0', grad_fn=<DivBackward0>), 2.932908535003662, 0.3806493282318115]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [39 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [39][0/196]	Time 0.040 (0.040)	Data 0.181 (0.181)	Loss 2.0423 (2.0423)	Acc@1 67.969 (67.969)	Acc@5 93.359 (93.359)
Epoch: [39][10/196]	Time 0.018 (0.018)	Data 0.001 (0.018)	Loss 1.9479 (2.0146)	Acc@1 73.047 (67.933)	Acc@5 91.797 (92.649)
Epoch: [39][20/196]	Time 0.014 (0.017)	Data 0.004 (0.011)	Loss 2.1181 (2.0357)	Acc@1 67.188 (67.374)	Acc@5 90.625 (92.243)
Epoch: [39][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.1251 (2.0463)	Acc@1 67.188 (67.188)	Acc@5 90.625 (92.351)
Epoch: [39][40/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 2.0947 (2.0507)	Acc@1 63.672 (67.216)	Acc@5 91.406 (92.407)
Epoch: [39][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.0483 (2.0416)	Acc@1 67.969 (67.525)	Acc@5 91.406 (92.463)
Epoch: [39][60/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0795 (2.0374)	Acc@1 68.750 (67.661)	Acc@5 91.797 (92.431)
Epoch: [39][70/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 1.9685 (2.0367)	Acc@1 69.922 (67.787)	Acc@5 91.797 (92.386)
Epoch: [39][80/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.0164 (2.0491)	Acc@1 69.531 (67.578)	Acc@5 93.750 (92.202)
Epoch: [39][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2055 (2.0532)	Acc@1 66.406 (67.544)	Acc@5 90.625 (92.110)
Epoch: [39][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0795 (2.0604)	Acc@1 66.016 (67.346)	Acc@5 93.359 (92.041)
Epoch: [39][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2314 (2.0693)	Acc@1 65.234 (67.159)	Acc@5 90.234 (91.973)
Epoch: [39][120/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0810 (2.0756)	Acc@1 67.188 (66.955)	Acc@5 91.406 (91.865)
Epoch: [39][130/196]	Time 0.019 (0.015)	Data 0.001 (0.005)	Loss 2.1470 (2.0760)	Acc@1 66.016 (66.907)	Acc@5 89.453 (91.862)
Epoch: [39][140/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.1441 (2.0782)	Acc@1 62.500 (66.852)	Acc@5 91.797 (91.825)
Epoch: [39][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1065 (2.0817)	Acc@1 68.359 (66.776)	Acc@5 88.672 (91.771)
Epoch: [39][160/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.1498 (2.0859)	Acc@1 69.141 (66.690)	Acc@5 89.844 (91.688)
Epoch: [39][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1545 (2.0903)	Acc@1 66.797 (66.614)	Acc@5 89.844 (91.564)
Epoch: [39][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1175 (2.0938)	Acc@1 63.672 (66.512)	Acc@5 89.844 (91.488)
Epoch: [39][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2572 (2.0976)	Acc@1 59.766 (66.427)	Acc@5 89.453 (91.433)
num momentum params: 26
[0.1, 2.098515005187988, 2.1300767314434053, 66.448, 47.79, tensor(0.4515, device='cuda:0', grad_fn=<DivBackward0>), 3.00010085105896, 0.38213634490966797]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [40 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [40][0/196]	Time 0.044 (0.044)	Data 0.179 (0.179)	Loss 1.8675 (1.8675)	Acc@1 71.875 (71.875)	Acc@5 92.578 (92.578)
Epoch: [40][10/196]	Time 0.017 (0.018)	Data 0.001 (0.018)	Loss 2.0820 (2.0270)	Acc@1 67.969 (69.070)	Acc@5 92.578 (92.898)
Epoch: [40][20/196]	Time 0.014 (0.017)	Data 0.004 (0.011)	Loss 1.9634 (2.0307)	Acc@1 70.703 (68.787)	Acc@5 93.750 (92.522)
Epoch: [40][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0741 (2.0262)	Acc@1 66.797 (69.002)	Acc@5 93.359 (92.717)
Epoch: [40][40/196]	Time 0.011 (0.016)	Data 0.022 (0.008)	Loss 2.0716 (2.0236)	Acc@1 65.625 (68.760)	Acc@5 91.797 (92.692)
Epoch: [40][50/196]	Time 0.018 (0.016)	Data 0.001 (0.007)	Loss 1.9564 (2.0305)	Acc@1 69.531 (68.375)	Acc@5 94.141 (92.639)
Epoch: [40][60/196]	Time 0.015 (0.016)	Data 0.009 (0.006)	Loss 1.7837 (2.0224)	Acc@1 73.438 (68.622)	Acc@5 95.703 (92.655)
Epoch: [40][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.9898 (2.0238)	Acc@1 69.141 (68.563)	Acc@5 92.578 (92.562)
Epoch: [40][80/196]	Time 0.015 (0.016)	Data 0.004 (0.005)	Loss 2.0970 (2.0280)	Acc@1 65.234 (68.451)	Acc@5 93.359 (92.564)
Epoch: [40][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1104 (2.0311)	Acc@1 64.844 (68.394)	Acc@5 94.922 (92.565)
Epoch: [40][100/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.2693 (2.0360)	Acc@1 57.422 (68.209)	Acc@5 91.797 (92.501)
Epoch: [40][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2401 (2.0418)	Acc@1 62.891 (68.022)	Acc@5 90.625 (92.406)
Epoch: [40][120/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0990 (2.0533)	Acc@1 64.062 (67.849)	Acc@5 89.844 (92.223)
Epoch: [40][130/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.1574 (2.0588)	Acc@1 65.234 (67.682)	Acc@5 91.016 (92.161)
Epoch: [40][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0921 (2.0622)	Acc@1 66.016 (67.617)	Acc@5 91.016 (92.127)
Epoch: [40][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1780 (2.0747)	Acc@1 66.016 (67.369)	Acc@5 89.844 (91.996)
Epoch: [40][160/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 2.1135 (2.0817)	Acc@1 64.844 (67.163)	Acc@5 93.359 (91.913)
Epoch: [40][170/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0574 (2.0875)	Acc@1 70.312 (67.000)	Acc@5 91.406 (91.865)
Epoch: [40][180/196]	Time 0.013 (0.015)	Data 0.018 (0.004)	Loss 2.3021 (2.0938)	Acc@1 62.500 (66.831)	Acc@5 88.672 (91.754)
Epoch: [40][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9467 (2.0974)	Acc@1 68.359 (66.736)	Acc@5 94.531 (91.738)
num momentum params: 26
[0.1, 2.0994154697418215, 1.957753630876541, 66.7, 50.96, tensor(0.4537, device='cuda:0', grad_fn=<DivBackward0>), 3.0292625427246094, 0.3894505500793457]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [41 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [41][0/196]	Time 0.039 (0.039)	Data 0.167 (0.167)	Loss 1.9866 (1.9866)	Acc@1 69.531 (69.531)	Acc@5 94.922 (94.922)
Epoch: [41][10/196]	Time 0.016 (0.018)	Data 0.002 (0.018)	Loss 1.9838 (2.0426)	Acc@1 71.875 (68.040)	Acc@5 93.359 (92.933)
Epoch: [41][20/196]	Time 0.012 (0.017)	Data 0.010 (0.011)	Loss 2.1827 (2.0508)	Acc@1 65.234 (67.876)	Acc@5 92.188 (92.913)
Epoch: [41][30/196]	Time 0.016 (0.017)	Data 0.001 (0.008)	Loss 1.9569 (2.0333)	Acc@1 69.141 (68.183)	Acc@5 91.016 (92.805)
Epoch: [41][40/196]	Time 0.012 (0.016)	Data 0.008 (0.007)	Loss 1.9720 (2.0400)	Acc@1 71.484 (68.007)	Acc@5 92.578 (92.626)
Epoch: [41][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9390 (2.0394)	Acc@1 69.531 (68.091)	Acc@5 91.797 (92.555)
Epoch: [41][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0296 (2.0390)	Acc@1 69.141 (68.276)	Acc@5 90.625 (92.585)
Epoch: [41][70/196]	Time 0.011 (0.016)	Data 0.013 (0.005)	Loss 2.0443 (2.0404)	Acc@1 66.797 (68.266)	Acc@5 91.016 (92.529)
Epoch: [41][80/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.2266 (2.0457)	Acc@1 65.625 (68.113)	Acc@5 90.234 (92.438)
Epoch: [41][90/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.1739 (2.0498)	Acc@1 63.672 (67.986)	Acc@5 91.797 (92.398)
Epoch: [41][100/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.0894 (2.0576)	Acc@1 66.406 (67.729)	Acc@5 91.797 (92.238)
Epoch: [41][110/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 2.0724 (2.0624)	Acc@1 68.750 (67.652)	Acc@5 93.359 (92.216)
Epoch: [41][120/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.2138 (2.0712)	Acc@1 64.453 (67.459)	Acc@5 91.016 (92.116)
Epoch: [41][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1966 (2.0763)	Acc@1 64.062 (67.343)	Acc@5 91.016 (92.053)
Epoch: [41][140/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.1591 (2.0820)	Acc@1 64.062 (67.188)	Acc@5 93.750 (92.021)
Epoch: [41][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1699 (2.0855)	Acc@1 67.188 (67.123)	Acc@5 93.359 (91.960)
Epoch: [41][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2146 (2.0925)	Acc@1 63.672 (66.947)	Acc@5 89.062 (91.814)
Epoch: [41][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2565 (2.0974)	Acc@1 66.406 (66.916)	Acc@5 89.453 (91.724)
Epoch: [41][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0485 (2.1017)	Acc@1 69.922 (66.808)	Acc@5 91.406 (91.685)
Epoch: [41][190/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.2321 (2.1063)	Acc@1 59.766 (66.652)	Acc@5 90.625 (91.643)
num momentum params: 26
[0.1, 2.1077461487579345, 1.813923546075821, 66.634, 53.24, tensor(0.4544, device='cuda:0', grad_fn=<DivBackward0>), 3.0213797092437744, 0.3909139633178711]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [42 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [42][0/196]	Time 0.041 (0.041)	Data 0.175 (0.175)	Loss 1.9427 (1.9427)	Acc@1 72.266 (72.266)	Acc@5 95.312 (95.312)
Epoch: [42][10/196]	Time 0.017 (0.018)	Data 0.000 (0.018)	Loss 2.0353 (2.0187)	Acc@1 67.969 (68.004)	Acc@5 92.578 (93.111)
Epoch: [42][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 1.9098 (2.0157)	Acc@1 72.266 (68.471)	Acc@5 94.922 (93.025)
Epoch: [42][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.1622 (2.0166)	Acc@1 60.156 (68.372)	Acc@5 92.188 (93.007)
Epoch: [42][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.0997 (2.0138)	Acc@1 65.625 (68.512)	Acc@5 91.406 (93.054)
Epoch: [42][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0289 (2.0191)	Acc@1 67.188 (68.436)	Acc@5 93.750 (92.923)
Epoch: [42][60/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 2.0828 (2.0284)	Acc@1 62.500 (68.327)	Acc@5 92.969 (92.706)
Epoch: [42][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0822 (2.0360)	Acc@1 66.406 (68.057)	Acc@5 92.578 (92.650)
Epoch: [42][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.0715 (2.0340)	Acc@1 66.406 (68.239)	Acc@5 91.797 (92.646)
Epoch: [42][90/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9960 (2.0399)	Acc@1 70.703 (68.033)	Acc@5 92.969 (92.608)
Epoch: [42][100/196]	Time 0.022 (0.015)	Data 0.005 (0.004)	Loss 2.0248 (2.0411)	Acc@1 67.578 (67.996)	Acc@5 94.922 (92.632)
Epoch: [42][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1916 (2.0417)	Acc@1 64.062 (68.011)	Acc@5 91.406 (92.634)
Epoch: [42][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1376 (2.0497)	Acc@1 66.406 (67.720)	Acc@5 91.016 (92.585)
Epoch: [42][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.9956 (2.0583)	Acc@1 69.922 (67.608)	Acc@5 92.578 (92.420)
Epoch: [42][140/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0418 (2.0643)	Acc@1 68.359 (67.442)	Acc@5 93.359 (92.379)
Epoch: [42][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1267 (2.0707)	Acc@1 64.062 (67.332)	Acc@5 90.234 (92.260)
Epoch: [42][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1228 (2.0764)	Acc@1 64.062 (67.161)	Acc@5 92.578 (92.207)
Epoch: [42][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0668 (2.0801)	Acc@1 67.578 (67.064)	Acc@5 91.797 (92.140)
Epoch: [42][180/196]	Time 0.012 (0.016)	Data 0.019 (0.004)	Loss 2.2252 (2.0858)	Acc@1 62.500 (66.900)	Acc@5 91.016 (92.067)
Epoch: [42][190/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2697 (2.0910)	Acc@1 58.984 (66.758)	Acc@5 91.016 (92.005)
num momentum params: 26
[0.1, 2.093783585662842, 2.0512883079051973, 66.714, 48.54, tensor(0.4590, device='cuda:0', grad_fn=<DivBackward0>), 3.045917272567749, 0.3884589672088623]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [43 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [43][0/196]	Time 0.042 (0.042)	Data 0.169 (0.169)	Loss 2.1678 (2.1678)	Acc@1 67.578 (67.578)	Acc@5 89.453 (89.453)
Epoch: [43][10/196]	Time 0.015 (0.017)	Data 0.003 (0.018)	Loss 2.0897 (2.0646)	Acc@1 64.062 (67.756)	Acc@5 93.750 (92.472)
Epoch: [43][20/196]	Time 0.013 (0.017)	Data 0.036 (0.012)	Loss 2.0790 (2.0681)	Acc@1 66.016 (67.429)	Acc@5 90.625 (92.578)
Epoch: [43][30/196]	Time 0.013 (0.016)	Data 0.005 (0.009)	Loss 2.1103 (2.0417)	Acc@1 65.625 (68.007)	Acc@5 93.359 (92.931)
Epoch: [43][40/196]	Time 0.019 (0.017)	Data 0.001 (0.008)	Loss 1.8684 (2.0212)	Acc@1 71.094 (68.455)	Acc@5 94.922 (93.255)
Epoch: [43][50/196]	Time 0.014 (0.017)	Data 0.005 (0.007)	Loss 2.0975 (2.0232)	Acc@1 66.406 (68.612)	Acc@5 94.141 (93.168)
Epoch: [43][60/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9768 (2.0233)	Acc@1 67.969 (68.468)	Acc@5 92.578 (93.122)
Epoch: [43][70/196]	Time 0.022 (0.017)	Data 0.000 (0.005)	Loss 2.0034 (2.0221)	Acc@1 70.312 (68.568)	Acc@5 92.188 (93.057)
Epoch: [43][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.2244 (2.0268)	Acc@1 63.672 (68.509)	Acc@5 91.797 (92.930)
Epoch: [43][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1130 (2.0372)	Acc@1 64.453 (68.171)	Acc@5 91.797 (92.737)
Epoch: [43][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2323 (2.0394)	Acc@1 62.500 (68.170)	Acc@5 92.188 (92.721)
Epoch: [43][110/196]	Time 0.020 (0.016)	Data 0.001 (0.004)	Loss 2.1122 (2.0492)	Acc@1 66.406 (67.990)	Acc@5 91.797 (92.561)
Epoch: [43][120/196]	Time 0.021 (0.016)	Data 0.000 (0.004)	Loss 2.3882 (2.0555)	Acc@1 64.453 (67.872)	Acc@5 86.328 (92.401)
Epoch: [43][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1938 (2.0637)	Acc@1 62.891 (67.656)	Acc@5 91.016 (92.286)
Epoch: [43][140/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.2473 (2.0683)	Acc@1 62.500 (67.584)	Acc@5 87.500 (92.218)
Epoch: [43][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.2355 (2.0753)	Acc@1 61.328 (67.438)	Acc@5 90.625 (92.120)
Epoch: [43][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2674 (2.0804)	Acc@1 66.797 (67.369)	Acc@5 91.406 (92.008)
Epoch: [43][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1155 (2.0851)	Acc@1 67.578 (67.306)	Acc@5 90.625 (91.920)
Epoch: [43][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0188 (2.0865)	Acc@1 69.141 (67.287)	Acc@5 96.094 (91.896)
Epoch: [43][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0985 (2.0854)	Acc@1 68.750 (67.325)	Acc@5 91.406 (91.891)
num momentum params: 26
[0.1, 2.0866500148773195, 1.8824111640453338, 67.284, 52.61, tensor(0.4621, device='cuda:0', grad_fn=<DivBackward0>), 3.1006112098693848, 0.3978419303894043]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [44 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [44][0/196]	Time 0.039 (0.039)	Data 0.179 (0.179)	Loss 1.9312 (1.9312)	Acc@1 71.484 (71.484)	Acc@5 94.922 (94.922)
Epoch: [44][10/196]	Time 0.021 (0.018)	Data 0.001 (0.018)	Loss 2.0937 (1.9769)	Acc@1 69.141 (70.987)	Acc@5 92.188 (93.359)
Epoch: [44][20/196]	Time 0.012 (0.016)	Data 0.006 (0.011)	Loss 1.9435 (1.9761)	Acc@1 69.141 (70.889)	Acc@5 95.312 (93.508)
Epoch: [44][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 2.1518 (1.9730)	Acc@1 65.234 (70.514)	Acc@5 90.234 (93.485)
Epoch: [44][40/196]	Time 0.012 (0.016)	Data 0.007 (0.007)	Loss 2.0387 (1.9820)	Acc@1 67.969 (70.265)	Acc@5 93.359 (93.445)
Epoch: [44][50/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.2047 (1.9923)	Acc@1 64.062 (70.006)	Acc@5 92.969 (93.183)
Epoch: [44][60/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9593 (1.9933)	Acc@1 69.922 (69.947)	Acc@5 92.969 (93.116)
Epoch: [44][70/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 1.9343 (2.0010)	Acc@1 71.875 (69.625)	Acc@5 94.531 (92.991)
Epoch: [44][80/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9489 (2.0056)	Acc@1 72.266 (69.579)	Acc@5 93.359 (92.925)
Epoch: [44][90/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.1358 (2.0094)	Acc@1 66.406 (69.501)	Acc@5 93.359 (92.913)
Epoch: [44][100/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0595 (2.0171)	Acc@1 66.016 (69.299)	Acc@5 92.969 (92.810)
Epoch: [44][110/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 2.0017 (2.0250)	Acc@1 71.484 (69.046)	Acc@5 91.406 (92.789)
Epoch: [44][120/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1387 (2.0342)	Acc@1 66.016 (68.798)	Acc@5 90.625 (92.720)
Epoch: [44][130/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1137 (2.0393)	Acc@1 71.094 (68.714)	Acc@5 92.188 (92.677)
Epoch: [44][140/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0654 (2.0440)	Acc@1 64.844 (68.531)	Acc@5 94.922 (92.617)
Epoch: [44][150/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.0768 (2.0506)	Acc@1 66.406 (68.349)	Acc@5 92.188 (92.542)
Epoch: [44][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2400 (2.0560)	Acc@1 64.844 (68.170)	Acc@5 91.406 (92.488)
Epoch: [44][170/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1457 (2.0596)	Acc@1 66.797 (68.115)	Acc@5 92.969 (92.439)
Epoch: [44][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8778 (2.0621)	Acc@1 76.562 (68.072)	Acc@5 94.531 (92.425)
Epoch: [44][190/196]	Time 0.011 (0.016)	Data 0.012 (0.004)	Loss 2.1008 (2.0656)	Acc@1 64.453 (67.971)	Acc@5 93.359 (92.367)
num momentum params: 26
[0.1, 2.067147061920166, 1.8610251128673554, 67.928, 52.68, tensor(0.4673, device='cuda:0', grad_fn=<DivBackward0>), 3.0694072246551514, 0.38623690605163574]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [45 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [45][0/196]	Time 0.039 (0.039)	Data 0.187 (0.187)	Loss 1.9738 (1.9738)	Acc@1 70.703 (70.703)	Acc@5 94.531 (94.531)
Epoch: [45][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.9943 (1.9841)	Acc@1 66.016 (69.886)	Acc@5 93.750 (93.821)
Epoch: [45][20/196]	Time 0.011 (0.017)	Data 0.011 (0.011)	Loss 1.8594 (1.9989)	Acc@1 74.219 (69.624)	Acc@5 93.750 (93.527)
Epoch: [45][30/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 2.1152 (1.9930)	Acc@1 66.797 (69.720)	Acc@5 91.016 (93.523)
Epoch: [45][40/196]	Time 0.012 (0.016)	Data 0.009 (0.007)	Loss 2.0602 (2.0046)	Acc@1 65.625 (69.255)	Acc@5 93.359 (93.464)
Epoch: [45][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9884 (2.0024)	Acc@1 69.922 (69.516)	Acc@5 93.359 (93.390)
Epoch: [45][60/196]	Time 0.012 (0.016)	Data 0.013 (0.006)	Loss 1.9438 (2.0017)	Acc@1 68.359 (69.371)	Acc@5 94.922 (93.449)
Epoch: [45][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9974 (2.0085)	Acc@1 70.703 (68.965)	Acc@5 94.141 (93.436)
Epoch: [45][80/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.1309 (2.0163)	Acc@1 67.578 (68.846)	Acc@5 91.406 (93.335)
Epoch: [45][90/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.0680 (2.0296)	Acc@1 66.016 (68.557)	Acc@5 94.531 (93.179)
Epoch: [45][100/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1893 (2.0428)	Acc@1 66.797 (68.301)	Acc@5 92.578 (92.922)
Epoch: [45][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3099 (2.0531)	Acc@1 62.891 (68.219)	Acc@5 86.328 (92.701)
Epoch: [45][120/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.0346 (2.0578)	Acc@1 70.312 (68.143)	Acc@5 93.750 (92.617)
Epoch: [45][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0174 (2.0615)	Acc@1 73.828 (68.064)	Acc@5 91.797 (92.554)
Epoch: [45][140/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1461 (2.0664)	Acc@1 66.406 (67.955)	Acc@5 89.844 (92.431)
Epoch: [45][150/196]	Time 0.015 (0.016)	Data 0.001 (0.004)	Loss 2.1028 (2.0695)	Acc@1 67.578 (67.868)	Acc@5 92.188 (92.394)
Epoch: [45][160/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0193 (2.0723)	Acc@1 71.094 (67.772)	Acc@5 91.797 (92.372)
Epoch: [45][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2357 (2.0752)	Acc@1 64.844 (67.740)	Acc@5 87.500 (92.293)
Epoch: [45][180/196]	Time 0.011 (0.016)	Data 0.007 (0.003)	Loss 2.1164 (2.0802)	Acc@1 67.578 (67.649)	Acc@5 92.188 (92.224)
Epoch: [45][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0433 (2.0817)	Acc@1 69.141 (67.607)	Acc@5 93.750 (92.190)
num momentum params: 26
[0.1, 2.0826781882476806, 1.728976867198944, 67.55, 54.85, tensor(0.4655, device='cuda:0', grad_fn=<DivBackward0>), 3.0853965282440186, 0.3941028118133545]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [46 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [46][0/196]	Time 0.042 (0.042)	Data 0.185 (0.185)	Loss 1.9296 (1.9296)	Acc@1 69.531 (69.531)	Acc@5 96.094 (96.094)
Epoch: [46][10/196]	Time 0.017 (0.018)	Data 0.003 (0.019)	Loss 1.9932 (2.0144)	Acc@1 69.141 (69.460)	Acc@5 91.797 (92.898)
Epoch: [46][20/196]	Time 0.011 (0.017)	Data 0.006 (0.012)	Loss 1.9579 (2.0244)	Acc@1 70.703 (69.141)	Acc@5 93.750 (92.801)
Epoch: [46][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.7521 (1.9903)	Acc@1 77.344 (70.350)	Acc@5 96.484 (93.233)
Epoch: [46][40/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 2.0058 (1.9853)	Acc@1 66.797 (70.179)	Acc@5 94.531 (93.331)
Epoch: [46][50/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 1.9177 (1.9959)	Acc@1 72.266 (69.930)	Acc@5 94.922 (93.222)
Epoch: [46][60/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 2.0642 (1.9939)	Acc@1 67.578 (69.954)	Acc@5 91.406 (93.276)
Epoch: [46][70/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9369 (1.9973)	Acc@1 71.094 (69.812)	Acc@5 94.141 (93.216)
Epoch: [46][80/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0096 (1.9966)	Acc@1 68.359 (69.753)	Acc@5 94.141 (93.248)
Epoch: [46][90/196]	Time 0.014 (0.015)	Data 0.022 (0.005)	Loss 2.1088 (2.0082)	Acc@1 66.797 (69.536)	Acc@5 93.750 (93.106)
Epoch: [46][100/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0674 (2.0269)	Acc@1 66.016 (69.079)	Acc@5 92.578 (92.849)
Epoch: [46][110/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.0128 (2.0320)	Acc@1 69.531 (68.982)	Acc@5 92.969 (92.800)
Epoch: [46][120/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.2085 (2.0376)	Acc@1 65.625 (68.850)	Acc@5 90.625 (92.782)
Epoch: [46][130/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.2596 (2.0476)	Acc@1 62.891 (68.523)	Acc@5 88.672 (92.671)
Epoch: [46][140/196]	Time 0.019 (0.015)	Data 0.000 (0.004)	Loss 2.1409 (2.0557)	Acc@1 67.578 (68.348)	Acc@5 89.844 (92.550)
Epoch: [46][150/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0636 (2.0615)	Acc@1 67.188 (68.181)	Acc@5 93.359 (92.472)
Epoch: [46][160/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0786 (2.0639)	Acc@1 66.797 (68.131)	Acc@5 92.969 (92.413)
Epoch: [46][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1012 (2.0718)	Acc@1 64.453 (67.914)	Acc@5 92.188 (92.297)
Epoch: [46][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2229 (2.0784)	Acc@1 65.234 (67.783)	Acc@5 89.062 (92.170)
Epoch: [46][190/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1046 (2.0835)	Acc@1 67.188 (67.674)	Acc@5 92.578 (92.132)
num momentum params: 26
[0.1, 2.0871352281951903, 1.8211045944690705, 67.586, 52.95, tensor(0.4661, device='cuda:0', grad_fn=<DivBackward0>), 3.016566276550293, 0.38352131843566895]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [47 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [47][0/196]	Time 0.042 (0.042)	Data 0.172 (0.172)	Loss 1.9095 (1.9095)	Acc@1 73.828 (73.828)	Acc@5 93.750 (93.750)
Epoch: [47][10/196]	Time 0.017 (0.018)	Data 0.001 (0.018)	Loss 1.9544 (2.0311)	Acc@1 73.047 (69.460)	Acc@5 92.188 (92.827)
Epoch: [47][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 2.0582 (2.0299)	Acc@1 66.406 (69.494)	Acc@5 90.625 (92.671)
Epoch: [47][30/196]	Time 0.019 (0.016)	Data 0.000 (0.008)	Loss 2.0996 (2.0268)	Acc@1 66.797 (69.216)	Acc@5 91.016 (92.755)
Epoch: [47][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 1.8803 (2.0231)	Acc@1 75.000 (69.264)	Acc@5 94.141 (92.873)
Epoch: [47][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0651 (2.0267)	Acc@1 71.875 (69.018)	Acc@5 90.625 (92.831)
Epoch: [47][60/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2272 (2.0271)	Acc@1 66.016 (69.096)	Acc@5 89.844 (92.828)
Epoch: [47][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1041 (2.0328)	Acc@1 68.750 (68.822)	Acc@5 91.016 (92.705)
Epoch: [47][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1907 (2.0416)	Acc@1 65.625 (68.663)	Acc@5 89.062 (92.641)
Epoch: [47][90/196]	Time 0.019 (0.015)	Data 0.002 (0.004)	Loss 2.1441 (2.0478)	Acc@1 63.672 (68.402)	Acc@5 92.578 (92.647)
Epoch: [47][100/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.0107 (2.0547)	Acc@1 67.578 (68.216)	Acc@5 92.969 (92.547)
Epoch: [47][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0754 (2.0616)	Acc@1 67.188 (68.106)	Acc@5 91.797 (92.448)
Epoch: [47][120/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.1672 (2.0650)	Acc@1 64.062 (68.062)	Acc@5 90.234 (92.368)
Epoch: [47][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0387 (2.0670)	Acc@1 70.703 (68.052)	Acc@5 93.750 (92.337)
Epoch: [47][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.0222 (2.0665)	Acc@1 71.875 (68.154)	Acc@5 93.359 (92.337)
Epoch: [47][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0718 (2.0678)	Acc@1 66.797 (68.121)	Acc@5 95.703 (92.322)
Epoch: [47][160/196]	Time 0.029 (0.015)	Data 0.003 (0.004)	Loss 2.2455 (2.0718)	Acc@1 63.281 (68.029)	Acc@5 89.844 (92.214)
Epoch: [47][170/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 2.0448 (2.0753)	Acc@1 71.094 (67.985)	Acc@5 89.453 (92.108)
Epoch: [47][180/196]	Time 0.014 (0.015)	Data 0.009 (0.004)	Loss 2.0885 (2.0768)	Acc@1 64.844 (67.882)	Acc@5 92.578 (92.116)
Epoch: [47][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1044 (2.0791)	Acc@1 67.188 (67.801)	Acc@5 91.797 (92.128)
num momentum params: 26
[0.1, 2.082575601272583, 1.9256843781471253, 67.706, 51.75, tensor(0.4685, device='cuda:0', grad_fn=<DivBackward0>), 3.0353286266326904, 0.3917825222015381]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [48 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [48][0/196]	Time 0.040 (0.040)	Data 0.176 (0.176)	Loss 2.0049 (2.0049)	Acc@1 69.531 (69.531)	Acc@5 94.922 (94.922)
Epoch: [48][10/196]	Time 0.019 (0.018)	Data 0.000 (0.018)	Loss 2.2172 (2.0733)	Acc@1 63.672 (68.040)	Acc@5 90.625 (92.685)
Epoch: [48][20/196]	Time 0.010 (0.016)	Data 0.006 (0.011)	Loss 1.9032 (2.0542)	Acc@1 70.703 (68.359)	Acc@5 94.141 (92.634)
Epoch: [48][30/196]	Time 0.015 (0.016)	Data 0.002 (0.009)	Loss 1.9637 (2.0397)	Acc@1 70.312 (69.103)	Acc@5 93.750 (92.641)
Epoch: [48][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 2.0170 (2.0478)	Acc@1 71.484 (68.721)	Acc@5 92.578 (92.683)
Epoch: [48][50/196]	Time 0.012 (0.015)	Data 0.008 (0.006)	Loss 2.0716 (2.0370)	Acc@1 64.453 (68.873)	Acc@5 91.797 (92.777)
Epoch: [48][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 1.9770 (2.0357)	Acc@1 71.875 (69.000)	Acc@5 92.188 (92.834)
Epoch: [48][70/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 1.8882 (2.0368)	Acc@1 71.484 (68.915)	Acc@5 95.312 (92.842)
Epoch: [48][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1668 (2.0320)	Acc@1 63.281 (69.054)	Acc@5 92.188 (92.906)
Epoch: [48][90/196]	Time 0.012 (0.015)	Data 0.015 (0.005)	Loss 2.1155 (2.0340)	Acc@1 65.625 (69.003)	Acc@5 89.844 (92.788)
Epoch: [48][100/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2222 (2.0407)	Acc@1 64.453 (68.843)	Acc@5 90.625 (92.686)
Epoch: [48][110/196]	Time 0.017 (0.015)	Data 0.008 (0.005)	Loss 2.2294 (2.0468)	Acc@1 62.500 (68.637)	Acc@5 90.625 (92.610)
Epoch: [48][120/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0624 (2.0547)	Acc@1 69.141 (68.440)	Acc@5 93.750 (92.494)
Epoch: [48][130/196]	Time 0.011 (0.015)	Data 0.011 (0.004)	Loss 2.0768 (2.0595)	Acc@1 67.578 (68.276)	Acc@5 92.969 (92.438)
Epoch: [48][140/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 2.1331 (2.0628)	Acc@1 69.531 (68.138)	Acc@5 91.016 (92.404)
Epoch: [48][150/196]	Time 0.012 (0.015)	Data 0.019 (0.004)	Loss 2.0927 (2.0670)	Acc@1 66.016 (68.002)	Acc@5 92.578 (92.366)
Epoch: [48][160/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1759 (2.0722)	Acc@1 66.797 (67.867)	Acc@5 90.234 (92.294)
Epoch: [48][170/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.1188 (2.0790)	Acc@1 66.797 (67.733)	Acc@5 89.453 (92.242)
Epoch: [48][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.3462 (2.0849)	Acc@1 61.328 (67.643)	Acc@5 89.453 (92.151)
Epoch: [48][190/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1209 (2.0874)	Acc@1 67.188 (67.582)	Acc@5 88.672 (92.112)
num momentum params: 26
[0.1, 2.0874447140502927, 1.816850529909134, 67.584, 53.41, tensor(0.4690, device='cuda:0', grad_fn=<DivBackward0>), 3.017129421234131, 0.38150525093078613]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [49 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [49][0/196]	Time 0.042 (0.042)	Data 0.212 (0.212)	Loss 2.0578 (2.0578)	Acc@1 67.188 (67.188)	Acc@5 93.750 (93.750)
Epoch: [49][10/196]	Time 0.016 (0.018)	Data 0.002 (0.021)	Loss 2.0668 (2.0226)	Acc@1 68.750 (68.786)	Acc@5 90.625 (93.537)
Epoch: [49][20/196]	Time 0.015 (0.017)	Data 0.002 (0.012)	Loss 1.8215 (1.9823)	Acc@1 75.391 (69.847)	Acc@5 94.531 (93.750)
Epoch: [49][30/196]	Time 0.016 (0.016)	Data 0.002 (0.009)	Loss 1.9372 (1.9720)	Acc@1 71.875 (70.262)	Acc@5 94.922 (93.851)
Epoch: [49][40/196]	Time 0.016 (0.016)	Data 0.003 (0.007)	Loss 1.9244 (1.9661)	Acc@1 72.266 (70.484)	Acc@5 94.141 (93.798)
Epoch: [49][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 1.9794 (1.9679)	Acc@1 72.656 (70.535)	Acc@5 94.531 (93.735)
Epoch: [49][60/196]	Time 0.011 (0.016)	Data 0.009 (0.006)	Loss 1.9515 (1.9739)	Acc@1 71.094 (70.332)	Acc@5 92.578 (93.718)
Epoch: [49][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1218 (1.9842)	Acc@1 66.797 (69.993)	Acc@5 92.578 (93.585)
Epoch: [49][80/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.1133 (1.9944)	Acc@1 67.969 (69.748)	Acc@5 92.188 (93.446)
Epoch: [49][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2188 (2.0124)	Acc@1 64.453 (69.291)	Acc@5 88.672 (93.243)
Epoch: [49][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2970 (2.0207)	Acc@1 65.625 (69.179)	Acc@5 89.062 (93.096)
Epoch: [49][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0317 (2.0298)	Acc@1 69.531 (68.877)	Acc@5 90.625 (92.986)
Epoch: [49][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0703 (2.0378)	Acc@1 71.875 (68.718)	Acc@5 91.406 (92.853)
Epoch: [49][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0338 (2.0412)	Acc@1 69.922 (68.637)	Acc@5 92.578 (92.811)
Epoch: [49][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0236 (2.0429)	Acc@1 70.703 (68.562)	Acc@5 92.969 (92.803)
Epoch: [49][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0724 (2.0483)	Acc@1 68.750 (68.398)	Acc@5 92.188 (92.754)
Epoch: [49][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0165 (2.0504)	Acc@1 68.750 (68.381)	Acc@5 93.750 (92.724)
Epoch: [49][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2129 (2.0539)	Acc@1 64.453 (68.359)	Acc@5 91.016 (92.642)
Epoch: [49][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1298 (2.0593)	Acc@1 66.797 (68.213)	Acc@5 91.406 (92.554)
Epoch: [49][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0803 (2.0623)	Acc@1 69.141 (68.147)	Acc@5 92.188 (92.527)
num momentum params: 26
[0.1, 2.0643107012939454, 1.8953933596611023, 68.098, 52.68, tensor(0.4752, device='cuda:0', grad_fn=<DivBackward0>), 3.0515551567077637, 0.39813113212585455]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [50 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [50][0/196]	Time 0.044 (0.044)	Data 0.181 (0.181)	Loss 2.1239 (2.1239)	Acc@1 66.797 (66.797)	Acc@5 91.406 (91.406)
Epoch: [50][10/196]	Time 0.017 (0.019)	Data 0.001 (0.018)	Loss 2.1077 (2.1355)	Acc@1 71.094 (66.655)	Acc@5 92.578 (91.832)
Epoch: [50][20/196]	Time 0.015 (0.017)	Data 0.003 (0.011)	Loss 2.1863 (2.0924)	Acc@1 66.016 (67.690)	Acc@5 92.969 (92.485)
Epoch: [50][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9101 (2.0615)	Acc@1 71.875 (68.284)	Acc@5 94.531 (92.717)
Epoch: [50][40/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 1.9661 (2.0559)	Acc@1 72.656 (68.588)	Acc@5 93.750 (92.873)
Epoch: [50][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1582 (2.0550)	Acc@1 63.672 (68.528)	Acc@5 93.359 (92.900)
Epoch: [50][60/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 1.9122 (2.0487)	Acc@1 72.266 (68.564)	Acc@5 92.969 (93.001)
Epoch: [50][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0666 (2.0447)	Acc@1 68.359 (68.744)	Acc@5 93.359 (93.084)
Epoch: [50][80/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 1.9311 (2.0478)	Acc@1 71.094 (68.620)	Acc@5 94.141 (92.998)
Epoch: [50][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2386 (2.0557)	Acc@1 66.016 (68.454)	Acc@5 87.891 (92.806)
Epoch: [50][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.0060 (2.0543)	Acc@1 71.484 (68.541)	Acc@5 91.797 (92.772)
Epoch: [50][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0872 (2.0524)	Acc@1 69.531 (68.623)	Acc@5 92.969 (92.765)
Epoch: [50][120/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 1.9860 (2.0563)	Acc@1 71.094 (68.472)	Acc@5 91.016 (92.659)
Epoch: [50][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1349 (2.0545)	Acc@1 66.406 (68.500)	Acc@5 91.797 (92.668)
Epoch: [50][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1123 (2.0581)	Acc@1 66.406 (68.426)	Acc@5 92.188 (92.584)
Epoch: [50][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9507 (2.0609)	Acc@1 72.266 (68.297)	Acc@5 92.578 (92.560)
Epoch: [50][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.1363 (2.0624)	Acc@1 61.719 (68.250)	Acc@5 92.188 (92.549)
Epoch: [50][170/196]	Time 0.019 (0.015)	Data 0.000 (0.004)	Loss 2.0066 (2.0643)	Acc@1 69.141 (68.245)	Acc@5 91.406 (92.473)
Epoch: [50][180/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.0712 (2.0682)	Acc@1 68.750 (68.144)	Acc@5 92.969 (92.442)
Epoch: [50][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1875 (2.0744)	Acc@1 66.016 (67.987)	Acc@5 93.750 (92.400)
num momentum params: 26
[0.1, 2.0761568901062013, 1.8739441001415253, 67.924, 53.13, tensor(0.4735, device='cuda:0', grad_fn=<DivBackward0>), 2.937462329864502, 0.3824174404144287]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [51 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [51][0/196]	Time 0.045 (0.045)	Data 0.181 (0.181)	Loss 1.9555 (1.9555)	Acc@1 70.312 (70.312)	Acc@5 92.578 (92.578)
Epoch: [51][10/196]	Time 0.018 (0.019)	Data 0.003 (0.019)	Loss 1.7722 (1.9967)	Acc@1 75.781 (70.348)	Acc@5 97.656 (93.501)
Epoch: [51][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 1.9279 (2.0041)	Acc@1 72.266 (70.089)	Acc@5 94.141 (93.452)
Epoch: [51][30/196]	Time 0.013 (0.016)	Data 0.008 (0.009)	Loss 2.0183 (1.9920)	Acc@1 66.797 (70.464)	Acc@5 94.141 (93.599)
Epoch: [51][40/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 2.0691 (1.9899)	Acc@1 66.016 (70.341)	Acc@5 92.188 (93.655)
Epoch: [51][50/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0916 (1.9882)	Acc@1 65.625 (70.351)	Acc@5 90.234 (93.620)
Epoch: [51][60/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.9024 (1.9873)	Acc@1 72.656 (70.274)	Acc@5 92.578 (93.571)
Epoch: [51][70/196]	Time 0.022 (0.016)	Data 0.000 (0.005)	Loss 2.0189 (1.9925)	Acc@1 69.531 (70.175)	Acc@5 92.188 (93.519)
Epoch: [51][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9913 (1.9986)	Acc@1 70.703 (70.057)	Acc@5 93.750 (93.403)
Epoch: [51][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0387 (2.0065)	Acc@1 69.141 (69.797)	Acc@5 92.188 (93.295)
Epoch: [51][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0245 (2.0120)	Acc@1 69.531 (69.605)	Acc@5 94.531 (93.286)
Epoch: [51][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0640 (2.0167)	Acc@1 69.922 (69.461)	Acc@5 91.406 (93.282)
Epoch: [51][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9644 (2.0254)	Acc@1 72.266 (69.228)	Acc@5 92.969 (93.224)
Epoch: [51][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0717 (2.0291)	Acc@1 69.531 (69.147)	Acc@5 90.625 (93.139)
Epoch: [51][140/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0639 (2.0363)	Acc@1 69.531 (69.049)	Acc@5 92.578 (93.074)
Epoch: [51][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2551 (2.0436)	Acc@1 62.109 (68.892)	Acc@5 90.625 (92.958)
Epoch: [51][160/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1887 (2.0479)	Acc@1 64.062 (68.772)	Acc@5 91.016 (92.903)
Epoch: [51][170/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 1.9501 (2.0477)	Acc@1 69.141 (68.768)	Acc@5 93.750 (92.871)
Epoch: [51][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0360 (2.0517)	Acc@1 72.266 (68.759)	Acc@5 93.359 (92.852)
Epoch: [51][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2639 (2.0598)	Acc@1 64.453 (68.513)	Acc@5 91.406 (92.748)
num momentum params: 26
[0.1, 2.0624547100830077, 1.7233954882621765, 68.444, 54.53, tensor(0.4781, device='cuda:0', grad_fn=<DivBackward0>), 3.078846931457519, 0.38367748260498047]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [52 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [52][0/196]	Time 0.043 (0.043)	Data 0.176 (0.176)	Loss 2.0596 (2.0596)	Acc@1 69.531 (69.531)	Acc@5 94.531 (94.531)
Epoch: [52][10/196]	Time 0.017 (0.019)	Data 0.001 (0.018)	Loss 1.8984 (1.9581)	Acc@1 71.484 (71.484)	Acc@5 96.094 (93.643)
Epoch: [52][20/196]	Time 0.015 (0.017)	Data 0.003 (0.011)	Loss 1.9964 (1.9582)	Acc@1 70.312 (71.131)	Acc@5 94.531 (93.880)
Epoch: [52][30/196]	Time 0.014 (0.017)	Data 0.002 (0.008)	Loss 1.8190 (1.9634)	Acc@1 73.438 (71.031)	Acc@5 96.094 (93.901)
Epoch: [52][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.0641 (1.9738)	Acc@1 69.141 (70.617)	Acc@5 94.531 (93.836)
Epoch: [52][50/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.9835 (1.9734)	Acc@1 71.875 (70.642)	Acc@5 91.406 (93.735)
Epoch: [52][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9792 (1.9792)	Acc@1 70.703 (70.485)	Acc@5 94.922 (93.654)
Epoch: [52][70/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.0579 (1.9877)	Acc@1 66.406 (70.384)	Acc@5 94.531 (93.541)
Epoch: [52][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.1213 (1.9957)	Acc@1 65.234 (70.086)	Acc@5 92.188 (93.475)
Epoch: [52][90/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3391 (2.0079)	Acc@1 59.766 (69.742)	Acc@5 90.625 (93.308)
Epoch: [52][100/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.0692 (2.0100)	Acc@1 68.359 (69.756)	Acc@5 92.969 (93.228)
Epoch: [52][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1263 (2.0132)	Acc@1 67.188 (69.640)	Acc@5 90.234 (93.212)
Epoch: [52][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0944 (2.0196)	Acc@1 68.359 (69.583)	Acc@5 89.844 (93.114)
Epoch: [52][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1572 (2.0303)	Acc@1 64.062 (69.370)	Acc@5 93.750 (92.975)
Epoch: [52][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0942 (2.0388)	Acc@1 65.625 (69.124)	Acc@5 95.312 (92.930)
Epoch: [52][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9356 (2.0408)	Acc@1 71.094 (69.047)	Acc@5 94.141 (92.914)
Epoch: [52][160/196]	Time 0.011 (0.015)	Data 0.005 (0.004)	Loss 1.8865 (2.0458)	Acc@1 72.656 (68.908)	Acc@5 95.312 (92.862)
Epoch: [52][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2696 (2.0499)	Acc@1 62.891 (68.761)	Acc@5 91.016 (92.809)
Epoch: [52][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.2233 (2.0538)	Acc@1 67.188 (68.657)	Acc@5 89.062 (92.785)
Epoch: [52][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0042 (2.0578)	Acc@1 72.656 (68.566)	Acc@5 92.969 (92.727)
num momentum params: 26
[0.1, 2.05970120223999, 1.9808702099323272, 68.518, 51.16, tensor(0.4802, device='cuda:0', grad_fn=<DivBackward0>), 2.969766616821289, 0.3887467384338379]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [53 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [53][0/196]	Time 0.042 (0.042)	Data 0.182 (0.182)	Loss 2.0782 (2.0782)	Acc@1 69.141 (69.141)	Acc@5 90.234 (90.234)
Epoch: [53][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 1.9478 (1.9961)	Acc@1 72.656 (70.419)	Acc@5 92.578 (93.004)
Epoch: [53][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 1.9574 (1.9618)	Acc@1 71.875 (71.205)	Acc@5 93.359 (93.229)
Epoch: [53][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 1.8851 (1.9552)	Acc@1 77.344 (71.333)	Acc@5 94.141 (93.397)
Epoch: [53][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9070 (1.9585)	Acc@1 73.438 (71.351)	Acc@5 95.312 (93.293)
Epoch: [53][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9806 (1.9537)	Acc@1 68.359 (71.301)	Acc@5 93.359 (93.490)
Epoch: [53][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.1531 (1.9627)	Acc@1 66.406 (70.985)	Acc@5 92.188 (93.539)
Epoch: [53][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9564 (1.9616)	Acc@1 70.703 (71.165)	Acc@5 92.969 (93.684)
Epoch: [53][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0654 (1.9692)	Acc@1 69.141 (70.939)	Acc@5 92.578 (93.620)
Epoch: [53][90/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.1686 (1.9805)	Acc@1 64.453 (70.592)	Acc@5 93.750 (93.600)
Epoch: [53][100/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2029 (1.9909)	Acc@1 65.234 (70.312)	Acc@5 91.797 (93.499)
Epoch: [53][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1822 (2.0012)	Acc@1 64.453 (70.038)	Acc@5 92.188 (93.380)
Epoch: [53][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1708 (2.0114)	Acc@1 63.672 (69.828)	Acc@5 90.625 (93.243)
Epoch: [53][130/196]	Time 0.021 (0.016)	Data 0.001 (0.004)	Loss 2.0748 (2.0203)	Acc@1 67.578 (69.564)	Acc@5 93.750 (93.163)
Epoch: [53][140/196]	Time 0.020 (0.016)	Data 0.002 (0.004)	Loss 2.1303 (2.0306)	Acc@1 67.578 (69.318)	Acc@5 91.016 (92.974)
Epoch: [53][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1242 (2.0392)	Acc@1 66.406 (69.076)	Acc@5 92.578 (92.855)
Epoch: [53][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0160 (2.0427)	Acc@1 71.484 (68.985)	Acc@5 93.750 (92.830)
Epoch: [53][170/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0106 (2.0462)	Acc@1 69.922 (68.887)	Acc@5 92.969 (92.777)
Epoch: [53][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2102 (2.0510)	Acc@1 66.016 (68.787)	Acc@5 90.234 (92.738)
Epoch: [53][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1538 (2.0538)	Acc@1 65.234 (68.697)	Acc@5 92.578 (92.705)
num momentum params: 26
[0.1, 2.05624035987854, 1.8946211969852447, 68.668, 52.86, tensor(0.4817, device='cuda:0', grad_fn=<DivBackward0>), 3.0630159378051753, 0.38161754608154297]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [54 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [54][0/196]	Time 0.043 (0.043)	Data 0.174 (0.174)	Loss 1.9773 (1.9773)	Acc@1 69.531 (69.531)	Acc@5 94.531 (94.531)
Epoch: [54][10/196]	Time 0.016 (0.019)	Data 0.002 (0.018)	Loss 1.9572 (2.0131)	Acc@1 71.484 (69.780)	Acc@5 92.969 (93.643)
Epoch: [54][20/196]	Time 0.012 (0.018)	Data 0.005 (0.011)	Loss 2.1903 (2.0123)	Acc@1 67.188 (69.885)	Acc@5 91.016 (93.397)
Epoch: [54][30/196]	Time 0.017 (0.017)	Data 0.001 (0.008)	Loss 1.9905 (2.0020)	Acc@1 67.188 (69.884)	Acc@5 92.578 (93.637)
Epoch: [54][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 1.9130 (1.9909)	Acc@1 69.922 (70.112)	Acc@5 96.094 (93.712)
Epoch: [54][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1586 (1.9896)	Acc@1 64.844 (70.106)	Acc@5 92.578 (93.796)
Epoch: [54][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9602 (1.9948)	Acc@1 75.000 (70.152)	Acc@5 94.531 (93.718)
Epoch: [54][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1956 (1.9963)	Acc@1 69.531 (70.191)	Acc@5 89.453 (93.645)
Epoch: [54][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.2237 (2.0033)	Acc@1 65.234 (70.105)	Acc@5 90.234 (93.562)
Epoch: [54][90/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.0221 (2.0076)	Acc@1 71.875 (69.939)	Acc@5 92.578 (93.505)
Epoch: [54][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1900 (2.0145)	Acc@1 65.625 (69.686)	Acc@5 90.625 (93.367)
Epoch: [54][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0403 (2.0200)	Acc@1 65.625 (69.514)	Acc@5 92.578 (93.275)
Epoch: [54][120/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9555 (2.0210)	Acc@1 69.922 (69.499)	Acc@5 95.312 (93.269)
Epoch: [54][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0432 (2.0264)	Acc@1 72.266 (69.391)	Acc@5 93.750 (93.219)
Epoch: [54][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1399 (2.0334)	Acc@1 67.188 (69.221)	Acc@5 91.797 (93.096)
Epoch: [54][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2759 (2.0394)	Acc@1 62.500 (69.024)	Acc@5 91.016 (93.018)
Epoch: [54][160/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.2521 (2.0470)	Acc@1 65.625 (68.859)	Acc@5 91.797 (92.969)
Epoch: [54][170/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.3077 (2.0538)	Acc@1 64.844 (68.695)	Acc@5 91.406 (92.914)
Epoch: [54][180/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.3087 (2.0577)	Acc@1 62.109 (68.629)	Acc@5 88.281 (92.839)
Epoch: [54][190/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.2050 (2.0600)	Acc@1 63.281 (68.556)	Acc@5 92.578 (92.834)
num momentum params: 26
[0.1, 2.0599622589874267, 1.6886401343345643, 68.582, 55.83, tensor(0.4820, device='cuda:0', grad_fn=<DivBackward0>), 3.0483474731445312, 0.3929378986358642]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [55 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [55][0/196]	Time 0.046 (0.046)	Data 0.187 (0.187)	Loss 1.8720 (1.8720)	Acc@1 73.828 (73.828)	Acc@5 95.312 (95.312)
Epoch: [55][10/196]	Time 0.013 (0.018)	Data 0.004 (0.019)	Loss 2.1342 (2.0206)	Acc@1 66.016 (69.176)	Acc@5 92.578 (93.430)
Epoch: [55][20/196]	Time 0.015 (0.017)	Data 0.004 (0.011)	Loss 2.0202 (2.0024)	Acc@1 68.359 (69.475)	Acc@5 91.406 (93.304)
Epoch: [55][30/196]	Time 0.013 (0.016)	Data 0.005 (0.009)	Loss 2.0672 (2.0097)	Acc@1 70.312 (69.342)	Acc@5 91.797 (93.170)
Epoch: [55][40/196]	Time 0.019 (0.016)	Data 0.001 (0.007)	Loss 1.9626 (2.0068)	Acc@1 72.266 (69.693)	Acc@5 94.531 (93.255)
Epoch: [55][50/196]	Time 0.011 (0.016)	Data 0.006 (0.006)	Loss 1.8427 (1.9940)	Acc@1 75.000 (70.244)	Acc@5 95.703 (93.474)
Epoch: [55][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0272 (1.9970)	Acc@1 71.094 (70.197)	Acc@5 92.188 (93.462)
Epoch: [55][70/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.0054 (2.0063)	Acc@1 69.141 (69.878)	Acc@5 98.438 (93.414)
Epoch: [55][80/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.1110 (2.0106)	Acc@1 68.359 (69.734)	Acc@5 93.750 (93.432)
Epoch: [55][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9712 (2.0123)	Acc@1 73.047 (69.832)	Acc@5 93.750 (93.424)
Epoch: [55][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1948 (2.0156)	Acc@1 64.844 (69.701)	Acc@5 91.797 (93.321)
Epoch: [55][110/196]	Time 0.015 (0.016)	Data 0.001 (0.005)	Loss 2.0479 (2.0247)	Acc@1 68.359 (69.535)	Acc@5 92.188 (93.215)
Epoch: [55][120/196]	Time 0.021 (0.016)	Data 0.001 (0.005)	Loss 2.1220 (2.0326)	Acc@1 65.625 (69.289)	Acc@5 92.188 (93.143)
Epoch: [55][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1518 (2.0396)	Acc@1 64.062 (69.161)	Acc@5 92.969 (93.052)
Epoch: [55][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0801 (2.0435)	Acc@1 69.531 (69.005)	Acc@5 93.359 (93.002)
Epoch: [55][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1892 (2.0492)	Acc@1 62.500 (68.903)	Acc@5 91.797 (92.935)
Epoch: [55][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2765 (2.0547)	Acc@1 63.672 (68.743)	Acc@5 87.891 (92.813)
Epoch: [55][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1478 (2.0582)	Acc@1 65.234 (68.711)	Acc@5 90.625 (92.747)
Epoch: [55][180/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.2692 (2.0602)	Acc@1 61.719 (68.621)	Acc@5 88.672 (92.721)
Epoch: [55][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2217 (2.0652)	Acc@1 66.406 (68.521)	Acc@5 91.016 (92.639)
num momentum params: 26
[0.1, 2.065766331100464, 1.9376279020309448, 68.538, 51.2, tensor(0.4824, device='cuda:0', grad_fn=<DivBackward0>), 3.060873508453369, 0.3963799476623535]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [56 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [56][0/196]	Time 0.047 (0.047)	Data 0.194 (0.194)	Loss 2.0338 (2.0338)	Acc@1 68.359 (68.359)	Acc@5 92.188 (92.188)
Epoch: [56][10/196]	Time 0.017 (0.019)	Data 0.000 (0.020)	Loss 2.0313 (1.9876)	Acc@1 68.359 (71.094)	Acc@5 92.578 (93.466)
Epoch: [56][20/196]	Time 0.011 (0.017)	Data 0.008 (0.012)	Loss 1.9347 (1.9829)	Acc@1 73.047 (71.112)	Acc@5 93.750 (93.750)
Epoch: [56][30/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 1.9989 (1.9855)	Acc@1 66.797 (70.514)	Acc@5 93.359 (93.788)
Epoch: [56][40/196]	Time 0.013 (0.017)	Data 0.007 (0.007)	Loss 1.9908 (1.9793)	Acc@1 70.703 (70.884)	Acc@5 93.750 (93.721)
Epoch: [56][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9818 (1.9737)	Acc@1 71.094 (71.140)	Acc@5 92.188 (93.819)
Epoch: [56][60/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0059 (1.9716)	Acc@1 70.703 (71.209)	Acc@5 94.531 (93.955)
Epoch: [56][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9123 (1.9778)	Acc@1 73.438 (71.110)	Acc@5 94.141 (93.844)
Epoch: [56][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0413 (1.9840)	Acc@1 70.312 (71.026)	Acc@5 94.141 (93.697)
Epoch: [56][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0909 (1.9907)	Acc@1 70.312 (70.909)	Acc@5 92.969 (93.600)
Epoch: [56][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0052 (1.9980)	Acc@1 70.703 (70.552)	Acc@5 92.969 (93.560)
Epoch: [56][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2296 (2.0039)	Acc@1 62.891 (70.365)	Acc@5 92.578 (93.528)
Epoch: [56][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0598 (2.0073)	Acc@1 66.797 (70.271)	Acc@5 94.922 (93.505)
Epoch: [56][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0555 (2.0113)	Acc@1 70.312 (70.122)	Acc@5 91.797 (93.473)
Epoch: [56][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1965 (2.0149)	Acc@1 65.625 (70.033)	Acc@5 91.016 (93.384)
Epoch: [56][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2079 (2.0200)	Acc@1 64.844 (69.922)	Acc@5 89.844 (93.308)
Epoch: [56][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0082 (2.0235)	Acc@1 69.141 (69.856)	Acc@5 92.969 (93.257)
Epoch: [56][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9914 (2.0273)	Acc@1 69.922 (69.705)	Acc@5 95.312 (93.215)
Epoch: [56][180/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1243 (2.0332)	Acc@1 64.453 (69.471)	Acc@5 91.016 (93.139)
Epoch: [56][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1674 (2.0385)	Acc@1 68.359 (69.364)	Acc@5 90.625 (93.059)
num momentum params: 26
[0.1, 2.039466079788208, 1.8214440882205962, 69.332, 54.0, tensor(0.4883, device='cuda:0', grad_fn=<DivBackward0>), 3.1163876056671143, 0.39498567581176763]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [57 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [57][0/196]	Time 0.045 (0.045)	Data 0.168 (0.168)	Loss 2.0674 (2.0674)	Acc@1 67.969 (67.969)	Acc@5 90.234 (90.234)
Epoch: [57][10/196]	Time 0.015 (0.018)	Data 0.003 (0.018)	Loss 2.1492 (2.0095)	Acc@1 66.406 (70.028)	Acc@5 92.188 (93.288)
Epoch: [57][20/196]	Time 0.014 (0.017)	Data 0.004 (0.011)	Loss 2.1035 (2.0255)	Acc@1 66.406 (69.531)	Acc@5 92.969 (93.397)
Epoch: [57][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9700 (2.0205)	Acc@1 73.047 (69.556)	Acc@5 93.359 (93.485)
Epoch: [57][40/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.8008 (2.0063)	Acc@1 75.000 (69.846)	Acc@5 95.703 (93.598)
Epoch: [57][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.7433 (1.9918)	Acc@1 77.344 (70.320)	Acc@5 95.312 (93.643)
Epoch: [57][60/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9242 (1.9876)	Acc@1 72.266 (70.498)	Acc@5 92.969 (93.705)
Epoch: [57][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9893 (1.9913)	Acc@1 69.922 (70.301)	Acc@5 92.578 (93.722)
Epoch: [57][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9164 (1.9926)	Acc@1 74.609 (70.245)	Acc@5 93.750 (93.707)
Epoch: [57][90/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.0277 (1.9956)	Acc@1 70.312 (70.265)	Acc@5 94.141 (93.664)
Epoch: [57][100/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.0007 (2.0007)	Acc@1 69.531 (70.080)	Acc@5 94.531 (93.588)
Epoch: [57][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8258 (2.0018)	Acc@1 74.609 (70.084)	Acc@5 94.531 (93.585)
Epoch: [57][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3136 (2.0124)	Acc@1 62.891 (69.854)	Acc@5 90.234 (93.424)
Epoch: [57][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1321 (2.0202)	Acc@1 63.672 (69.680)	Acc@5 92.188 (93.341)
Epoch: [57][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0678 (2.0245)	Acc@1 67.578 (69.614)	Acc@5 94.531 (93.329)
Epoch: [57][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1998 (2.0303)	Acc@1 65.234 (69.495)	Acc@5 90.234 (93.266)
Epoch: [57][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0219 (2.0357)	Acc@1 63.281 (69.284)	Acc@5 97.266 (93.255)
Epoch: [57][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0112 (2.0422)	Acc@1 69.531 (69.175)	Acc@5 93.750 (93.202)
Epoch: [57][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2189 (2.0488)	Acc@1 64.062 (68.996)	Acc@5 91.406 (93.100)
Epoch: [57][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.2877 (2.0561)	Acc@1 60.938 (68.797)	Acc@5 91.016 (93.020)
num momentum params: 26
[0.1, 2.05768609500885, 1.9667882132530212, 68.766, 52.06, tensor(0.4847, device='cuda:0', grad_fn=<DivBackward0>), 3.0652573108673096, 0.3853166103363037]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [58 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [58][0/196]	Time 0.046 (0.046)	Data 0.187 (0.187)	Loss 1.9156 (1.9156)	Acc@1 72.266 (72.266)	Acc@5 94.922 (94.922)
Epoch: [58][10/196]	Time 0.017 (0.019)	Data 0.001 (0.019)	Loss 1.9522 (1.9750)	Acc@1 70.312 (70.419)	Acc@5 96.484 (93.999)
Epoch: [58][20/196]	Time 0.011 (0.017)	Data 0.013 (0.011)	Loss 1.9491 (1.9566)	Acc@1 72.266 (71.708)	Acc@5 92.578 (94.196)
Epoch: [58][30/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 2.0005 (1.9593)	Acc@1 71.484 (71.598)	Acc@5 90.625 (93.939)
Epoch: [58][40/196]	Time 0.011 (0.016)	Data 0.009 (0.007)	Loss 2.0921 (1.9702)	Acc@1 69.531 (71.351)	Acc@5 90.625 (93.760)
Epoch: [58][50/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 2.0718 (1.9761)	Acc@1 70.312 (71.239)	Acc@5 93.359 (93.819)
Epoch: [58][60/196]	Time 0.011 (0.016)	Data 0.007 (0.006)	Loss 2.0596 (1.9841)	Acc@1 71.094 (71.100)	Acc@5 89.844 (93.635)
Epoch: [58][70/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.0341 (1.9832)	Acc@1 67.578 (71.083)	Acc@5 92.578 (93.629)
Epoch: [58][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1479 (1.9920)	Acc@1 67.188 (70.896)	Acc@5 91.797 (93.547)
Epoch: [58][90/196]	Time 0.019 (0.016)	Data 0.000 (0.005)	Loss 2.0051 (2.0031)	Acc@1 72.266 (70.600)	Acc@5 93.750 (93.488)
Epoch: [58][100/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.1054 (2.0092)	Acc@1 69.531 (70.510)	Acc@5 93.359 (93.417)
Epoch: [58][110/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0411 (2.0135)	Acc@1 71.484 (70.386)	Acc@5 95.312 (93.366)
Epoch: [58][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9424 (2.0130)	Acc@1 71.875 (70.277)	Acc@5 95.312 (93.414)
Epoch: [58][130/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0227 (2.0164)	Acc@1 69.141 (70.151)	Acc@5 95.703 (93.404)
Epoch: [58][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0408 (2.0243)	Acc@1 66.406 (69.908)	Acc@5 92.969 (93.287)
Epoch: [58][150/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0694 (2.0307)	Acc@1 69.531 (69.715)	Acc@5 92.969 (93.258)
Epoch: [58][160/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1960 (2.0350)	Acc@1 67.578 (69.616)	Acc@5 90.234 (93.187)
Epoch: [58][170/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1701 (2.0435)	Acc@1 65.625 (69.333)	Acc@5 92.578 (93.106)
Epoch: [58][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1518 (2.0494)	Acc@1 64.453 (69.190)	Acc@5 93.750 (93.027)
Epoch: [58][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1735 (2.0530)	Acc@1 68.750 (69.098)	Acc@5 90.625 (92.973)
num momentum params: 26
[0.1, 2.057621226272583, 1.832046058177948, 68.956, 53.79, tensor(0.4862, device='cuda:0', grad_fn=<DivBackward0>), 3.047518491744995, 0.3860313892364502]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [59 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [59][0/196]	Time 0.042 (0.042)	Data 0.178 (0.178)	Loss 1.9804 (1.9804)	Acc@1 73.047 (73.047)	Acc@5 95.312 (95.312)
Epoch: [59][10/196]	Time 0.013 (0.018)	Data 0.005 (0.019)	Loss 1.9770 (1.9905)	Acc@1 69.922 (70.099)	Acc@5 92.969 (94.460)
Epoch: [59][20/196]	Time 0.012 (0.016)	Data 0.005 (0.011)	Loss 1.9110 (1.9996)	Acc@1 69.141 (69.754)	Acc@5 95.312 (94.048)
Epoch: [59][30/196]	Time 0.012 (0.016)	Data 0.005 (0.008)	Loss 2.0640 (2.0096)	Acc@1 71.094 (70.098)	Acc@5 91.016 (93.712)
Epoch: [59][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.9915 (2.0223)	Acc@1 71.875 (69.960)	Acc@5 94.141 (93.512)
Epoch: [59][50/196]	Time 0.011 (0.016)	Data 0.004 (0.006)	Loss 2.0445 (2.0082)	Acc@1 67.188 (70.305)	Acc@5 94.531 (93.666)
Epoch: [59][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.8640 (2.0080)	Acc@1 73.047 (70.300)	Acc@5 94.531 (93.680)
Epoch: [59][70/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0436 (2.0092)	Acc@1 68.359 (70.202)	Acc@5 92.578 (93.596)
Epoch: [59][80/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0447 (2.0075)	Acc@1 68.359 (70.245)	Acc@5 92.578 (93.639)
Epoch: [59][90/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1733 (2.0072)	Acc@1 61.719 (70.235)	Acc@5 93.750 (93.613)
Epoch: [59][100/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.0374 (2.0146)	Acc@1 70.312 (70.050)	Acc@5 93.750 (93.468)
Epoch: [59][110/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.1416 (2.0161)	Acc@1 65.625 (70.056)	Acc@5 91.797 (93.476)
Epoch: [59][120/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.1338 (2.0193)	Acc@1 68.750 (69.954)	Acc@5 87.891 (93.395)
Epoch: [59][130/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 1.9876 (2.0174)	Acc@1 71.484 (70.008)	Acc@5 93.359 (93.452)
Epoch: [59][140/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.0088 (2.0190)	Acc@1 68.750 (69.902)	Acc@5 92.578 (93.412)
Epoch: [59][150/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 1.9522 (2.0252)	Acc@1 72.656 (69.689)	Acc@5 93.750 (93.367)
Epoch: [59][160/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0712 (2.0320)	Acc@1 70.703 (69.536)	Acc@5 92.969 (93.287)
Epoch: [59][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0202 (2.0385)	Acc@1 72.656 (69.417)	Acc@5 92.188 (93.183)
Epoch: [59][180/196]	Time 0.014 (0.015)	Data 0.002 (0.003)	Loss 2.2506 (2.0437)	Acc@1 67.188 (69.348)	Acc@5 90.234 (93.109)
Epoch: [59][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.1605 (2.0478)	Acc@1 67.188 (69.278)	Acc@5 89.453 (93.024)
num momentum params: 26
[0.1, 2.049348945541382, 1.8831676971912383, 69.282, 52.58, tensor(0.4893, device='cuda:0', grad_fn=<DivBackward0>), 2.9762604236602783, 0.3818833827972412]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [60 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [60][0/196]	Time 0.048 (0.048)	Data 0.175 (0.175)	Loss 2.1015 (2.1015)	Acc@1 60.938 (60.938)	Acc@5 94.531 (94.531)
Epoch: [60][10/196]	Time 0.017 (0.018)	Data 0.001 (0.018)	Loss 2.0951 (1.9974)	Acc@1 67.969 (70.526)	Acc@5 92.969 (94.389)
Epoch: [60][20/196]	Time 0.011 (0.017)	Data 0.006 (0.011)	Loss 1.8676 (1.9874)	Acc@1 75.781 (71.001)	Acc@5 93.750 (94.289)
Epoch: [60][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.1194 (1.9937)	Acc@1 66.797 (70.527)	Acc@5 94.141 (94.292)
Epoch: [60][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.0381 (1.9968)	Acc@1 70.312 (70.312)	Acc@5 91.797 (94.036)
Epoch: [60][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1266 (2.0056)	Acc@1 65.625 (69.914)	Acc@5 93.750 (93.995)
Epoch: [60][60/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 1.9450 (2.0099)	Acc@1 72.266 (69.890)	Acc@5 94.922 (93.852)
Epoch: [60][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0621 (2.0131)	Acc@1 71.094 (69.768)	Acc@5 92.188 (93.866)
Epoch: [60][80/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.8466 (2.0119)	Acc@1 74.219 (69.782)	Acc@5 95.312 (93.842)
Epoch: [60][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.8693 (2.0184)	Acc@1 75.391 (69.823)	Acc@5 95.312 (93.668)
Epoch: [60][100/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.0308 (2.0246)	Acc@1 70.312 (69.775)	Acc@5 92.188 (93.530)
Epoch: [60][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0021 (2.0298)	Acc@1 71.875 (69.665)	Acc@5 92.578 (93.468)
Epoch: [60][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1583 (2.0306)	Acc@1 66.016 (69.706)	Acc@5 91.797 (93.472)
Epoch: [60][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9831 (2.0291)	Acc@1 71.875 (69.806)	Acc@5 92.969 (93.520)
Epoch: [60][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0769 (2.0294)	Acc@1 69.531 (69.800)	Acc@5 92.969 (93.481)
Epoch: [60][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0672 (2.0315)	Acc@1 67.188 (69.751)	Acc@5 94.141 (93.437)
Epoch: [60][160/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1292 (2.0379)	Acc@1 68.750 (69.568)	Acc@5 92.578 (93.367)
Epoch: [60][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0366 (2.0407)	Acc@1 69.922 (69.506)	Acc@5 94.922 (93.316)
Epoch: [60][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1475 (2.0460)	Acc@1 65.625 (69.395)	Acc@5 92.188 (93.202)
Epoch: [60][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1696 (2.0505)	Acc@1 66.797 (69.302)	Acc@5 91.797 (93.143)
num momentum params: 26
[0.1, 2.052603524398804, 1.972258332967758, 69.222, 51.38, tensor(0.4899, device='cuda:0', grad_fn=<DivBackward0>), 3.0327205657958984, 0.38513898849487305]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [61 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [61][0/196]	Time 0.046 (0.046)	Data 0.179 (0.179)	Loss 1.9838 (1.9838)	Acc@1 72.266 (72.266)	Acc@5 93.750 (93.750)
Epoch: [61][10/196]	Time 0.017 (0.018)	Data 0.001 (0.018)	Loss 1.7050 (2.0244)	Acc@1 78.125 (69.673)	Acc@5 96.875 (93.501)
Epoch: [61][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 2.0792 (2.0079)	Acc@1 69.922 (70.257)	Acc@5 94.922 (93.527)
Epoch: [61][30/196]	Time 0.013 (0.016)	Data 0.005 (0.008)	Loss 1.9322 (1.9985)	Acc@1 70.703 (70.376)	Acc@5 94.922 (93.750)
Epoch: [61][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9585 (1.9843)	Acc@1 71.094 (70.779)	Acc@5 95.703 (93.969)
Epoch: [61][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 1.8997 (1.9847)	Acc@1 75.000 (70.964)	Acc@5 94.531 (94.018)
Epoch: [61][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9893 (1.9883)	Acc@1 70.312 (70.870)	Acc@5 94.141 (94.006)
Epoch: [61][70/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0179 (1.9923)	Acc@1 69.531 (70.786)	Acc@5 94.531 (93.904)
Epoch: [61][80/196]	Time 0.022 (0.016)	Data 0.003 (0.005)	Loss 2.0805 (1.9926)	Acc@1 69.922 (70.785)	Acc@5 92.578 (93.885)
Epoch: [61][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1627 (1.9934)	Acc@1 63.281 (70.639)	Acc@5 92.188 (93.909)
Epoch: [61][100/196]	Time 0.013 (0.016)	Data 0.006 (0.004)	Loss 1.9926 (1.9985)	Acc@1 73.047 (70.521)	Acc@5 92.578 (93.827)
Epoch: [61][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9942 (2.0081)	Acc@1 66.797 (70.277)	Acc@5 94.922 (93.743)
Epoch: [61][120/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.1749 (2.0174)	Acc@1 67.188 (70.038)	Acc@5 91.406 (93.631)
Epoch: [61][130/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 2.0646 (2.0232)	Acc@1 67.578 (69.892)	Acc@5 93.359 (93.604)
Epoch: [61][140/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0505 (2.0303)	Acc@1 66.406 (69.731)	Acc@5 96.094 (93.487)
Epoch: [61][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.3330 (2.0407)	Acc@1 58.984 (69.464)	Acc@5 90.234 (93.372)
Epoch: [61][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0751 (2.0471)	Acc@1 67.969 (69.306)	Acc@5 90.234 (93.296)
Epoch: [61][170/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 2.1209 (2.0507)	Acc@1 68.750 (69.216)	Acc@5 93.359 (93.252)
Epoch: [61][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1888 (2.0566)	Acc@1 66.406 (69.093)	Acc@5 92.578 (93.163)
Epoch: [61][190/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.1860 (2.0597)	Acc@1 67.188 (69.026)	Acc@5 89.453 (93.130)
num momentum params: 26
[0.1, 2.0606457274627688, 1.8710078704357147, 68.984, 53.3, tensor(0.4894, device='cuda:0', grad_fn=<DivBackward0>), 2.990532636642456, 0.3870673179626465]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [62 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [62][0/196]	Time 0.044 (0.044)	Data 0.187 (0.187)	Loss 1.8980 (1.8980)	Acc@1 71.094 (71.094)	Acc@5 96.484 (96.484)
Epoch: [62][10/196]	Time 0.014 (0.020)	Data 0.002 (0.019)	Loss 1.9875 (1.9687)	Acc@1 70.312 (71.342)	Acc@5 93.750 (94.141)
Epoch: [62][20/196]	Time 0.013 (0.018)	Data 0.004 (0.011)	Loss 2.0876 (1.9644)	Acc@1 67.969 (71.373)	Acc@5 95.312 (94.364)
Epoch: [62][30/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 1.9210 (1.9691)	Acc@1 73.047 (71.144)	Acc@5 95.703 (94.380)
Epoch: [62][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 1.8504 (1.9602)	Acc@1 75.781 (71.503)	Acc@5 95.703 (94.388)
Epoch: [62][50/196]	Time 0.012 (0.016)	Data 0.019 (0.007)	Loss 2.0039 (1.9572)	Acc@1 71.875 (71.814)	Acc@5 93.359 (94.286)
Epoch: [62][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9803 (1.9609)	Acc@1 71.094 (71.702)	Acc@5 92.969 (94.179)
Epoch: [62][70/196]	Time 0.012 (0.016)	Data 0.041 (0.006)	Loss 1.9526 (1.9656)	Acc@1 73.047 (71.589)	Acc@5 92.969 (94.141)
Epoch: [62][80/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0692 (1.9722)	Acc@1 68.750 (71.412)	Acc@5 94.141 (94.121)
Epoch: [62][90/196]	Time 0.011 (0.016)	Data 0.009 (0.006)	Loss 2.1784 (1.9894)	Acc@1 65.625 (70.961)	Acc@5 91.016 (93.905)
Epoch: [62][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0870 (1.9973)	Acc@1 70.312 (70.792)	Acc@5 91.797 (93.804)
Epoch: [62][110/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0179 (2.0026)	Acc@1 72.266 (70.752)	Acc@5 92.578 (93.683)
Epoch: [62][120/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.1768 (2.0072)	Acc@1 66.016 (70.600)	Acc@5 91.016 (93.618)
Epoch: [62][130/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1811 (2.0154)	Acc@1 66.406 (70.360)	Acc@5 90.625 (93.503)
Epoch: [62][140/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.1820 (2.0206)	Acc@1 64.844 (70.202)	Acc@5 89.844 (93.459)
Epoch: [62][150/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 2.0050 (2.0239)	Acc@1 74.219 (70.090)	Acc@5 92.969 (93.416)
Epoch: [62][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2371 (2.0300)	Acc@1 62.891 (69.876)	Acc@5 91.797 (93.350)
Epoch: [62][170/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.1072 (2.0359)	Acc@1 65.625 (69.705)	Acc@5 94.531 (93.268)
Epoch: [62][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0521 (2.0378)	Acc@1 69.531 (69.689)	Acc@5 92.188 (93.221)
Epoch: [62][190/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0681 (2.0424)	Acc@1 68.359 (69.578)	Acc@5 92.969 (93.171)
num momentum params: 26
[0.1, 2.045067691192627, 2.038997198343277, 69.508, 49.69, tensor(0.4936, device='cuda:0', grad_fn=<DivBackward0>), 3.0866072177886963, 0.395486831665039]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [63 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [63][0/196]	Time 0.043 (0.043)	Data 0.190 (0.190)	Loss 1.9692 (1.9692)	Acc@1 71.094 (71.094)	Acc@5 95.312 (95.312)
Epoch: [63][10/196]	Time 0.019 (0.018)	Data 0.000 (0.020)	Loss 1.9776 (2.0197)	Acc@1 69.922 (70.277)	Acc@5 94.922 (93.928)
Epoch: [63][20/196]	Time 0.012 (0.017)	Data 0.006 (0.012)	Loss 1.9380 (1.9938)	Acc@1 71.875 (71.112)	Acc@5 91.797 (93.657)
Epoch: [63][30/196]	Time 0.017 (0.016)	Data 0.000 (0.009)	Loss 1.9846 (1.9911)	Acc@1 69.922 (71.119)	Acc@5 94.141 (93.763)
Epoch: [63][40/196]	Time 0.014 (0.016)	Data 0.005 (0.007)	Loss 2.0599 (1.9825)	Acc@1 66.406 (71.160)	Acc@5 92.578 (93.941)
Epoch: [63][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0718 (1.9737)	Acc@1 67.969 (71.438)	Acc@5 93.750 (93.964)
Epoch: [63][60/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 1.9858 (1.9696)	Acc@1 71.094 (71.452)	Acc@5 92.188 (94.057)
Epoch: [63][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8327 (1.9658)	Acc@1 73.828 (71.517)	Acc@5 95.703 (94.025)
Epoch: [63][80/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 1.8817 (1.9717)	Acc@1 73.828 (71.460)	Acc@5 94.141 (93.861)
Epoch: [63][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1336 (1.9784)	Acc@1 65.234 (71.308)	Acc@5 91.016 (93.729)
Epoch: [63][100/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0758 (1.9886)	Acc@1 71.484 (71.098)	Acc@5 91.406 (93.595)
Epoch: [63][110/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 2.1251 (1.9975)	Acc@1 67.969 (70.798)	Acc@5 92.969 (93.521)
Epoch: [63][120/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0668 (2.0007)	Acc@1 70.703 (70.716)	Acc@5 92.969 (93.456)
Epoch: [63][130/196]	Time 0.012 (0.016)	Data 0.032 (0.005)	Loss 2.0606 (2.0070)	Acc@1 69.922 (70.500)	Acc@5 92.578 (93.410)
Epoch: [63][140/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.2019 (2.0115)	Acc@1 64.844 (70.362)	Acc@5 91.016 (93.384)
Epoch: [63][150/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.1328 (2.0125)	Acc@1 65.625 (70.310)	Acc@5 93.750 (93.385)
Epoch: [63][160/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0036 (2.0163)	Acc@1 69.531 (70.225)	Acc@5 94.531 (93.340)
Epoch: [63][170/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9925 (2.0200)	Acc@1 71.484 (70.137)	Acc@5 92.188 (93.295)
Epoch: [63][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0423 (2.0257)	Acc@1 72.266 (69.961)	Acc@5 92.578 (93.206)
Epoch: [63][190/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1147 (2.0296)	Acc@1 67.188 (69.883)	Acc@5 93.359 (93.157)
num momentum params: 26
[0.1, 2.030854177246094, 1.8010090029239654, 69.856, 54.22, tensor(0.4971, device='cuda:0', grad_fn=<DivBackward0>), 3.0411791801452637, 0.378753662109375]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [64 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [64][0/196]	Time 0.042 (0.042)	Data 0.182 (0.182)	Loss 1.8690 (1.8690)	Acc@1 75.000 (75.000)	Acc@5 96.484 (96.484)
Epoch: [64][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 1.9003 (1.9638)	Acc@1 75.000 (72.479)	Acc@5 94.922 (94.141)
Epoch: [64][20/196]	Time 0.018 (0.017)	Data 0.003 (0.011)	Loss 1.8778 (1.9791)	Acc@1 73.438 (71.931)	Acc@5 94.922 (93.806)
Epoch: [64][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.9049 (1.9554)	Acc@1 74.219 (72.253)	Acc@5 94.531 (94.115)
Epoch: [64][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 2.0494 (1.9498)	Acc@1 69.531 (72.294)	Acc@5 94.141 (94.274)
Epoch: [64][50/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 1.8696 (1.9575)	Acc@1 74.609 (71.898)	Acc@5 94.922 (94.171)
Epoch: [64][60/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 2.0200 (1.9631)	Acc@1 68.359 (71.676)	Acc@5 92.969 (94.096)
Epoch: [64][70/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0528 (1.9654)	Acc@1 70.312 (71.539)	Acc@5 93.750 (94.069)
Epoch: [64][80/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0185 (1.9813)	Acc@1 67.578 (71.118)	Acc@5 94.531 (93.991)
Epoch: [64][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9908 (1.9876)	Acc@1 70.312 (70.982)	Acc@5 94.531 (93.819)
Epoch: [64][100/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 1.9246 (1.9913)	Acc@1 72.656 (70.873)	Acc@5 94.922 (93.727)
Epoch: [64][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1084 (1.9979)	Acc@1 70.703 (70.717)	Acc@5 90.625 (93.690)
Epoch: [64][120/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 2.1085 (2.0078)	Acc@1 67.969 (70.442)	Acc@5 93.359 (93.621)
Epoch: [64][130/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0376 (2.0133)	Acc@1 71.484 (70.348)	Acc@5 93.359 (93.583)
Epoch: [64][140/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.1133 (2.0208)	Acc@1 71.094 (70.224)	Acc@5 90.234 (93.451)
Epoch: [64][150/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.1126 (2.0268)	Acc@1 70.703 (70.139)	Acc@5 93.359 (93.398)
Epoch: [64][160/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.0876 (2.0311)	Acc@1 67.969 (70.036)	Acc@5 92.188 (93.325)
Epoch: [64][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0595 (2.0347)	Acc@1 69.922 (69.922)	Acc@5 92.578 (93.311)
Epoch: [64][180/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.2135 (2.0401)	Acc@1 60.938 (69.775)	Acc@5 93.359 (93.241)
Epoch: [64][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1637 (2.0456)	Acc@1 64.062 (69.582)	Acc@5 90.625 (93.186)
num momentum params: 26
[0.1, 2.0494367684173582, 2.0126193594932555, 69.504, 50.85, tensor(0.4933, device='cuda:0', grad_fn=<DivBackward0>), 3.091079950332641, 0.3882408142089844]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [65 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [65][0/196]	Time 0.044 (0.044)	Data 0.182 (0.182)	Loss 1.9315 (1.9315)	Acc@1 69.922 (69.922)	Acc@5 94.531 (94.531)
Epoch: [65][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 1.8533 (2.0017)	Acc@1 75.391 (70.987)	Acc@5 93.750 (93.643)
Epoch: [65][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 2.0131 (1.9835)	Acc@1 71.875 (71.373)	Acc@5 91.797 (94.066)
Epoch: [65][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9365 (1.9774)	Acc@1 72.266 (71.623)	Acc@5 96.094 (94.078)
Epoch: [65][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 2.0468 (1.9764)	Acc@1 67.578 (71.751)	Acc@5 92.969 (94.131)
Epoch: [65][50/196]	Time 0.020 (0.016)	Data 0.000 (0.006)	Loss 1.9171 (1.9669)	Acc@1 69.531 (71.684)	Acc@5 95.703 (94.263)
Epoch: [65][60/196]	Time 0.011 (0.016)	Data 0.029 (0.006)	Loss 2.0665 (1.9681)	Acc@1 71.875 (71.728)	Acc@5 92.188 (94.185)
Epoch: [65][70/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.6880 (1.9629)	Acc@1 80.078 (71.892)	Acc@5 96.875 (94.267)
Epoch: [65][80/196]	Time 0.012 (0.016)	Data 0.027 (0.006)	Loss 2.2559 (1.9701)	Acc@1 62.891 (71.600)	Acc@5 89.453 (94.227)
Epoch: [65][90/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1378 (1.9782)	Acc@1 63.281 (71.338)	Acc@5 92.969 (94.145)
Epoch: [65][100/196]	Time 0.012 (0.016)	Data 0.033 (0.007)	Loss 2.0020 (1.9906)	Acc@1 72.266 (71.032)	Acc@5 94.531 (93.986)
Epoch: [65][110/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9461 (1.9958)	Acc@1 72.266 (70.805)	Acc@5 94.531 (93.915)
Epoch: [65][120/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 1.9153 (1.9991)	Acc@1 76.953 (70.735)	Acc@5 92.969 (93.808)
Epoch: [65][130/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0298 (2.0017)	Acc@1 68.359 (70.614)	Acc@5 92.188 (93.786)
Epoch: [65][140/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 2.1248 (2.0089)	Acc@1 67.969 (70.468)	Acc@5 93.359 (93.706)
Epoch: [65][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9938 (2.0167)	Acc@1 71.484 (70.253)	Acc@5 94.531 (93.621)
Epoch: [65][160/196]	Time 0.012 (0.016)	Data 0.013 (0.005)	Loss 2.0339 (2.0243)	Acc@1 70.703 (70.133)	Acc@5 92.969 (93.532)
Epoch: [65][170/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1525 (2.0303)	Acc@1 66.797 (69.956)	Acc@5 93.359 (93.487)
Epoch: [65][180/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 2.1702 (2.0352)	Acc@1 66.797 (69.807)	Acc@5 91.406 (93.385)
Epoch: [65][190/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.2168 (2.0411)	Acc@1 65.234 (69.652)	Acc@5 90.625 (93.310)
num momentum params: 26
[0.1, 2.0440870347595217, 1.8654478287696838, 69.592, 52.57, tensor(0.4960, device='cuda:0', grad_fn=<DivBackward0>), 3.1035683155059814, 0.38849854469299316]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [66 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [66][0/196]	Time 0.043 (0.043)	Data 0.180 (0.180)	Loss 2.1026 (2.1026)	Acc@1 63.281 (63.281)	Acc@5 96.875 (96.875)
Epoch: [66][10/196]	Time 0.019 (0.019)	Data 0.001 (0.019)	Loss 1.8303 (2.0437)	Acc@1 75.781 (68.643)	Acc@5 96.094 (94.034)
Epoch: [66][20/196]	Time 0.013 (0.017)	Data 0.005 (0.011)	Loss 1.8701 (2.0186)	Acc@1 74.219 (69.717)	Acc@5 94.922 (93.899)
Epoch: [66][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.9590 (2.0045)	Acc@1 74.219 (70.275)	Acc@5 92.188 (93.901)
Epoch: [66][40/196]	Time 0.021 (0.017)	Data 0.003 (0.007)	Loss 1.9165 (1.9835)	Acc@1 73.828 (70.903)	Acc@5 94.531 (94.026)
Epoch: [66][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.8696 (1.9854)	Acc@1 74.609 (70.941)	Acc@5 94.922 (93.911)
Epoch: [66][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0188 (1.9862)	Acc@1 71.094 (70.972)	Acc@5 94.922 (93.949)
Epoch: [66][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9571 (1.9894)	Acc@1 73.438 (70.813)	Acc@5 94.141 (93.998)
Epoch: [66][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1745 (1.9903)	Acc@1 66.797 (70.906)	Acc@5 93.359 (94.059)
Epoch: [66][90/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.8212 (1.9939)	Acc@1 75.000 (70.828)	Acc@5 96.484 (94.046)
Epoch: [66][100/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1282 (1.9977)	Acc@1 68.750 (70.746)	Acc@5 89.844 (93.959)
Epoch: [66][110/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.0338 (2.0050)	Acc@1 67.969 (70.506)	Acc@5 93.359 (93.817)
Epoch: [66][120/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0831 (2.0085)	Acc@1 66.797 (70.484)	Acc@5 91.797 (93.802)
Epoch: [66][130/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0516 (2.0169)	Acc@1 71.094 (70.321)	Acc@5 93.359 (93.696)
Epoch: [66][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0537 (2.0221)	Acc@1 69.531 (70.199)	Acc@5 94.531 (93.650)
Epoch: [66][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0512 (2.0242)	Acc@1 70.703 (70.131)	Acc@5 93.750 (93.654)
Epoch: [66][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2097 (2.0268)	Acc@1 64.453 (70.070)	Acc@5 91.016 (93.602)
Epoch: [66][170/196]	Time 0.021 (0.016)	Data 0.002 (0.004)	Loss 2.1963 (2.0339)	Acc@1 64.453 (69.913)	Acc@5 91.406 (93.483)
Epoch: [66][180/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.1249 (2.0360)	Acc@1 72.266 (69.931)	Acc@5 93.750 (93.415)
Epoch: [66][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1410 (2.0431)	Acc@1 66.406 (69.764)	Acc@5 92.188 (93.333)
num momentum params: 26
[0.1, 2.043399159927368, 1.767597223520279, 69.77, 53.63, tensor(0.4968, device='cuda:0', grad_fn=<DivBackward0>), 3.0423240661621094, 0.3874356746673584]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [67 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [67][0/196]	Time 0.046 (0.046)	Data 0.173 (0.173)	Loss 1.9996 (1.9996)	Acc@1 70.703 (70.703)	Acc@5 94.531 (94.531)
Epoch: [67][10/196]	Time 0.015 (0.019)	Data 0.003 (0.018)	Loss 2.1364 (2.0417)	Acc@1 70.703 (70.135)	Acc@5 94.141 (93.643)
Epoch: [67][20/196]	Time 0.014 (0.017)	Data 0.004 (0.011)	Loss 2.1220 (2.0016)	Acc@1 66.406 (71.131)	Acc@5 91.406 (93.880)
Epoch: [67][30/196]	Time 0.012 (0.016)	Data 0.006 (0.008)	Loss 1.9983 (1.9902)	Acc@1 71.484 (71.346)	Acc@5 94.531 (93.889)
Epoch: [67][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.8801 (1.9793)	Acc@1 76.562 (71.399)	Acc@5 95.703 (94.083)
Epoch: [67][50/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.0125 (1.9730)	Acc@1 72.656 (71.523)	Acc@5 92.578 (94.056)
Epoch: [67][60/196]	Time 0.014 (0.016)	Data 0.005 (0.006)	Loss 1.9657 (1.9698)	Acc@1 68.750 (71.606)	Acc@5 94.141 (94.077)
Epoch: [67][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 1.8284 (1.9705)	Acc@1 76.562 (71.677)	Acc@5 96.094 (94.042)
Epoch: [67][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.1867 (1.9790)	Acc@1 68.750 (71.518)	Acc@5 93.359 (94.010)
Epoch: [67][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1572 (1.9878)	Acc@1 71.094 (71.257)	Acc@5 91.797 (93.969)
Epoch: [67][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1405 (1.9964)	Acc@1 68.359 (71.074)	Acc@5 91.016 (93.858)
Epoch: [67][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0490 (2.0015)	Acc@1 68.359 (70.935)	Acc@5 92.969 (93.785)
Epoch: [67][120/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1174 (2.0075)	Acc@1 69.922 (70.781)	Acc@5 92.188 (93.724)
Epoch: [67][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0566 (2.0151)	Acc@1 67.188 (70.638)	Acc@5 94.531 (93.669)
Epoch: [67][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0139 (2.0223)	Acc@1 73.828 (70.504)	Acc@5 91.406 (93.592)
Epoch: [67][150/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.0918 (2.0260)	Acc@1 67.578 (70.481)	Acc@5 92.969 (93.528)
Epoch: [67][160/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0262 (2.0299)	Acc@1 72.656 (70.395)	Acc@5 92.578 (93.454)
Epoch: [67][170/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1241 (2.0332)	Acc@1 65.625 (70.274)	Acc@5 92.578 (93.391)
Epoch: [67][180/196]	Time 0.015 (0.016)	Data 0.005 (0.003)	Loss 2.0930 (2.0368)	Acc@1 69.141 (70.148)	Acc@5 91.406 (93.372)
Epoch: [67][190/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 1.9518 (2.0395)	Acc@1 70.312 (70.069)	Acc@5 93.750 (93.333)
num momentum params: 26
[0.1, 2.0403944607543947, 1.8497471106052399, 70.044, 53.2, tensor(0.4980, device='cuda:0', grad_fn=<DivBackward0>), 3.0556180477142334, 0.40577459335327143]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [68 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [68][0/196]	Time 0.042 (0.042)	Data 0.172 (0.172)	Loss 1.9460 (1.9460)	Acc@1 73.828 (73.828)	Acc@5 94.922 (94.922)
Epoch: [68][10/196]	Time 0.015 (0.018)	Data 0.003 (0.018)	Loss 2.1935 (1.9593)	Acc@1 67.578 (72.798)	Acc@5 92.188 (94.141)
Epoch: [68][20/196]	Time 0.012 (0.017)	Data 0.019 (0.011)	Loss 1.8316 (1.9181)	Acc@1 77.734 (73.958)	Acc@5 95.312 (94.475)
Epoch: [68][30/196]	Time 0.017 (0.017)	Data 0.001 (0.008)	Loss 1.9999 (1.9254)	Acc@1 69.922 (73.778)	Acc@5 94.141 (94.531)
Epoch: [68][40/196]	Time 0.011 (0.017)	Data 0.008 (0.007)	Loss 2.1344 (1.9436)	Acc@1 67.969 (72.580)	Acc@5 90.234 (94.303)
Epoch: [68][50/196]	Time 0.017 (0.017)	Data 0.001 (0.007)	Loss 1.9180 (1.9424)	Acc@1 69.141 (72.411)	Acc@5 96.094 (94.401)
Epoch: [68][60/196]	Time 0.014 (0.017)	Data 0.009 (0.006)	Loss 2.0034 (1.9466)	Acc@1 73.438 (72.304)	Acc@5 95.312 (94.461)
Epoch: [68][70/196]	Time 0.018 (0.017)	Data 0.001 (0.006)	Loss 2.0540 (1.9563)	Acc@1 68.750 (71.963)	Acc@5 92.578 (94.372)
Epoch: [68][80/196]	Time 0.012 (0.016)	Data 0.011 (0.005)	Loss 1.9846 (1.9672)	Acc@1 71.484 (71.726)	Acc@5 94.922 (94.213)
Epoch: [68][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9995 (1.9746)	Acc@1 71.094 (71.527)	Acc@5 95.312 (94.115)
Epoch: [68][100/196]	Time 0.011 (0.016)	Data 0.011 (0.005)	Loss 2.0805 (1.9814)	Acc@1 67.578 (71.357)	Acc@5 92.969 (94.021)
Epoch: [68][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1279 (1.9905)	Acc@1 64.062 (71.013)	Acc@5 92.969 (93.954)
Epoch: [68][120/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 1.9326 (1.9965)	Acc@1 71.875 (70.790)	Acc@5 94.922 (93.963)
Epoch: [68][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1469 (2.0012)	Acc@1 63.281 (70.638)	Acc@5 92.578 (93.860)
Epoch: [68][140/196]	Time 0.012 (0.016)	Data 0.026 (0.005)	Loss 2.1914 (2.0082)	Acc@1 67.188 (70.454)	Acc@5 91.797 (93.742)
Epoch: [68][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1631 (2.0155)	Acc@1 66.406 (70.243)	Acc@5 89.844 (93.615)
Epoch: [68][160/196]	Time 0.012 (0.016)	Data 0.027 (0.005)	Loss 2.1385 (2.0216)	Acc@1 69.922 (70.138)	Acc@5 91.016 (93.500)
Epoch: [68][170/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1249 (2.0233)	Acc@1 68.359 (70.091)	Acc@5 92.578 (93.535)
Epoch: [68][180/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.0051 (2.0278)	Acc@1 71.484 (70.034)	Acc@5 95.703 (93.454)
Epoch: [68][190/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0957 (2.0338)	Acc@1 66.406 (69.897)	Acc@5 95.312 (93.384)
num momentum params: 26
[0.1, 2.036512528762817, 1.856871235370636, 69.836, 53.7, tensor(0.4994, device='cuda:0', grad_fn=<DivBackward0>), 3.11885404586792, 0.3885834217071533]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [69 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [69][0/196]	Time 0.043 (0.043)	Data 0.184 (0.184)	Loss 1.8382 (1.8382)	Acc@1 76.562 (76.562)	Acc@5 96.484 (96.484)
Epoch: [69][10/196]	Time 0.014 (0.018)	Data 0.002 (0.018)	Loss 1.9381 (1.9284)	Acc@1 76.172 (73.544)	Acc@5 94.141 (94.922)
Epoch: [69][20/196]	Time 0.015 (0.017)	Data 0.003 (0.011)	Loss 1.9200 (1.9529)	Acc@1 75.391 (72.879)	Acc@5 93.359 (94.401)
Epoch: [69][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.8261 (1.9331)	Acc@1 75.391 (73.022)	Acc@5 96.484 (94.708)
Epoch: [69][40/196]	Time 0.014 (0.016)	Data 0.002 (0.007)	Loss 1.9965 (1.9272)	Acc@1 72.266 (73.095)	Acc@5 94.531 (94.979)
Epoch: [69][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9146 (1.9325)	Acc@1 72.656 (72.848)	Acc@5 94.922 (94.899)
Epoch: [69][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8792 (1.9311)	Acc@1 74.219 (72.868)	Acc@5 94.141 (94.794)
Epoch: [69][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.1151 (1.9394)	Acc@1 65.625 (72.590)	Acc@5 93.359 (94.757)
Epoch: [69][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1157 (1.9511)	Acc@1 67.188 (72.217)	Acc@5 92.188 (94.604)
Epoch: [69][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0454 (1.9608)	Acc@1 66.016 (71.944)	Acc@5 94.141 (94.428)
Epoch: [69][100/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0843 (1.9687)	Acc@1 66.797 (71.697)	Acc@5 93.750 (94.311)
Epoch: [69][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2315 (1.9784)	Acc@1 64.844 (71.428)	Acc@5 90.625 (94.200)
Epoch: [69][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9563 (1.9844)	Acc@1 74.609 (71.294)	Acc@5 95.312 (94.137)
Epoch: [69][130/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0098 (1.9919)	Acc@1 69.531 (71.094)	Acc@5 92.188 (94.048)
Epoch: [69][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1480 (2.0014)	Acc@1 63.672 (70.842)	Acc@5 92.969 (93.936)
Epoch: [69][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2039 (2.0105)	Acc@1 62.109 (70.532)	Acc@5 91.406 (93.887)
Epoch: [69][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0904 (2.0189)	Acc@1 67.969 (70.354)	Acc@5 92.969 (93.803)
Epoch: [69][170/196]	Time 0.017 (0.015)	Data 0.002 (0.003)	Loss 2.0772 (2.0261)	Acc@1 72.656 (70.214)	Acc@5 92.578 (93.700)
Epoch: [69][180/196]	Time 0.017 (0.015)	Data 0.003 (0.003)	Loss 2.0713 (2.0347)	Acc@1 67.969 (69.989)	Acc@5 92.188 (93.560)
Epoch: [69][190/196]	Time 0.014 (0.015)	Data 0.003 (0.003)	Loss 2.0822 (2.0419)	Acc@1 69.141 (69.756)	Acc@5 92.578 (93.470)
num momentum params: 26
[0.1, 2.0438212573242187, 1.8179536855220795, 69.696, 53.8, tensor(0.4983, device='cuda:0', grad_fn=<DivBackward0>), 3.0193629264831543, 0.3835582733154297]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [70 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [70][0/196]	Time 0.042 (0.042)	Data 0.177 (0.177)	Loss 1.9804 (1.9804)	Acc@1 71.484 (71.484)	Acc@5 94.531 (94.531)
Epoch: [70][10/196]	Time 0.018 (0.018)	Data 0.003 (0.019)	Loss 1.8966 (1.9829)	Acc@1 73.828 (70.348)	Acc@5 95.703 (93.963)
Epoch: [70][20/196]	Time 0.016 (0.018)	Data 0.002 (0.011)	Loss 2.0837 (1.9936)	Acc@1 67.578 (70.461)	Acc@5 94.141 (93.973)
Epoch: [70][30/196]	Time 0.017 (0.017)	Data 0.001 (0.008)	Loss 1.9351 (1.9793)	Acc@1 73.438 (71.220)	Acc@5 94.922 (94.317)
Epoch: [70][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.8168 (1.9690)	Acc@1 76.172 (71.675)	Acc@5 94.922 (94.360)
Epoch: [70][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8608 (1.9593)	Acc@1 74.609 (71.929)	Acc@5 95.703 (94.508)
Epoch: [70][60/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0525 (1.9580)	Acc@1 67.969 (72.054)	Acc@5 91.406 (94.429)
Epoch: [70][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0305 (1.9652)	Acc@1 70.312 (71.847)	Acc@5 91.406 (94.223)
Epoch: [70][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0406 (1.9746)	Acc@1 72.266 (71.590)	Acc@5 90.625 (94.083)
Epoch: [70][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0171 (1.9778)	Acc@1 69.141 (71.553)	Acc@5 91.406 (94.020)
Epoch: [70][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1048 (1.9830)	Acc@1 70.312 (71.457)	Acc@5 91.406 (93.943)
Epoch: [70][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1145 (1.9885)	Acc@1 69.531 (71.294)	Acc@5 91.016 (93.866)
Epoch: [70][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9192 (1.9966)	Acc@1 74.609 (71.071)	Acc@5 94.922 (93.837)
Epoch: [70][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 1.9440 (1.9979)	Acc@1 71.094 (71.028)	Acc@5 95.703 (93.905)
Epoch: [70][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1921 (2.0036)	Acc@1 65.625 (70.850)	Acc@5 92.578 (93.861)
Epoch: [70][150/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2549 (2.0094)	Acc@1 62.109 (70.685)	Acc@5 92.969 (93.797)
Epoch: [70][160/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0276 (2.0118)	Acc@1 68.359 (70.616)	Acc@5 94.531 (93.760)
Epoch: [70][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1041 (2.0148)	Acc@1 69.531 (70.557)	Acc@5 92.969 (93.734)
Epoch: [70][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0881 (2.0176)	Acc@1 67.969 (70.487)	Acc@5 91.016 (93.692)
Epoch: [70][190/196]	Time 0.020 (0.016)	Data 0.000 (0.004)	Loss 2.0461 (2.0213)	Acc@1 66.016 (70.415)	Acc@5 92.578 (93.631)
num momentum params: 26
[0.1, 2.024437730255127, 2.3290094101428984, 70.338, 46.18, tensor(0.5033, device='cuda:0', grad_fn=<DivBackward0>), 3.0561819076538086, 0.38643312454223633]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [71 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [71][0/196]	Time 0.046 (0.046)	Data 0.177 (0.177)	Loss 2.3404 (2.3404)	Acc@1 63.672 (63.672)	Acc@5 87.891 (87.891)
Epoch: [71][10/196]	Time 0.017 (0.020)	Data 0.001 (0.018)	Loss 2.0092 (2.0651)	Acc@1 70.703 (70.455)	Acc@5 92.969 (93.040)
Epoch: [71][20/196]	Time 0.012 (0.018)	Data 0.008 (0.011)	Loss 1.9595 (2.0195)	Acc@1 70.703 (71.001)	Acc@5 93.750 (93.601)
Epoch: [71][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 2.0365 (2.0034)	Acc@1 70.703 (71.384)	Acc@5 92.188 (93.662)
Epoch: [71][40/196]	Time 0.011 (0.017)	Data 0.007 (0.007)	Loss 1.8753 (1.9877)	Acc@1 74.219 (71.723)	Acc@5 95.312 (93.855)
Epoch: [71][50/196]	Time 0.017 (0.017)	Data 0.000 (0.006)	Loss 2.0005 (1.9793)	Acc@1 70.703 (71.829)	Acc@5 94.141 (94.095)
Epoch: [71][60/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 1.9166 (1.9823)	Acc@1 71.094 (71.753)	Acc@5 94.531 (94.019)
Epoch: [71][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1646 (1.9861)	Acc@1 67.578 (71.677)	Acc@5 89.453 (93.943)
Epoch: [71][80/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 1.9430 (1.9911)	Acc@1 73.438 (71.571)	Acc@5 92.969 (93.933)
Epoch: [71][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9402 (1.9932)	Acc@1 72.266 (71.544)	Acc@5 94.531 (93.870)
Epoch: [71][100/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.0813 (2.0029)	Acc@1 68.750 (71.210)	Acc@5 92.969 (93.777)
Epoch: [71][110/196]	Time 0.014 (0.016)	Data 0.000 (0.004)	Loss 1.9969 (2.0083)	Acc@1 72.656 (71.083)	Acc@5 92.188 (93.673)
Epoch: [71][120/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 1.9702 (2.0125)	Acc@1 73.047 (70.939)	Acc@5 93.359 (93.601)
Epoch: [71][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0114 (2.0160)	Acc@1 72.266 (70.888)	Acc@5 91.797 (93.556)
Epoch: [71][140/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9973 (2.0211)	Acc@1 66.406 (70.684)	Acc@5 95.703 (93.501)
Epoch: [71][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9169 (2.0223)	Acc@1 75.781 (70.711)	Acc@5 94.531 (93.502)
Epoch: [71][160/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1589 (2.0285)	Acc@1 67.969 (70.516)	Acc@5 91.016 (93.435)
Epoch: [71][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9582 (2.0321)	Acc@1 71.484 (70.379)	Acc@5 95.312 (93.398)
Epoch: [71][180/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.1022 (2.0407)	Acc@1 70.703 (70.123)	Acc@5 90.234 (93.301)
Epoch: [71][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0556 (2.0428)	Acc@1 69.531 (70.055)	Acc@5 94.141 (93.255)
num momentum params: 26
[0.1, 2.0442110342407225, 1.8506234741210938, 70.02, 53.92, tensor(0.4989, device='cuda:0', grad_fn=<DivBackward0>), 3.042102813720703, 0.3809173107147217]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [72 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [72][0/196]	Time 0.046 (0.046)	Data 0.187 (0.187)	Loss 1.8865 (1.8865)	Acc@1 73.828 (73.828)	Acc@5 95.703 (95.703)
Epoch: [72][10/196]	Time 0.017 (0.020)	Data 0.002 (0.019)	Loss 2.0535 (1.9528)	Acc@1 69.922 (72.159)	Acc@5 93.359 (94.496)
Epoch: [72][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 2.0019 (1.9552)	Acc@1 68.359 (72.061)	Acc@5 94.531 (94.252)
Epoch: [72][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.8975 (1.9577)	Acc@1 71.875 (71.888)	Acc@5 94.531 (94.241)
Epoch: [72][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.8556 (1.9633)	Acc@1 75.391 (71.542)	Acc@5 96.875 (94.093)
Epoch: [72][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.9581 (1.9655)	Acc@1 69.531 (71.469)	Acc@5 94.141 (94.102)
Epoch: [72][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9544 (1.9712)	Acc@1 71.484 (71.408)	Acc@5 91.797 (94.057)
Epoch: [72][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0130 (1.9724)	Acc@1 69.141 (71.512)	Acc@5 89.453 (93.970)
Epoch: [72][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0250 (1.9785)	Acc@1 67.578 (71.359)	Acc@5 95.703 (93.957)
Epoch: [72][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0194 (1.9820)	Acc@1 66.406 (71.278)	Acc@5 96.094 (93.947)
Epoch: [72][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0793 (1.9820)	Acc@1 67.188 (71.287)	Acc@5 92.969 (93.889)
Epoch: [72][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0141 (1.9902)	Acc@1 71.484 (71.101)	Acc@5 94.141 (93.799)
Epoch: [72][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9486 (1.9938)	Acc@1 73.828 (71.065)	Acc@5 93.359 (93.763)
Epoch: [72][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1054 (1.9980)	Acc@1 66.797 (70.912)	Acc@5 93.359 (93.786)
Epoch: [72][140/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9420 (2.0037)	Acc@1 73.438 (70.767)	Acc@5 94.922 (93.736)
Epoch: [72][150/196]	Time 0.021 (0.016)	Data 0.002 (0.003)	Loss 2.0195 (2.0082)	Acc@1 67.188 (70.587)	Acc@5 95.312 (93.688)
Epoch: [72][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1049 (2.0109)	Acc@1 69.141 (70.521)	Acc@5 92.188 (93.648)
Epoch: [72][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1665 (2.0147)	Acc@1 62.500 (70.459)	Acc@5 93.750 (93.640)
Epoch: [72][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0545 (2.0171)	Acc@1 67.188 (70.330)	Acc@5 93.359 (93.649)
Epoch: [72][190/196]	Time 0.016 (0.016)	Data 0.001 (0.003)	Loss 2.2422 (2.0231)	Acc@1 64.844 (70.169)	Acc@5 90.625 (93.560)
num momentum params: 26
[0.1, 2.0262036736297606, 1.7872245132923126, 70.072, 54.6, tensor(0.5036, device='cuda:0', grad_fn=<DivBackward0>), 3.0982220172882085, 0.39018821716308594]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [73 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [73][0/196]	Time 0.047 (0.047)	Data 0.179 (0.179)	Loss 1.8211 (1.8211)	Acc@1 75.781 (75.781)	Acc@5 96.484 (96.484)
Epoch: [73][10/196]	Time 0.018 (0.019)	Data 0.001 (0.019)	Loss 1.9262 (1.9399)	Acc@1 72.266 (73.260)	Acc@5 96.094 (94.531)
Epoch: [73][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 1.8873 (1.9548)	Acc@1 73.828 (72.414)	Acc@5 95.703 (94.364)
Epoch: [73][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 2.0264 (1.9524)	Acc@1 71.094 (72.392)	Acc@5 93.359 (94.355)
Epoch: [73][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 2.1123 (1.9590)	Acc@1 67.578 (72.228)	Acc@5 94.141 (94.341)
Epoch: [73][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9637 (1.9539)	Acc@1 69.531 (72.304)	Acc@5 93.750 (94.309)
Epoch: [73][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8283 (1.9528)	Acc@1 78.516 (72.298)	Acc@5 94.922 (94.314)
Epoch: [73][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8430 (1.9628)	Acc@1 77.734 (71.958)	Acc@5 96.094 (94.240)
Epoch: [73][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9219 (1.9695)	Acc@1 73.047 (71.769)	Acc@5 93.750 (94.160)
Epoch: [73][90/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 1.9928 (1.9730)	Acc@1 70.703 (71.673)	Acc@5 93.359 (94.046)
Epoch: [73][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1474 (1.9813)	Acc@1 69.531 (71.484)	Acc@5 90.234 (93.916)
Epoch: [73][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1299 (1.9820)	Acc@1 68.359 (71.446)	Acc@5 94.922 (93.940)
Epoch: [73][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0769 (1.9883)	Acc@1 66.406 (71.174)	Acc@5 96.484 (93.921)
Epoch: [73][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1029 (1.9967)	Acc@1 67.969 (71.019)	Acc@5 92.969 (93.804)
Epoch: [73][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0632 (2.0002)	Acc@1 69.531 (70.916)	Acc@5 92.969 (93.803)
Epoch: [73][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2567 (2.0080)	Acc@1 64.453 (70.757)	Acc@5 89.062 (93.709)
Epoch: [73][160/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.0688 (2.0135)	Acc@1 71.094 (70.526)	Acc@5 93.359 (93.677)
Epoch: [73][170/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0979 (2.0192)	Acc@1 67.578 (70.365)	Acc@5 94.922 (93.665)
Epoch: [73][180/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 1.9602 (2.0222)	Acc@1 75.391 (70.308)	Acc@5 92.969 (93.612)
Epoch: [73][190/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.1833 (2.0279)	Acc@1 66.406 (70.182)	Acc@5 91.406 (93.513)
num momentum params: 26
[0.1, 2.031112099761963, 1.8134535300731658, 70.128, 53.56, tensor(0.5040, device='cuda:0', grad_fn=<DivBackward0>), 3.050731897354126, 0.39063262939453125]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [74 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [74][0/196]	Time 0.049 (0.049)	Data 0.176 (0.176)	Loss 1.9675 (1.9675)	Acc@1 71.484 (71.484)	Acc@5 94.141 (94.141)
Epoch: [74][10/196]	Time 0.017 (0.019)	Data 0.001 (0.018)	Loss 1.9050 (1.9940)	Acc@1 73.047 (71.768)	Acc@5 96.484 (94.709)
Epoch: [74][20/196]	Time 0.015 (0.017)	Data 0.003 (0.011)	Loss 2.0648 (1.9844)	Acc@1 69.531 (72.191)	Acc@5 94.531 (94.438)
Epoch: [74][30/196]	Time 0.016 (0.017)	Data 0.000 (0.008)	Loss 1.9648 (1.9568)	Acc@1 73.438 (72.996)	Acc@5 94.141 (94.619)
Epoch: [74][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 1.8898 (1.9480)	Acc@1 71.875 (73.047)	Acc@5 97.266 (94.655)
Epoch: [74][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9409 (1.9570)	Acc@1 73.438 (72.679)	Acc@5 95.312 (94.570)
Epoch: [74][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.3844 (1.9726)	Acc@1 64.062 (72.182)	Acc@5 90.625 (94.480)
Epoch: [74][70/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0035 (1.9831)	Acc@1 72.656 (71.831)	Acc@5 92.578 (94.416)
Epoch: [74][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9839 (1.9845)	Acc@1 69.922 (71.726)	Acc@5 95.312 (94.314)
Epoch: [74][90/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0260 (1.9885)	Acc@1 71.875 (71.605)	Acc@5 93.359 (94.261)
Epoch: [74][100/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.0162 (1.9958)	Acc@1 72.656 (71.403)	Acc@5 93.359 (94.133)
Epoch: [74][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1672 (2.0052)	Acc@1 66.797 (71.136)	Acc@5 91.406 (94.017)
Epoch: [74][120/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.2555 (2.0154)	Acc@1 62.109 (70.865)	Acc@5 92.969 (93.892)
Epoch: [74][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1017 (2.0220)	Acc@1 66.797 (70.655)	Acc@5 92.188 (93.732)
Epoch: [74][140/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.0888 (2.0254)	Acc@1 69.531 (70.565)	Acc@5 92.578 (93.659)
Epoch: [74][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.2324 (2.0308)	Acc@1 67.188 (70.403)	Acc@5 91.797 (93.603)
Epoch: [74][160/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1498 (2.0350)	Acc@1 68.359 (70.259)	Acc@5 89.062 (93.544)
Epoch: [74][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1712 (2.0411)	Acc@1 67.578 (70.114)	Acc@5 91.016 (93.506)
Epoch: [74][180/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1412 (2.0427)	Acc@1 68.359 (70.079)	Acc@5 90.625 (93.482)
Epoch: [74][190/196]	Time 0.012 (0.016)	Data 0.003 (0.004)	Loss 2.0727 (2.0450)	Acc@1 69.531 (70.014)	Acc@5 91.797 (93.443)
num momentum params: 26
[0.1, 2.0458159587860107, 1.7939019513130188, 69.97, 54.1, tensor(0.5020, device='cuda:0', grad_fn=<DivBackward0>), 3.0687477588653564, 0.38532543182373047]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [75 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [75][0/196]	Time 0.049 (0.049)	Data 0.185 (0.185)	Loss 1.9449 (1.9449)	Acc@1 73.438 (73.438)	Acc@5 94.141 (94.141)
Epoch: [75][10/196]	Time 0.016 (0.019)	Data 0.002 (0.019)	Loss 1.8053 (1.9478)	Acc@1 74.609 (72.408)	Acc@5 94.922 (94.425)
Epoch: [75][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.9696 (1.9377)	Acc@1 72.656 (72.917)	Acc@5 92.578 (94.234)
Epoch: [75][30/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 1.9530 (1.9364)	Acc@1 75.000 (73.085)	Acc@5 95.703 (94.430)
Epoch: [75][40/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 1.8762 (1.9288)	Acc@1 74.219 (73.171)	Acc@5 96.094 (94.407)
Epoch: [75][50/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 1.9791 (1.9247)	Acc@1 69.922 (73.154)	Acc@5 95.312 (94.524)
Epoch: [75][60/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 1.9074 (1.9411)	Acc@1 72.656 (72.682)	Acc@5 96.094 (94.448)
Epoch: [75][70/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0408 (1.9455)	Acc@1 68.359 (72.590)	Acc@5 93.359 (94.355)
Epoch: [75][80/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9330 (1.9501)	Acc@1 75.391 (72.536)	Acc@5 93.359 (94.290)
Epoch: [75][90/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9583 (1.9563)	Acc@1 73.438 (72.309)	Acc@5 95.312 (94.299)
Epoch: [75][100/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1680 (1.9635)	Acc@1 65.234 (72.107)	Acc@5 92.188 (94.237)
Epoch: [75][110/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0970 (1.9741)	Acc@1 70.312 (71.734)	Acc@5 94.531 (94.176)
Epoch: [75][120/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1156 (1.9855)	Acc@1 68.359 (71.488)	Acc@5 92.188 (94.079)
Epoch: [75][130/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.0114 (1.9942)	Acc@1 70.312 (71.299)	Acc@5 94.922 (94.009)
Epoch: [75][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1881 (2.0047)	Acc@1 66.797 (71.036)	Acc@5 93.359 (93.861)
Epoch: [75][150/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0973 (2.0137)	Acc@1 67.969 (70.796)	Acc@5 93.359 (93.758)
Epoch: [75][160/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0691 (2.0198)	Acc@1 67.969 (70.604)	Acc@5 92.969 (93.687)
Epoch: [75][170/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0591 (2.0249)	Acc@1 69.922 (70.472)	Acc@5 94.922 (93.654)
Epoch: [75][180/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 2.2137 (2.0320)	Acc@1 65.625 (70.304)	Acc@5 90.234 (93.571)
Epoch: [75][190/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9960 (2.0328)	Acc@1 70.312 (70.259)	Acc@5 94.141 (93.545)
num momentum params: 26
[0.1, 2.0356680824279785, 1.7490955781936646, 70.198, 54.73, tensor(0.5049, device='cuda:0', grad_fn=<DivBackward0>), 3.0745561122894287, 0.3898153305053711]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [76 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [76][0/196]	Time 0.049 (0.049)	Data 0.191 (0.191)	Loss 1.8579 (1.8579)	Acc@1 75.391 (75.391)	Acc@5 96.094 (96.094)
Epoch: [76][10/196]	Time 0.016 (0.019)	Data 0.002 (0.019)	Loss 1.8852 (1.9316)	Acc@1 73.828 (73.686)	Acc@5 95.312 (94.851)
Epoch: [76][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 1.8468 (1.9114)	Acc@1 74.609 (73.679)	Acc@5 95.703 (95.275)
Epoch: [76][30/196]	Time 0.015 (0.016)	Data 0.002 (0.009)	Loss 1.9024 (1.9101)	Acc@1 75.391 (73.879)	Acc@5 94.141 (95.098)
Epoch: [76][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 2.0206 (1.9142)	Acc@1 71.875 (73.723)	Acc@5 93.359 (95.084)
Epoch: [76][50/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 1.9332 (1.9286)	Acc@1 73.828 (73.330)	Acc@5 96.484 (94.868)
Epoch: [76][60/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 2.0882 (1.9362)	Acc@1 67.188 (73.105)	Acc@5 91.797 (94.749)
Epoch: [76][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0546 (1.9551)	Acc@1 69.141 (72.508)	Acc@5 94.141 (94.542)
Epoch: [76][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0938 (1.9665)	Acc@1 66.016 (72.082)	Acc@5 91.797 (94.416)
Epoch: [76][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0649 (1.9760)	Acc@1 69.531 (71.759)	Acc@5 91.797 (94.261)
Epoch: [76][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0813 (1.9809)	Acc@1 69.141 (71.612)	Acc@5 93.750 (94.156)
Epoch: [76][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0881 (1.9845)	Acc@1 66.797 (71.565)	Acc@5 94.141 (94.056)
Epoch: [76][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 1.9528 (1.9876)	Acc@1 73.828 (71.533)	Acc@5 94.141 (93.976)
Epoch: [76][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9431 (1.9906)	Acc@1 74.219 (71.422)	Acc@5 96.094 (93.956)
Epoch: [76][140/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9918 (1.9936)	Acc@1 70.703 (71.268)	Acc@5 94.531 (93.955)
Epoch: [76][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0894 (1.9990)	Acc@1 66.797 (71.070)	Acc@5 93.750 (93.890)
Epoch: [76][160/196]	Time 0.011 (0.016)	Data 0.004 (0.004)	Loss 2.0744 (2.0020)	Acc@1 70.312 (71.038)	Acc@5 92.188 (93.862)
Epoch: [76][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9712 (2.0061)	Acc@1 73.828 (70.996)	Acc@5 92.969 (93.823)
Epoch: [76][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1009 (2.0126)	Acc@1 69.141 (70.841)	Acc@5 92.969 (93.754)
Epoch: [76][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1967 (2.0190)	Acc@1 66.016 (70.705)	Acc@5 91.406 (93.689)
num momentum params: 26
[0.1, 2.0228606182861326, 2.048015034198761, 70.6, 50.8, tensor(0.5075, device='cuda:0', grad_fn=<DivBackward0>), 3.049541473388672, 0.39064097404479975]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [77 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [77][0/196]	Time 0.047 (0.047)	Data 0.194 (0.194)	Loss 1.9230 (1.9230)	Acc@1 73.828 (73.828)	Acc@5 95.312 (95.312)
Epoch: [77][10/196]	Time 0.017 (0.020)	Data 0.000 (0.020)	Loss 1.9930 (2.0062)	Acc@1 71.094 (71.378)	Acc@5 93.750 (93.892)
Epoch: [77][20/196]	Time 0.016 (0.018)	Data 0.002 (0.012)	Loss 1.8710 (2.0177)	Acc@1 76.953 (71.429)	Acc@5 93.750 (93.638)
Epoch: [77][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9805 (1.9988)	Acc@1 73.828 (71.711)	Acc@5 92.578 (93.813)
Epoch: [77][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.8412 (1.9920)	Acc@1 75.391 (71.627)	Acc@5 96.094 (93.979)
Epoch: [77][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0058 (1.9796)	Acc@1 70.703 (71.913)	Acc@5 94.141 (94.079)
Epoch: [77][60/196]	Time 0.011 (0.016)	Data 0.007 (0.006)	Loss 2.0842 (1.9771)	Acc@1 71.094 (72.080)	Acc@5 93.750 (94.141)
Epoch: [77][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.8117 (1.9698)	Acc@1 75.391 (72.233)	Acc@5 96.094 (94.295)
Epoch: [77][80/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0850 (1.9748)	Acc@1 69.141 (72.087)	Acc@5 91.797 (94.266)
Epoch: [77][90/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9645 (1.9794)	Acc@1 69.531 (71.884)	Acc@5 92.969 (94.162)
Epoch: [77][100/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 1.9917 (1.9801)	Acc@1 73.828 (71.964)	Acc@5 91.797 (94.144)
Epoch: [77][110/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 1.9829 (1.9841)	Acc@1 72.266 (71.822)	Acc@5 94.922 (94.074)
Epoch: [77][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2254 (1.9874)	Acc@1 65.625 (71.762)	Acc@5 92.188 (94.021)
Epoch: [77][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1214 (1.9909)	Acc@1 67.969 (71.678)	Acc@5 93.359 (93.974)
Epoch: [77][140/196]	Time 0.011 (0.015)	Data 0.015 (0.004)	Loss 2.1074 (1.9952)	Acc@1 68.359 (71.529)	Acc@5 90.625 (93.913)
Epoch: [77][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1381 (2.0024)	Acc@1 66.797 (71.272)	Acc@5 93.359 (93.848)
Epoch: [77][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1644 (2.0118)	Acc@1 64.844 (71.023)	Acc@5 92.969 (93.760)
Epoch: [77][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0255 (2.0189)	Acc@1 73.047 (70.767)	Acc@5 94.141 (93.707)
Epoch: [77][180/196]	Time 0.018 (0.015)	Data 0.010 (0.004)	Loss 2.0294 (2.0247)	Acc@1 67.578 (70.610)	Acc@5 94.141 (93.621)
Epoch: [77][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0131 (2.0273)	Acc@1 70.703 (70.517)	Acc@5 95.703 (93.633)
num momentum params: 26
[0.1, 2.0304221342468263, 1.8287661468982697, 70.402, 53.98, tensor(0.5066, device='cuda:0', grad_fn=<DivBackward0>), 3.038778781890869, 0.3875718116760254]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [78 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [78][0/196]	Time 0.051 (0.051)	Data 0.179 (0.179)	Loss 1.9445 (1.9445)	Acc@1 71.484 (71.484)	Acc@5 92.969 (92.969)
Epoch: [78][10/196]	Time 0.016 (0.020)	Data 0.002 (0.018)	Loss 1.9995 (1.9916)	Acc@1 71.094 (71.733)	Acc@5 91.797 (93.928)
Epoch: [78][20/196]	Time 0.014 (0.018)	Data 0.002 (0.011)	Loss 1.8961 (1.9808)	Acc@1 75.000 (71.949)	Acc@5 96.094 (94.196)
Epoch: [78][30/196]	Time 0.016 (0.017)	Data 0.000 (0.008)	Loss 1.9463 (1.9712)	Acc@1 70.703 (72.203)	Acc@5 92.578 (94.229)
Epoch: [78][40/196]	Time 0.018 (0.016)	Data 0.002 (0.007)	Loss 1.9598 (1.9746)	Acc@1 73.828 (71.961)	Acc@5 95.703 (94.293)
Epoch: [78][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9478 (1.9771)	Acc@1 71.094 (72.005)	Acc@5 95.703 (94.393)
Epoch: [78][60/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9696 (1.9790)	Acc@1 71.484 (71.843)	Acc@5 95.703 (94.486)
Epoch: [78][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9377 (1.9799)	Acc@1 71.094 (71.869)	Acc@5 96.484 (94.388)
Epoch: [78][80/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 2.1835 (1.9823)	Acc@1 67.578 (71.817)	Acc@5 93.359 (94.387)
Epoch: [78][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8896 (1.9792)	Acc@1 75.000 (71.901)	Acc@5 94.141 (94.385)
Epoch: [78][100/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0033 (1.9815)	Acc@1 72.656 (71.856)	Acc@5 94.531 (94.307)
Epoch: [78][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 1.8996 (1.9799)	Acc@1 71.875 (71.924)	Acc@5 93.750 (94.310)
Epoch: [78][120/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 2.1829 (1.9874)	Acc@1 67.578 (71.752)	Acc@5 91.016 (94.215)
Epoch: [78][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9904 (1.9919)	Acc@1 71.484 (71.592)	Acc@5 92.969 (94.156)
Epoch: [78][140/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0750 (1.9970)	Acc@1 68.359 (71.448)	Acc@5 90.625 (94.027)
Epoch: [78][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0441 (2.0030)	Acc@1 73.047 (71.301)	Acc@5 93.359 (93.983)
Epoch: [78][160/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9596 (2.0031)	Acc@1 71.484 (71.300)	Acc@5 92.969 (93.964)
Epoch: [78][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9387 (2.0069)	Acc@1 72.266 (71.192)	Acc@5 94.531 (93.901)
Epoch: [78][180/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9674 (2.0111)	Acc@1 72.266 (71.089)	Acc@5 93.359 (93.826)
Epoch: [78][190/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0235 (2.0153)	Acc@1 71.875 (70.991)	Acc@5 90.625 (93.740)
num momentum params: 26
[0.1, 2.018936629333496, 2.1672145855426788, 70.896, 48.62, tensor(0.5094, device='cuda:0', grad_fn=<DivBackward0>), 3.0569255352020264, 0.3957357406616211]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [79 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [79][0/196]	Time 0.053 (0.053)	Data 0.171 (0.171)	Loss 1.8720 (1.8720)	Acc@1 74.219 (74.219)	Acc@5 94.922 (94.922)
Epoch: [79][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 1.8371 (1.9929)	Acc@1 73.828 (72.088)	Acc@5 96.094 (94.070)
Epoch: [79][20/196]	Time 0.015 (0.018)	Data 0.003 (0.011)	Loss 1.9465 (1.9961)	Acc@1 73.828 (71.875)	Acc@5 92.969 (94.122)
Epoch: [79][30/196]	Time 0.014 (0.017)	Data 0.002 (0.008)	Loss 1.9275 (1.9791)	Acc@1 73.438 (72.228)	Acc@5 94.141 (94.267)
Epoch: [79][40/196]	Time 0.014 (0.017)	Data 0.005 (0.007)	Loss 1.7993 (1.9586)	Acc@1 76.562 (72.771)	Acc@5 95.312 (94.322)
Epoch: [79][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8444 (1.9608)	Acc@1 76.172 (72.511)	Acc@5 96.094 (94.416)
Epoch: [79][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9865 (1.9555)	Acc@1 71.094 (72.631)	Acc@5 94.141 (94.512)
Epoch: [79][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0000 (1.9568)	Acc@1 71.875 (72.491)	Acc@5 92.578 (94.597)
Epoch: [79][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.8849 (1.9542)	Acc@1 77.344 (72.526)	Acc@5 92.969 (94.541)
Epoch: [79][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0137 (1.9578)	Acc@1 72.266 (72.369)	Acc@5 94.141 (94.501)
Epoch: [79][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0310 (1.9616)	Acc@1 70.312 (72.312)	Acc@5 93.359 (94.435)
Epoch: [79][110/196]	Time 0.020 (0.016)	Data 0.005 (0.004)	Loss 2.1441 (1.9663)	Acc@1 69.922 (72.245)	Acc@5 92.969 (94.355)
Epoch: [79][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9556 (1.9719)	Acc@1 69.922 (72.069)	Acc@5 94.531 (94.296)
Epoch: [79][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1114 (1.9833)	Acc@1 68.750 (71.759)	Acc@5 92.188 (94.179)
Epoch: [79][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.3032 (1.9935)	Acc@1 63.281 (71.465)	Acc@5 91.016 (94.082)
Epoch: [79][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0353 (1.9981)	Acc@1 66.406 (71.358)	Acc@5 96.875 (94.037)
Epoch: [79][160/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0185 (2.0029)	Acc@1 71.484 (71.254)	Acc@5 93.359 (93.988)
Epoch: [79][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1152 (2.0083)	Acc@1 67.969 (71.087)	Acc@5 92.969 (93.935)
Epoch: [79][180/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1736 (2.0154)	Acc@1 64.453 (70.882)	Acc@5 92.578 (93.841)
Epoch: [79][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.2304 (2.0213)	Acc@1 64.062 (70.746)	Acc@5 89.844 (93.734)
num momentum params: 26
[0.1, 2.023624709625244, 1.982776139974594, 70.684, 51.9, tensor(0.5078, device='cuda:0', grad_fn=<DivBackward0>), 3.0372438430786133, 0.3888988494873047]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [80 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [80][0/196]	Time 0.051 (0.051)	Data 0.177 (0.177)	Loss 1.9538 (1.9538)	Acc@1 74.609 (74.609)	Acc@5 93.750 (93.750)
Epoch: [80][10/196]	Time 0.017 (0.019)	Data 0.002 (0.018)	Loss 1.8756 (1.9821)	Acc@1 77.344 (71.946)	Acc@5 95.703 (94.638)
Epoch: [80][20/196]	Time 0.015 (0.018)	Data 0.002 (0.010)	Loss 1.8498 (1.9570)	Acc@1 75.391 (72.879)	Acc@5 94.922 (94.773)
Epoch: [80][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9467 (1.9557)	Acc@1 75.000 (72.770)	Acc@5 95.703 (94.859)
Epoch: [80][40/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 1.8200 (1.9490)	Acc@1 73.047 (72.866)	Acc@5 96.875 (94.769)
Epoch: [80][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.7724 (1.9470)	Acc@1 77.734 (72.840)	Acc@5 95.312 (94.700)
Epoch: [80][60/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.9629 (1.9492)	Acc@1 72.656 (72.739)	Acc@5 94.531 (94.576)
Epoch: [80][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.8060 (1.9529)	Acc@1 75.000 (72.645)	Acc@5 95.312 (94.531)
Epoch: [80][80/196]	Time 0.012 (0.016)	Data 0.022 (0.005)	Loss 1.8574 (1.9492)	Acc@1 72.266 (72.690)	Acc@5 96.484 (94.512)
Epoch: [80][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0093 (1.9553)	Acc@1 69.141 (72.472)	Acc@5 94.531 (94.497)
Epoch: [80][100/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0898 (1.9595)	Acc@1 70.312 (72.339)	Acc@5 93.359 (94.469)
Epoch: [80][110/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0756 (1.9737)	Acc@1 69.141 (71.882)	Acc@5 93.359 (94.355)
Epoch: [80][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0904 (1.9838)	Acc@1 68.750 (71.646)	Acc@5 94.141 (94.286)
Epoch: [80][130/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 2.2156 (1.9903)	Acc@1 64.062 (71.458)	Acc@5 93.359 (94.269)
Epoch: [80][140/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9117 (1.9945)	Acc@1 75.000 (71.429)	Acc@5 94.922 (94.163)
Epoch: [80][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1747 (2.0023)	Acc@1 69.141 (71.200)	Acc@5 89.062 (94.050)
Epoch: [80][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1316 (2.0091)	Acc@1 67.578 (70.994)	Acc@5 93.359 (93.939)
Epoch: [80][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0361 (2.0115)	Acc@1 70.703 (70.925)	Acc@5 93.750 (93.908)
Epoch: [80][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0712 (2.0157)	Acc@1 68.750 (70.828)	Acc@5 94.141 (93.860)
Epoch: [80][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9746 (2.0187)	Acc@1 74.219 (70.818)	Acc@5 92.578 (93.811)
num momentum params: 26
[0.1, 2.020618801727295, 1.9566779458522796, 70.748, 52.94, tensor(0.5091, device='cuda:0', grad_fn=<DivBackward0>), 3.0842061042785645, 0.3958923816680908]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [81 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [81][0/196]	Time 0.052 (0.052)	Data 0.178 (0.178)	Loss 1.8798 (1.8798)	Acc@1 75.391 (75.391)	Acc@5 96.875 (96.875)
Epoch: [81][10/196]	Time 0.016 (0.020)	Data 0.002 (0.018)	Loss 1.9348 (1.9208)	Acc@1 73.828 (73.935)	Acc@5 96.875 (95.703)
Epoch: [81][20/196]	Time 0.016 (0.018)	Data 0.004 (0.011)	Loss 1.8198 (1.9074)	Acc@1 77.344 (74.033)	Acc@5 95.703 (95.592)
Epoch: [81][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9568 (1.9122)	Acc@1 74.219 (73.727)	Acc@5 95.312 (95.401)
Epoch: [81][40/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 2.0200 (1.9175)	Acc@1 70.703 (73.676)	Acc@5 92.578 (95.236)
Epoch: [81][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9791 (1.9164)	Acc@1 69.531 (73.537)	Acc@5 94.531 (95.129)
Epoch: [81][60/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.1042 (1.9266)	Acc@1 69.141 (73.239)	Acc@5 92.578 (95.005)
Epoch: [81][70/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 2.0673 (1.9297)	Acc@1 69.531 (73.074)	Acc@5 92.188 (94.982)
Epoch: [81][80/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.7805 (1.9406)	Acc@1 76.953 (72.758)	Acc@5 97.656 (94.796)
Epoch: [81][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8913 (1.9435)	Acc@1 75.781 (72.734)	Acc@5 95.703 (94.686)
Epoch: [81][100/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9633 (1.9499)	Acc@1 73.047 (72.521)	Acc@5 94.141 (94.585)
Epoch: [81][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0798 (1.9586)	Acc@1 69.531 (72.245)	Acc@5 93.750 (94.542)
Epoch: [81][120/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 2.2107 (1.9697)	Acc@1 63.672 (71.965)	Acc@5 93.359 (94.434)
Epoch: [81][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1338 (1.9823)	Acc@1 69.531 (71.681)	Acc@5 92.188 (94.260)
Epoch: [81][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1278 (1.9919)	Acc@1 69.531 (71.479)	Acc@5 91.016 (94.113)
Epoch: [81][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1136 (1.9978)	Acc@1 68.750 (71.365)	Acc@5 93.750 (94.060)
Epoch: [81][160/196]	Time 0.012 (0.016)	Data 0.005 (0.003)	Loss 2.1642 (2.0047)	Acc@1 67.188 (71.220)	Acc@5 93.750 (94.012)
Epoch: [81][170/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.0658 (2.0087)	Acc@1 67.578 (71.158)	Acc@5 93.750 (93.958)
Epoch: [81][180/196]	Time 0.013 (0.015)	Data 0.004 (0.003)	Loss 2.2644 (2.0132)	Acc@1 63.281 (71.068)	Acc@5 91.016 (93.927)
Epoch: [81][190/196]	Time 0.017 (0.015)	Data 0.002 (0.003)	Loss 2.3361 (2.0185)	Acc@1 64.062 (70.928)	Acc@5 87.891 (93.844)
num momentum params: 26
[0.1, 2.021711321105957, 1.8402184522151948, 70.846, 53.32, tensor(0.5098, device='cuda:0', grad_fn=<DivBackward0>), 3.0347938537597656, 0.38739490509033203]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [82 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [82][0/196]	Time 0.053 (0.053)	Data 0.194 (0.194)	Loss 1.8222 (1.8222)	Acc@1 77.344 (77.344)	Acc@5 96.094 (96.094)
Epoch: [82][10/196]	Time 0.017 (0.020)	Data 0.002 (0.019)	Loss 1.7560 (1.9329)	Acc@1 76.172 (72.159)	Acc@5 97.266 (95.455)
Epoch: [82][20/196]	Time 0.013 (0.017)	Data 0.004 (0.012)	Loss 2.0089 (1.9425)	Acc@1 72.266 (72.191)	Acc@5 92.969 (95.015)
Epoch: [82][30/196]	Time 0.016 (0.017)	Data 0.002 (0.009)	Loss 1.9653 (1.9416)	Acc@1 70.703 (72.228)	Acc@5 96.875 (95.035)
Epoch: [82][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 1.8992 (1.9396)	Acc@1 75.391 (72.437)	Acc@5 95.703 (94.884)
Epoch: [82][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9605 (1.9276)	Acc@1 70.312 (72.802)	Acc@5 94.141 (95.060)
Epoch: [82][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.7318 (1.9232)	Acc@1 78.516 (72.829)	Acc@5 96.875 (95.108)
Epoch: [82][70/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9372 (1.9272)	Acc@1 72.266 (72.667)	Acc@5 95.312 (95.048)
Epoch: [82][80/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0385 (1.9352)	Acc@1 70.312 (72.372)	Acc@5 94.141 (94.970)
Epoch: [82][90/196]	Time 0.012 (0.016)	Data 0.022 (0.005)	Loss 2.0495 (1.9438)	Acc@1 67.188 (72.270)	Acc@5 92.969 (94.870)
Epoch: [82][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9605 (1.9492)	Acc@1 75.781 (72.177)	Acc@5 94.922 (94.817)
Epoch: [82][110/196]	Time 0.013 (0.016)	Data 0.016 (0.005)	Loss 2.0388 (1.9592)	Acc@1 70.312 (71.995)	Acc@5 92.578 (94.609)
Epoch: [82][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0974 (1.9667)	Acc@1 66.797 (71.785)	Acc@5 92.969 (94.570)
Epoch: [82][130/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 1.8980 (1.9713)	Acc@1 73.828 (71.708)	Acc@5 94.531 (94.481)
Epoch: [82][140/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1467 (1.9774)	Acc@1 67.188 (71.584)	Acc@5 93.359 (94.393)
Epoch: [82][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0867 (1.9875)	Acc@1 70.312 (71.316)	Acc@5 93.750 (94.254)
Epoch: [82][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0175 (1.9941)	Acc@1 66.406 (71.174)	Acc@5 94.922 (94.182)
Epoch: [82][170/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0767 (2.0031)	Acc@1 69.922 (70.977)	Acc@5 92.969 (94.049)
Epoch: [82][180/196]	Time 0.022 (0.016)	Data 0.001 (0.004)	Loss 2.0885 (2.0087)	Acc@1 64.844 (70.807)	Acc@5 94.531 (93.981)
Epoch: [82][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1510 (2.0136)	Acc@1 69.531 (70.707)	Acc@5 92.969 (93.930)
num momentum params: 26
[0.1, 2.0159329539489748, 1.9308737218379974, 70.658, 52.06, tensor(0.5118, device='cuda:0', grad_fn=<DivBackward0>), 3.04927396774292, 0.3894791603088379]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [83 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [83][0/196]	Time 0.054 (0.054)	Data 0.178 (0.178)	Loss 1.8824 (1.8824)	Acc@1 75.781 (75.781)	Acc@5 96.484 (96.484)
Epoch: [83][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 2.0358 (1.9457)	Acc@1 68.359 (73.224)	Acc@5 92.188 (95.028)
Epoch: [83][20/196]	Time 0.020 (0.018)	Data 0.000 (0.011)	Loss 1.8785 (1.9773)	Acc@1 73.438 (72.042)	Acc@5 92.969 (94.494)
Epoch: [83][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 1.9032 (1.9670)	Acc@1 73.047 (72.077)	Acc@5 96.094 (94.481)
Epoch: [83][40/196]	Time 0.017 (0.017)	Data 0.001 (0.007)	Loss 1.8614 (1.9519)	Acc@1 76.562 (72.494)	Acc@5 94.922 (94.598)
Epoch: [83][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 2.1591 (1.9635)	Acc@1 66.016 (72.289)	Acc@5 92.969 (94.539)
Epoch: [83][60/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0130 (1.9641)	Acc@1 68.750 (72.272)	Acc@5 95.312 (94.557)
Epoch: [83][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9373 (1.9692)	Acc@1 71.484 (72.062)	Acc@5 94.531 (94.460)
Epoch: [83][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9423 (1.9713)	Acc@1 73.828 (72.005)	Acc@5 92.969 (94.372)
Epoch: [83][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.2445 (1.9794)	Acc@1 66.016 (71.751)	Acc@5 93.359 (94.338)
Epoch: [83][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0156 (1.9870)	Acc@1 70.703 (71.566)	Acc@5 94.922 (94.264)
Epoch: [83][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1667 (1.9977)	Acc@1 65.625 (71.182)	Acc@5 94.531 (94.183)
Epoch: [83][120/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.2076 (2.0034)	Acc@1 62.500 (71.032)	Acc@5 91.797 (94.047)
Epoch: [83][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0259 (2.0095)	Acc@1 67.969 (70.909)	Acc@5 94.141 (93.968)
Epoch: [83][140/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 2.0132 (2.0117)	Acc@1 69.141 (70.803)	Acc@5 94.141 (93.952)
Epoch: [83][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0824 (2.0145)	Acc@1 71.094 (70.737)	Acc@5 94.531 (93.905)
Epoch: [83][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9586 (2.0177)	Acc@1 70.703 (70.667)	Acc@5 96.094 (93.842)
Epoch: [83][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0730 (2.0206)	Acc@1 70.703 (70.584)	Acc@5 91.797 (93.807)
Epoch: [83][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0882 (2.0230)	Acc@1 69.922 (70.515)	Acc@5 91.797 (93.780)
Epoch: [83][190/196]	Time 0.014 (0.016)	Data 0.010 (0.004)	Loss 2.1476 (2.0268)	Acc@1 66.406 (70.390)	Acc@5 91.016 (93.766)
num momentum params: 26
[0.1, 2.0301183190155028, 1.9321419262886048, 70.294, 53.13, tensor(0.5087, device='cuda:0', grad_fn=<DivBackward0>), 3.111341953277588, 0.39103031158447266]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [84 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [84][0/196]	Time 0.057 (0.057)	Data 0.182 (0.182)	Loss 1.9468 (1.9468)	Acc@1 72.266 (72.266)	Acc@5 94.922 (94.922)
Epoch: [84][10/196]	Time 0.017 (0.020)	Data 0.002 (0.018)	Loss 2.0795 (2.0543)	Acc@1 66.797 (69.531)	Acc@5 97.266 (93.857)
Epoch: [84][20/196]	Time 0.016 (0.018)	Data 0.002 (0.011)	Loss 1.8379 (1.9766)	Acc@1 78.516 (72.470)	Acc@5 95.312 (94.401)
Epoch: [84][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 2.0474 (1.9673)	Acc@1 67.969 (72.467)	Acc@5 94.141 (94.569)
Epoch: [84][40/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 1.9416 (1.9527)	Acc@1 73.828 (72.828)	Acc@5 94.141 (94.808)
Epoch: [84][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.9143 (1.9520)	Acc@1 73.047 (72.786)	Acc@5 94.141 (94.707)
Epoch: [84][60/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9698 (1.9513)	Acc@1 72.656 (72.752)	Acc@5 94.922 (94.666)
Epoch: [84][70/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9937 (1.9479)	Acc@1 75.000 (72.788)	Acc@5 93.359 (94.685)
Epoch: [84][80/196]	Time 0.020 (0.016)	Data 0.001 (0.005)	Loss 2.0306 (1.9540)	Acc@1 69.141 (72.459)	Acc@5 93.750 (94.652)
Epoch: [84][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0849 (1.9661)	Acc@1 70.703 (72.184)	Acc@5 91.797 (94.493)
Epoch: [84][100/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1044 (1.9717)	Acc@1 67.188 (72.018)	Acc@5 92.578 (94.438)
Epoch: [84][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0959 (1.9834)	Acc@1 67.578 (71.678)	Acc@5 94.531 (94.288)
Epoch: [84][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9957 (1.9869)	Acc@1 74.219 (71.630)	Acc@5 92.969 (94.244)
Epoch: [84][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8053 (1.9865)	Acc@1 78.906 (71.684)	Acc@5 95.703 (94.251)
Epoch: [84][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9741 (1.9896)	Acc@1 74.219 (71.620)	Acc@5 93.750 (94.249)
Epoch: [84][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9390 (1.9935)	Acc@1 72.656 (71.482)	Acc@5 94.531 (94.195)
Epoch: [84][160/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1868 (2.0014)	Acc@1 65.625 (71.290)	Acc@5 91.797 (94.107)
Epoch: [84][170/196]	Time 0.023 (0.016)	Data 0.002 (0.004)	Loss 1.9715 (2.0072)	Acc@1 74.609 (71.178)	Acc@5 94.141 (94.058)
Epoch: [84][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1120 (2.0113)	Acc@1 66.406 (71.040)	Acc@5 92.578 (94.007)
Epoch: [84][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1397 (2.0161)	Acc@1 68.359 (70.938)	Acc@5 94.141 (93.938)
num momentum params: 26
[0.1, 2.0158770692443846, 1.7583087646961213, 70.95, 54.95, tensor(0.5129, device='cuda:0', grad_fn=<DivBackward0>), 3.1042897701263428, 0.3919048309326172]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [85 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [85][0/196]	Time 0.050 (0.050)	Data 0.177 (0.177)	Loss 1.9600 (1.9600)	Acc@1 71.484 (71.484)	Acc@5 96.484 (96.484)
Epoch: [85][10/196]	Time 0.014 (0.019)	Data 0.004 (0.018)	Loss 2.0131 (1.9267)	Acc@1 70.703 (72.834)	Acc@5 95.703 (95.384)
Epoch: [85][20/196]	Time 0.014 (0.017)	Data 0.003 (0.011)	Loss 1.9378 (1.9180)	Acc@1 74.219 (73.307)	Acc@5 94.141 (95.108)
Epoch: [85][30/196]	Time 0.011 (0.017)	Data 0.008 (0.008)	Loss 1.9333 (1.9151)	Acc@1 69.531 (73.185)	Acc@5 94.141 (95.149)
Epoch: [85][40/196]	Time 0.019 (0.016)	Data 0.000 (0.007)	Loss 1.8409 (1.9140)	Acc@1 77.344 (73.428)	Acc@5 94.531 (95.179)
Epoch: [85][50/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 1.8298 (1.9222)	Acc@1 75.391 (73.100)	Acc@5 96.094 (95.075)
Epoch: [85][60/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.8972 (1.9234)	Acc@1 71.094 (73.098)	Acc@5 95.703 (95.076)
Epoch: [85][70/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9957 (1.9273)	Acc@1 71.875 (73.091)	Acc@5 94.141 (94.982)
Epoch: [85][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9611 (1.9377)	Acc@1 72.266 (72.719)	Acc@5 94.141 (94.854)
Epoch: [85][90/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9244 (1.9457)	Acc@1 69.922 (72.566)	Acc@5 94.531 (94.742)
Epoch: [85][100/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9626 (1.9505)	Acc@1 72.266 (72.463)	Acc@5 94.922 (94.616)
Epoch: [85][110/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1238 (1.9586)	Acc@1 69.922 (72.245)	Acc@5 88.281 (94.486)
Epoch: [85][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1915 (1.9660)	Acc@1 64.844 (72.033)	Acc@5 91.406 (94.421)
Epoch: [85][130/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0242 (1.9691)	Acc@1 71.094 (71.935)	Acc@5 92.578 (94.370)
Epoch: [85][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9903 (1.9745)	Acc@1 72.266 (71.770)	Acc@5 93.750 (94.299)
Epoch: [85][150/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.0965 (1.9776)	Acc@1 68.359 (71.655)	Acc@5 91.406 (94.247)
Epoch: [85][160/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.0615 (1.9836)	Acc@1 67.188 (71.472)	Acc@5 95.703 (94.211)
Epoch: [85][170/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.0873 (1.9892)	Acc@1 67.188 (71.297)	Acc@5 94.141 (94.163)
Epoch: [85][180/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.2068 (1.9974)	Acc@1 64.453 (71.061)	Acc@5 91.406 (94.087)
Epoch: [85][190/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1096 (2.0013)	Acc@1 71.094 (70.989)	Acc@5 93.750 (94.032)
num momentum params: 26
[0.1, 2.003580407485962, 1.8005439531803131, 70.922, 55.16, tensor(0.5154, device='cuda:0', grad_fn=<DivBackward0>), 2.99668025970459, 0.3966102600097656]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [86 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [86][0/196]	Time 0.055 (0.055)	Data 0.175 (0.175)	Loss 1.8320 (1.8320)	Acc@1 74.609 (74.609)	Acc@5 94.922 (94.922)
Epoch: [86][10/196]	Time 0.016 (0.020)	Data 0.002 (0.018)	Loss 1.8952 (1.9294)	Acc@1 72.656 (73.722)	Acc@5 96.094 (94.780)
Epoch: [86][20/196]	Time 0.011 (0.018)	Data 0.009 (0.011)	Loss 1.9542 (1.9229)	Acc@1 71.875 (74.014)	Acc@5 94.922 (94.680)
Epoch: [86][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 1.8900 (1.9183)	Acc@1 73.047 (74.257)	Acc@5 96.875 (94.720)
Epoch: [86][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.8330 (1.9186)	Acc@1 77.344 (74.095)	Acc@5 96.094 (94.912)
Epoch: [86][50/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.8342 (1.9158)	Acc@1 76.953 (74.112)	Acc@5 96.094 (94.991)
Epoch: [86][60/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 2.0453 (1.9199)	Acc@1 69.141 (73.687)	Acc@5 94.141 (94.941)
Epoch: [86][70/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.0580 (1.9335)	Acc@1 69.531 (73.206)	Acc@5 92.578 (94.740)
Epoch: [86][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9407 (1.9386)	Acc@1 71.484 (72.999)	Acc@5 94.531 (94.661)
Epoch: [86][90/196]	Time 0.020 (0.016)	Data 0.001 (0.004)	Loss 2.0074 (1.9431)	Acc@1 71.484 (72.927)	Acc@5 94.141 (94.609)
Epoch: [86][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2035 (1.9509)	Acc@1 69.141 (72.672)	Acc@5 91.016 (94.485)
Epoch: [86][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0291 (1.9607)	Acc@1 71.484 (72.368)	Acc@5 94.531 (94.412)
Epoch: [86][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1365 (1.9644)	Acc@1 70.312 (72.259)	Acc@5 91.016 (94.367)
Epoch: [86][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9520 (1.9765)	Acc@1 72.656 (71.908)	Acc@5 94.922 (94.218)
Epoch: [86][140/196]	Time 0.011 (0.016)	Data 0.014 (0.004)	Loss 2.1822 (1.9827)	Acc@1 66.016 (71.753)	Acc@5 91.406 (94.174)
Epoch: [86][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0864 (1.9865)	Acc@1 69.531 (71.640)	Acc@5 92.578 (94.130)
Epoch: [86][160/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1362 (1.9945)	Acc@1 67.578 (71.404)	Acc@5 92.578 (94.036)
Epoch: [86][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0450 (2.0013)	Acc@1 69.141 (71.185)	Acc@5 91.406 (93.949)
Epoch: [86][180/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.0037 (2.0062)	Acc@1 70.703 (71.042)	Acc@5 92.188 (93.918)
Epoch: [86][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0365 (2.0107)	Acc@1 67.188 (70.883)	Acc@5 94.922 (93.926)
num momentum params: 26
[0.1, 2.011156746368408, 1.7168494200706481, 70.88, 55.6, tensor(0.5143, device='cuda:0', grad_fn=<DivBackward0>), 3.053218126296997, 0.3987247943878174]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [87 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [87][0/196]	Time 0.051 (0.051)	Data 0.185 (0.185)	Loss 1.9421 (1.9421)	Acc@1 73.047 (73.047)	Acc@5 93.750 (93.750)
Epoch: [87][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 1.9568 (1.9367)	Acc@1 73.047 (72.869)	Acc@5 95.312 (95.135)
Epoch: [87][20/196]	Time 0.016 (0.018)	Data 0.002 (0.011)	Loss 1.8950 (1.9194)	Acc@1 71.875 (73.344)	Acc@5 94.141 (95.108)
Epoch: [87][30/196]	Time 0.015 (0.017)	Data 0.003 (0.008)	Loss 1.9546 (1.9232)	Acc@1 74.609 (73.337)	Acc@5 96.094 (95.199)
Epoch: [87][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.7879 (1.9147)	Acc@1 77.734 (73.542)	Acc@5 97.266 (95.227)
Epoch: [87][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.8788 (1.9106)	Acc@1 75.000 (73.552)	Acc@5 96.094 (95.312)
Epoch: [87][60/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 2.1414 (1.9173)	Acc@1 69.141 (73.393)	Acc@5 92.188 (95.197)
Epoch: [87][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.9542 (1.9241)	Acc@1 73.047 (73.267)	Acc@5 95.312 (95.065)
Epoch: [87][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9192 (1.9269)	Acc@1 73.828 (73.187)	Acc@5 94.141 (94.975)
Epoch: [87][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9453 (1.9381)	Acc@1 71.875 (72.922)	Acc@5 94.922 (94.785)
Epoch: [87][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0237 (1.9486)	Acc@1 71.094 (72.641)	Acc@5 92.578 (94.651)
Epoch: [87][110/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0795 (1.9588)	Acc@1 68.359 (72.403)	Acc@5 94.141 (94.552)
Epoch: [87][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2859 (1.9756)	Acc@1 65.234 (72.011)	Acc@5 89.844 (94.386)
Epoch: [87][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1973 (1.9875)	Acc@1 64.062 (71.648)	Acc@5 91.797 (94.266)
Epoch: [87][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1596 (1.9948)	Acc@1 67.578 (71.476)	Acc@5 92.969 (94.202)
Epoch: [87][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1528 (2.0000)	Acc@1 66.797 (71.345)	Acc@5 94.531 (94.172)
Epoch: [87][160/196]	Time 0.012 (0.016)	Data 0.007 (0.003)	Loss 2.2006 (2.0052)	Acc@1 66.406 (71.210)	Acc@5 89.062 (94.090)
Epoch: [87][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1076 (2.0074)	Acc@1 65.625 (71.126)	Acc@5 91.406 (94.045)
Epoch: [87][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0625 (2.0126)	Acc@1 70.312 (70.969)	Acc@5 92.969 (93.977)
Epoch: [87][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0915 (2.0176)	Acc@1 70.703 (70.850)	Acc@5 92.188 (93.924)
num momentum params: 26
[0.1, 2.0191698025512697, 1.9383763325214387, 70.834, 52.5, tensor(0.5129, device='cuda:0', grad_fn=<DivBackward0>), 3.0558111667633057, 0.38973569869995117]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [88 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [88][0/196]	Time 0.058 (0.058)	Data 0.187 (0.187)	Loss 1.8721 (1.8721)	Acc@1 75.781 (75.781)	Acc@5 94.531 (94.531)
Epoch: [88][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 1.9804 (1.9769)	Acc@1 69.141 (71.236)	Acc@5 94.531 (94.247)
Epoch: [88][20/196]	Time 0.017 (0.018)	Data 0.003 (0.011)	Loss 2.0068 (1.9721)	Acc@1 75.781 (72.173)	Acc@5 94.141 (94.420)
Epoch: [88][30/196]	Time 0.015 (0.017)	Data 0.003 (0.008)	Loss 1.8116 (1.9557)	Acc@1 76.953 (72.631)	Acc@5 94.922 (94.657)
Epoch: [88][40/196]	Time 0.017 (0.017)	Data 0.002 (0.007)	Loss 1.9605 (1.9572)	Acc@1 72.656 (72.685)	Acc@5 95.312 (94.617)
Epoch: [88][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8152 (1.9511)	Acc@1 75.000 (72.809)	Acc@5 97.266 (94.684)
Epoch: [88][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9810 (1.9555)	Acc@1 71.484 (72.707)	Acc@5 93.750 (94.621)
Epoch: [88][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8710 (1.9579)	Acc@1 73.438 (72.722)	Acc@5 96.094 (94.564)
Epoch: [88][80/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 1.9446 (1.9642)	Acc@1 70.703 (72.536)	Acc@5 95.312 (94.541)
Epoch: [88][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9424 (1.9679)	Acc@1 72.266 (72.485)	Acc@5 94.141 (94.437)
Epoch: [88][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1169 (1.9754)	Acc@1 69.531 (72.269)	Acc@5 93.750 (94.365)
Epoch: [88][110/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0996 (1.9802)	Acc@1 69.922 (72.100)	Acc@5 91.797 (94.285)
Epoch: [88][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1309 (1.9833)	Acc@1 70.312 (72.020)	Acc@5 92.578 (94.289)
Epoch: [88][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9886 (1.9876)	Acc@1 68.750 (71.842)	Acc@5 92.969 (94.290)
Epoch: [88][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9943 (1.9878)	Acc@1 71.094 (71.825)	Acc@5 92.578 (94.293)
Epoch: [88][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1376 (1.9910)	Acc@1 65.625 (71.601)	Acc@5 92.578 (94.226)
Epoch: [88][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.8712 (1.9947)	Acc@1 73.828 (71.497)	Acc@5 94.141 (94.145)
Epoch: [88][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0572 (2.0005)	Acc@1 67.969 (71.347)	Acc@5 92.578 (94.097)
Epoch: [88][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0962 (2.0075)	Acc@1 67.578 (71.148)	Acc@5 92.969 (94.037)
Epoch: [88][190/196]	Time 0.016 (0.016)	Data 0.001 (0.003)	Loss 2.2083 (2.0161)	Acc@1 62.891 (70.924)	Acc@5 91.797 (93.912)
num momentum params: 26
[0.1, 2.018497345275879, 1.8947152626514434, 70.886, 53.29, tensor(0.5135, device='cuda:0', grad_fn=<DivBackward0>), 3.1029479503631587, 0.39082145690917974]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [89 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [89][0/196]	Time 0.057 (0.057)	Data 0.196 (0.196)	Loss 2.0451 (2.0451)	Acc@1 71.875 (71.875)	Acc@5 94.531 (94.531)
Epoch: [89][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 1.8813 (1.9679)	Acc@1 74.609 (72.514)	Acc@5 96.484 (94.673)
Epoch: [89][20/196]	Time 0.016 (0.018)	Data 0.002 (0.011)	Loss 1.8537 (1.9440)	Acc@1 75.000 (73.140)	Acc@5 95.703 (94.754)
Epoch: [89][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.8872 (1.9324)	Acc@1 74.609 (73.564)	Acc@5 94.531 (94.808)
Epoch: [89][40/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 2.0772 (1.9384)	Acc@1 69.531 (73.190)	Acc@5 94.141 (94.855)
Epoch: [89][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0486 (1.9508)	Acc@1 68.750 (72.901)	Acc@5 94.922 (94.707)
Epoch: [89][60/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0041 (1.9575)	Acc@1 73.047 (72.797)	Acc@5 91.797 (94.525)
Epoch: [89][70/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.9821 (1.9571)	Acc@1 73.828 (72.761)	Acc@5 94.141 (94.592)
Epoch: [89][80/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8055 (1.9582)	Acc@1 78.516 (72.753)	Acc@5 95.703 (94.623)
Epoch: [89][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1284 (1.9650)	Acc@1 68.359 (72.527)	Acc@5 94.531 (94.613)
Epoch: [89][100/196]	Time 0.018 (0.016)	Data 0.002 (0.005)	Loss 2.1244 (1.9704)	Acc@1 68.750 (72.474)	Acc@5 92.578 (94.516)
Epoch: [89][110/196]	Time 0.021 (0.016)	Data 0.001 (0.005)	Loss 2.1171 (1.9724)	Acc@1 68.750 (72.378)	Acc@5 89.062 (94.440)
Epoch: [89][120/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1673 (1.9786)	Acc@1 68.359 (72.191)	Acc@5 93.359 (94.367)
Epoch: [89][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8868 (1.9811)	Acc@1 72.266 (72.084)	Acc@5 95.312 (94.334)
Epoch: [89][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.8476 (1.9839)	Acc@1 74.219 (72.008)	Acc@5 94.922 (94.307)
Epoch: [89][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9765 (1.9906)	Acc@1 70.312 (71.844)	Acc@5 96.094 (94.254)
Epoch: [89][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1080 (1.9944)	Acc@1 68.750 (71.734)	Acc@5 93.359 (94.206)
Epoch: [89][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1666 (2.0030)	Acc@1 67.188 (71.468)	Acc@5 93.359 (94.131)
Epoch: [89][180/196]	Time 0.018 (0.016)	Data 0.003 (0.004)	Loss 2.3018 (2.0124)	Acc@1 64.844 (71.225)	Acc@5 90.625 (94.065)
Epoch: [89][190/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0382 (2.0182)	Acc@1 71.094 (71.090)	Acc@5 92.578 (93.993)
num momentum params: 26
[0.1, 2.0192174279785156, 1.7895013856887818, 71.06, 54.96, tensor(0.5137, device='cuda:0', grad_fn=<DivBackward0>), 3.08418083190918, 0.4036431312561035]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [90 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [90][0/196]	Time 0.055 (0.055)	Data 0.186 (0.186)	Loss 1.9037 (1.9037)	Acc@1 77.344 (77.344)	Acc@5 94.531 (94.531)
Epoch: [90][10/196]	Time 0.018 (0.022)	Data 0.002 (0.019)	Loss 1.8665 (1.9279)	Acc@1 75.391 (73.580)	Acc@5 95.703 (94.851)
Epoch: [90][20/196]	Time 0.015 (0.019)	Data 0.003 (0.011)	Loss 1.9023 (1.9006)	Acc@1 74.609 (74.126)	Acc@5 94.922 (95.052)
Epoch: [90][30/196]	Time 0.017 (0.018)	Data 0.000 (0.009)	Loss 1.7887 (1.9016)	Acc@1 77.734 (74.017)	Acc@5 94.531 (94.997)
Epoch: [90][40/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 2.0661 (1.9086)	Acc@1 69.531 (73.742)	Acc@5 95.312 (94.970)
Epoch: [90][50/196]	Time 0.017 (0.017)	Data 0.001 (0.008)	Loss 2.0882 (1.9104)	Acc@1 67.578 (73.790)	Acc@5 92.578 (94.953)
Epoch: [90][60/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 1.9346 (1.9180)	Acc@1 71.875 (73.495)	Acc@5 94.922 (94.909)
Epoch: [90][70/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 2.2256 (1.9333)	Acc@1 66.016 (73.124)	Acc@5 92.578 (94.757)
Epoch: [90][80/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.9341 (1.9395)	Acc@1 73.438 (72.917)	Acc@5 94.141 (94.652)
Epoch: [90][90/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.2779 (1.9517)	Acc@1 63.672 (72.566)	Acc@5 91.406 (94.604)
Epoch: [90][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9981 (1.9632)	Acc@1 72.656 (72.355)	Acc@5 94.141 (94.481)
Epoch: [90][110/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.0213 (1.9710)	Acc@1 71.875 (72.135)	Acc@5 94.531 (94.405)
Epoch: [90][120/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1912 (1.9829)	Acc@1 68.359 (71.839)	Acc@5 91.406 (94.286)
Epoch: [90][130/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0580 (1.9905)	Acc@1 68.359 (71.604)	Acc@5 94.531 (94.194)
Epoch: [90][140/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.1336 (1.9923)	Acc@1 69.141 (71.587)	Acc@5 94.531 (94.177)
Epoch: [90][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9780 (1.9967)	Acc@1 71.484 (71.487)	Acc@5 94.141 (94.146)
Epoch: [90][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9230 (2.0001)	Acc@1 74.609 (71.429)	Acc@5 95.703 (94.131)
Epoch: [90][170/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0074 (2.0037)	Acc@1 72.656 (71.384)	Acc@5 93.359 (94.072)
Epoch: [90][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2090 (2.0096)	Acc@1 67.188 (71.243)	Acc@5 89.844 (93.992)
Epoch: [90][190/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1098 (2.0131)	Acc@1 69.922 (71.159)	Acc@5 94.922 (93.963)
num momentum params: 26
[0.1, 2.0136701829528807, 1.894838490486145, 71.142, 53.23, tensor(0.5158, device='cuda:0', grad_fn=<DivBackward0>), 3.0839850902557373, 0.40130615234375]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [91 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [91][0/196]	Time 0.055 (0.055)	Data 0.192 (0.192)	Loss 1.8405 (1.8405)	Acc@1 77.734 (77.734)	Acc@5 96.094 (96.094)
Epoch: [91][10/196]	Time 0.017 (0.020)	Data 0.003 (0.019)	Loss 1.8567 (1.9743)	Acc@1 76.172 (71.982)	Acc@5 96.484 (94.531)
Epoch: [91][20/196]	Time 0.013 (0.018)	Data 0.005 (0.011)	Loss 1.9247 (1.9701)	Acc@1 73.438 (72.247)	Acc@5 94.141 (94.773)
Epoch: [91][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.7838 (1.9573)	Acc@1 78.125 (72.341)	Acc@5 97.266 (94.884)
Epoch: [91][40/196]	Time 0.016 (0.017)	Data 0.003 (0.007)	Loss 1.9971 (1.9639)	Acc@1 72.656 (72.247)	Acc@5 94.141 (94.798)
Epoch: [91][50/196]	Time 0.016 (0.017)	Data 0.001 (0.006)	Loss 1.9935 (1.9696)	Acc@1 70.312 (71.906)	Acc@5 91.797 (94.593)
Epoch: [91][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9150 (1.9628)	Acc@1 74.609 (72.189)	Acc@5 93.750 (94.659)
Epoch: [91][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1274 (1.9722)	Acc@1 65.625 (71.809)	Acc@5 92.578 (94.619)
Epoch: [91][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0642 (1.9749)	Acc@1 68.359 (71.807)	Acc@5 92.578 (94.633)
Epoch: [91][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9728 (1.9725)	Acc@1 72.266 (71.918)	Acc@5 93.750 (94.643)
Epoch: [91][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0152 (1.9752)	Acc@1 69.922 (71.840)	Acc@5 94.922 (94.593)
Epoch: [91][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9921 (1.9775)	Acc@1 71.875 (71.819)	Acc@5 92.969 (94.577)
Epoch: [91][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1575 (1.9846)	Acc@1 66.797 (71.607)	Acc@5 93.750 (94.460)
Epoch: [91][130/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2245 (1.9917)	Acc@1 65.234 (71.461)	Acc@5 93.359 (94.403)
Epoch: [91][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0731 (1.9994)	Acc@1 68.750 (71.315)	Acc@5 92.188 (94.301)
Epoch: [91][150/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0486 (2.0065)	Acc@1 75.391 (71.192)	Acc@5 96.094 (94.229)
Epoch: [91][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1520 (2.0147)	Acc@1 68.750 (70.975)	Acc@5 93.750 (94.141)
Epoch: [91][170/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0834 (2.0212)	Acc@1 68.359 (70.824)	Acc@5 94.922 (94.065)
Epoch: [91][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0859 (2.0265)	Acc@1 69.141 (70.658)	Acc@5 94.141 (94.000)
Epoch: [91][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.1857 (2.0327)	Acc@1 64.453 (70.495)	Acc@5 91.016 (93.897)
num momentum params: 26
[0.1, 2.033937413406372, 1.7957415449619294, 70.476, 54.28, tensor(0.5112, device='cuda:0', grad_fn=<DivBackward0>), 3.04668927192688, 0.4003570079803466]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [92 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [92][0/196]	Time 0.052 (0.052)	Data 0.184 (0.184)	Loss 1.9820 (1.9820)	Acc@1 72.266 (72.266)	Acc@5 94.531 (94.531)
Epoch: [92][10/196]	Time 0.017 (0.021)	Data 0.002 (0.019)	Loss 2.0834 (1.9739)	Acc@1 68.750 (72.798)	Acc@5 91.797 (94.141)
Epoch: [92][20/196]	Time 0.013 (0.018)	Data 0.004 (0.011)	Loss 1.9828 (1.9882)	Acc@1 73.828 (72.433)	Acc@5 94.141 (94.215)
Epoch: [92][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.8365 (1.9574)	Acc@1 75.000 (73.387)	Acc@5 94.531 (94.531)
Epoch: [92][40/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9886 (1.9349)	Acc@1 72.266 (73.809)	Acc@5 94.531 (94.846)
Epoch: [92][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8114 (1.9296)	Acc@1 76.172 (73.935)	Acc@5 94.922 (94.930)
Epoch: [92][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.7583 (1.9283)	Acc@1 77.734 (73.841)	Acc@5 95.703 (94.915)
Epoch: [92][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0666 (1.9349)	Acc@1 67.969 (73.460)	Acc@5 94.531 (94.933)
Epoch: [92][80/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8923 (1.9392)	Acc@1 75.000 (73.327)	Acc@5 95.312 (94.893)
Epoch: [92][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0949 (1.9430)	Acc@1 68.359 (73.240)	Acc@5 91.797 (94.780)
Epoch: [92][100/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0135 (1.9493)	Acc@1 72.266 (73.058)	Acc@5 92.578 (94.694)
Epoch: [92][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1413 (1.9553)	Acc@1 67.969 (72.836)	Acc@5 94.141 (94.658)
Epoch: [92][120/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0197 (1.9620)	Acc@1 71.875 (72.640)	Acc@5 92.188 (94.554)
Epoch: [92][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0333 (1.9705)	Acc@1 69.141 (72.346)	Acc@5 93.359 (94.412)
Epoch: [92][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0837 (1.9790)	Acc@1 70.312 (72.116)	Acc@5 91.406 (94.318)
Epoch: [92][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1146 (1.9860)	Acc@1 69.922 (71.960)	Acc@5 92.578 (94.273)
Epoch: [92][160/196]	Time 0.011 (0.016)	Data 0.007 (0.003)	Loss 2.2306 (1.9898)	Acc@1 66.406 (71.846)	Acc@5 91.406 (94.245)
Epoch: [92][170/196]	Time 0.011 (0.015)	Data 0.007 (0.003)	Loss 2.0653 (1.9949)	Acc@1 71.875 (71.690)	Acc@5 91.406 (94.193)
Epoch: [92][180/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0467 (2.0031)	Acc@1 69.531 (71.489)	Acc@5 91.797 (94.085)
Epoch: [92][190/196]	Time 0.013 (0.015)	Data 0.004 (0.003)	Loss 2.1121 (2.0070)	Acc@1 69.141 (71.386)	Acc@5 92.188 (94.040)
num momentum params: 26
[0.1, 2.0094575900268556, 1.9148802399635314, 71.324, 52.7, tensor(0.5172, device='cuda:0', grad_fn=<DivBackward0>), 3.046097993850708, 0.4177587032318115]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [93 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [93][0/196]	Time 0.061 (0.061)	Data 0.183 (0.183)	Loss 1.9681 (1.9681)	Acc@1 75.000 (75.000)	Acc@5 95.703 (95.703)
Epoch: [93][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 1.9319 (1.9935)	Acc@1 75.000 (72.159)	Acc@5 94.531 (93.999)
Epoch: [93][20/196]	Time 0.013 (0.018)	Data 0.005 (0.011)	Loss 2.0544 (1.9517)	Acc@1 69.531 (73.065)	Acc@5 94.141 (94.587)
Epoch: [93][30/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 1.9796 (1.9314)	Acc@1 72.656 (73.601)	Acc@5 91.797 (94.720)
Epoch: [93][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 1.9338 (1.9420)	Acc@1 74.609 (73.114)	Acc@5 95.703 (94.769)
Epoch: [93][50/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 1.8874 (1.9405)	Acc@1 74.609 (73.100)	Acc@5 95.312 (94.830)
Epoch: [93][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9048 (1.9393)	Acc@1 74.219 (73.322)	Acc@5 95.312 (94.807)
Epoch: [93][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8730 (1.9397)	Acc@1 73.047 (73.129)	Acc@5 94.531 (94.845)
Epoch: [93][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9776 (1.9375)	Acc@1 71.094 (73.192)	Acc@5 94.141 (94.878)
Epoch: [93][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9203 (1.9427)	Acc@1 71.875 (72.957)	Acc@5 96.094 (94.810)
Epoch: [93][100/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0251 (1.9476)	Acc@1 73.438 (72.904)	Acc@5 93.359 (94.744)
Epoch: [93][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9607 (1.9527)	Acc@1 75.391 (72.825)	Acc@5 93.750 (94.651)
Epoch: [93][120/196]	Time 0.011 (0.016)	Data 0.019 (0.004)	Loss 1.9437 (1.9615)	Acc@1 73.828 (72.605)	Acc@5 94.141 (94.534)
Epoch: [93][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9726 (1.9677)	Acc@1 73.438 (72.391)	Acc@5 94.922 (94.492)
Epoch: [93][140/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0748 (1.9750)	Acc@1 70.703 (72.124)	Acc@5 92.188 (94.368)
Epoch: [93][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0856 (1.9763)	Acc@1 67.969 (72.028)	Acc@5 92.188 (94.389)
Epoch: [93][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0641 (1.9839)	Acc@1 71.094 (71.848)	Acc@5 93.359 (94.296)
Epoch: [93][170/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.1663 (1.9907)	Acc@1 69.531 (71.711)	Acc@5 90.625 (94.200)
Epoch: [93][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9662 (1.9978)	Acc@1 72.266 (71.469)	Acc@5 93.359 (94.123)
Epoch: [93][190/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9679 (2.0000)	Acc@1 71.484 (71.353)	Acc@5 93.750 (94.077)
num momentum params: 26
[0.1, 2.000235778579712, 2.0141340458393096, 71.38, 51.18, tensor(0.5194, device='cuda:0', grad_fn=<DivBackward0>), 3.0796380043029785, 0.4081997871398926]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [94 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [94][0/196]	Time 0.057 (0.057)	Data 0.185 (0.185)	Loss 1.8700 (1.8700)	Acc@1 75.391 (75.391)	Acc@5 94.531 (94.531)
Epoch: [94][10/196]	Time 0.015 (0.021)	Data 0.002 (0.019)	Loss 2.1066 (1.9601)	Acc@1 68.750 (72.479)	Acc@5 94.141 (94.851)
Epoch: [94][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 2.0400 (1.9202)	Acc@1 71.094 (73.400)	Acc@5 94.531 (95.294)
Epoch: [94][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.8578 (1.9236)	Acc@1 75.000 (73.324)	Acc@5 93.750 (95.312)
Epoch: [94][40/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9643 (1.9205)	Acc@1 72.266 (73.399)	Acc@5 95.703 (95.255)
Epoch: [94][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.8702 (1.9113)	Acc@1 75.391 (73.744)	Acc@5 96.094 (95.251)
Epoch: [94][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0866 (1.9195)	Acc@1 67.188 (73.412)	Acc@5 94.141 (95.165)
Epoch: [94][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.8688 (1.9253)	Acc@1 77.734 (73.278)	Acc@5 94.141 (95.048)
Epoch: [94][80/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0470 (1.9375)	Acc@1 68.359 (72.902)	Acc@5 94.141 (94.893)
Epoch: [94][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8790 (1.9459)	Acc@1 74.219 (72.695)	Acc@5 96.094 (94.763)
Epoch: [94][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8905 (1.9558)	Acc@1 75.391 (72.370)	Acc@5 94.922 (94.686)
Epoch: [94][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0382 (1.9671)	Acc@1 69.141 (72.107)	Acc@5 92.188 (94.545)
Epoch: [94][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1554 (1.9718)	Acc@1 67.188 (72.001)	Acc@5 92.969 (94.493)
Epoch: [94][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1409 (1.9740)	Acc@1 68.750 (71.982)	Acc@5 94.531 (94.466)
Epoch: [94][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1142 (1.9804)	Acc@1 67.188 (71.786)	Acc@5 91.797 (94.382)
Epoch: [94][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0854 (1.9893)	Acc@1 70.703 (71.544)	Acc@5 92.969 (94.257)
Epoch: [94][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1242 (1.9957)	Acc@1 70.703 (71.363)	Acc@5 92.188 (94.165)
Epoch: [94][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1191 (2.0010)	Acc@1 69.531 (71.286)	Acc@5 92.578 (94.086)
Epoch: [94][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9951 (2.0045)	Acc@1 71.875 (71.202)	Acc@5 93.750 (94.015)
Epoch: [94][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1755 (2.0107)	Acc@1 65.234 (71.039)	Acc@5 92.578 (93.983)
num momentum params: 26
[0.1, 2.0133119506073, 2.0258819532394408, 70.988, 51.06, tensor(0.5171, device='cuda:0', grad_fn=<DivBackward0>), 3.057490110397339, 0.3918645381927491]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [95 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [95][0/196]	Time 0.059 (0.059)	Data 0.197 (0.197)	Loss 2.0836 (2.0836)	Acc@1 70.312 (70.312)	Acc@5 93.750 (93.750)
Epoch: [95][10/196]	Time 0.017 (0.020)	Data 0.001 (0.020)	Loss 1.9789 (2.0356)	Acc@1 71.484 (69.780)	Acc@5 94.922 (93.821)
Epoch: [95][20/196]	Time 0.011 (0.018)	Data 0.016 (0.012)	Loss 1.9427 (1.9819)	Acc@1 70.703 (71.819)	Acc@5 94.922 (94.606)
Epoch: [95][30/196]	Time 0.021 (0.018)	Data 0.001 (0.009)	Loss 1.8005 (1.9711)	Acc@1 75.000 (72.215)	Acc@5 97.266 (94.834)
Epoch: [95][40/196]	Time 0.011 (0.017)	Data 0.008 (0.008)	Loss 1.9514 (1.9587)	Acc@1 73.047 (72.685)	Acc@5 94.141 (94.855)
Epoch: [95][50/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 2.0846 (1.9576)	Acc@1 69.531 (72.587)	Acc@5 94.922 (94.953)
Epoch: [95][60/196]	Time 0.011 (0.016)	Data 0.015 (0.006)	Loss 1.9381 (1.9636)	Acc@1 71.484 (72.362)	Acc@5 95.312 (94.877)
Epoch: [95][70/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9724 (1.9697)	Acc@1 71.875 (72.150)	Acc@5 94.531 (94.757)
Epoch: [95][80/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0364 (1.9709)	Acc@1 70.703 (72.217)	Acc@5 94.922 (94.724)
Epoch: [95][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9346 (1.9768)	Acc@1 73.438 (72.141)	Acc@5 95.312 (94.664)
Epoch: [95][100/196]	Time 0.014 (0.016)	Data 0.020 (0.005)	Loss 1.8998 (1.9842)	Acc@1 73.828 (71.968)	Acc@5 95.312 (94.566)
Epoch: [95][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0351 (1.9888)	Acc@1 71.094 (71.843)	Acc@5 94.531 (94.500)
Epoch: [95][120/196]	Time 0.011 (0.016)	Data 0.021 (0.005)	Loss 2.0760 (1.9929)	Acc@1 70.312 (71.788)	Acc@5 92.578 (94.392)
Epoch: [95][130/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1387 (1.9971)	Acc@1 69.922 (71.708)	Acc@5 91.797 (94.323)
Epoch: [95][140/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.1200 (2.0012)	Acc@1 67.969 (71.587)	Acc@5 91.797 (94.251)
Epoch: [95][150/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1109 (2.0061)	Acc@1 71.484 (71.513)	Acc@5 92.969 (94.213)
Epoch: [95][160/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0352 (2.0121)	Acc@1 68.750 (71.351)	Acc@5 94.141 (94.073)
Epoch: [95][170/196]	Time 0.013 (0.016)	Data 0.001 (0.004)	Loss 1.9589 (2.0162)	Acc@1 72.656 (71.201)	Acc@5 94.922 (94.054)
Epoch: [95][180/196]	Time 0.012 (0.016)	Data 0.011 (0.004)	Loss 2.1013 (2.0199)	Acc@1 66.797 (71.128)	Acc@5 94.141 (93.987)
Epoch: [95][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0866 (2.0220)	Acc@1 68.750 (71.057)	Acc@5 93.359 (93.950)
num momentum params: 26
[0.1, 2.022724621429443, 1.833769519329071, 71.024, 54.52, tensor(0.5154, device='cuda:0', grad_fn=<DivBackward0>), 3.1458089351654053, 0.3938379287719726]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [96 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [96][0/196]	Time 0.059 (0.059)	Data 0.198 (0.198)	Loss 1.8168 (1.8168)	Acc@1 76.953 (76.953)	Acc@5 96.875 (96.875)
Epoch: [96][10/196]	Time 0.018 (0.020)	Data 0.001 (0.020)	Loss 2.1357 (1.9806)	Acc@1 64.453 (72.017)	Acc@5 94.141 (94.780)
Epoch: [96][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 1.8768 (1.9527)	Acc@1 76.953 (72.768)	Acc@5 96.484 (95.145)
Epoch: [96][30/196]	Time 0.017 (0.017)	Data 0.000 (0.009)	Loss 1.8495 (1.9502)	Acc@1 78.125 (72.858)	Acc@5 94.922 (94.947)
Epoch: [96][40/196]	Time 0.011 (0.016)	Data 0.007 (0.008)	Loss 1.8303 (1.9438)	Acc@1 76.953 (72.904)	Acc@5 96.094 (94.989)
Epoch: [96][50/196]	Time 0.018 (0.016)	Data 0.000 (0.006)	Loss 1.8333 (1.9339)	Acc@1 76.562 (73.330)	Acc@5 94.922 (95.014)
Epoch: [96][60/196]	Time 0.011 (0.016)	Data 0.006 (0.006)	Loss 1.8647 (1.9315)	Acc@1 76.562 (73.604)	Acc@5 95.703 (95.146)
Epoch: [96][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9110 (1.9358)	Acc@1 75.781 (73.278)	Acc@5 94.922 (95.120)
Epoch: [96][80/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9897 (1.9400)	Acc@1 69.531 (73.134)	Acc@5 94.922 (95.095)
Epoch: [96][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.8945 (1.9443)	Acc@1 71.875 (73.017)	Acc@5 96.094 (95.003)
Epoch: [96][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0385 (1.9501)	Acc@1 71.484 (72.915)	Acc@5 92.969 (94.926)
Epoch: [96][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8874 (1.9552)	Acc@1 74.219 (72.772)	Acc@5 96.484 (94.827)
Epoch: [96][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 1.9138 (1.9590)	Acc@1 73.438 (72.689)	Acc@5 95.703 (94.760)
Epoch: [96][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 1.9693 (1.9628)	Acc@1 67.969 (72.531)	Acc@5 97.656 (94.728)
Epoch: [96][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 1.9221 (1.9684)	Acc@1 76.562 (72.396)	Acc@5 94.531 (94.625)
Epoch: [96][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0412 (1.9730)	Acc@1 66.797 (72.268)	Acc@5 92.969 (94.544)
Epoch: [96][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0903 (1.9772)	Acc@1 70.703 (72.171)	Acc@5 92.188 (94.483)
Epoch: [96][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0753 (1.9828)	Acc@1 69.922 (71.994)	Acc@5 94.141 (94.422)
Epoch: [96][180/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 2.2374 (1.9909)	Acc@1 64.844 (71.752)	Acc@5 92.188 (94.333)
Epoch: [96][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2184 (1.9978)	Acc@1 64.844 (71.578)	Acc@5 94.531 (94.239)
num momentum params: 26
[0.1, 1.999022924194336, 1.8442633879184722, 71.536, 54.51, tensor(0.5214, device='cuda:0', grad_fn=<DivBackward0>), 3.0130019187927246, 0.4032623767852783]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [97 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [97][0/196]	Time 0.058 (0.058)	Data 0.180 (0.180)	Loss 1.9020 (1.9020)	Acc@1 73.438 (73.438)	Acc@5 95.703 (95.703)
Epoch: [97][10/196]	Time 0.019 (0.020)	Data 0.002 (0.019)	Loss 1.8089 (1.9150)	Acc@1 74.219 (73.970)	Acc@5 95.703 (94.567)
Epoch: [97][20/196]	Time 0.018 (0.018)	Data 0.001 (0.011)	Loss 1.8897 (1.9040)	Acc@1 73.438 (73.996)	Acc@5 96.094 (95.275)
Epoch: [97][30/196]	Time 0.019 (0.018)	Data 0.002 (0.008)	Loss 1.7377 (1.8836)	Acc@1 76.953 (74.773)	Acc@5 96.484 (95.502)
Epoch: [97][40/196]	Time 0.016 (0.017)	Data 0.001 (0.007)	Loss 2.1371 (1.9127)	Acc@1 70.312 (73.857)	Acc@5 91.797 (95.379)
Epoch: [97][50/196]	Time 0.017 (0.017)	Data 0.002 (0.006)	Loss 1.9440 (1.9187)	Acc@1 71.484 (73.591)	Acc@5 94.531 (95.335)
Epoch: [97][60/196]	Time 0.018 (0.017)	Data 0.007 (0.005)	Loss 1.9650 (1.9227)	Acc@1 71.094 (73.386)	Acc@5 94.922 (95.255)
Epoch: [97][70/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 1.9737 (1.9212)	Acc@1 71.875 (73.493)	Acc@5 94.922 (95.208)
Epoch: [97][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9958 (1.9296)	Acc@1 73.047 (73.211)	Acc@5 92.188 (95.110)
Epoch: [97][90/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0293 (1.9361)	Acc@1 70.703 (73.068)	Acc@5 91.016 (94.948)
Epoch: [97][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 1.9783 (1.9422)	Acc@1 71.484 (72.912)	Acc@5 94.531 (94.918)
Epoch: [97][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1041 (1.9513)	Acc@1 66.016 (72.625)	Acc@5 95.703 (94.834)
Epoch: [97][120/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 1.9764 (1.9588)	Acc@1 73.047 (72.433)	Acc@5 93.750 (94.715)
Epoch: [97][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1360 (1.9723)	Acc@1 71.094 (72.173)	Acc@5 91.406 (94.537)
Epoch: [97][140/196]	Time 0.013 (0.016)	Data 0.009 (0.004)	Loss 2.2138 (1.9811)	Acc@1 66.406 (71.983)	Acc@5 93.750 (94.440)
Epoch: [97][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9893 (1.9885)	Acc@1 71.875 (71.831)	Acc@5 92.578 (94.337)
Epoch: [97][160/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0795 (1.9941)	Acc@1 71.094 (71.744)	Acc@5 93.359 (94.233)
Epoch: [97][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1883 (2.0022)	Acc@1 65.625 (71.562)	Acc@5 91.406 (94.134)
Epoch: [97][180/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0796 (2.0067)	Acc@1 72.266 (71.504)	Acc@5 92.578 (94.056)
Epoch: [97][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.9245 (2.0086)	Acc@1 72.266 (71.417)	Acc@5 95.703 (94.051)
num momentum params: 26
[0.1, 2.0110585047149656, 2.3598513531684877, 71.378, 48.03, tensor(0.5188, device='cuda:0', grad_fn=<DivBackward0>), 3.1341688632965083, 0.4077513217926026]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [98 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [98][0/196]	Time 0.063 (0.063)	Data 0.188 (0.188)	Loss 1.9177 (1.9177)	Acc@1 77.344 (77.344)	Acc@5 95.703 (95.703)
Epoch: [98][10/196]	Time 0.016 (0.021)	Data 0.002 (0.019)	Loss 1.9778 (1.9389)	Acc@1 73.828 (74.325)	Acc@5 94.922 (95.028)
Epoch: [98][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.8233 (1.9193)	Acc@1 77.734 (74.237)	Acc@5 96.094 (95.089)
Epoch: [98][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.9709 (1.9183)	Acc@1 72.266 (74.118)	Acc@5 94.922 (95.300)
Epoch: [98][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.7526 (1.9227)	Acc@1 81.250 (74.123)	Acc@5 96.484 (95.160)
Epoch: [98][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8907 (1.9188)	Acc@1 76.172 (74.081)	Acc@5 96.094 (95.351)
Epoch: [98][60/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 1.9882 (1.9243)	Acc@1 75.391 (73.988)	Acc@5 96.484 (95.383)
Epoch: [98][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.8478 (1.9293)	Acc@1 75.781 (73.861)	Acc@5 96.094 (95.279)
Epoch: [98][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.2487 (1.9399)	Acc@1 64.844 (73.510)	Acc@5 92.188 (95.115)
Epoch: [98][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0017 (1.9430)	Acc@1 68.750 (73.390)	Acc@5 96.484 (95.081)
Epoch: [98][100/196]	Time 0.011 (0.016)	Data 0.025 (0.005)	Loss 1.9684 (1.9520)	Acc@1 70.703 (73.020)	Acc@5 94.531 (94.964)
Epoch: [98][110/196]	Time 0.015 (0.016)	Data 0.000 (0.005)	Loss 1.9153 (1.9604)	Acc@1 75.781 (72.804)	Acc@5 94.531 (94.866)
Epoch: [98][120/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0520 (1.9665)	Acc@1 72.656 (72.627)	Acc@5 94.141 (94.780)
Epoch: [98][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1626 (1.9764)	Acc@1 68.359 (72.409)	Acc@5 92.969 (94.677)
Epoch: [98][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0398 (1.9813)	Acc@1 69.922 (72.260)	Acc@5 93.359 (94.578)
Epoch: [98][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1575 (1.9881)	Acc@1 67.969 (72.072)	Acc@5 92.578 (94.480)
Epoch: [98][160/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 2.1305 (1.9932)	Acc@1 67.188 (71.943)	Acc@5 94.922 (94.403)
Epoch: [98][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.2360 (2.0003)	Acc@1 65.625 (71.711)	Acc@5 91.016 (94.317)
Epoch: [98][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2418 (2.0075)	Acc@1 65.625 (71.508)	Acc@5 92.188 (94.259)
Epoch: [98][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0286 (2.0132)	Acc@1 71.484 (71.362)	Acc@5 93.750 (94.165)
num momentum params: 26
[0.1, 2.0141859260559083, 1.8030845105648041, 71.34, 54.18, tensor(0.5193, device='cuda:0', grad_fn=<DivBackward0>), 3.0866420269012456, 0.3940303325653077]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [99 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [99][0/196]	Time 0.059 (0.059)	Data 0.179 (0.179)	Loss 1.9089 (1.9089)	Acc@1 72.266 (72.266)	Acc@5 96.875 (96.875)
Epoch: [99][10/196]	Time 0.014 (0.020)	Data 0.004 (0.019)	Loss 1.9804 (1.9516)	Acc@1 73.047 (72.940)	Acc@5 94.922 (94.993)
Epoch: [99][20/196]	Time 0.016 (0.018)	Data 0.003 (0.011)	Loss 1.9064 (1.9280)	Acc@1 72.266 (73.251)	Acc@5 95.703 (95.108)
Epoch: [99][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9053 (1.9320)	Acc@1 75.000 (73.362)	Acc@5 94.141 (95.186)
Epoch: [99][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.9773 (1.9304)	Acc@1 72.656 (73.495)	Acc@5 93.750 (95.198)
Epoch: [99][50/196]	Time 0.013 (0.017)	Data 0.006 (0.006)	Loss 1.9748 (1.9336)	Acc@1 73.438 (73.476)	Acc@5 94.922 (95.129)
Epoch: [99][60/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0870 (1.9448)	Acc@1 71.094 (73.329)	Acc@5 93.750 (94.973)
Epoch: [99][70/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0778 (1.9498)	Acc@1 70.703 (73.212)	Acc@5 93.750 (94.911)
Epoch: [99][80/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.8232 (1.9602)	Acc@1 76.953 (72.955)	Acc@5 96.094 (94.748)
Epoch: [99][90/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9420 (1.9661)	Acc@1 73.438 (72.768)	Acc@5 96.484 (94.690)
Epoch: [99][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1743 (1.9709)	Acc@1 64.844 (72.606)	Acc@5 91.016 (94.616)
Epoch: [99][110/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0262 (1.9755)	Acc@1 71.875 (72.505)	Acc@5 93.750 (94.545)
Epoch: [99][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0490 (1.9793)	Acc@1 68.359 (72.414)	Acc@5 95.312 (94.483)
Epoch: [99][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0247 (1.9819)	Acc@1 70.312 (72.352)	Acc@5 92.969 (94.466)
Epoch: [99][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0483 (1.9857)	Acc@1 70.703 (72.271)	Acc@5 93.359 (94.415)
Epoch: [99][150/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1664 (1.9973)	Acc@1 70.312 (71.953)	Acc@5 92.188 (94.285)
Epoch: [99][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1455 (2.0004)	Acc@1 66.016 (71.800)	Acc@5 90.625 (94.228)
Epoch: [99][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2587 (2.0094)	Acc@1 64.062 (71.578)	Acc@5 94.141 (94.118)
Epoch: [99][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8497 (2.0132)	Acc@1 75.000 (71.469)	Acc@5 96.094 (94.076)
Epoch: [99][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1240 (2.0166)	Acc@1 66.797 (71.378)	Acc@5 90.625 (94.036)
num momentum params: 26
[0.1, 2.0175697271728517, 1.8186337637901306, 71.356, 53.92, tensor(0.5185, device='cuda:0', grad_fn=<DivBackward0>), 3.059699773788452, 0.3904128074645996]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [100 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [100][0/196]	Time 0.066 (0.066)	Data 0.185 (0.185)	Loss 1.9162 (1.9162)	Acc@1 71.484 (71.484)	Acc@5 95.703 (95.703)
Epoch: [100][10/196]	Time 0.016 (0.021)	Data 0.002 (0.018)	Loss 1.9309 (1.9540)	Acc@1 73.047 (72.976)	Acc@5 94.141 (94.673)
Epoch: [100][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 1.9294 (1.9449)	Acc@1 73.828 (73.624)	Acc@5 94.922 (94.773)
Epoch: [100][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 2.0009 (1.9205)	Acc@1 70.703 (74.055)	Acc@5 95.312 (95.035)
Epoch: [100][40/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 1.8238 (1.9171)	Acc@1 78.516 (74.038)	Acc@5 95.312 (95.103)
Epoch: [100][50/196]	Time 0.017 (0.017)	Data 0.002 (0.006)	Loss 1.9604 (1.9125)	Acc@1 72.266 (74.357)	Acc@5 94.922 (95.090)
Epoch: [100][60/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 2.0651 (1.9130)	Acc@1 67.188 (74.251)	Acc@5 94.531 (95.088)
Epoch: [100][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.8033 (1.9109)	Acc@1 76.562 (74.180)	Acc@5 95.312 (95.180)
Epoch: [100][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.9405 (1.9206)	Acc@1 71.094 (73.915)	Acc@5 95.312 (95.081)
Epoch: [100][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0116 (1.9310)	Acc@1 66.797 (73.669)	Acc@5 95.703 (94.999)
Epoch: [100][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0995 (1.9388)	Acc@1 68.750 (73.372)	Acc@5 95.312 (94.980)
Epoch: [100][110/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.9598 (1.9439)	Acc@1 72.656 (73.269)	Acc@5 93.359 (94.873)
Epoch: [100][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0825 (1.9486)	Acc@1 70.703 (73.124)	Acc@5 90.625 (94.809)
Epoch: [100][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1501 (1.9569)	Acc@1 67.188 (72.877)	Acc@5 90.625 (94.719)
Epoch: [100][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.2608 (1.9665)	Acc@1 63.672 (72.609)	Acc@5 91.016 (94.637)
Epoch: [100][150/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 2.2075 (1.9758)	Acc@1 66.016 (72.395)	Acc@5 93.359 (94.526)
Epoch: [100][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0813 (1.9830)	Acc@1 70.312 (72.244)	Acc@5 91.406 (94.424)
Epoch: [100][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0394 (1.9919)	Acc@1 69.141 (72.051)	Acc@5 92.969 (94.312)
Epoch: [100][180/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.0576 (1.9967)	Acc@1 72.656 (71.938)	Acc@5 92.578 (94.240)
Epoch: [100][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.1920 (2.0066)	Acc@1 63.672 (71.644)	Acc@5 91.406 (94.153)
num momentum params: 26
[0.1, 2.0078674904632567, 1.901431360244751, 71.582, 52.53, tensor(0.5209, device='cuda:0', grad_fn=<DivBackward0>), 3.0904257297515865, 0.39552211761474615]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [101 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [101][0/196]	Time 0.059 (0.059)	Data 0.183 (0.183)	Loss 1.8720 (1.8720)	Acc@1 75.000 (75.000)	Acc@5 94.531 (94.531)
Epoch: [101][10/196]	Time 0.016 (0.021)	Data 0.002 (0.019)	Loss 2.0500 (1.9939)	Acc@1 71.875 (72.763)	Acc@5 92.969 (94.070)
Epoch: [101][20/196]	Time 0.020 (0.018)	Data 0.003 (0.011)	Loss 1.9074 (1.9770)	Acc@1 73.047 (72.731)	Acc@5 94.531 (94.457)
Epoch: [101][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.9015 (1.9545)	Acc@1 74.219 (73.324)	Acc@5 96.484 (94.771)
Epoch: [101][40/196]	Time 0.017 (0.017)	Data 0.002 (0.006)	Loss 1.7801 (1.9450)	Acc@1 78.906 (73.161)	Acc@5 96.094 (94.817)
Epoch: [101][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8894 (1.9376)	Acc@1 74.609 (73.223)	Acc@5 96.484 (94.937)
Epoch: [101][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9777 (1.9337)	Acc@1 73.047 (73.361)	Acc@5 96.094 (94.973)
Epoch: [101][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9672 (1.9332)	Acc@1 71.875 (73.438)	Acc@5 94.922 (94.999)
Epoch: [101][80/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9737 (1.9434)	Acc@1 73.047 (73.187)	Acc@5 95.312 (94.864)
Epoch: [101][90/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 2.0894 (1.9461)	Acc@1 67.188 (73.068)	Acc@5 93.359 (94.810)
Epoch: [101][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0834 (1.9531)	Acc@1 71.094 (72.997)	Acc@5 92.578 (94.728)
Epoch: [101][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9057 (1.9558)	Acc@1 76.172 (72.987)	Acc@5 95.312 (94.697)
Epoch: [101][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0153 (1.9653)	Acc@1 72.656 (72.750)	Acc@5 94.141 (94.602)
Epoch: [101][130/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0579 (1.9713)	Acc@1 69.531 (72.591)	Acc@5 93.359 (94.510)
Epoch: [101][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1210 (1.9793)	Acc@1 69.141 (72.410)	Acc@5 92.578 (94.420)
Epoch: [101][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1454 (1.9871)	Acc@1 65.234 (72.134)	Acc@5 93.359 (94.363)
Epoch: [101][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1253 (1.9919)	Acc@1 65.234 (72.008)	Acc@5 94.531 (94.296)
Epoch: [101][170/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 1.9593 (1.9973)	Acc@1 72.266 (71.861)	Acc@5 92.578 (94.191)
Epoch: [101][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0707 (1.9990)	Acc@1 71.094 (71.849)	Acc@5 93.750 (94.179)
Epoch: [101][190/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 1.9810 (2.0040)	Acc@1 73.438 (71.746)	Acc@5 94.922 (94.159)
num momentum params: 26
[0.1, 2.0058292030334472, 1.7860811626911164, 71.664, 54.61, tensor(0.5219, device='cuda:0', grad_fn=<DivBackward0>), 3.079210042953491, 0.39846301078796387]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [102 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [102][0/196]	Time 0.059 (0.059)	Data 0.179 (0.179)	Loss 1.8625 (1.8625)	Acc@1 74.609 (74.609)	Acc@5 95.312 (95.312)
Epoch: [102][10/196]	Time 0.016 (0.021)	Data 0.003 (0.018)	Loss 1.9174 (1.9286)	Acc@1 74.219 (73.118)	Acc@5 94.922 (95.170)
Epoch: [102][20/196]	Time 0.016 (0.019)	Data 0.003 (0.011)	Loss 1.8355 (1.9226)	Acc@1 77.344 (73.754)	Acc@5 96.484 (95.089)
Epoch: [102][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.7930 (1.9044)	Acc@1 78.516 (74.269)	Acc@5 95.703 (95.300)
Epoch: [102][40/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.7066 (1.8868)	Acc@1 81.641 (74.714)	Acc@5 96.484 (95.436)
Epoch: [102][50/196]	Time 0.012 (0.017)	Data 0.005 (0.006)	Loss 1.9385 (1.8843)	Acc@1 72.656 (74.862)	Acc@5 96.094 (95.512)
Epoch: [102][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.8971 (1.8893)	Acc@1 75.391 (74.782)	Acc@5 96.094 (95.383)
Epoch: [102][70/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9939 (1.8977)	Acc@1 74.609 (74.466)	Acc@5 92.969 (95.340)
Epoch: [102][80/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.8502 (1.9053)	Acc@1 75.781 (74.286)	Acc@5 95.703 (95.255)
Epoch: [102][90/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.8365 (1.9093)	Acc@1 73.828 (74.189)	Acc@5 96.484 (95.171)
Epoch: [102][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9296 (1.9192)	Acc@1 72.266 (73.863)	Acc@5 95.312 (95.084)
Epoch: [102][110/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9195 (1.9239)	Acc@1 74.609 (73.726)	Acc@5 96.094 (95.070)
Epoch: [102][120/196]	Time 0.017 (0.016)	Data 0.011 (0.004)	Loss 2.1569 (1.9343)	Acc@1 68.750 (73.441)	Acc@5 92.969 (94.977)
Epoch: [102][130/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0920 (1.9473)	Acc@1 69.922 (73.071)	Acc@5 94.922 (94.835)
Epoch: [102][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1241 (1.9587)	Acc@1 67.578 (72.759)	Acc@5 92.188 (94.656)
Epoch: [102][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1789 (1.9666)	Acc@1 67.969 (72.599)	Acc@5 92.578 (94.542)
Epoch: [102][160/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.2347 (1.9757)	Acc@1 64.453 (72.297)	Acc@5 92.188 (94.449)
Epoch: [102][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0290 (1.9855)	Acc@1 72.266 (72.023)	Acc@5 93.359 (94.330)
Epoch: [102][180/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.0386 (1.9918)	Acc@1 71.484 (71.834)	Acc@5 95.312 (94.290)
Epoch: [102][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0158 (2.0004)	Acc@1 68.359 (71.609)	Acc@5 95.312 (94.210)
num momentum params: 26
[0.1, 2.0010431658935546, 1.7436170434951783, 71.594, 55.84, tensor(0.5228, device='cuda:0', grad_fn=<DivBackward0>), 3.0563161373138428, 0.3945267200469971]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [103 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [103][0/196]	Time 0.057 (0.057)	Data 0.184 (0.184)	Loss 1.9768 (1.9768)	Acc@1 71.484 (71.484)	Acc@5 95.703 (95.703)
Epoch: [103][10/196]	Time 0.017 (0.021)	Data 0.002 (0.019)	Loss 1.9849 (1.9546)	Acc@1 70.703 (72.372)	Acc@5 93.750 (95.028)
Epoch: [103][20/196]	Time 0.020 (0.018)	Data 0.000 (0.011)	Loss 1.9958 (1.9557)	Acc@1 71.094 (72.526)	Acc@5 94.531 (94.699)
Epoch: [103][30/196]	Time 0.017 (0.018)	Data 0.001 (0.008)	Loss 1.8398 (1.9290)	Acc@1 76.953 (73.589)	Acc@5 94.922 (94.985)
Epoch: [103][40/196]	Time 0.018 (0.018)	Data 0.001 (0.007)	Loss 1.8188 (1.9212)	Acc@1 76.953 (73.981)	Acc@5 96.094 (95.112)
Epoch: [103][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.7537 (1.9150)	Acc@1 75.000 (74.089)	Acc@5 96.875 (95.282)
Epoch: [103][60/196]	Time 0.016 (0.017)	Data 0.000 (0.005)	Loss 1.8751 (1.9114)	Acc@1 75.000 (74.187)	Acc@5 95.703 (95.274)
Epoch: [103][70/196]	Time 0.022 (0.017)	Data 0.001 (0.005)	Loss 2.1339 (1.9176)	Acc@1 68.750 (73.993)	Acc@5 93.359 (95.246)
Epoch: [103][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0590 (1.9243)	Acc@1 71.484 (73.833)	Acc@5 92.188 (95.071)
Epoch: [103][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0384 (1.9325)	Acc@1 71.875 (73.549)	Acc@5 94.531 (95.025)
Epoch: [103][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0770 (1.9458)	Acc@1 66.406 (73.167)	Acc@5 95.703 (94.875)
Epoch: [103][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9736 (1.9526)	Acc@1 69.531 (72.927)	Acc@5 94.141 (94.799)
Epoch: [103][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0784 (1.9640)	Acc@1 71.484 (72.621)	Acc@5 94.141 (94.628)
Epoch: [103][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0204 (1.9699)	Acc@1 68.359 (72.424)	Acc@5 94.141 (94.531)
Epoch: [103][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0352 (1.9762)	Acc@1 71.875 (72.255)	Acc@5 95.703 (94.481)
Epoch: [103][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0480 (1.9851)	Acc@1 71.875 (72.051)	Acc@5 92.969 (94.353)
Epoch: [103][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0741 (1.9896)	Acc@1 68.750 (71.919)	Acc@5 93.750 (94.291)
Epoch: [103][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1021 (1.9939)	Acc@1 67.969 (71.806)	Acc@5 92.969 (94.243)
Epoch: [103][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0716 (1.9976)	Acc@1 71.484 (71.730)	Acc@5 92.969 (94.169)
Epoch: [103][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0773 (2.0000)	Acc@1 70.312 (71.662)	Acc@5 92.578 (94.149)
num momentum params: 26
[0.1, 2.0031819533538817, 1.8692219209671022, 71.56, 53.92, tensor(0.5222, device='cuda:0', grad_fn=<DivBackward0>), 3.048983335494995, 0.4019420146942138]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [104 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [104][0/196]	Time 0.059 (0.059)	Data 0.189 (0.189)	Loss 1.9392 (1.9392)	Acc@1 73.828 (73.828)	Acc@5 93.750 (93.750)
Epoch: [104][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 1.9737 (1.9498)	Acc@1 74.219 (73.189)	Acc@5 93.359 (95.028)
Epoch: [104][20/196]	Time 0.014 (0.019)	Data 0.002 (0.011)	Loss 1.8236 (1.9339)	Acc@1 77.344 (73.586)	Acc@5 95.703 (95.108)
Epoch: [104][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 2.0565 (1.9235)	Acc@1 70.703 (73.904)	Acc@5 94.922 (95.186)
Epoch: [104][40/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 2.1020 (1.9329)	Acc@1 68.750 (73.685)	Acc@5 93.750 (95.151)
Epoch: [104][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8524 (1.9414)	Acc@1 76.562 (73.460)	Acc@5 95.703 (94.899)
Epoch: [104][60/196]	Time 0.013 (0.017)	Data 0.005 (0.005)	Loss 1.6830 (1.9352)	Acc@1 82.422 (73.732)	Acc@5 96.094 (94.941)
Epoch: [104][70/196]	Time 0.016 (0.017)	Data 0.003 (0.005)	Loss 2.0746 (1.9420)	Acc@1 72.266 (73.537)	Acc@5 91.016 (94.867)
Epoch: [104][80/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0981 (1.9469)	Acc@1 68.750 (73.322)	Acc@5 92.188 (94.830)
Epoch: [104][90/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9257 (1.9501)	Acc@1 72.266 (73.227)	Acc@5 96.094 (94.815)
Epoch: [104][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9734 (1.9507)	Acc@1 71.484 (73.163)	Acc@5 94.531 (94.810)
Epoch: [104][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0007 (1.9540)	Acc@1 71.875 (73.093)	Acc@5 92.578 (94.700)
Epoch: [104][120/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 1.9546 (1.9554)	Acc@1 73.828 (72.998)	Acc@5 94.531 (94.667)
Epoch: [104][130/196]	Time 0.028 (0.016)	Data 0.001 (0.004)	Loss 2.0312 (1.9631)	Acc@1 66.406 (72.773)	Acc@5 94.141 (94.516)
Epoch: [104][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0648 (1.9678)	Acc@1 68.750 (72.620)	Acc@5 94.141 (94.456)
Epoch: [104][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1088 (1.9745)	Acc@1 69.531 (72.405)	Acc@5 91.406 (94.373)
Epoch: [104][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0466 (1.9799)	Acc@1 67.578 (72.212)	Acc@5 92.578 (94.335)
Epoch: [104][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9177 (1.9856)	Acc@1 75.000 (72.090)	Acc@5 95.703 (94.255)
Epoch: [104][180/196]	Time 0.020 (0.016)	Data 0.000 (0.004)	Loss 2.1201 (1.9924)	Acc@1 70.312 (71.974)	Acc@5 89.844 (94.147)
Epoch: [104][190/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.3058 (1.9995)	Acc@1 66.406 (71.812)	Acc@5 89.062 (94.069)
num momentum params: 26
[0.1, 2.003235072402954, 2.091843942403793, 71.708, 49.46, tensor(0.5227, device='cuda:0', grad_fn=<DivBackward0>), 3.1192259788513184, 0.4236788749694824]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [105 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [105][0/196]	Time 0.058 (0.058)	Data 0.184 (0.184)	Loss 1.9735 (1.9735)	Acc@1 70.312 (70.312)	Acc@5 93.750 (93.750)
Epoch: [105][10/196]	Time 0.016 (0.021)	Data 0.002 (0.019)	Loss 1.8819 (1.9448)	Acc@1 73.828 (72.514)	Acc@5 95.312 (95.028)
Epoch: [105][20/196]	Time 0.013 (0.019)	Data 0.002 (0.011)	Loss 1.8683 (1.9531)	Acc@1 74.219 (72.433)	Acc@5 96.875 (95.089)
Epoch: [105][30/196]	Time 0.013 (0.018)	Data 0.004 (0.008)	Loss 2.0089 (1.9375)	Acc@1 68.750 (73.047)	Acc@5 96.484 (95.098)
Epoch: [105][40/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 1.9187 (1.9289)	Acc@1 74.609 (73.523)	Acc@5 94.922 (95.198)
Epoch: [105][50/196]	Time 0.015 (0.017)	Data 0.008 (0.006)	Loss 1.9641 (1.9246)	Acc@1 74.219 (73.752)	Acc@5 96.094 (95.274)
Epoch: [105][60/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0080 (1.9322)	Acc@1 70.703 (73.540)	Acc@5 93.750 (95.165)
Epoch: [105][70/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.8743 (1.9371)	Acc@1 75.781 (73.438)	Acc@5 97.656 (95.098)
Epoch: [105][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9128 (1.9435)	Acc@1 76.953 (73.312)	Acc@5 95.703 (95.033)
Epoch: [105][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9056 (1.9462)	Acc@1 78.125 (73.219)	Acc@5 92.969 (94.952)
Epoch: [105][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0050 (1.9498)	Acc@1 69.531 (73.078)	Acc@5 94.531 (94.852)
Epoch: [105][110/196]	Time 0.015 (0.016)	Data 0.005 (0.004)	Loss 1.9440 (1.9520)	Acc@1 74.609 (72.994)	Acc@5 93.359 (94.792)
Epoch: [105][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0819 (1.9594)	Acc@1 69.922 (72.731)	Acc@5 92.578 (94.738)
Epoch: [105][130/196]	Time 0.014 (0.016)	Data 0.006 (0.004)	Loss 1.9634 (1.9602)	Acc@1 73.047 (72.734)	Acc@5 93.359 (94.722)
Epoch: [105][140/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 2.2765 (1.9640)	Acc@1 62.891 (72.634)	Acc@5 91.406 (94.653)
Epoch: [105][150/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0807 (1.9669)	Acc@1 72.656 (72.623)	Acc@5 94.531 (94.593)
Epoch: [105][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1764 (1.9733)	Acc@1 66.797 (72.433)	Acc@5 92.578 (94.519)
Epoch: [105][170/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1158 (1.9814)	Acc@1 68.750 (72.275)	Acc@5 92.578 (94.410)
Epoch: [105][180/196]	Time 0.021 (0.016)	Data 0.000 (0.004)	Loss 2.0236 (1.9863)	Acc@1 68.359 (72.147)	Acc@5 94.531 (94.331)
Epoch: [105][190/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0944 (1.9925)	Acc@1 67.969 (71.922)	Acc@5 94.141 (94.255)
num momentum params: 26
[0.1, 1.99475029586792, 1.842228502035141, 71.86, 53.86, tensor(0.5242, device='cuda:0', grad_fn=<DivBackward0>), 3.167661666870117, 0.39516162872314453]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [106 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [106][0/196]	Time 0.060 (0.060)	Data 0.195 (0.195)	Loss 1.7863 (1.7863)	Acc@1 75.781 (75.781)	Acc@5 96.484 (96.484)
Epoch: [106][10/196]	Time 0.012 (0.022)	Data 0.005 (0.019)	Loss 1.8685 (1.9498)	Acc@1 75.000 (73.295)	Acc@5 95.312 (94.744)
Epoch: [106][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.9649 (1.9436)	Acc@1 69.922 (73.344)	Acc@5 94.922 (95.033)
Epoch: [106][30/196]	Time 0.013 (0.018)	Data 0.004 (0.009)	Loss 2.0232 (1.9299)	Acc@1 73.047 (73.513)	Acc@5 93.750 (95.312)
Epoch: [106][40/196]	Time 0.017 (0.018)	Data 0.000 (0.008)	Loss 1.8868 (1.9214)	Acc@1 76.953 (73.514)	Acc@5 96.094 (95.322)
Epoch: [106][50/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 1.7388 (1.9153)	Acc@1 76.562 (73.759)	Acc@5 97.656 (95.466)
Epoch: [106][60/196]	Time 0.024 (0.017)	Data 0.002 (0.006)	Loss 2.1424 (1.9153)	Acc@1 66.797 (73.841)	Acc@5 94.141 (95.428)
Epoch: [106][70/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.9764 (1.9151)	Acc@1 71.094 (73.812)	Acc@5 93.359 (95.417)
Epoch: [106][80/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 2.0505 (1.9195)	Acc@1 70.312 (73.659)	Acc@5 94.922 (95.361)
Epoch: [106][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.8246 (1.9243)	Acc@1 78.906 (73.613)	Acc@5 96.875 (95.312)
Epoch: [106][100/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8822 (1.9286)	Acc@1 73.438 (73.519)	Acc@5 94.922 (95.181)
Epoch: [106][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1408 (1.9383)	Acc@1 70.703 (73.325)	Acc@5 92.578 (95.045)
Epoch: [106][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9933 (1.9445)	Acc@1 72.266 (73.212)	Acc@5 94.141 (94.932)
Epoch: [106][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2336 (1.9546)	Acc@1 65.234 (72.889)	Acc@5 89.844 (94.797)
Epoch: [106][140/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0023 (1.9618)	Acc@1 69.141 (72.673)	Acc@5 93.750 (94.711)
Epoch: [106][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0629 (1.9713)	Acc@1 69.531 (72.413)	Acc@5 94.141 (94.578)
Epoch: [106][160/196]	Time 0.012 (0.016)	Data 0.022 (0.005)	Loss 2.0565 (1.9807)	Acc@1 69.531 (72.173)	Acc@5 92.578 (94.449)
Epoch: [106][170/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0360 (1.9877)	Acc@1 72.266 (72.010)	Acc@5 94.922 (94.296)
Epoch: [106][180/196]	Time 0.012 (0.016)	Data 0.023 (0.005)	Loss 1.9221 (1.9922)	Acc@1 72.656 (71.860)	Acc@5 95.703 (94.240)
Epoch: [106][190/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1500 (1.9969)	Acc@1 66.406 (71.754)	Acc@5 92.578 (94.179)
num momentum params: 26
[0.1, 1.9987367646789551, 1.8305265879631043, 71.718, 53.16, tensor(0.5235, device='cuda:0', grad_fn=<DivBackward0>), 3.174474954605103, 0.39043593406677246]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [107 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [107][0/196]	Time 0.058 (0.058)	Data 0.176 (0.176)	Loss 2.0391 (2.0391)	Acc@1 67.969 (67.969)	Acc@5 93.359 (93.359)
Epoch: [107][10/196]	Time 0.016 (0.021)	Data 0.002 (0.018)	Loss 1.9126 (2.0210)	Acc@1 76.172 (71.804)	Acc@5 94.922 (93.821)
Epoch: [107][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.9584 (1.9896)	Acc@1 73.438 (72.024)	Acc@5 93.359 (94.327)
Epoch: [107][30/196]	Time 0.015 (0.017)	Data 0.003 (0.008)	Loss 1.7728 (1.9545)	Acc@1 78.516 (73.072)	Acc@5 96.484 (94.909)
Epoch: [107][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.9412 (1.9399)	Acc@1 71.094 (73.409)	Acc@5 96.094 (95.112)
Epoch: [107][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8387 (1.9407)	Acc@1 76.562 (73.269)	Acc@5 96.484 (95.083)
Epoch: [107][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.9206 (1.9427)	Acc@1 74.219 (73.201)	Acc@5 94.531 (95.012)
Epoch: [107][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9289 (1.9393)	Acc@1 73.828 (73.201)	Acc@5 93.750 (95.065)
Epoch: [107][80/196]	Time 0.012 (0.016)	Data 0.004 (0.004)	Loss 2.0559 (1.9428)	Acc@1 68.359 (73.013)	Acc@5 93.750 (95.028)
Epoch: [107][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0181 (1.9519)	Acc@1 71.484 (72.768)	Acc@5 94.531 (95.003)
Epoch: [107][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0710 (1.9542)	Acc@1 64.453 (72.560)	Acc@5 93.750 (94.968)
Epoch: [107][110/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0324 (1.9614)	Acc@1 69.922 (72.318)	Acc@5 94.531 (94.925)
Epoch: [107][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9682 (1.9660)	Acc@1 73.047 (72.214)	Acc@5 95.703 (94.851)
Epoch: [107][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2437 (1.9725)	Acc@1 66.797 (72.048)	Acc@5 92.969 (94.826)
Epoch: [107][140/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0272 (1.9772)	Acc@1 69.531 (71.941)	Acc@5 92.969 (94.764)
Epoch: [107][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9427 (1.9825)	Acc@1 70.703 (71.839)	Acc@5 94.922 (94.728)
Epoch: [107][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0705 (1.9861)	Acc@1 69.141 (71.812)	Acc@5 92.969 (94.650)
Epoch: [107][170/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1661 (1.9923)	Acc@1 69.922 (71.724)	Acc@5 93.750 (94.547)
Epoch: [107][180/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0542 (1.9946)	Acc@1 71.875 (71.683)	Acc@5 94.531 (94.503)
Epoch: [107][190/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1025 (2.0014)	Acc@1 69.922 (71.531)	Acc@5 90.234 (94.372)
num momentum params: 26
[0.1, 2.0040276150512697, 1.7764010953903198, 71.44, 55.62, tensor(0.5220, device='cuda:0', grad_fn=<DivBackward0>), 3.080360651016236, 0.39576244354248047]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [108 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [108][0/196]	Time 0.058 (0.058)	Data 0.199 (0.199)	Loss 1.8354 (1.8354)	Acc@1 75.781 (75.781)	Acc@5 94.922 (94.922)
Epoch: [108][10/196]	Time 0.016 (0.021)	Data 0.002 (0.020)	Loss 2.0471 (2.0218)	Acc@1 67.969 (70.845)	Acc@5 94.531 (94.460)
Epoch: [108][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 2.0329 (2.0129)	Acc@1 68.359 (71.243)	Acc@5 92.969 (94.308)
Epoch: [108][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 2.0020 (2.0025)	Acc@1 76.172 (71.913)	Acc@5 93.359 (94.141)
Epoch: [108][40/196]	Time 0.016 (0.017)	Data 0.003 (0.007)	Loss 1.7904 (1.9805)	Acc@1 78.516 (72.742)	Acc@5 95.312 (94.284)
Epoch: [108][50/196]	Time 0.019 (0.017)	Data 0.002 (0.006)	Loss 1.9232 (1.9657)	Acc@1 73.438 (73.047)	Acc@5 97.266 (94.531)
Epoch: [108][60/196]	Time 0.014 (0.017)	Data 0.004 (0.005)	Loss 1.9231 (1.9605)	Acc@1 75.000 (73.162)	Acc@5 94.531 (94.614)
Epoch: [108][70/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9423 (1.9572)	Acc@1 75.000 (73.393)	Acc@5 93.750 (94.641)
Epoch: [108][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9605 (1.9604)	Acc@1 73.828 (73.259)	Acc@5 92.969 (94.560)
Epoch: [108][90/196]	Time 0.026 (0.016)	Data 0.003 (0.004)	Loss 1.9311 (1.9680)	Acc@1 73.438 (72.995)	Acc@5 94.922 (94.514)
Epoch: [108][100/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 1.9774 (1.9733)	Acc@1 72.266 (72.842)	Acc@5 94.531 (94.458)
Epoch: [108][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0040 (1.9791)	Acc@1 73.438 (72.649)	Acc@5 92.969 (94.387)
Epoch: [108][120/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0023 (1.9820)	Acc@1 72.266 (72.543)	Acc@5 92.188 (94.338)
Epoch: [108][130/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0585 (1.9846)	Acc@1 70.703 (72.465)	Acc@5 92.969 (94.358)
Epoch: [108][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1553 (1.9883)	Acc@1 69.141 (72.357)	Acc@5 94.922 (94.359)
Epoch: [108][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0793 (1.9879)	Acc@1 70.312 (72.416)	Acc@5 92.578 (94.340)
Epoch: [108][160/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1214 (1.9922)	Acc@1 70.312 (72.258)	Acc@5 91.406 (94.310)
Epoch: [108][170/196]	Time 0.023 (0.016)	Data 0.000 (0.004)	Loss 2.1381 (1.9967)	Acc@1 69.531 (72.113)	Acc@5 94.922 (94.301)
Epoch: [108][180/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 1.9881 (1.9996)	Acc@1 73.047 (72.054)	Acc@5 95.312 (94.302)
Epoch: [108][190/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2079 (2.0058)	Acc@1 69.531 (71.846)	Acc@5 91.797 (94.245)
num momentum params: 26
[0.1, 2.0085967724609377, 1.9312382292747499, 71.764, 53.4, tensor(0.5208, device='cuda:0', grad_fn=<DivBackward0>), 3.1352462768554683, 0.39643430709838867]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [109 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [109][0/196]	Time 0.068 (0.068)	Data 0.191 (0.191)	Loss 1.8977 (1.8977)	Acc@1 72.266 (72.266)	Acc@5 95.312 (95.312)
Epoch: [109][10/196]	Time 0.015 (0.022)	Data 0.002 (0.019)	Loss 1.7498 (1.8995)	Acc@1 82.031 (75.355)	Acc@5 96.484 (95.312)
Epoch: [109][20/196]	Time 0.011 (0.019)	Data 0.007 (0.012)	Loss 1.9013 (1.9083)	Acc@1 75.781 (75.037)	Acc@5 96.094 (95.294)
Epoch: [109][30/196]	Time 0.015 (0.018)	Data 0.003 (0.009)	Loss 1.9541 (1.9115)	Acc@1 72.266 (74.546)	Acc@5 94.922 (95.174)
Epoch: [109][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.8829 (1.9164)	Acc@1 77.344 (74.419)	Acc@5 94.141 (95.151)
Epoch: [109][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9116 (1.9234)	Acc@1 74.609 (73.997)	Acc@5 96.875 (95.083)
Epoch: [109][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9811 (1.9298)	Acc@1 71.875 (73.713)	Acc@5 94.531 (94.935)
Epoch: [109][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9018 (1.9334)	Acc@1 73.828 (73.454)	Acc@5 95.312 (94.938)
Epoch: [109][80/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 2.0524 (1.9415)	Acc@1 69.141 (73.047)	Acc@5 92.188 (94.893)
Epoch: [109][90/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.8557 (1.9462)	Acc@1 74.219 (72.888)	Acc@5 95.703 (94.815)
Epoch: [109][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.8988 (1.9514)	Acc@1 75.000 (72.803)	Acc@5 96.484 (94.740)
Epoch: [109][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0503 (1.9586)	Acc@1 71.875 (72.575)	Acc@5 93.359 (94.672)
Epoch: [109][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1529 (1.9650)	Acc@1 67.578 (72.398)	Acc@5 92.578 (94.622)
Epoch: [109][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0495 (1.9702)	Acc@1 72.266 (72.298)	Acc@5 93.359 (94.552)
Epoch: [109][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9070 (1.9716)	Acc@1 74.609 (72.277)	Acc@5 95.312 (94.517)
Epoch: [109][150/196]	Time 0.013 (0.016)	Data 0.010 (0.004)	Loss 2.0192 (1.9774)	Acc@1 73.047 (72.139)	Acc@5 94.922 (94.487)
Epoch: [109][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9339 (1.9832)	Acc@1 71.094 (72.001)	Acc@5 96.875 (94.434)
Epoch: [109][170/196]	Time 0.013 (0.016)	Data 0.009 (0.004)	Loss 2.1719 (1.9888)	Acc@1 69.922 (71.884)	Acc@5 91.797 (94.369)
Epoch: [109][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2392 (1.9954)	Acc@1 65.625 (71.674)	Acc@5 89.844 (94.305)
Epoch: [109][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.3180 (2.0006)	Acc@1 62.500 (71.558)	Acc@5 90.625 (94.231)
num momentum params: 26
[0.1, 2.0031393406677247, 2.0339582574367525, 71.506, 51.06, tensor(0.5219, device='cuda:0', grad_fn=<DivBackward0>), 3.090990304946899, 0.3998527526855469]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [110 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [110][0/196]	Time 0.060 (0.060)	Data 0.198 (0.198)	Loss 1.8447 (1.8447)	Acc@1 74.219 (74.219)	Acc@5 96.094 (96.094)
Epoch: [110][10/196]	Time 0.017 (0.022)	Data 0.003 (0.020)	Loss 1.9030 (1.9731)	Acc@1 72.266 (72.443)	Acc@5 95.703 (94.957)
Epoch: [110][20/196]	Time 0.015 (0.019)	Data 0.003 (0.012)	Loss 1.8939 (1.9321)	Acc@1 75.000 (73.679)	Acc@5 97.266 (95.238)
Epoch: [110][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 1.7638 (1.9267)	Acc@1 78.125 (74.068)	Acc@5 95.703 (95.174)
Epoch: [110][40/196]	Time 0.014 (0.017)	Data 0.006 (0.007)	Loss 1.8869 (1.9244)	Acc@1 76.172 (73.952)	Acc@5 95.312 (95.151)
Epoch: [110][50/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.7971 (1.9252)	Acc@1 78.125 (73.874)	Acc@5 96.875 (95.190)
Epoch: [110][60/196]	Time 0.012 (0.017)	Data 0.006 (0.006)	Loss 1.9080 (1.9301)	Acc@1 74.609 (73.572)	Acc@5 94.922 (95.191)
Epoch: [110][70/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 2.1119 (1.9389)	Acc@1 66.797 (73.421)	Acc@5 90.625 (94.960)
Epoch: [110][80/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 1.8243 (1.9442)	Acc@1 75.000 (73.288)	Acc@5 96.094 (94.912)
Epoch: [110][90/196]	Time 0.014 (0.016)	Data 0.001 (0.004)	Loss 1.9260 (1.9440)	Acc@1 73.828 (73.334)	Acc@5 94.531 (94.857)
Epoch: [110][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9447 (1.9499)	Acc@1 73.828 (73.167)	Acc@5 95.312 (94.821)
Epoch: [110][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8851 (1.9518)	Acc@1 75.781 (73.163)	Acc@5 95.703 (94.753)
Epoch: [110][120/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 2.0229 (1.9560)	Acc@1 69.531 (73.018)	Acc@5 93.750 (94.715)
Epoch: [110][130/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.1828 (1.9671)	Acc@1 68.359 (72.743)	Acc@5 94.141 (94.633)
Epoch: [110][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1317 (1.9742)	Acc@1 66.406 (72.496)	Acc@5 91.016 (94.576)
Epoch: [110][150/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1354 (1.9778)	Acc@1 66.016 (72.379)	Acc@5 94.531 (94.549)
Epoch: [110][160/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.1531 (1.9825)	Acc@1 67.969 (72.220)	Acc@5 89.062 (94.444)
Epoch: [110][170/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9605 (1.9870)	Acc@1 74.219 (72.129)	Acc@5 92.969 (94.374)
Epoch: [110][180/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0088 (1.9904)	Acc@1 71.484 (72.011)	Acc@5 93.359 (94.322)
Epoch: [110][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1453 (1.9971)	Acc@1 71.094 (71.801)	Acc@5 94.141 (94.249)
num momentum params: 26
[0.1, 1.9996291914367677, 2.006521043777466, 71.734, 51.28, tensor(0.5238, device='cuda:0', grad_fn=<DivBackward0>), 3.0664091110229497, 0.39468169212341314]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [111 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [111][0/196]	Time 0.060 (0.060)	Data 0.184 (0.184)	Loss 1.8526 (1.8526)	Acc@1 76.172 (76.172)	Acc@5 96.094 (96.094)
Epoch: [111][10/196]	Time 0.016 (0.021)	Data 0.002 (0.018)	Loss 1.9141 (1.9474)	Acc@1 74.219 (73.224)	Acc@5 95.312 (95.241)
Epoch: [111][20/196]	Time 0.016 (0.019)	Data 0.002 (0.010)	Loss 1.9310 (1.9333)	Acc@1 75.000 (73.884)	Acc@5 94.531 (95.294)
Epoch: [111][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 2.0224 (1.9397)	Acc@1 71.094 (74.030)	Acc@5 94.531 (95.149)
Epoch: [111][40/196]	Time 0.017 (0.017)	Data 0.002 (0.006)	Loss 1.8242 (1.9238)	Acc@1 78.906 (74.419)	Acc@5 96.094 (95.389)
Epoch: [111][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8847 (1.9262)	Acc@1 76.562 (74.058)	Acc@5 95.703 (95.343)
Epoch: [111][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.9141 (1.9291)	Acc@1 77.344 (74.033)	Acc@5 95.312 (95.223)
Epoch: [111][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0371 (1.9313)	Acc@1 71.094 (73.960)	Acc@5 94.141 (95.153)
Epoch: [111][80/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1389 (1.9386)	Acc@1 67.969 (73.741)	Acc@5 92.969 (95.047)
Epoch: [111][90/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0015 (1.9464)	Acc@1 71.484 (73.506)	Acc@5 94.531 (95.042)
Epoch: [111][100/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 1.9945 (1.9515)	Acc@1 71.484 (73.372)	Acc@5 94.531 (95.026)
Epoch: [111][110/196]	Time 0.022 (0.016)	Data 0.000 (0.004)	Loss 2.1102 (1.9556)	Acc@1 68.359 (73.170)	Acc@5 92.969 (95.020)
Epoch: [111][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9543 (1.9621)	Acc@1 75.781 (73.066)	Acc@5 95.312 (94.922)
Epoch: [111][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9875 (1.9669)	Acc@1 73.438 (73.005)	Acc@5 94.141 (94.829)
Epoch: [111][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0267 (1.9738)	Acc@1 71.484 (72.786)	Acc@5 93.750 (94.728)
Epoch: [111][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9907 (1.9790)	Acc@1 74.219 (72.625)	Acc@5 95.312 (94.658)
Epoch: [111][160/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0950 (1.9829)	Acc@1 68.750 (72.511)	Acc@5 92.188 (94.638)
Epoch: [111][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1606 (1.9893)	Acc@1 69.141 (72.316)	Acc@5 91.406 (94.568)
Epoch: [111][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1372 (1.9920)	Acc@1 68.750 (72.184)	Acc@5 94.531 (94.544)
Epoch: [111][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9901 (1.9978)	Acc@1 73.047 (72.012)	Acc@5 94.531 (94.484)
num momentum params: 26
[0.1, 1.9993867835998536, 1.7208802092075348, 71.936, 56.15, tensor(0.5244, device='cuda:0', grad_fn=<DivBackward0>), 3.098897933959961, 0.39093828201293945]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [112 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [112][0/196]	Time 0.060 (0.060)	Data 0.188 (0.188)	Loss 1.9495 (1.9495)	Acc@1 71.875 (71.875)	Acc@5 95.703 (95.703)
Epoch: [112][10/196]	Time 0.016 (0.022)	Data 0.002 (0.019)	Loss 1.9802 (1.9419)	Acc@1 75.391 (73.757)	Acc@5 93.750 (95.064)
Epoch: [112][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 1.8102 (1.9549)	Acc@1 76.953 (72.972)	Acc@5 96.484 (95.126)
Epoch: [112][30/196]	Time 0.012 (0.017)	Data 0.005 (0.008)	Loss 2.1325 (1.9509)	Acc@1 68.359 (72.959)	Acc@5 92.578 (95.086)
Epoch: [112][40/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 1.9001 (1.9458)	Acc@1 75.781 (73.037)	Acc@5 94.922 (95.084)
Epoch: [112][50/196]	Time 0.011 (0.017)	Data 0.007 (0.006)	Loss 1.9321 (1.9344)	Acc@1 75.000 (73.552)	Acc@5 94.141 (95.221)
Epoch: [112][60/196]	Time 0.016 (0.017)	Data 0.000 (0.006)	Loss 1.8755 (1.9335)	Acc@1 78.125 (73.681)	Acc@5 95.312 (95.242)
Epoch: [112][70/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.0030 (1.9270)	Acc@1 71.094 (73.911)	Acc@5 95.312 (95.285)
Epoch: [112][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9762 (1.9325)	Acc@1 73.047 (73.818)	Acc@5 94.922 (95.134)
Epoch: [112][90/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.9979 (1.9382)	Acc@1 73.438 (73.704)	Acc@5 92.578 (95.012)
Epoch: [112][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.8781 (1.9415)	Acc@1 76.172 (73.623)	Acc@5 96.484 (94.999)
Epoch: [112][110/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 1.9958 (1.9517)	Acc@1 71.094 (73.297)	Acc@5 94.141 (94.873)
Epoch: [112][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9531 (1.9603)	Acc@1 73.828 (73.021)	Acc@5 92.578 (94.751)
Epoch: [112][130/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.0776 (1.9692)	Acc@1 68.750 (72.710)	Acc@5 93.750 (94.674)
Epoch: [112][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9507 (1.9749)	Acc@1 73.047 (72.515)	Acc@5 94.922 (94.659)
Epoch: [112][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.9660 (1.9786)	Acc@1 73.438 (72.460)	Acc@5 94.141 (94.591)
Epoch: [112][160/196]	Time 0.012 (0.016)	Data 0.029 (0.004)	Loss 2.0922 (1.9847)	Acc@1 66.797 (72.300)	Acc@5 95.703 (94.507)
Epoch: [112][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9725 (1.9892)	Acc@1 71.875 (72.183)	Acc@5 94.531 (94.431)
Epoch: [112][180/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1548 (1.9928)	Acc@1 68.750 (72.084)	Acc@5 91.016 (94.374)
Epoch: [112][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0103 (1.9977)	Acc@1 69.531 (71.940)	Acc@5 94.922 (94.300)
num momentum params: 26
[0.1, 1.998359026031494, 1.9062146055698395, 71.954, 52.6, tensor(0.5253, device='cuda:0', grad_fn=<DivBackward0>), 3.121588230133057, 0.4007205963134765]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [113 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [113][0/196]	Time 0.056 (0.056)	Data 0.202 (0.202)	Loss 2.0150 (2.0150)	Acc@1 73.438 (73.438)	Acc@5 92.969 (92.969)
Epoch: [113][10/196]	Time 0.016 (0.021)	Data 0.002 (0.020)	Loss 1.8284 (1.8974)	Acc@1 76.953 (74.112)	Acc@5 94.922 (95.206)
Epoch: [113][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 1.7682 (1.8923)	Acc@1 78.516 (74.665)	Acc@5 98.047 (95.461)
Epoch: [113][30/196]	Time 0.017 (0.018)	Data 0.002 (0.008)	Loss 2.0501 (1.9034)	Acc@1 70.312 (74.534)	Acc@5 92.188 (95.262)
Epoch: [113][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.9455 (1.9049)	Acc@1 72.266 (74.438)	Acc@5 94.922 (95.332)
Epoch: [113][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 2.0577 (1.9110)	Acc@1 71.484 (74.288)	Acc@5 94.531 (95.351)
Epoch: [113][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.1726 (1.9210)	Acc@1 68.359 (74.078)	Acc@5 93.359 (95.223)
Epoch: [113][70/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 2.0863 (1.9247)	Acc@1 70.703 (73.933)	Acc@5 93.359 (95.202)
Epoch: [113][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 2.0357 (1.9329)	Acc@1 71.094 (73.601)	Acc@5 93.359 (95.105)
Epoch: [113][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0780 (1.9401)	Acc@1 72.266 (73.433)	Acc@5 93.750 (95.025)
Epoch: [113][100/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 1.9793 (1.9465)	Acc@1 74.609 (73.267)	Acc@5 94.141 (94.910)
Epoch: [113][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0636 (1.9514)	Acc@1 68.359 (73.131)	Acc@5 92.969 (94.851)
Epoch: [113][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1680 (1.9570)	Acc@1 66.797 (72.992)	Acc@5 93.359 (94.780)
Epoch: [113][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2074 (1.9645)	Acc@1 66.406 (72.752)	Acc@5 93.359 (94.692)
Epoch: [113][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1963 (1.9731)	Acc@1 67.188 (72.576)	Acc@5 92.969 (94.598)
Epoch: [113][150/196]	Time 0.017 (0.016)	Data 0.006 (0.003)	Loss 2.1172 (1.9794)	Acc@1 71.484 (72.426)	Acc@5 93.359 (94.521)
Epoch: [113][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0823 (1.9843)	Acc@1 69.141 (72.275)	Acc@5 91.406 (94.461)
Epoch: [113][170/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.0033 (1.9911)	Acc@1 72.266 (72.099)	Acc@5 93.750 (94.344)
Epoch: [113][180/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 1.9017 (1.9949)	Acc@1 75.000 (71.959)	Acc@5 95.703 (94.285)
Epoch: [113][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0901 (1.9988)	Acc@1 71.094 (71.904)	Acc@5 92.578 (94.247)
num momentum params: 26
[0.1, 2.0006374252319334, 1.8565165507793426, 71.864, 53.74, tensor(0.5245, device='cuda:0', grad_fn=<DivBackward0>), 3.1316730976104736, 0.4001312255859375]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [114 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [114][0/196]	Time 0.067 (0.067)	Data 0.182 (0.182)	Loss 1.9010 (1.9010)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [114][10/196]	Time 0.015 (0.021)	Data 0.002 (0.018)	Loss 1.7937 (1.9156)	Acc@1 78.906 (74.467)	Acc@5 97.656 (95.774)
Epoch: [114][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.8475 (1.9263)	Acc@1 76.562 (74.144)	Acc@5 95.703 (95.350)
Epoch: [114][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.8985 (1.9085)	Acc@1 74.219 (74.521)	Acc@5 94.922 (95.476)
Epoch: [114][40/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 1.9255 (1.9150)	Acc@1 73.438 (74.419)	Acc@5 94.922 (95.389)
Epoch: [114][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8471 (1.9179)	Acc@1 76.172 (74.311)	Acc@5 95.312 (95.236)
Epoch: [114][60/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 1.8633 (1.9150)	Acc@1 76.953 (74.392)	Acc@5 95.703 (95.338)
Epoch: [114][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9205 (1.9193)	Acc@1 75.000 (74.169)	Acc@5 94.531 (95.219)
Epoch: [114][80/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0647 (1.9268)	Acc@1 71.875 (73.958)	Acc@5 92.578 (95.115)
Epoch: [114][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.8094 (1.9288)	Acc@1 76.562 (73.935)	Acc@5 97.266 (95.106)
Epoch: [114][100/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0426 (1.9331)	Acc@1 68.359 (73.751)	Acc@5 94.141 (95.022)
Epoch: [114][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0731 (1.9426)	Acc@1 68.750 (73.423)	Acc@5 93.750 (94.922)
Epoch: [114][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0635 (1.9489)	Acc@1 68.359 (73.218)	Acc@5 93.750 (94.883)
Epoch: [114][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9121 (1.9533)	Acc@1 77.344 (73.184)	Acc@5 94.141 (94.847)
Epoch: [114][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0286 (1.9583)	Acc@1 69.141 (73.011)	Acc@5 94.531 (94.745)
Epoch: [114][150/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0654 (1.9651)	Acc@1 71.094 (72.796)	Acc@5 90.625 (94.642)
Epoch: [114][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0176 (1.9748)	Acc@1 69.922 (72.593)	Acc@5 96.875 (94.512)
Epoch: [114][170/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0686 (1.9783)	Acc@1 69.141 (72.540)	Acc@5 94.141 (94.442)
Epoch: [114][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9359 (1.9838)	Acc@1 72.266 (72.326)	Acc@5 94.531 (94.389)
Epoch: [114][190/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.2452 (1.9873)	Acc@1 67.969 (72.221)	Acc@5 90.625 (94.359)
num momentum params: 26
[0.1, 1.9895061727905274, 1.8606880354881286, 72.158, 54.45, tensor(0.5263, device='cuda:0', grad_fn=<DivBackward0>), 3.063098430633545, 0.3991067409515381]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [115 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [115][0/196]	Time 0.062 (0.062)	Data 0.189 (0.189)	Loss 1.7022 (1.7022)	Acc@1 79.297 (79.297)	Acc@5 97.266 (97.266)
Epoch: [115][10/196]	Time 0.018 (0.023)	Data 0.002 (0.019)	Loss 1.9794 (1.8949)	Acc@1 74.609 (74.574)	Acc@5 94.531 (95.384)
Epoch: [115][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.7157 (1.9218)	Acc@1 78.516 (73.903)	Acc@5 95.703 (95.294)
Epoch: [115][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 1.8164 (1.9107)	Acc@1 78.906 (74.269)	Acc@5 95.703 (95.312)
Epoch: [115][40/196]	Time 0.013 (0.018)	Data 0.004 (0.006)	Loss 1.9528 (1.9070)	Acc@1 73.047 (74.257)	Acc@5 95.312 (95.474)
Epoch: [115][50/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 2.0044 (1.9128)	Acc@1 71.094 (74.027)	Acc@5 92.578 (95.374)
Epoch: [115][60/196]	Time 0.011 (0.017)	Data 0.007 (0.005)	Loss 1.9439 (1.9171)	Acc@1 68.750 (73.796)	Acc@5 96.875 (95.409)
Epoch: [115][70/196]	Time 0.016 (0.017)	Data 0.001 (0.005)	Loss 1.8163 (1.9188)	Acc@1 75.000 (73.883)	Acc@5 95.312 (95.301)
Epoch: [115][80/196]	Time 0.011 (0.017)	Data 0.006 (0.004)	Loss 1.9715 (1.9274)	Acc@1 70.312 (73.736)	Acc@5 94.922 (95.274)
Epoch: [115][90/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9906 (1.9329)	Acc@1 71.484 (73.472)	Acc@5 93.750 (95.252)
Epoch: [115][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.1058 (1.9407)	Acc@1 69.531 (73.260)	Acc@5 92.578 (95.119)
Epoch: [115][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0561 (1.9469)	Acc@1 71.484 (73.184)	Acc@5 93.359 (95.003)
Epoch: [115][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0514 (1.9525)	Acc@1 67.969 (72.950)	Acc@5 92.969 (94.912)
Epoch: [115][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0645 (1.9596)	Acc@1 68.750 (72.710)	Acc@5 93.359 (94.841)
Epoch: [115][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0226 (1.9642)	Acc@1 70.312 (72.620)	Acc@5 92.578 (94.758)
Epoch: [115][150/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.9394 (1.9668)	Acc@1 72.266 (72.498)	Acc@5 94.531 (94.702)
Epoch: [115][160/196]	Time 0.011 (0.016)	Data 0.006 (0.003)	Loss 1.8766 (1.9743)	Acc@1 77.344 (72.283)	Acc@5 94.531 (94.568)
Epoch: [115][170/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.1715 (1.9834)	Acc@1 67.188 (72.007)	Acc@5 92.188 (94.467)
Epoch: [115][180/196]	Time 0.011 (0.016)	Data 0.006 (0.003)	Loss 2.2218 (1.9899)	Acc@1 62.109 (71.845)	Acc@5 92.188 (94.393)
Epoch: [115][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.1196 (1.9951)	Acc@1 69.141 (71.726)	Acc@5 92.188 (94.302)
num momentum params: 26
[0.1, 1.995964979248047, 1.976662632226944, 71.73, 52.46, tensor(0.5251, device='cuda:0', grad_fn=<DivBackward0>), 3.0994822978973393, 0.39069485664367676]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [116 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [116][0/196]	Time 0.061 (0.061)	Data 0.183 (0.183)	Loss 1.8229 (1.8229)	Acc@1 76.953 (76.953)	Acc@5 96.875 (96.875)
Epoch: [116][10/196]	Time 0.016 (0.021)	Data 0.002 (0.018)	Loss 1.9015 (1.9741)	Acc@1 75.781 (73.722)	Acc@5 96.875 (94.815)
Epoch: [116][20/196]	Time 0.015 (0.018)	Data 0.003 (0.011)	Loss 1.8186 (1.9409)	Acc@1 76.953 (74.386)	Acc@5 96.875 (94.829)
Epoch: [116][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 2.0042 (1.9487)	Acc@1 71.875 (74.118)	Acc@5 93.750 (94.846)
Epoch: [116][40/196]	Time 0.014 (0.017)	Data 0.002 (0.006)	Loss 1.8563 (1.9465)	Acc@1 72.656 (73.857)	Acc@5 98.047 (94.989)
Epoch: [116][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8657 (1.9382)	Acc@1 73.047 (73.912)	Acc@5 96.484 (95.175)
Epoch: [116][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.8355 (1.9338)	Acc@1 75.000 (73.886)	Acc@5 96.875 (95.236)
Epoch: [116][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8844 (1.9333)	Acc@1 75.000 (73.773)	Acc@5 96.094 (95.230)
Epoch: [116][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9622 (1.9352)	Acc@1 70.312 (73.708)	Acc@5 94.141 (95.231)
Epoch: [116][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9442 (1.9381)	Acc@1 72.266 (73.669)	Acc@5 95.312 (95.111)
Epoch: [116][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9305 (1.9404)	Acc@1 71.484 (73.503)	Acc@5 96.094 (95.135)
Epoch: [116][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0595 (1.9429)	Acc@1 71.484 (73.441)	Acc@5 92.188 (95.126)
Epoch: [116][120/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0619 (1.9473)	Acc@1 71.484 (73.354)	Acc@5 94.141 (95.074)
Epoch: [116][130/196]	Time 0.012 (0.016)	Data 0.004 (0.004)	Loss 2.0218 (1.9555)	Acc@1 72.656 (73.047)	Acc@5 94.531 (95.002)
Epoch: [116][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2194 (1.9637)	Acc@1 65.234 (72.834)	Acc@5 92.188 (94.875)
Epoch: [116][150/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 1.9515 (1.9718)	Acc@1 74.219 (72.597)	Acc@5 93.359 (94.769)
Epoch: [116][160/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.2056 (1.9799)	Acc@1 64.844 (72.326)	Acc@5 92.578 (94.672)
Epoch: [116][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1524 (1.9871)	Acc@1 64.062 (72.074)	Acc@5 93.359 (94.600)
Epoch: [116][180/196]	Time 0.012 (0.016)	Data 0.005 (0.003)	Loss 2.1895 (1.9913)	Acc@1 67.578 (72.007)	Acc@5 90.234 (94.501)
Epoch: [116][190/196]	Time 0.014 (0.015)	Data 0.003 (0.003)	Loss 1.9792 (1.9927)	Acc@1 70.703 (72.000)	Acc@5 95.312 (94.464)
num momentum params: 26
[0.1, 1.9940375645065307, 1.7853794932365417, 71.928, 54.56, tensor(0.5259, device='cuda:0', grad_fn=<DivBackward0>), 3.0436437129974365, 0.3963756561279297]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [117 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [117][0/196]	Time 0.063 (0.063)	Data 0.186 (0.186)	Loss 1.8383 (1.8383)	Acc@1 75.781 (75.781)	Acc@5 96.094 (96.094)
Epoch: [117][10/196]	Time 0.017 (0.021)	Data 0.001 (0.018)	Loss 1.9803 (1.9359)	Acc@1 69.922 (72.727)	Acc@5 94.922 (95.490)
Epoch: [117][20/196]	Time 0.014 (0.018)	Data 0.003 (0.011)	Loss 1.8356 (1.9292)	Acc@1 73.438 (73.047)	Acc@5 96.484 (95.238)
Epoch: [117][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.8391 (1.9045)	Acc@1 76.562 (74.005)	Acc@5 95.312 (95.300)
Epoch: [117][40/196]	Time 0.016 (0.017)	Data 0.003 (0.006)	Loss 1.8242 (1.9086)	Acc@1 76.562 (74.019)	Acc@5 96.484 (95.303)
Epoch: [117][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9359 (1.9148)	Acc@1 75.000 (74.073)	Acc@5 94.531 (95.175)
Epoch: [117][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0911 (1.9166)	Acc@1 68.750 (73.943)	Acc@5 94.141 (95.204)
Epoch: [117][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9784 (1.9265)	Acc@1 74.219 (73.801)	Acc@5 93.750 (95.125)
Epoch: [117][80/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 1.9621 (1.9397)	Acc@1 71.484 (73.375)	Acc@5 94.141 (94.936)
Epoch: [117][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8820 (1.9438)	Acc@1 71.875 (73.193)	Acc@5 97.266 (94.913)
Epoch: [117][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9087 (1.9502)	Acc@1 74.219 (72.973)	Acc@5 96.094 (94.879)
Epoch: [117][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1484 (1.9632)	Acc@1 68.750 (72.586)	Acc@5 94.531 (94.760)
Epoch: [117][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1342 (1.9720)	Acc@1 66.406 (72.343)	Acc@5 93.750 (94.628)
Epoch: [117][130/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 1.9331 (1.9788)	Acc@1 75.391 (72.164)	Acc@5 95.703 (94.570)
Epoch: [117][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1508 (1.9902)	Acc@1 68.750 (71.955)	Acc@5 92.188 (94.434)
Epoch: [117][150/196]	Time 0.020 (0.016)	Data 0.002 (0.003)	Loss 1.9759 (1.9963)	Acc@1 73.047 (71.777)	Acc@5 93.750 (94.407)
Epoch: [117][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0399 (2.0028)	Acc@1 75.000 (71.628)	Acc@5 93.359 (94.364)
Epoch: [117][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9378 (2.0045)	Acc@1 72.266 (71.544)	Acc@5 94.531 (94.355)
Epoch: [117][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0195 (2.0086)	Acc@1 70.312 (71.430)	Acc@5 92.969 (94.287)
Epoch: [117][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2044 (2.0163)	Acc@1 67.578 (71.278)	Acc@5 90.234 (94.204)
num momentum params: 26
[0.1, 2.0185902327728273, 2.0401528704166414, 71.236, 50.82, tensor(0.5205, device='cuda:0', grad_fn=<DivBackward0>), 3.1167006492614746, 0.4035966396331787]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [118 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [118][0/196]	Time 0.059 (0.059)	Data 0.189 (0.189)	Loss 2.0708 (2.0708)	Acc@1 67.969 (67.969)	Acc@5 94.531 (94.531)
Epoch: [118][10/196]	Time 0.017 (0.021)	Data 0.002 (0.019)	Loss 1.9068 (1.9459)	Acc@1 76.562 (73.544)	Acc@5 94.922 (95.419)
Epoch: [118][20/196]	Time 0.016 (0.019)	Data 0.003 (0.011)	Loss 1.9132 (1.9663)	Acc@1 73.828 (72.861)	Acc@5 94.141 (95.238)
Epoch: [118][30/196]	Time 0.019 (0.018)	Data 0.002 (0.008)	Loss 1.7586 (1.9372)	Acc@1 77.344 (73.526)	Acc@5 97.656 (95.376)
Epoch: [118][40/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9236 (1.9285)	Acc@1 75.781 (73.857)	Acc@5 94.531 (95.332)
Epoch: [118][50/196]	Time 0.017 (0.017)	Data 0.002 (0.006)	Loss 1.9348 (1.9173)	Acc@1 71.484 (74.188)	Acc@5 96.484 (95.450)
Epoch: [118][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.9526 (1.9156)	Acc@1 73.828 (74.078)	Acc@5 93.359 (95.383)
Epoch: [118][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.8655 (1.9167)	Acc@1 74.219 (74.026)	Acc@5 94.922 (95.428)
Epoch: [118][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 2.0131 (1.9192)	Acc@1 72.266 (73.929)	Acc@5 92.578 (95.341)
Epoch: [118][90/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 1.9738 (1.9214)	Acc@1 71.094 (73.845)	Acc@5 94.141 (95.291)
Epoch: [118][100/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.9102 (1.9294)	Acc@1 75.000 (73.650)	Acc@5 94.531 (95.177)
Epoch: [118][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1822 (1.9375)	Acc@1 64.844 (73.452)	Acc@5 92.969 (95.098)
Epoch: [118][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0207 (1.9447)	Acc@1 71.094 (73.237)	Acc@5 94.922 (94.986)
Epoch: [118][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9284 (1.9506)	Acc@1 72.266 (73.056)	Acc@5 95.312 (94.913)
Epoch: [118][140/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.0823 (1.9588)	Acc@1 69.141 (72.858)	Acc@5 92.188 (94.811)
Epoch: [118][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0175 (1.9640)	Acc@1 70.312 (72.703)	Acc@5 94.531 (94.767)
Epoch: [118][160/196]	Time 0.012 (0.016)	Data 0.005 (0.003)	Loss 2.0542 (1.9680)	Acc@1 71.875 (72.579)	Acc@5 92.188 (94.704)
Epoch: [118][170/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.2146 (1.9738)	Acc@1 66.406 (72.467)	Acc@5 90.625 (94.620)
Epoch: [118][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1676 (1.9797)	Acc@1 68.750 (72.309)	Acc@5 92.188 (94.538)
Epoch: [118][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.9451 (1.9870)	Acc@1 74.609 (72.165)	Acc@5 95.312 (94.443)
num momentum params: 26
[0.1, 1.9901107568359375, 1.8861377322673798, 72.104, 53.71, tensor(0.5279, device='cuda:0', grad_fn=<DivBackward0>), 3.105884552001953, 0.388594388961792]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [119 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [119][0/196]	Time 0.060 (0.060)	Data 0.185 (0.185)	Loss 2.0076 (2.0076)	Acc@1 72.266 (72.266)	Acc@5 94.531 (94.531)
Epoch: [119][10/196]	Time 0.015 (0.020)	Data 0.002 (0.019)	Loss 2.0312 (1.9773)	Acc@1 72.266 (73.189)	Acc@5 93.359 (94.496)
Epoch: [119][20/196]	Time 0.016 (0.018)	Data 0.002 (0.011)	Loss 2.0188 (1.9371)	Acc@1 71.094 (73.512)	Acc@5 94.141 (95.294)
Epoch: [119][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 2.0027 (1.9317)	Acc@1 67.188 (73.501)	Acc@5 95.312 (95.212)
Epoch: [119][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.9814 (1.9353)	Acc@1 73.047 (73.314)	Acc@5 94.531 (95.065)
Epoch: [119][50/196]	Time 0.017 (0.017)	Data 0.002 (0.006)	Loss 1.9631 (1.9341)	Acc@1 75.391 (73.384)	Acc@5 94.922 (95.067)
Epoch: [119][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9252 (1.9327)	Acc@1 75.391 (73.617)	Acc@5 95.312 (94.947)
Epoch: [119][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0020 (1.9256)	Acc@1 70.703 (73.861)	Acc@5 93.359 (95.021)
Epoch: [119][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8887 (1.9263)	Acc@1 73.828 (73.823)	Acc@5 95.312 (95.042)
Epoch: [119][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9061 (1.9289)	Acc@1 74.219 (73.717)	Acc@5 95.703 (95.085)
Epoch: [119][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1174 (1.9319)	Acc@1 67.969 (73.627)	Acc@5 93.359 (95.011)
Epoch: [119][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9843 (1.9385)	Acc@1 71.875 (73.402)	Acc@5 94.141 (94.950)
Epoch: [119][120/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 2.1079 (1.9470)	Acc@1 69.141 (73.118)	Acc@5 93.750 (94.844)
Epoch: [119][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0729 (1.9559)	Acc@1 67.578 (72.913)	Acc@5 92.969 (94.776)
Epoch: [119][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9680 (1.9594)	Acc@1 73.438 (72.795)	Acc@5 95.312 (94.778)
Epoch: [119][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0640 (1.9653)	Acc@1 68.750 (72.592)	Acc@5 93.750 (94.712)
Epoch: [119][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0956 (1.9738)	Acc@1 71.094 (72.360)	Acc@5 93.750 (94.616)
Epoch: [119][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1817 (1.9805)	Acc@1 65.625 (72.142)	Acc@5 94.531 (94.531)
Epoch: [119][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0608 (1.9861)	Acc@1 72.656 (72.041)	Acc@5 92.969 (94.456)
Epoch: [119][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0122 (1.9920)	Acc@1 69.531 (71.893)	Acc@5 94.531 (94.404)
num momentum params: 26
[0.1, 1.9931009768676757, 1.9506312859058381, 71.858, 51.74, tensor(0.5267, device='cuda:0', grad_fn=<DivBackward0>), 3.0910794734954834, 0.39709234237670893]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [120 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [120][0/196]	Time 0.066 (0.066)	Data 0.189 (0.189)	Loss 1.9227 (1.9227)	Acc@1 73.828 (73.828)	Acc@5 92.188 (92.188)
Epoch: [120][10/196]	Time 0.018 (0.022)	Data 0.002 (0.019)	Loss 1.9653 (1.9928)	Acc@1 73.438 (71.911)	Acc@5 94.922 (94.389)
Epoch: [120][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 2.1076 (1.9752)	Acc@1 68.750 (72.749)	Acc@5 94.141 (94.829)
Epoch: [120][30/196]	Time 0.016 (0.018)	Data 0.003 (0.008)	Loss 2.0359 (1.9556)	Acc@1 70.312 (73.148)	Acc@5 95.312 (95.023)
Epoch: [120][40/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9423 (1.9553)	Acc@1 71.875 (73.171)	Acc@5 95.703 (94.931)
Epoch: [120][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 2.0099 (1.9544)	Acc@1 75.000 (73.246)	Acc@5 93.750 (94.914)
Epoch: [120][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.9832 (1.9544)	Acc@1 69.922 (73.169)	Acc@5 94.531 (94.980)
Epoch: [120][70/196]	Time 0.014 (0.017)	Data 0.003 (0.005)	Loss 1.8776 (1.9502)	Acc@1 73.047 (73.212)	Acc@5 94.922 (95.043)
Epoch: [120][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9325 (1.9523)	Acc@1 71.484 (73.114)	Acc@5 96.484 (95.004)
Epoch: [120][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9564 (1.9599)	Acc@1 72.266 (72.940)	Acc@5 95.703 (94.892)
Epoch: [120][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0069 (1.9624)	Acc@1 73.047 (72.884)	Acc@5 91.797 (94.829)
Epoch: [120][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9606 (1.9685)	Acc@1 73.828 (72.748)	Acc@5 95.703 (94.711)
Epoch: [120][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0614 (1.9709)	Acc@1 69.922 (72.659)	Acc@5 94.922 (94.722)
Epoch: [120][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9305 (1.9757)	Acc@1 77.344 (72.638)	Acc@5 95.312 (94.683)
Epoch: [120][140/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.0362 (1.9831)	Acc@1 68.750 (72.390)	Acc@5 94.141 (94.612)
Epoch: [120][150/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0979 (1.9859)	Acc@1 73.438 (72.338)	Acc@5 92.578 (94.552)
Epoch: [120][160/196]	Time 0.017 (0.016)	Data 0.003 (0.003)	Loss 1.8678 (1.9892)	Acc@1 76.953 (72.232)	Acc@5 96.094 (94.500)
Epoch: [120][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1595 (1.9952)	Acc@1 66.406 (72.044)	Acc@5 90.625 (94.431)
Epoch: [120][180/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 1.9292 (1.9995)	Acc@1 73.828 (71.955)	Acc@5 95.312 (94.404)
Epoch: [120][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0204 (2.0041)	Acc@1 71.484 (71.846)	Acc@5 93.750 (94.382)
num momentum params: 26
[0.1, 2.0066561028289795, 1.9954814159870147, 71.746, 51.53, tensor(0.5242, device='cuda:0', grad_fn=<DivBackward0>), 3.042491912841797, 0.39766073226928705]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [121 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [121][0/196]	Time 0.062 (0.062)	Data 0.195 (0.195)	Loss 1.9774 (1.9774)	Acc@1 72.656 (72.656)	Acc@5 94.922 (94.922)
Epoch: [121][10/196]	Time 0.016 (0.022)	Data 0.002 (0.019)	Loss 1.9506 (1.9911)	Acc@1 75.781 (72.159)	Acc@5 94.531 (94.460)
Epoch: [121][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.8356 (1.9518)	Acc@1 78.125 (73.512)	Acc@5 95.312 (94.773)
Epoch: [121][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.9376 (1.9307)	Acc@1 71.875 (74.105)	Acc@5 96.484 (95.023)
Epoch: [121][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.8719 (1.9202)	Acc@1 75.000 (74.314)	Acc@5 95.312 (95.217)
Epoch: [121][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8778 (1.9137)	Acc@1 78.125 (74.533)	Acc@5 95.312 (95.282)
Epoch: [121][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.8945 (1.9075)	Acc@1 75.391 (74.526)	Acc@5 94.922 (95.338)
Epoch: [121][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0619 (1.9094)	Acc@1 67.188 (74.389)	Acc@5 94.531 (95.290)
Epoch: [121][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8922 (1.9163)	Acc@1 69.531 (74.166)	Acc@5 95.312 (95.197)
Epoch: [121][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9734 (1.9242)	Acc@1 72.266 (73.914)	Acc@5 95.312 (95.098)
Epoch: [121][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0159 (1.9323)	Acc@1 73.438 (73.546)	Acc@5 93.359 (95.015)
Epoch: [121][110/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.1253 (1.9398)	Acc@1 68.750 (73.328)	Acc@5 91.797 (94.964)
Epoch: [121][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1652 (1.9485)	Acc@1 66.797 (73.037)	Acc@5 92.578 (94.851)
Epoch: [121][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9885 (1.9542)	Acc@1 71.484 (72.817)	Acc@5 95.703 (94.877)
Epoch: [121][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0028 (1.9591)	Acc@1 69.922 (72.584)	Acc@5 94.531 (94.864)
Epoch: [121][150/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0990 (1.9624)	Acc@1 73.828 (72.496)	Acc@5 91.016 (94.798)
Epoch: [121][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0959 (1.9705)	Acc@1 69.922 (72.309)	Acc@5 93.750 (94.650)
Epoch: [121][170/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.1524 (1.9741)	Acc@1 68.750 (72.229)	Acc@5 91.406 (94.613)
Epoch: [121][180/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.1009 (1.9759)	Acc@1 71.484 (72.242)	Acc@5 92.578 (94.602)
Epoch: [121][190/196]	Time 0.015 (0.015)	Data 0.002 (0.003)	Loss 2.0036 (1.9856)	Acc@1 73.438 (72.010)	Acc@5 93.359 (94.511)
num momentum params: 26
[0.1, 1.9877230937576293, 2.01532776594162, 71.98, 51.11, tensor(0.5289, device='cuda:0', grad_fn=<DivBackward0>), 3.0346741676330566, 0.40267276763916016]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [122 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [122][0/196]	Time 0.067 (0.067)	Data 0.189 (0.189)	Loss 1.9529 (1.9529)	Acc@1 74.219 (74.219)	Acc@5 95.703 (95.703)
Epoch: [122][10/196]	Time 0.018 (0.022)	Data 0.002 (0.018)	Loss 1.9500 (1.9179)	Acc@1 70.312 (74.325)	Acc@5 95.312 (94.602)
Epoch: [122][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.8265 (1.9002)	Acc@1 74.609 (74.814)	Acc@5 96.094 (95.033)
Epoch: [122][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.7654 (1.8952)	Acc@1 79.688 (74.723)	Acc@5 97.266 (95.413)
Epoch: [122][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 1.7618 (1.9004)	Acc@1 76.562 (74.419)	Acc@5 97.266 (95.494)
Epoch: [122][50/196]	Time 0.014 (0.017)	Data 0.002 (0.006)	Loss 1.8309 (1.8997)	Acc@1 78.516 (74.449)	Acc@5 94.922 (95.535)
Epoch: [122][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9240 (1.9000)	Acc@1 71.094 (74.219)	Acc@5 94.141 (95.473)
Epoch: [122][70/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 1.9649 (1.9044)	Acc@1 70.703 (74.103)	Acc@5 96.094 (95.516)
Epoch: [122][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9803 (1.9071)	Acc@1 69.531 (73.992)	Acc@5 95.312 (95.467)
Epoch: [122][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9563 (1.9094)	Acc@1 75.391 (74.008)	Acc@5 96.094 (95.484)
Epoch: [122][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1018 (1.9190)	Acc@1 70.312 (73.789)	Acc@5 94.922 (95.390)
Epoch: [122][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9264 (1.9288)	Acc@1 73.438 (73.536)	Acc@5 96.094 (95.260)
Epoch: [122][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0119 (1.9430)	Acc@1 73.047 (73.250)	Acc@5 93.750 (95.074)
Epoch: [122][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9909 (1.9535)	Acc@1 70.312 (72.907)	Acc@5 94.531 (94.973)
Epoch: [122][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0386 (1.9569)	Acc@1 70.703 (72.895)	Acc@5 95.703 (94.905)
Epoch: [122][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0556 (1.9621)	Acc@1 69.922 (72.762)	Acc@5 93.750 (94.837)
Epoch: [122][160/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0153 (1.9658)	Acc@1 71.094 (72.671)	Acc@5 94.531 (94.813)
Epoch: [122][170/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0493 (1.9711)	Acc@1 69.922 (72.551)	Acc@5 93.359 (94.730)
Epoch: [122][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1312 (1.9773)	Acc@1 67.969 (72.335)	Acc@5 93.750 (94.633)
Epoch: [122][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.1013 (1.9826)	Acc@1 68.359 (72.182)	Acc@5 92.578 (94.560)
num momentum params: 26
[0.1, 1.9834729153442383, 1.900966796875, 72.156, 53.65, tensor(0.5297, device='cuda:0', grad_fn=<DivBackward0>), 3.0747663974761963, 0.3945043087005615]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [123 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [123][0/196]	Time 0.064 (0.064)	Data 0.186 (0.186)	Loss 1.7518 (1.7518)	Acc@1 82.031 (82.031)	Acc@5 94.531 (94.531)
Epoch: [123][10/196]	Time 0.016 (0.021)	Data 0.002 (0.019)	Loss 1.8913 (1.9375)	Acc@1 73.828 (74.432)	Acc@5 97.656 (95.277)
Epoch: [123][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.9168 (1.9352)	Acc@1 72.656 (73.847)	Acc@5 95.703 (95.201)
Epoch: [123][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.8763 (1.9101)	Acc@1 73.047 (74.257)	Acc@5 95.703 (95.401)
Epoch: [123][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 2.0277 (1.9052)	Acc@1 69.922 (74.381)	Acc@5 95.312 (95.503)
Epoch: [123][50/196]	Time 0.017 (0.017)	Data 0.000 (0.006)	Loss 1.9118 (1.9119)	Acc@1 77.344 (74.234)	Acc@5 94.531 (95.343)
Epoch: [123][60/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.8793 (1.9122)	Acc@1 73.828 (74.129)	Acc@5 96.875 (95.370)
Epoch: [123][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8077 (1.9170)	Acc@1 76.562 (73.977)	Acc@5 97.266 (95.379)
Epoch: [123][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0736 (1.9227)	Acc@1 71.094 (73.828)	Acc@5 94.531 (95.332)
Epoch: [123][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0950 (1.9320)	Acc@1 69.141 (73.626)	Acc@5 92.969 (95.184)
Epoch: [123][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8764 (1.9399)	Acc@1 75.781 (73.349)	Acc@5 93.750 (95.092)
Epoch: [123][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 1.9318 (1.9463)	Acc@1 70.703 (73.135)	Acc@5 94.922 (95.034)
Epoch: [123][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3185 (1.9553)	Acc@1 64.453 (72.953)	Acc@5 87.891 (94.896)
Epoch: [123][130/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0418 (1.9661)	Acc@1 69.922 (72.638)	Acc@5 92.969 (94.758)
Epoch: [123][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0921 (1.9696)	Acc@1 68.359 (72.554)	Acc@5 92.188 (94.736)
Epoch: [123][150/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0062 (1.9760)	Acc@1 73.047 (72.382)	Acc@5 93.359 (94.658)
Epoch: [123][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2596 (1.9858)	Acc@1 66.016 (72.122)	Acc@5 91.406 (94.519)
Epoch: [123][170/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 1.9225 (1.9877)	Acc@1 75.781 (72.110)	Acc@5 95.703 (94.504)
Epoch: [123][180/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.9965 (1.9935)	Acc@1 73.828 (71.944)	Acc@5 93.359 (94.421)
Epoch: [123][190/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.3096 (1.9977)	Acc@1 67.578 (71.873)	Acc@5 88.672 (94.362)
num momentum params: 26
[0.1, 1.9977874482727052, 1.87464817404747, 71.864, 53.1, tensor(0.5264, device='cuda:0', grad_fn=<DivBackward0>), 3.065661430358887, 0.4003000259399414]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [124 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [124][0/196]	Time 0.066 (0.066)	Data 0.193 (0.193)	Loss 1.8647 (1.8647)	Acc@1 77.344 (77.344)	Acc@5 96.094 (96.094)
Epoch: [124][10/196]	Time 0.014 (0.022)	Data 0.002 (0.020)	Loss 2.0772 (1.9624)	Acc@1 70.703 (74.148)	Acc@5 94.531 (94.922)
Epoch: [124][20/196]	Time 0.017 (0.020)	Data 0.002 (0.011)	Loss 2.0538 (1.9558)	Acc@1 71.094 (73.642)	Acc@5 92.969 (94.810)
Epoch: [124][30/196]	Time 0.015 (0.018)	Data 0.003 (0.008)	Loss 1.9723 (1.9351)	Acc@1 72.656 (73.891)	Acc@5 94.141 (95.023)
Epoch: [124][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8052 (1.9235)	Acc@1 77.344 (74.009)	Acc@5 97.266 (95.198)
Epoch: [124][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 2.0087 (1.9257)	Acc@1 71.094 (73.874)	Acc@5 92.969 (95.136)
Epoch: [124][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.0242 (1.9315)	Acc@1 69.922 (73.655)	Acc@5 91.797 (95.037)
Epoch: [124][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9353 (1.9398)	Acc@1 75.391 (73.454)	Acc@5 96.484 (94.982)
Epoch: [124][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1894 (1.9440)	Acc@1 66.797 (73.380)	Acc@5 92.578 (94.985)
Epoch: [124][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1685 (1.9472)	Acc@1 66.016 (73.146)	Acc@5 92.188 (94.961)
Epoch: [124][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9014 (1.9541)	Acc@1 75.781 (73.039)	Acc@5 93.359 (94.841)
Epoch: [124][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0012 (1.9549)	Acc@1 72.266 (72.941)	Acc@5 92.578 (94.813)
Epoch: [124][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9986 (1.9589)	Acc@1 75.391 (72.950)	Acc@5 91.406 (94.754)
Epoch: [124][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0510 (1.9635)	Acc@1 72.656 (72.877)	Acc@5 93.359 (94.728)
Epoch: [124][140/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 1.9533 (1.9665)	Acc@1 71.484 (72.798)	Acc@5 94.531 (94.706)
Epoch: [124][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1134 (1.9694)	Acc@1 70.703 (72.731)	Acc@5 90.234 (94.655)
Epoch: [124][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0638 (1.9760)	Acc@1 71.094 (72.600)	Acc@5 92.969 (94.582)
Epoch: [124][170/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 1.9556 (1.9801)	Acc@1 72.656 (72.464)	Acc@5 92.969 (94.524)
Epoch: [124][180/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.1687 (1.9833)	Acc@1 68.359 (72.391)	Acc@5 91.016 (94.488)
Epoch: [124][190/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 2.2258 (1.9890)	Acc@1 67.578 (72.268)	Acc@5 91.797 (94.431)
num momentum params: 26
[0.1, 1.9902093000030519, 1.8864813017845155, 72.23, 53.2, tensor(0.5288, device='cuda:0', grad_fn=<DivBackward0>), 3.083514451980591, 0.3960256576538086]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [125 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [125][0/196]	Time 0.062 (0.062)	Data 0.181 (0.181)	Loss 1.7814 (1.7814)	Acc@1 77.344 (77.344)	Acc@5 96.875 (96.875)
Epoch: [125][10/196]	Time 0.016 (0.021)	Data 0.002 (0.018)	Loss 1.8230 (1.9402)	Acc@1 76.562 (73.082)	Acc@5 97.656 (94.957)
Epoch: [125][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.7353 (1.8935)	Acc@1 80.078 (74.740)	Acc@5 96.484 (95.461)
Epoch: [125][30/196]	Time 0.017 (0.018)	Data 0.002 (0.008)	Loss 1.8272 (1.8853)	Acc@1 75.391 (74.710)	Acc@5 95.703 (95.628)
Epoch: [125][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8233 (1.8866)	Acc@1 75.391 (74.581)	Acc@5 96.094 (95.655)
Epoch: [125][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8797 (1.8902)	Acc@1 74.609 (74.586)	Acc@5 95.312 (95.565)
Epoch: [125][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9487 (1.8939)	Acc@1 73.047 (74.501)	Acc@5 94.922 (95.537)
Epoch: [125][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8967 (1.9012)	Acc@1 73.828 (74.268)	Acc@5 95.312 (95.434)
Epoch: [125][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9927 (1.9155)	Acc@1 73.438 (73.891)	Acc@5 94.922 (95.250)
Epoch: [125][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0231 (1.9260)	Acc@1 72.656 (73.635)	Acc@5 92.578 (95.149)
Epoch: [125][100/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 1.9364 (1.9310)	Acc@1 73.438 (73.557)	Acc@5 95.703 (95.127)
Epoch: [125][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9335 (1.9353)	Acc@1 73.828 (73.452)	Acc@5 93.359 (95.084)
Epoch: [125][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0870 (1.9434)	Acc@1 71.484 (73.276)	Acc@5 93.359 (94.993)
Epoch: [125][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1098 (1.9506)	Acc@1 70.703 (73.071)	Acc@5 91.406 (94.937)
Epoch: [125][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0978 (1.9597)	Acc@1 68.359 (72.803)	Acc@5 91.797 (94.828)
Epoch: [125][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0285 (1.9665)	Acc@1 69.531 (72.625)	Acc@5 94.922 (94.749)
Epoch: [125][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0964 (1.9719)	Acc@1 67.969 (72.486)	Acc@5 92.578 (94.670)
Epoch: [125][170/196]	Time 0.013 (0.016)	Data 0.006 (0.003)	Loss 1.8568 (1.9775)	Acc@1 73.828 (72.327)	Acc@5 95.312 (94.627)
Epoch: [125][180/196]	Time 0.018 (0.016)	Data 0.002 (0.003)	Loss 2.0738 (1.9837)	Acc@1 70.312 (72.192)	Acc@5 93.359 (94.540)
Epoch: [125][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1509 (1.9900)	Acc@1 67.578 (72.020)	Acc@5 92.188 (94.451)
num momentum params: 26
[0.1, 1.9921201274108886, 1.982331155538559, 71.962, 52.34, tensor(0.5281, device='cuda:0', grad_fn=<DivBackward0>), 3.080026149749756, 0.4030771255493164]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [126 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [126][0/196]	Time 0.064 (0.064)	Data 0.183 (0.183)	Loss 1.8346 (1.8346)	Acc@1 75.781 (75.781)	Acc@5 96.484 (96.484)
Epoch: [126][10/196]	Time 0.016 (0.022)	Data 0.002 (0.018)	Loss 1.7788 (1.9050)	Acc@1 78.125 (74.254)	Acc@5 97.266 (95.526)
Epoch: [126][20/196]	Time 0.017 (0.019)	Data 0.002 (0.010)	Loss 1.8611 (1.8985)	Acc@1 76.172 (74.833)	Acc@5 95.703 (95.443)
Epoch: [126][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.7021 (1.8890)	Acc@1 78.906 (74.836)	Acc@5 96.875 (95.476)
Epoch: [126][40/196]	Time 0.016 (0.017)	Data 0.003 (0.006)	Loss 1.9445 (1.8773)	Acc@1 72.656 (74.981)	Acc@5 93.750 (95.598)
Epoch: [126][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8245 (1.8769)	Acc@1 78.125 (75.054)	Acc@5 97.266 (95.558)
Epoch: [126][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.8548 (1.8872)	Acc@1 75.391 (74.866)	Acc@5 96.484 (95.453)
Epoch: [126][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.9770 (1.8852)	Acc@1 72.266 (74.890)	Acc@5 95.703 (95.423)
Epoch: [126][80/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0581 (1.8969)	Acc@1 71.094 (74.523)	Acc@5 94.141 (95.274)
Epoch: [126][90/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.0077 (1.8972)	Acc@1 74.219 (74.541)	Acc@5 94.141 (95.278)
Epoch: [126][100/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8751 (1.9054)	Acc@1 75.000 (74.261)	Acc@5 96.875 (95.239)
Epoch: [126][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0597 (1.9188)	Acc@1 71.484 (73.899)	Acc@5 94.531 (95.105)
Epoch: [126][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9489 (1.9283)	Acc@1 72.656 (73.693)	Acc@5 92.969 (94.948)
Epoch: [126][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0966 (1.9329)	Acc@1 69.141 (73.494)	Acc@5 92.578 (94.928)
Epoch: [126][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9614 (1.9404)	Acc@1 72.266 (73.310)	Acc@5 94.141 (94.825)
Epoch: [126][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0917 (1.9474)	Acc@1 70.312 (73.093)	Acc@5 92.969 (94.759)
Epoch: [126][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9471 (1.9520)	Acc@1 74.609 (72.986)	Acc@5 96.094 (94.704)
Epoch: [126][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9638 (1.9572)	Acc@1 72.266 (72.841)	Acc@5 94.141 (94.659)
Epoch: [126][180/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 1.9683 (1.9632)	Acc@1 71.875 (72.738)	Acc@5 94.141 (94.609)
Epoch: [126][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0123 (1.9704)	Acc@1 74.219 (72.538)	Acc@5 91.797 (94.554)
num momentum params: 26
[0.1, 1.9728462733459473, 1.9472618973255158, 72.484, 52.19, tensor(0.5329, device='cuda:0', grad_fn=<DivBackward0>), 3.1447865962982178, 0.4031248092651367]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [127 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [127][0/196]	Time 0.076 (0.076)	Data 0.182 (0.182)	Loss 1.9361 (1.9361)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [127][10/196]	Time 0.016 (0.022)	Data 0.002 (0.018)	Loss 1.8317 (1.9576)	Acc@1 76.953 (72.585)	Acc@5 96.484 (94.993)
Epoch: [127][20/196]	Time 0.016 (0.019)	Data 0.002 (0.010)	Loss 1.9565 (1.9414)	Acc@1 71.094 (73.326)	Acc@5 96.094 (95.126)
Epoch: [127][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.8617 (1.9190)	Acc@1 75.000 (74.131)	Acc@5 96.484 (95.502)
Epoch: [127][40/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.7799 (1.9072)	Acc@1 76.953 (74.543)	Acc@5 96.484 (95.570)
Epoch: [127][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9118 (1.9021)	Acc@1 73.828 (74.395)	Acc@5 96.484 (95.718)
Epoch: [127][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.8325 (1.9109)	Acc@1 75.391 (74.142)	Acc@5 96.094 (95.601)
Epoch: [127][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9328 (1.9213)	Acc@1 75.000 (73.812)	Acc@5 94.531 (95.494)
Epoch: [127][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 2.0826 (1.9271)	Acc@1 70.312 (73.698)	Acc@5 94.141 (95.366)
Epoch: [127][90/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0132 (1.9357)	Acc@1 74.219 (73.510)	Acc@5 91.016 (95.270)
Epoch: [127][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9469 (1.9406)	Acc@1 72.656 (73.449)	Acc@5 94.141 (95.158)
Epoch: [127][110/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1384 (1.9515)	Acc@1 67.188 (73.135)	Acc@5 92.969 (95.027)
Epoch: [127][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0295 (1.9543)	Acc@1 69.922 (73.011)	Acc@5 95.312 (94.999)
Epoch: [127][130/196]	Time 0.012 (0.016)	Data 0.010 (0.004)	Loss 1.8862 (1.9605)	Acc@1 74.609 (72.847)	Acc@5 96.094 (94.955)
Epoch: [127][140/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.0538 (1.9672)	Acc@1 71.094 (72.706)	Acc@5 92.578 (94.897)
Epoch: [127][150/196]	Time 0.012 (0.016)	Data 0.006 (0.003)	Loss 1.9694 (1.9701)	Acc@1 73.438 (72.633)	Acc@5 94.922 (94.837)
Epoch: [127][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.8927 (1.9734)	Acc@1 73.047 (72.574)	Acc@5 94.141 (94.767)
Epoch: [127][170/196]	Time 0.012 (0.016)	Data 0.005 (0.003)	Loss 2.1289 (1.9778)	Acc@1 66.406 (72.464)	Acc@5 92.969 (94.696)
Epoch: [127][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9698 (1.9843)	Acc@1 72.656 (72.328)	Acc@5 96.094 (94.626)
Epoch: [127][190/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.1043 (1.9863)	Acc@1 68.750 (72.268)	Acc@5 94.141 (94.623)
num momentum params: 26
[0.1, 1.9885933980560302, 1.9259857261180877, 72.196, 53.44, tensor(0.5293, device='cuda:0', grad_fn=<DivBackward0>), 3.116791009902954, 0.39790415763854986]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [128 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [128][0/196]	Time 0.065 (0.065)	Data 0.193 (0.193)	Loss 1.9607 (1.9607)	Acc@1 73.438 (73.438)	Acc@5 94.141 (94.141)
Epoch: [128][10/196]	Time 0.016 (0.021)	Data 0.002 (0.019)	Loss 1.8867 (1.9394)	Acc@1 76.562 (72.479)	Acc@5 95.312 (95.348)
Epoch: [128][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.9422 (1.9214)	Acc@1 73.828 (73.326)	Acc@5 94.531 (95.592)
Epoch: [128][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.9093 (1.9029)	Acc@1 74.609 (74.005)	Acc@5 96.875 (95.917)
Epoch: [128][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8446 (1.8949)	Acc@1 76.172 (74.390)	Acc@5 97.266 (95.808)
Epoch: [128][50/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 1.6885 (1.8947)	Acc@1 81.641 (74.403)	Acc@5 97.656 (95.680)
Epoch: [128][60/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9398 (1.8982)	Acc@1 72.266 (74.206)	Acc@5 95.312 (95.742)
Epoch: [128][70/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.8381 (1.9023)	Acc@1 74.219 (74.180)	Acc@5 96.875 (95.566)
Epoch: [128][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9742 (1.9128)	Acc@1 70.312 (73.857)	Acc@5 94.531 (95.443)
Epoch: [128][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0482 (1.9206)	Acc@1 71.094 (73.712)	Acc@5 94.531 (95.377)
Epoch: [128][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.8781 (1.9277)	Acc@1 75.391 (73.565)	Acc@5 96.484 (95.336)
Epoch: [128][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9624 (1.9385)	Acc@1 76.562 (73.381)	Acc@5 94.141 (95.094)
Epoch: [128][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0120 (1.9477)	Acc@1 70.703 (73.031)	Acc@5 94.922 (95.032)
Epoch: [128][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0963 (1.9557)	Acc@1 67.188 (72.838)	Acc@5 92.578 (94.919)
Epoch: [128][140/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0303 (1.9594)	Acc@1 72.266 (72.712)	Acc@5 93.359 (94.875)
Epoch: [128][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2567 (1.9644)	Acc@1 64.062 (72.610)	Acc@5 92.188 (94.813)
Epoch: [128][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0635 (1.9680)	Acc@1 68.359 (72.501)	Acc@5 92.969 (94.767)
Epoch: [128][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0909 (1.9729)	Acc@1 69.141 (72.311)	Acc@5 91.797 (94.698)
Epoch: [128][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9162 (1.9759)	Acc@1 74.609 (72.253)	Acc@5 96.094 (94.652)
Epoch: [128][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.1142 (1.9797)	Acc@1 66.406 (72.188)	Acc@5 94.531 (94.593)
num momentum params: 26
[0.1, 1.982039418334961, 1.894906885623932, 72.132, 53.16, tensor(0.5311, device='cuda:0', grad_fn=<DivBackward0>), 3.0571165084838867, 0.40528297424316406]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [129 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [129][0/196]	Time 0.068 (0.068)	Data 0.185 (0.185)	Loss 1.8272 (1.8272)	Acc@1 75.391 (75.391)	Acc@5 94.531 (94.531)
Epoch: [129][10/196]	Time 0.017 (0.023)	Data 0.002 (0.018)	Loss 1.9930 (1.9236)	Acc@1 73.828 (74.325)	Acc@5 92.188 (95.312)
Epoch: [129][20/196]	Time 0.015 (0.020)	Data 0.002 (0.010)	Loss 2.1392 (1.9090)	Acc@1 69.531 (74.740)	Acc@5 92.578 (95.461)
Epoch: [129][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.8709 (1.9125)	Acc@1 77.344 (74.609)	Acc@5 94.531 (95.388)
Epoch: [129][40/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 1.9570 (1.9115)	Acc@1 73.438 (74.647)	Acc@5 94.531 (95.227)
Epoch: [129][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8519 (1.9109)	Acc@1 77.344 (74.617)	Acc@5 94.922 (95.175)
Epoch: [129][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.8034 (1.9150)	Acc@1 75.391 (74.533)	Acc@5 97.266 (95.140)
Epoch: [129][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.8256 (1.9227)	Acc@1 74.219 (74.290)	Acc@5 96.484 (95.032)
Epoch: [129][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 2.0612 (1.9333)	Acc@1 73.047 (74.060)	Acc@5 94.141 (94.917)
Epoch: [129][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1546 (1.9422)	Acc@1 67.578 (73.789)	Acc@5 93.359 (94.866)
Epoch: [129][100/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0083 (1.9503)	Acc@1 70.312 (73.480)	Acc@5 96.484 (94.860)
Epoch: [129][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0911 (1.9584)	Acc@1 68.359 (73.163)	Acc@5 95.312 (94.827)
Epoch: [129][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0628 (1.9629)	Acc@1 69.922 (73.069)	Acc@5 93.750 (94.793)
Epoch: [129][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1526 (1.9671)	Acc@1 64.844 (72.925)	Acc@5 94.141 (94.788)
Epoch: [129][140/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.8797 (1.9721)	Acc@1 73.438 (72.842)	Acc@5 96.484 (94.714)
Epoch: [129][150/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 1.9552 (1.9764)	Acc@1 72.266 (72.708)	Acc@5 94.531 (94.692)
Epoch: [129][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2047 (1.9810)	Acc@1 62.500 (72.574)	Acc@5 93.359 (94.636)
Epoch: [129][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.9638 (1.9845)	Acc@1 73.828 (72.437)	Acc@5 96.484 (94.625)
Epoch: [129][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1177 (1.9883)	Acc@1 69.531 (72.350)	Acc@5 95.312 (94.568)
Epoch: [129][190/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.0111 (1.9946)	Acc@1 69.922 (72.192)	Acc@5 92.969 (94.490)
num momentum params: 26
[0.1, 1.9954678845977782, 2.0508579897880552, 72.154, 50.91, tensor(0.5280, device='cuda:0', grad_fn=<DivBackward0>), 3.1122870445251465, 0.39992547035217285]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [130 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [130][0/196]	Time 0.069 (0.069)	Data 0.186 (0.186)	Loss 1.9452 (1.9452)	Acc@1 75.781 (75.781)	Acc@5 92.969 (92.969)
Epoch: [130][10/196]	Time 0.016 (0.021)	Data 0.002 (0.019)	Loss 1.9792 (1.9154)	Acc@1 69.141 (73.935)	Acc@5 95.312 (95.490)
Epoch: [130][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.9063 (1.9143)	Acc@1 74.219 (74.330)	Acc@5 93.750 (95.592)
Epoch: [130][30/196]	Time 0.016 (0.018)	Data 0.006 (0.008)	Loss 1.9107 (1.9110)	Acc@1 71.094 (74.131)	Acc@5 94.922 (95.590)
Epoch: [130][40/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8669 (1.9125)	Acc@1 75.000 (74.162)	Acc@5 94.922 (95.608)
Epoch: [130][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8994 (1.9029)	Acc@1 75.391 (74.494)	Acc@5 93.359 (95.588)
Epoch: [130][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.8585 (1.9019)	Acc@1 75.781 (74.475)	Acc@5 94.531 (95.524)
Epoch: [130][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0061 (1.9090)	Acc@1 69.531 (74.274)	Acc@5 96.875 (95.450)
Epoch: [130][80/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.9384 (1.9124)	Acc@1 72.266 (74.161)	Acc@5 95.312 (95.409)
Epoch: [130][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0112 (1.9185)	Acc@1 73.438 (73.983)	Acc@5 93.359 (95.278)
Epoch: [130][100/196]	Time 0.021 (0.016)	Data 0.001 (0.004)	Loss 2.1259 (1.9343)	Acc@1 67.188 (73.511)	Acc@5 93.359 (95.073)
Epoch: [130][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0964 (1.9481)	Acc@1 67.188 (73.131)	Acc@5 92.188 (94.929)
Epoch: [130][120/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.2873 (1.9604)	Acc@1 62.891 (72.747)	Acc@5 91.406 (94.825)
Epoch: [130][130/196]	Time 0.021 (0.016)	Data 0.002 (0.004)	Loss 2.0780 (1.9690)	Acc@1 69.531 (72.591)	Acc@5 93.750 (94.710)
Epoch: [130][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0776 (1.9767)	Acc@1 69.531 (72.418)	Acc@5 92.578 (94.601)
Epoch: [130][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.8993 (1.9863)	Acc@1 74.219 (72.201)	Acc@5 95.312 (94.511)
Epoch: [130][160/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.0888 (1.9891)	Acc@1 71.875 (72.149)	Acc@5 94.531 (94.485)
Epoch: [130][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 1.9494 (1.9947)	Acc@1 75.000 (72.010)	Acc@5 94.141 (94.428)
Epoch: [130][180/196]	Time 0.018 (0.016)	Data 0.003 (0.003)	Loss 2.0545 (1.9986)	Acc@1 67.969 (71.858)	Acc@5 93.750 (94.415)
Epoch: [130][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9260 (2.0000)	Acc@1 72.266 (71.832)	Acc@5 96.875 (94.427)
num momentum params: 26
[0.1, 2.0010403343200682, 1.8844678175449372, 71.816, 53.67, tensor(0.5274, device='cuda:0', grad_fn=<DivBackward0>), 3.1142587661743164, 0.39845561981201166]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [131 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [131][0/196]	Time 0.069 (0.069)	Data 0.187 (0.187)	Loss 1.9291 (1.9291)	Acc@1 75.391 (75.391)	Acc@5 96.094 (96.094)
Epoch: [131][10/196]	Time 0.014 (0.022)	Data 0.002 (0.019)	Loss 1.8872 (1.9216)	Acc@1 74.219 (74.219)	Acc@5 94.922 (95.099)
Epoch: [131][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 1.9258 (1.8804)	Acc@1 74.609 (75.112)	Acc@5 95.312 (95.703)
Epoch: [131][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.8787 (1.8717)	Acc@1 76.953 (75.302)	Acc@5 96.094 (95.766)
Epoch: [131][40/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8983 (1.8672)	Acc@1 74.609 (75.524)	Acc@5 94.922 (95.989)
Epoch: [131][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8814 (1.8677)	Acc@1 74.609 (75.528)	Acc@5 96.094 (96.032)
Epoch: [131][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 2.0751 (1.8760)	Acc@1 73.047 (75.327)	Acc@5 91.797 (95.934)
Epoch: [131][70/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9797 (1.8888)	Acc@1 72.266 (74.890)	Acc@5 95.312 (95.802)
Epoch: [131][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9544 (1.8979)	Acc@1 74.219 (74.571)	Acc@5 94.531 (95.611)
Epoch: [131][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0306 (1.9072)	Acc@1 70.312 (74.446)	Acc@5 93.359 (95.373)
Epoch: [131][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0388 (1.9151)	Acc@1 70.312 (74.199)	Acc@5 94.922 (95.301)
Epoch: [131][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2631 (1.9228)	Acc@1 64.062 (73.986)	Acc@5 93.359 (95.242)
Epoch: [131][120/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 1.9723 (1.9356)	Acc@1 69.922 (73.586)	Acc@5 97.266 (95.177)
Epoch: [131][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9862 (1.9407)	Acc@1 71.875 (73.458)	Acc@5 95.312 (95.122)
Epoch: [131][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0915 (1.9483)	Acc@1 67.188 (73.210)	Acc@5 95.312 (95.055)
Epoch: [131][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0244 (1.9550)	Acc@1 74.219 (73.052)	Acc@5 93.359 (95.010)
Epoch: [131][160/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.1757 (1.9655)	Acc@1 68.750 (72.734)	Acc@5 94.531 (94.907)
Epoch: [131][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.3039 (1.9759)	Acc@1 67.188 (72.462)	Acc@5 91.016 (94.794)
Epoch: [131][180/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.0142 (1.9811)	Acc@1 72.656 (72.343)	Acc@5 94.922 (94.697)
Epoch: [131][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0569 (1.9860)	Acc@1 67.578 (72.178)	Acc@5 94.531 (94.646)
num momentum params: 26
[0.1, 1.987752082977295, 1.79828329205513, 72.12, 55.04, tensor(0.5314, device='cuda:0', grad_fn=<DivBackward0>), 3.0407583713531494, 0.3988225460052491]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [132 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [132][0/196]	Time 0.069 (0.069)	Data 0.194 (0.194)	Loss 1.8477 (1.8477)	Acc@1 77.344 (77.344)	Acc@5 95.312 (95.312)
Epoch: [132][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 2.0564 (1.9376)	Acc@1 70.312 (74.645)	Acc@5 93.750 (95.312)
Epoch: [132][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.8019 (1.9038)	Acc@1 77.734 (74.591)	Acc@5 95.312 (95.424)
Epoch: [132][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.7805 (1.8917)	Acc@1 78.906 (75.025)	Acc@5 97.656 (95.678)
Epoch: [132][40/196]	Time 0.013 (0.018)	Data 0.005 (0.007)	Loss 2.0006 (1.9043)	Acc@1 67.969 (74.600)	Acc@5 95.703 (95.446)
Epoch: [132][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9080 (1.9139)	Acc@1 72.266 (74.295)	Acc@5 96.094 (95.320)
Epoch: [132][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 2.0311 (1.9107)	Acc@1 73.047 (74.276)	Acc@5 92.969 (95.453)
Epoch: [132][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.1136 (1.9206)	Acc@1 71.875 (74.048)	Acc@5 94.141 (95.296)
Epoch: [132][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 2.0883 (1.9253)	Acc@1 69.922 (73.823)	Acc@5 93.359 (95.298)
Epoch: [132][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0669 (1.9285)	Acc@1 69.922 (73.729)	Acc@5 93.359 (95.244)
Epoch: [132][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0270 (1.9371)	Acc@1 70.312 (73.496)	Acc@5 92.969 (95.158)
Epoch: [132][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9698 (1.9450)	Acc@1 71.875 (73.265)	Acc@5 95.703 (95.077)
Epoch: [132][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9716 (1.9525)	Acc@1 74.609 (73.160)	Acc@5 93.359 (94.948)
Epoch: [132][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0889 (1.9547)	Acc@1 68.750 (73.130)	Acc@5 92.969 (94.889)
Epoch: [132][140/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 1.8891 (1.9540)	Acc@1 77.734 (73.232)	Acc@5 95.312 (94.886)
Epoch: [132][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9945 (1.9577)	Acc@1 71.094 (73.130)	Acc@5 95.312 (94.831)
Epoch: [132][160/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 2.1810 (1.9666)	Acc@1 66.406 (72.913)	Acc@5 92.578 (94.740)
Epoch: [132][170/196]	Time 0.014 (0.016)	Data 0.002 (0.003)	Loss 2.2260 (1.9683)	Acc@1 67.188 (72.908)	Acc@5 93.750 (94.741)
Epoch: [132][180/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0545 (1.9740)	Acc@1 71.875 (72.781)	Acc@5 91.406 (94.611)
Epoch: [132][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1308 (1.9778)	Acc@1 68.750 (72.640)	Acc@5 91.797 (94.560)
num momentum params: 26
[0.1, 1.9787213069915772, 2.022388801574707, 72.64, 50.49, tensor(0.5335, device='cuda:0', grad_fn=<DivBackward0>), 3.0825002193450928, 0.4122133255004883]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [133 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [133][0/196]	Time 0.068 (0.068)	Data 0.184 (0.184)	Loss 1.8168 (1.8168)	Acc@1 76.953 (76.953)	Acc@5 96.094 (96.094)
Epoch: [133][10/196]	Time 0.016 (0.022)	Data 0.002 (0.018)	Loss 1.9187 (1.9648)	Acc@1 73.828 (72.479)	Acc@5 96.875 (95.455)
Epoch: [133][20/196]	Time 0.016 (0.019)	Data 0.001 (0.011)	Loss 1.8682 (1.9615)	Acc@1 77.344 (72.768)	Acc@5 96.875 (95.201)
Epoch: [133][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.9440 (1.9384)	Acc@1 75.000 (73.639)	Acc@5 93.359 (95.224)
Epoch: [133][40/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9713 (1.9321)	Acc@1 71.484 (73.599)	Acc@5 94.141 (95.208)
Epoch: [133][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.7463 (1.9275)	Acc@1 78.516 (73.675)	Acc@5 97.266 (95.228)
Epoch: [133][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9120 (1.9254)	Acc@1 73.828 (73.662)	Acc@5 95.312 (95.255)
Epoch: [133][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9857 (1.9326)	Acc@1 73.047 (73.515)	Acc@5 94.141 (95.103)
Epoch: [133][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8324 (1.9299)	Acc@1 76.562 (73.597)	Acc@5 96.484 (95.134)
Epoch: [133][90/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9766 (1.9296)	Acc@1 71.094 (73.536)	Acc@5 93.359 (95.154)
Epoch: [133][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9741 (1.9312)	Acc@1 73.828 (73.546)	Acc@5 94.922 (95.127)
Epoch: [133][110/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9101 (1.9377)	Acc@1 75.781 (73.364)	Acc@5 95.312 (95.098)
Epoch: [133][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0092 (1.9462)	Acc@1 70.703 (73.108)	Acc@5 93.359 (95.032)
Epoch: [133][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0805 (1.9534)	Acc@1 70.312 (72.948)	Acc@5 92.969 (94.928)
Epoch: [133][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0052 (1.9575)	Acc@1 71.484 (72.928)	Acc@5 95.312 (94.858)
Epoch: [133][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1476 (1.9662)	Acc@1 67.969 (72.667)	Acc@5 90.625 (94.741)
Epoch: [133][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9792 (1.9685)	Acc@1 71.875 (72.659)	Acc@5 94.531 (94.718)
Epoch: [133][170/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.1999 (1.9747)	Acc@1 67.578 (72.508)	Acc@5 91.797 (94.655)
Epoch: [133][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1211 (1.9826)	Acc@1 69.922 (72.302)	Acc@5 91.406 (94.561)
Epoch: [133][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0678 (1.9893)	Acc@1 69.531 (72.188)	Acc@5 92.969 (94.470)
num momentum params: 26
[0.1, 1.9909561087036134, 1.7355711543560028, 72.142, 55.88, tensor(0.5304, device='cuda:0', grad_fn=<DivBackward0>), 3.05618953704834, 0.40700864791870117]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [134 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [134][0/196]	Time 0.074 (0.074)	Data 0.181 (0.181)	Loss 1.8603 (1.8603)	Acc@1 77.734 (77.734)	Acc@5 96.484 (96.484)
Epoch: [134][10/196]	Time 0.016 (0.023)	Data 0.002 (0.018)	Loss 1.9656 (1.9446)	Acc@1 73.828 (73.509)	Acc@5 95.312 (95.703)
Epoch: [134][20/196]	Time 0.015 (0.019)	Data 0.003 (0.010)	Loss 1.9359 (1.9224)	Acc@1 75.391 (74.312)	Acc@5 94.141 (95.554)
Epoch: [134][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.8857 (1.9106)	Acc@1 73.438 (74.672)	Acc@5 96.484 (95.628)
Epoch: [134][40/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 1.9490 (1.9008)	Acc@1 72.266 (75.000)	Acc@5 94.531 (95.598)
Epoch: [134][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 2.0849 (1.9008)	Acc@1 69.922 (75.000)	Acc@5 94.922 (95.642)
Epoch: [134][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9832 (1.9044)	Acc@1 71.094 (74.808)	Acc@5 96.484 (95.613)
Epoch: [134][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.8405 (1.9031)	Acc@1 76.562 (74.785)	Acc@5 95.703 (95.604)
Epoch: [134][80/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.8486 (1.9038)	Acc@1 77.344 (74.619)	Acc@5 96.094 (95.597)
Epoch: [134][90/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1359 (1.9105)	Acc@1 68.750 (74.554)	Acc@5 92.969 (95.488)
Epoch: [134][100/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 1.9574 (1.9191)	Acc@1 73.828 (74.354)	Acc@5 92.969 (95.413)
Epoch: [134][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0139 (1.9255)	Acc@1 69.531 (74.134)	Acc@5 94.922 (95.355)
Epoch: [134][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0371 (1.9301)	Acc@1 69.531 (73.957)	Acc@5 93.359 (95.251)
Epoch: [134][130/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0096 (1.9419)	Acc@1 73.047 (73.625)	Acc@5 92.969 (95.104)
Epoch: [134][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8549 (1.9497)	Acc@1 73.828 (73.410)	Acc@5 96.484 (95.044)
Epoch: [134][150/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.0425 (1.9623)	Acc@1 70.703 (73.068)	Acc@5 94.531 (94.932)
Epoch: [134][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0494 (1.9657)	Acc@1 73.828 (72.962)	Acc@5 92.969 (94.900)
Epoch: [134][170/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.1898 (1.9732)	Acc@1 67.578 (72.748)	Acc@5 90.234 (94.817)
Epoch: [134][180/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 1.9845 (1.9783)	Acc@1 71.094 (72.566)	Acc@5 95.312 (94.760)
Epoch: [134][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0315 (1.9862)	Acc@1 70.703 (72.407)	Acc@5 93.750 (94.634)
num momentum params: 26
[0.1, 1.9885603853607177, 1.953449741601944, 72.282, 52.48, tensor(0.5320, device='cuda:0', grad_fn=<DivBackward0>), 3.0830047130584712, 0.4030005931854248]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [135 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [135][0/196]	Time 0.069 (0.069)	Data 0.180 (0.180)	Loss 1.8099 (1.8099)	Acc@1 78.516 (78.516)	Acc@5 97.266 (97.266)
Epoch: [135][10/196]	Time 0.016 (0.022)	Data 0.002 (0.019)	Loss 1.9020 (1.9109)	Acc@1 74.609 (74.432)	Acc@5 96.094 (95.810)
Epoch: [135][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 1.9425 (1.9131)	Acc@1 73.047 (74.089)	Acc@5 93.359 (95.406)
Epoch: [135][30/196]	Time 0.021 (0.019)	Data 0.003 (0.008)	Loss 1.8262 (1.8989)	Acc@1 76.562 (74.622)	Acc@5 94.531 (95.665)
Epoch: [135][40/196]	Time 0.014 (0.018)	Data 0.003 (0.006)	Loss 1.9553 (1.9001)	Acc@1 73.047 (74.686)	Acc@5 96.094 (95.570)
Epoch: [135][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8968 (1.8972)	Acc@1 76.562 (74.870)	Acc@5 94.141 (95.558)
Epoch: [135][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.7291 (1.9016)	Acc@1 78.906 (74.725)	Acc@5 96.875 (95.479)
Epoch: [135][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.8083 (1.8995)	Acc@1 78.125 (74.730)	Acc@5 96.094 (95.560)
Epoch: [135][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.8858 (1.9074)	Acc@1 75.391 (74.465)	Acc@5 97.656 (95.501)
Epoch: [135][90/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 2.1010 (1.9127)	Acc@1 68.750 (74.214)	Acc@5 93.750 (95.441)
Epoch: [135][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1195 (1.9226)	Acc@1 68.359 (73.936)	Acc@5 94.141 (95.343)
Epoch: [135][110/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0081 (1.9292)	Acc@1 73.047 (73.737)	Acc@5 94.531 (95.284)
Epoch: [135][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1007 (1.9351)	Acc@1 71.094 (73.618)	Acc@5 91.797 (95.174)
Epoch: [135][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0650 (1.9359)	Acc@1 68.359 (73.569)	Acc@5 91.406 (95.148)
Epoch: [135][140/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 1.9732 (1.9417)	Acc@1 71.875 (73.346)	Acc@5 95.703 (95.124)
Epoch: [135][150/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.1470 (1.9492)	Acc@1 67.969 (73.119)	Acc@5 93.359 (95.041)
Epoch: [135][160/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.0759 (1.9546)	Acc@1 69.531 (73.037)	Acc@5 94.531 (94.941)
Epoch: [135][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0806 (1.9585)	Acc@1 68.359 (72.892)	Acc@5 94.141 (94.920)
Epoch: [135][180/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.0145 (1.9631)	Acc@1 73.047 (72.807)	Acc@5 93.750 (94.825)
Epoch: [135][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2998 (1.9708)	Acc@1 66.406 (72.658)	Acc@5 88.672 (94.687)
num momentum params: 26
[0.1, 1.9734442102050782, 2.0001911449432375, 72.6, 51.29, tensor(0.5350, device='cuda:0', grad_fn=<DivBackward0>), 3.085599184036255, 0.4074583053588867]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [136 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [136][0/196]	Time 0.072 (0.072)	Data 0.196 (0.196)	Loss 1.8859 (1.8859)	Acc@1 72.266 (72.266)	Acc@5 96.484 (96.484)
Epoch: [136][10/196]	Time 0.015 (0.022)	Data 0.003 (0.020)	Loss 1.9084 (1.9592)	Acc@1 73.828 (73.402)	Acc@5 96.094 (94.567)
Epoch: [136][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.9043 (1.9285)	Acc@1 74.609 (74.014)	Acc@5 96.484 (95.219)
Epoch: [136][30/196]	Time 0.017 (0.018)	Data 0.002 (0.008)	Loss 1.9313 (1.9256)	Acc@1 75.000 (74.080)	Acc@5 95.312 (95.212)
Epoch: [136][40/196]	Time 0.019 (0.018)	Data 0.002 (0.007)	Loss 1.7731 (1.9193)	Acc@1 77.734 (74.257)	Acc@5 96.484 (95.151)
Epoch: [136][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8818 (1.9126)	Acc@1 73.438 (74.295)	Acc@5 96.875 (95.358)
Epoch: [136][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.8525 (1.9123)	Acc@1 73.438 (74.225)	Acc@5 97.266 (95.396)
Epoch: [136][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9932 (1.9145)	Acc@1 67.969 (74.010)	Acc@5 96.094 (95.461)
Epoch: [136][80/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.8532 (1.9192)	Acc@1 72.656 (73.958)	Acc@5 98.047 (95.370)
Epoch: [136][90/196]	Time 0.018 (0.017)	Data 0.003 (0.004)	Loss 1.9898 (1.9255)	Acc@1 70.703 (73.768)	Acc@5 95.703 (95.287)
Epoch: [136][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8347 (1.9269)	Acc@1 77.734 (73.666)	Acc@5 96.875 (95.297)
Epoch: [136][110/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.0725 (1.9338)	Acc@1 69.531 (73.483)	Acc@5 92.578 (95.189)
Epoch: [136][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9819 (1.9343)	Acc@1 71.875 (73.415)	Acc@5 96.094 (95.200)
Epoch: [136][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9376 (1.9372)	Acc@1 75.781 (73.315)	Acc@5 93.750 (95.181)
Epoch: [136][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1423 (1.9483)	Acc@1 70.312 (73.094)	Acc@5 90.625 (95.027)
Epoch: [136][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 1.8748 (1.9536)	Acc@1 77.344 (72.954)	Acc@5 94.531 (94.901)
Epoch: [136][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0602 (1.9616)	Acc@1 68.750 (72.756)	Acc@5 93.359 (94.830)
Epoch: [136][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1389 (1.9672)	Acc@1 68.750 (72.640)	Acc@5 92.969 (94.771)
Epoch: [136][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2423 (1.9738)	Acc@1 64.844 (72.419)	Acc@5 92.969 (94.732)
Epoch: [136][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1669 (1.9814)	Acc@1 67.578 (72.217)	Acc@5 92.969 (94.666)
num momentum params: 26
[0.1, 1.984450284576416, 2.025049651861191, 72.114, 50.81, tensor(0.5322, device='cuda:0', grad_fn=<DivBackward0>), 3.1449811458587646, 0.3964941501617431]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [137 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [137][0/196]	Time 0.067 (0.067)	Data 0.181 (0.181)	Loss 1.7901 (1.7901)	Acc@1 77.344 (77.344)	Acc@5 96.094 (96.094)
Epoch: [137][10/196]	Time 0.016 (0.021)	Data 0.002 (0.018)	Loss 2.0094 (1.9788)	Acc@1 73.828 (72.124)	Acc@5 92.969 (94.851)
Epoch: [137][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 1.9274 (1.9581)	Acc@1 73.438 (73.047)	Acc@5 96.094 (95.126)
Epoch: [137][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.8099 (1.9356)	Acc@1 81.250 (73.702)	Acc@5 94.531 (95.350)
Epoch: [137][40/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.7985 (1.9265)	Acc@1 76.562 (74.085)	Acc@5 96.875 (95.370)
Epoch: [137][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8792 (1.9180)	Acc@1 74.609 (74.288)	Acc@5 96.484 (95.504)
Epoch: [137][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.0481 (1.9247)	Acc@1 68.750 (73.943)	Acc@5 92.188 (95.421)
Epoch: [137][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0056 (1.9233)	Acc@1 71.484 (73.938)	Acc@5 93.750 (95.395)
Epoch: [137][80/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0520 (1.9202)	Acc@1 71.094 (74.166)	Acc@5 92.188 (95.341)
Epoch: [137][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0619 (1.9284)	Acc@1 69.531 (73.978)	Acc@5 92.188 (95.257)
Epoch: [137][100/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0289 (1.9289)	Acc@1 73.047 (74.010)	Acc@5 92.188 (95.247)
Epoch: [137][110/196]	Time 0.015 (0.016)	Data 0.005 (0.004)	Loss 2.0316 (1.9345)	Acc@1 69.531 (73.842)	Acc@5 94.141 (95.179)
Epoch: [137][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1441 (1.9440)	Acc@1 68.750 (73.550)	Acc@5 93.359 (95.109)
Epoch: [137][130/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9287 (1.9477)	Acc@1 73.438 (73.503)	Acc@5 92.969 (95.065)
Epoch: [137][140/196]	Time 0.021 (0.016)	Data 0.000 (0.004)	Loss 2.0972 (1.9572)	Acc@1 66.016 (73.274)	Acc@5 93.359 (94.916)
Epoch: [137][150/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1207 (1.9630)	Acc@1 66.797 (73.086)	Acc@5 92.188 (94.849)
Epoch: [137][160/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1150 (1.9674)	Acc@1 68.750 (72.952)	Acc@5 92.969 (94.788)
Epoch: [137][170/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9905 (1.9695)	Acc@1 75.000 (72.901)	Acc@5 94.141 (94.805)
Epoch: [137][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1580 (1.9753)	Acc@1 68.359 (72.745)	Acc@5 92.969 (94.738)
Epoch: [137][190/196]	Time 0.015 (0.016)	Data 0.004 (0.003)	Loss 2.1837 (1.9835)	Acc@1 69.922 (72.515)	Acc@5 92.969 (94.662)
num momentum params: 26
[0.1, 1.9859585738372802, 1.9402934062480925, 72.434, 52.59, tensor(0.5321, device='cuda:0', grad_fn=<DivBackward0>), 3.0960841178894047, 0.4009263515472412]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [138 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [138][0/196]	Time 0.072 (0.072)	Data 0.188 (0.188)	Loss 1.8631 (1.8631)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [138][10/196]	Time 0.017 (0.022)	Data 0.002 (0.018)	Loss 1.9279 (1.9671)	Acc@1 73.828 (73.224)	Acc@5 95.703 (94.602)
Epoch: [138][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.7626 (1.9320)	Acc@1 76.562 (74.126)	Acc@5 96.484 (94.978)
Epoch: [138][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.8600 (1.9216)	Acc@1 73.828 (74.307)	Acc@5 97.266 (95.262)
Epoch: [138][40/196]	Time 0.014 (0.018)	Data 0.003 (0.006)	Loss 1.8709 (1.9140)	Acc@1 73.828 (74.419)	Acc@5 97.266 (95.370)
Epoch: [138][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9303 (1.9046)	Acc@1 76.172 (74.778)	Acc@5 96.094 (95.435)
Epoch: [138][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.0322 (1.9049)	Acc@1 71.484 (74.705)	Acc@5 95.312 (95.479)
Epoch: [138][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.1392 (1.9154)	Acc@1 68.359 (74.488)	Acc@5 92.578 (95.335)
Epoch: [138][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.9289 (1.9153)	Acc@1 75.391 (74.291)	Acc@5 94.922 (95.356)
Epoch: [138][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8880 (1.9185)	Acc@1 75.000 (74.193)	Acc@5 96.094 (95.312)
Epoch: [138][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0067 (1.9196)	Acc@1 68.359 (74.103)	Acc@5 94.141 (95.254)
Epoch: [138][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0559 (1.9248)	Acc@1 73.047 (74.032)	Acc@5 94.141 (95.175)
Epoch: [138][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9838 (1.9309)	Acc@1 72.266 (73.789)	Acc@5 92.578 (95.128)
Epoch: [138][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1847 (1.9398)	Acc@1 67.578 (73.587)	Acc@5 91.797 (95.059)
Epoch: [138][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8767 (1.9464)	Acc@1 76.172 (73.404)	Acc@5 94.141 (94.991)
Epoch: [138][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 1.9977 (1.9548)	Acc@1 72.266 (73.156)	Acc@5 93.359 (94.873)
Epoch: [138][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9405 (1.9595)	Acc@1 71.484 (73.010)	Acc@5 94.922 (94.803)
Epoch: [138][170/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2287 (1.9668)	Acc@1 67.578 (72.839)	Acc@5 92.188 (94.703)
Epoch: [138][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9904 (1.9700)	Acc@1 70.312 (72.725)	Acc@5 95.703 (94.687)
Epoch: [138][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9833 (1.9767)	Acc@1 70.312 (72.470)	Acc@5 94.141 (94.595)
num momentum params: 26
[0.1, 1.9764294622039795, 1.8323033702373506, 72.486, 54.19, tensor(0.5353, device='cuda:0', grad_fn=<DivBackward0>), 3.122247457504272, 0.407860279083252]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [139 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [139][0/196]	Time 0.075 (0.075)	Data 0.186 (0.186)	Loss 1.8366 (1.8366)	Acc@1 73.828 (73.828)	Acc@5 96.875 (96.875)
Epoch: [139][10/196]	Time 0.018 (0.023)	Data 0.002 (0.019)	Loss 2.0719 (1.9194)	Acc@1 69.922 (74.148)	Acc@5 94.531 (95.312)
Epoch: [139][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.9382 (1.9159)	Acc@1 72.266 (74.144)	Acc@5 93.750 (95.275)
Epoch: [139][30/196]	Time 0.017 (0.018)	Data 0.002 (0.008)	Loss 1.9414 (1.9065)	Acc@1 73.047 (74.509)	Acc@5 94.922 (95.451)
Epoch: [139][40/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 1.8642 (1.9033)	Acc@1 76.172 (74.638)	Acc@5 94.531 (95.522)
Epoch: [139][50/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 1.9960 (1.8983)	Acc@1 71.875 (74.793)	Acc@5 97.266 (95.650)
Epoch: [139][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.9260 (1.9137)	Acc@1 74.609 (74.353)	Acc@5 95.703 (95.441)
Epoch: [139][70/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.9984 (1.9198)	Acc@1 72.656 (74.164)	Acc@5 94.922 (95.401)
Epoch: [139][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.8886 (1.9281)	Acc@1 74.609 (74.002)	Acc@5 96.484 (95.370)
Epoch: [139][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.8824 (1.9335)	Acc@1 74.609 (73.854)	Acc@5 97.656 (95.347)
Epoch: [139][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8927 (1.9438)	Acc@1 74.219 (73.554)	Acc@5 96.094 (95.193)
Epoch: [139][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1216 (1.9535)	Acc@1 68.359 (73.290)	Acc@5 94.922 (95.070)
Epoch: [139][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0670 (1.9621)	Acc@1 69.922 (73.115)	Acc@5 92.969 (94.954)
Epoch: [139][130/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0418 (1.9704)	Acc@1 69.922 (72.916)	Acc@5 95.312 (94.877)
Epoch: [139][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0630 (1.9761)	Acc@1 71.875 (72.775)	Acc@5 94.141 (94.797)
Epoch: [139][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1673 (1.9811)	Acc@1 65.625 (72.605)	Acc@5 92.969 (94.772)
Epoch: [139][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.8900 (1.9827)	Acc@1 74.219 (72.533)	Acc@5 94.922 (94.745)
Epoch: [139][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1016 (1.9840)	Acc@1 70.312 (72.492)	Acc@5 93.750 (94.719)
Epoch: [139][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0395 (1.9868)	Acc@1 75.000 (72.462)	Acc@5 91.797 (94.652)
Epoch: [139][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1552 (1.9894)	Acc@1 68.750 (72.423)	Acc@5 92.188 (94.589)
num momentum params: 26
[0.1, 1.991933519897461, 1.9888446068763732, 72.348, 52.47, tensor(0.5320, device='cuda:0', grad_fn=<DivBackward0>), 3.1333439350128174, 0.4014029502868653]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [140 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [140][0/196]	Time 0.071 (0.071)	Data 0.192 (0.192)	Loss 2.0069 (2.0069)	Acc@1 74.609 (74.609)	Acc@5 95.312 (95.312)
Epoch: [140][10/196]	Time 0.018 (0.023)	Data 0.002 (0.019)	Loss 1.9324 (1.9204)	Acc@1 73.828 (75.036)	Acc@5 95.703 (96.094)
Epoch: [140][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.9718 (1.9064)	Acc@1 70.703 (75.112)	Acc@5 94.141 (96.019)
Epoch: [140][30/196]	Time 0.015 (0.019)	Data 0.002 (0.008)	Loss 1.9290 (1.9141)	Acc@1 74.219 (74.899)	Acc@5 93.750 (95.640)
Epoch: [140][40/196]	Time 0.015 (0.018)	Data 0.003 (0.006)	Loss 1.9126 (1.9158)	Acc@1 76.172 (74.924)	Acc@5 96.484 (95.627)
Epoch: [140][50/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 2.0420 (1.9135)	Acc@1 67.969 (74.786)	Acc@5 94.531 (95.619)
Epoch: [140][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9102 (1.9155)	Acc@1 74.219 (74.481)	Acc@5 95.312 (95.594)
Epoch: [140][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9320 (1.9295)	Acc@1 72.266 (73.977)	Acc@5 95.312 (95.511)
Epoch: [140][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.9649 (1.9376)	Acc@1 73.438 (73.717)	Acc@5 95.703 (95.356)
Epoch: [140][90/196]	Time 0.017 (0.017)	Data 0.000 (0.004)	Loss 1.9910 (1.9447)	Acc@1 72.266 (73.553)	Acc@5 92.578 (95.252)
Epoch: [140][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1060 (1.9457)	Acc@1 67.578 (73.468)	Acc@5 92.188 (95.220)
Epoch: [140][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0688 (1.9507)	Acc@1 71.484 (73.395)	Acc@5 92.578 (95.122)
Epoch: [140][120/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0234 (1.9561)	Acc@1 71.875 (73.289)	Acc@5 92.578 (95.028)
Epoch: [140][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1192 (1.9616)	Acc@1 64.844 (73.121)	Acc@5 93.359 (94.934)
Epoch: [140][140/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 2.1485 (1.9703)	Acc@1 67.578 (72.911)	Acc@5 92.578 (94.847)
Epoch: [140][150/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0406 (1.9755)	Acc@1 68.750 (72.767)	Acc@5 94.141 (94.774)
Epoch: [140][160/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 1.9624 (1.9794)	Acc@1 71.875 (72.654)	Acc@5 95.312 (94.723)
Epoch: [140][170/196]	Time 0.019 (0.016)	Data 0.001 (0.003)	Loss 2.0896 (1.9834)	Acc@1 70.703 (72.608)	Acc@5 92.578 (94.632)
Epoch: [140][180/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.2026 (1.9889)	Acc@1 66.016 (72.434)	Acc@5 93.750 (94.590)
Epoch: [140][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0062 (1.9960)	Acc@1 73.047 (72.294)	Acc@5 92.969 (94.505)
num momentum params: 26
[0.1, 1.9985648248291015, 2.154538093805313, 72.24, 50.4, tensor(0.5311, device='cuda:0', grad_fn=<DivBackward0>), 3.1121480464935303, 0.39581036567687994]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [141 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [141][0/196]	Time 0.074 (0.074)	Data 0.192 (0.192)	Loss 1.9699 (1.9699)	Acc@1 73.828 (73.828)	Acc@5 94.531 (94.531)
Epoch: [141][10/196]	Time 0.016 (0.022)	Data 0.002 (0.019)	Loss 2.1560 (1.9786)	Acc@1 68.359 (73.082)	Acc@5 94.141 (95.455)
Epoch: [141][20/196]	Time 0.018 (0.020)	Data 0.002 (0.011)	Loss 1.9257 (1.9517)	Acc@1 73.047 (73.419)	Acc@5 97.266 (95.573)
Epoch: [141][30/196]	Time 0.012 (0.018)	Data 0.005 (0.008)	Loss 1.8027 (1.9202)	Acc@1 78.516 (74.244)	Acc@5 96.484 (95.766)
Epoch: [141][40/196]	Time 0.024 (0.018)	Data 0.002 (0.007)	Loss 1.7845 (1.8964)	Acc@1 78.516 (74.838)	Acc@5 94.531 (95.903)
Epoch: [141][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9156 (1.8884)	Acc@1 74.609 (75.199)	Acc@5 94.922 (95.864)
Epoch: [141][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.8924 (1.8987)	Acc@1 76.562 (74.910)	Acc@5 94.922 (95.690)
Epoch: [141][70/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.8885 (1.9042)	Acc@1 73.828 (74.708)	Acc@5 95.703 (95.593)
Epoch: [141][80/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.9370 (1.9076)	Acc@1 69.141 (74.421)	Acc@5 97.266 (95.602)
Epoch: [141][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9085 (1.9135)	Acc@1 77.344 (74.236)	Acc@5 94.922 (95.510)
Epoch: [141][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0392 (1.9274)	Acc@1 72.266 (73.898)	Acc@5 92.578 (95.328)
Epoch: [141][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8992 (1.9303)	Acc@1 75.000 (73.877)	Acc@5 95.703 (95.235)
Epoch: [141][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9494 (1.9358)	Acc@1 73.438 (73.757)	Acc@5 94.922 (95.180)
Epoch: [141][130/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.2289 (1.9460)	Acc@1 65.234 (73.509)	Acc@5 92.969 (95.101)
Epoch: [141][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0282 (1.9539)	Acc@1 71.094 (73.307)	Acc@5 94.922 (95.047)
Epoch: [141][150/196]	Time 0.019 (0.016)	Data 0.002 (0.004)	Loss 2.0892 (1.9620)	Acc@1 65.625 (73.026)	Acc@5 92.969 (94.927)
Epoch: [141][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0564 (1.9692)	Acc@1 69.531 (72.804)	Acc@5 94.141 (94.866)
Epoch: [141][170/196]	Time 0.011 (0.016)	Data 0.006 (0.003)	Loss 2.2178 (1.9780)	Acc@1 64.062 (72.537)	Acc@5 90.234 (94.757)
Epoch: [141][180/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.2342 (1.9848)	Acc@1 67.578 (72.399)	Acc@5 89.453 (94.669)
Epoch: [141][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0018 (1.9865)	Acc@1 70.703 (72.313)	Acc@5 94.922 (94.662)
num momentum params: 26
[0.1, 1.9903741510009765, 1.8879915177822113, 72.22, 52.99, tensor(0.5335, device='cuda:0', grad_fn=<DivBackward0>), 3.1080069541931152, 0.39799451828002924]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [142 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [142][0/196]	Time 0.066 (0.066)	Data 0.201 (0.201)	Loss 1.9018 (1.9018)	Acc@1 75.781 (75.781)	Acc@5 94.141 (94.141)
Epoch: [142][10/196]	Time 0.017 (0.022)	Data 0.002 (0.020)	Loss 1.9442 (1.9715)	Acc@1 73.047 (72.514)	Acc@5 95.312 (95.384)
Epoch: [142][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 1.8462 (1.9374)	Acc@1 75.000 (73.512)	Acc@5 98.047 (95.480)
Epoch: [142][30/196]	Time 0.011 (0.018)	Data 0.006 (0.008)	Loss 1.8207 (1.9157)	Acc@1 76.953 (74.257)	Acc@5 97.266 (95.489)
Epoch: [142][40/196]	Time 0.017 (0.017)	Data 0.000 (0.007)	Loss 1.8903 (1.9106)	Acc@1 73.828 (74.362)	Acc@5 96.484 (95.665)
Epoch: [142][50/196]	Time 0.012 (0.017)	Data 0.016 (0.007)	Loss 1.8559 (1.9135)	Acc@1 76.172 (74.234)	Acc@5 96.875 (95.657)
Epoch: [142][60/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.9354 (1.9139)	Acc@1 71.875 (74.340)	Acc@5 96.094 (95.633)
Epoch: [142][70/196]	Time 0.014 (0.017)	Data 0.016 (0.006)	Loss 1.9846 (1.9259)	Acc@1 73.438 (73.971)	Acc@5 92.188 (95.544)
Epoch: [142][80/196]	Time 0.019 (0.017)	Data 0.000 (0.005)	Loss 2.0110 (1.9346)	Acc@1 71.484 (73.679)	Acc@5 94.531 (95.380)
Epoch: [142][90/196]	Time 0.014 (0.017)	Data 0.006 (0.005)	Loss 1.7637 (1.9401)	Acc@1 76.953 (73.575)	Acc@5 98.047 (95.287)
Epoch: [142][100/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 2.0126 (1.9411)	Acc@1 72.656 (73.612)	Acc@5 94.922 (95.231)
Epoch: [142][110/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0964 (1.9467)	Acc@1 69.141 (73.455)	Acc@5 94.531 (95.158)
Epoch: [142][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9130 (1.9518)	Acc@1 76.172 (73.357)	Acc@5 94.531 (95.145)
Epoch: [142][130/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1107 (1.9572)	Acc@1 69.141 (73.262)	Acc@5 94.922 (95.050)
Epoch: [142][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1062 (1.9623)	Acc@1 67.188 (73.158)	Acc@5 94.141 (95.002)
Epoch: [142][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9576 (1.9650)	Acc@1 74.219 (73.062)	Acc@5 95.312 (94.976)
Epoch: [142][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0735 (1.9719)	Acc@1 71.484 (72.896)	Acc@5 92.578 (94.849)
Epoch: [142][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0455 (1.9796)	Acc@1 71.875 (72.677)	Acc@5 91.406 (94.753)
Epoch: [142][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0318 (1.9862)	Acc@1 69.141 (72.533)	Acc@5 96.094 (94.689)
Epoch: [142][190/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1630 (1.9933)	Acc@1 68.750 (72.315)	Acc@5 92.578 (94.609)
num momentum params: 26
[0.1, 1.9938046049499512, 1.8357958590984345, 72.282, 54.45, tensor(0.5323, device='cuda:0', grad_fn=<DivBackward0>), 3.1317694187164307, 0.4160013198852539]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [143 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [143][0/196]	Time 0.074 (0.074)	Data 0.184 (0.184)	Loss 1.9653 (1.9653)	Acc@1 73.438 (73.438)	Acc@5 94.922 (94.922)
Epoch: [143][10/196]	Time 0.016 (0.023)	Data 0.002 (0.018)	Loss 2.0695 (1.9357)	Acc@1 68.359 (73.438)	Acc@5 94.531 (95.206)
Epoch: [143][20/196]	Time 0.015 (0.019)	Data 0.002 (0.010)	Loss 2.0149 (1.9331)	Acc@1 72.656 (73.456)	Acc@5 96.484 (95.517)
Epoch: [143][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.7980 (1.9257)	Acc@1 78.516 (73.828)	Acc@5 96.875 (95.401)
Epoch: [143][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 1.8204 (1.9321)	Acc@1 75.781 (73.819)	Acc@5 97.266 (95.246)
Epoch: [143][50/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 2.0402 (1.9301)	Acc@1 72.656 (73.775)	Acc@5 92.578 (95.213)
Epoch: [143][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 2.0200 (1.9359)	Acc@1 69.922 (73.617)	Acc@5 94.922 (95.191)
Epoch: [143][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9048 (1.9365)	Acc@1 76.562 (73.520)	Acc@5 95.312 (95.219)
Epoch: [143][80/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0910 (1.9442)	Acc@1 67.969 (73.235)	Acc@5 94.141 (95.226)
Epoch: [143][90/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9918 (1.9492)	Acc@1 73.828 (73.146)	Acc@5 93.750 (95.085)
Epoch: [143][100/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0427 (1.9526)	Acc@1 67.578 (73.062)	Acc@5 94.531 (95.061)
Epoch: [143][110/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.8584 (1.9569)	Acc@1 73.828 (72.969)	Acc@5 96.484 (95.042)
Epoch: [143][120/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9151 (1.9554)	Acc@1 75.000 (73.031)	Acc@5 93.359 (94.986)
Epoch: [143][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9620 (1.9583)	Acc@1 74.219 (72.957)	Acc@5 96.484 (94.958)
Epoch: [143][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1256 (1.9602)	Acc@1 70.312 (72.925)	Acc@5 92.188 (94.930)
Epoch: [143][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0750 (1.9662)	Acc@1 70.703 (72.749)	Acc@5 92.969 (94.865)
Epoch: [143][160/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 1.9992 (1.9674)	Acc@1 69.922 (72.697)	Acc@5 94.531 (94.849)
Epoch: [143][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9774 (1.9711)	Acc@1 73.828 (72.615)	Acc@5 92.969 (94.833)
Epoch: [143][180/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0192 (1.9754)	Acc@1 71.875 (72.514)	Acc@5 92.969 (94.771)
Epoch: [143][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1318 (1.9767)	Acc@1 68.359 (72.489)	Acc@5 93.750 (94.746)
num momentum params: 26
[0.1, 1.9776462837219237, 1.7819203519821167, 72.452, 55.57, tensor(0.5355, device='cuda:0', grad_fn=<DivBackward0>), 3.0547990798950195, 0.40437388420104975]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [144 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [144][0/196]	Time 0.066 (0.066)	Data 0.192 (0.192)	Loss 1.9816 (1.9816)	Acc@1 71.875 (71.875)	Acc@5 93.750 (93.750)
Epoch: [144][10/196]	Time 0.018 (0.022)	Data 0.002 (0.019)	Loss 1.8396 (1.8967)	Acc@1 79.688 (75.426)	Acc@5 95.703 (95.987)
Epoch: [144][20/196]	Time 0.015 (0.019)	Data 0.002 (0.011)	Loss 2.0069 (1.9186)	Acc@1 71.875 (74.405)	Acc@5 94.141 (95.219)
Epoch: [144][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.7258 (1.9172)	Acc@1 79.297 (74.105)	Acc@5 97.656 (95.439)
Epoch: [144][40/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8542 (1.9209)	Acc@1 76.562 (74.057)	Acc@5 96.875 (95.398)
Epoch: [144][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9163 (1.9232)	Acc@1 74.609 (74.073)	Acc@5 96.094 (95.412)
Epoch: [144][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 2.0107 (1.9217)	Acc@1 71.484 (74.084)	Acc@5 95.703 (95.428)
Epoch: [144][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0255 (1.9273)	Acc@1 69.922 (73.905)	Acc@5 95.703 (95.390)
Epoch: [144][80/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0835 (1.9363)	Acc@1 68.750 (73.655)	Acc@5 90.625 (95.192)
Epoch: [144][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.8701 (1.9382)	Acc@1 73.828 (73.635)	Acc@5 95.312 (95.167)
Epoch: [144][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9478 (1.9361)	Acc@1 69.922 (73.697)	Acc@5 95.703 (95.204)
Epoch: [144][110/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0787 (1.9377)	Acc@1 72.266 (73.698)	Acc@5 91.406 (95.172)
Epoch: [144][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0925 (1.9477)	Acc@1 71.094 (73.412)	Acc@5 93.750 (95.045)
Epoch: [144][130/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9225 (1.9506)	Acc@1 72.656 (73.315)	Acc@5 96.094 (94.999)
Epoch: [144][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0457 (1.9574)	Acc@1 68.359 (73.119)	Acc@5 94.531 (94.947)
Epoch: [144][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0563 (1.9629)	Acc@1 72.266 (73.011)	Acc@5 93.750 (94.878)
Epoch: [144][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0983 (1.9707)	Acc@1 69.141 (72.821)	Acc@5 92.188 (94.769)
Epoch: [144][170/196]	Time 0.012 (0.016)	Data 0.004 (0.003)	Loss 2.1553 (1.9764)	Acc@1 64.062 (72.656)	Acc@5 90.625 (94.700)
Epoch: [144][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.8833 (1.9788)	Acc@1 75.391 (72.579)	Acc@5 96.875 (94.702)
Epoch: [144][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.1149 (1.9831)	Acc@1 68.359 (72.460)	Acc@5 93.750 (94.656)
num momentum params: 26
[0.1, 1.9850420526123047, 2.2089709782600404, 72.388, 49.91, tensor(0.5336, device='cuda:0', grad_fn=<DivBackward0>), 3.089863538742065, 0.4052703380584717]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [145 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [145][0/196]	Time 0.071 (0.071)	Data 0.187 (0.187)	Loss 1.8695 (1.8695)	Acc@1 72.266 (72.266)	Acc@5 98.047 (98.047)
Epoch: [145][10/196]	Time 0.019 (0.022)	Data 0.002 (0.019)	Loss 1.9159 (1.9128)	Acc@1 75.391 (73.651)	Acc@5 95.312 (95.632)
Epoch: [145][20/196]	Time 0.015 (0.019)	Data 0.003 (0.011)	Loss 1.9315 (1.8953)	Acc@1 76.953 (74.740)	Acc@5 94.922 (95.610)
Epoch: [145][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.9339 (1.8690)	Acc@1 71.094 (75.391)	Acc@5 96.094 (95.955)
Epoch: [145][40/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 1.9808 (1.8692)	Acc@1 72.266 (75.210)	Acc@5 96.094 (95.989)
Epoch: [145][50/196]	Time 0.017 (0.017)	Data 0.000 (0.006)	Loss 1.8800 (1.8734)	Acc@1 74.609 (75.115)	Acc@5 95.312 (95.948)
Epoch: [145][60/196]	Time 0.011 (0.017)	Data 0.014 (0.006)	Loss 1.8319 (1.8763)	Acc@1 75.391 (74.968)	Acc@5 96.875 (95.863)
Epoch: [145][70/196]	Time 0.016 (0.017)	Data 0.000 (0.005)	Loss 1.8427 (1.8800)	Acc@1 74.609 (74.835)	Acc@5 94.531 (95.720)
Epoch: [145][80/196]	Time 0.012 (0.017)	Data 0.008 (0.005)	Loss 1.8876 (1.8891)	Acc@1 75.781 (74.605)	Acc@5 94.531 (95.660)
Epoch: [145][90/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 1.7847 (1.8932)	Acc@1 80.078 (74.536)	Acc@5 96.094 (95.630)
Epoch: [145][100/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 1.8560 (1.8996)	Acc@1 76.562 (74.381)	Acc@5 96.094 (95.560)
Epoch: [145][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9781 (1.9103)	Acc@1 71.094 (74.166)	Acc@5 94.922 (95.457)
Epoch: [145][120/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.1418 (1.9190)	Acc@1 67.969 (73.967)	Acc@5 93.750 (95.319)
Epoch: [145][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0798 (1.9275)	Acc@1 68.359 (73.724)	Acc@5 93.359 (95.223)
Epoch: [145][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0102 (1.9342)	Acc@1 75.781 (73.648)	Acc@5 92.969 (95.088)
Epoch: [145][150/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9929 (1.9429)	Acc@1 71.094 (73.409)	Acc@5 93.750 (94.992)
Epoch: [145][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9445 (1.9485)	Acc@1 75.781 (73.275)	Acc@5 96.875 (94.944)
Epoch: [145][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1204 (1.9535)	Acc@1 71.094 (73.186)	Acc@5 93.359 (94.851)
Epoch: [145][180/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0134 (1.9599)	Acc@1 72.656 (72.965)	Acc@5 94.922 (94.797)
Epoch: [145][190/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.2341 (1.9677)	Acc@1 66.016 (72.748)	Acc@5 91.797 (94.711)
num momentum params: 26
[0.1, 1.9707368250274657, 1.8522178196907044, 72.67, 53.08, tensor(0.5376, device='cuda:0', grad_fn=<DivBackward0>), 3.092430353164673, 0.4011988639831543]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [146 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [146][0/196]	Time 0.069 (0.069)	Data 0.189 (0.189)	Loss 1.9861 (1.9861)	Acc@1 69.922 (69.922)	Acc@5 94.141 (94.141)
Epoch: [146][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 2.0006 (1.9310)	Acc@1 70.312 (73.686)	Acc@5 94.922 (94.886)
Epoch: [146][20/196]	Time 0.016 (0.019)	Data 0.002 (0.010)	Loss 1.8628 (1.9403)	Acc@1 74.609 (73.456)	Acc@5 96.875 (95.089)
Epoch: [146][30/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.7355 (1.9170)	Acc@1 79.297 (74.068)	Acc@5 96.875 (95.312)
Epoch: [146][40/196]	Time 0.016 (0.017)	Data 0.003 (0.006)	Loss 1.8767 (1.9045)	Acc@1 74.609 (74.371)	Acc@5 97.266 (95.570)
Epoch: [146][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8234 (1.9029)	Acc@1 75.781 (74.586)	Acc@5 98.047 (95.588)
Epoch: [146][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.9761 (1.9015)	Acc@1 69.922 (74.443)	Acc@5 97.266 (95.639)
Epoch: [146][70/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 2.0297 (1.9074)	Acc@1 70.703 (74.268)	Acc@5 94.531 (95.593)
Epoch: [146][80/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9645 (1.9153)	Acc@1 72.266 (74.069)	Acc@5 94.922 (95.530)
Epoch: [146][90/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0567 (1.9267)	Acc@1 73.047 (73.747)	Acc@5 95.312 (95.424)
Epoch: [146][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0697 (1.9291)	Acc@1 71.094 (73.724)	Acc@5 94.922 (95.359)
Epoch: [146][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0618 (1.9356)	Acc@1 73.047 (73.550)	Acc@5 94.141 (95.277)
Epoch: [146][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.8367 (1.9390)	Acc@1 77.344 (73.415)	Acc@5 96.484 (95.248)
Epoch: [146][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1473 (1.9502)	Acc@1 67.578 (73.145)	Acc@5 90.234 (95.086)
Epoch: [146][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9324 (1.9584)	Acc@1 71.094 (72.972)	Acc@5 94.531 (94.911)
Epoch: [146][150/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0654 (1.9661)	Acc@1 70.703 (72.765)	Acc@5 94.141 (94.875)
Epoch: [146][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9681 (1.9693)	Acc@1 72.656 (72.642)	Acc@5 94.141 (94.837)
Epoch: [146][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0663 (1.9742)	Acc@1 67.969 (72.494)	Acc@5 93.359 (94.748)
Epoch: [146][180/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.9714 (1.9815)	Acc@1 73.828 (72.356)	Acc@5 93.359 (94.667)
Epoch: [146][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9932 (1.9874)	Acc@1 72.656 (72.270)	Acc@5 93.750 (94.601)
num momentum params: 26
[0.1, 1.9889230723571778, 1.8906242871284484, 72.218, 53.2, tensor(0.5333, device='cuda:0', grad_fn=<DivBackward0>), 3.057647705078125, 0.3957009315490723]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [147 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [147][0/196]	Time 0.069 (0.069)	Data 0.190 (0.190)	Loss 1.7646 (1.7646)	Acc@1 79.297 (79.297)	Acc@5 98.438 (98.438)
Epoch: [147][10/196]	Time 0.015 (0.026)	Data 0.002 (0.019)	Loss 1.7264 (1.8766)	Acc@1 80.078 (75.746)	Acc@5 96.875 (95.774)
Epoch: [147][20/196]	Time 0.016 (0.021)	Data 0.002 (0.011)	Loss 1.9144 (1.8910)	Acc@1 72.656 (75.577)	Acc@5 94.922 (95.499)
Epoch: [147][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 1.8807 (1.8866)	Acc@1 75.781 (75.428)	Acc@5 96.875 (95.716)
Epoch: [147][40/196]	Time 0.017 (0.018)	Data 0.000 (0.007)	Loss 1.9027 (1.8919)	Acc@1 74.219 (75.400)	Acc@5 94.531 (95.570)
Epoch: [147][50/196]	Time 0.017 (0.017)	Data 0.000 (0.006)	Loss 2.0273 (1.8898)	Acc@1 71.875 (75.207)	Acc@5 93.359 (95.627)
Epoch: [147][60/196]	Time 0.017 (0.017)	Data 0.000 (0.006)	Loss 1.8471 (1.8881)	Acc@1 78.125 (75.237)	Acc@5 94.922 (95.665)
Epoch: [147][70/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 1.9059 (1.8899)	Acc@1 73.047 (75.028)	Acc@5 93.750 (95.643)
Epoch: [147][80/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 2.0263 (1.8984)	Acc@1 72.266 (74.822)	Acc@5 93.750 (95.452)
Epoch: [147][90/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 2.0347 (1.9093)	Acc@1 71.875 (74.502)	Acc@5 94.531 (95.368)
Epoch: [147][100/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9376 (1.9139)	Acc@1 73.438 (74.524)	Acc@5 96.484 (95.305)
Epoch: [147][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0472 (1.9201)	Acc@1 67.969 (74.247)	Acc@5 94.141 (95.270)
Epoch: [147][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0874 (1.9261)	Acc@1 70.312 (74.125)	Acc@5 92.969 (95.193)
Epoch: [147][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1290 (1.9353)	Acc@1 71.875 (73.849)	Acc@5 92.188 (95.107)
Epoch: [147][140/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9733 (1.9418)	Acc@1 72.656 (73.690)	Acc@5 94.531 (95.027)
Epoch: [147][150/196]	Time 0.020 (0.016)	Data 0.001 (0.004)	Loss 2.0661 (1.9502)	Acc@1 72.656 (73.536)	Acc@5 91.016 (94.932)
Epoch: [147][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0680 (1.9580)	Acc@1 72.656 (73.311)	Acc@5 93.750 (94.861)
Epoch: [147][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0898 (1.9639)	Acc@1 69.531 (73.113)	Acc@5 91.797 (94.808)
Epoch: [147][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9898 (1.9702)	Acc@1 69.922 (72.952)	Acc@5 95.703 (94.719)
Epoch: [147][190/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2380 (1.9750)	Acc@1 67.188 (72.859)	Acc@5 91.406 (94.640)
num momentum params: 26
[0.1, 1.9765665576934814, 2.272124789953232, 72.818, 47.36, tensor(0.5368, device='cuda:0', grad_fn=<DivBackward0>), 3.1432881355285645, 0.39430236816406256]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [148 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [148][0/196]	Time 0.066 (0.066)	Data 0.187 (0.187)	Loss 1.8571 (1.8571)	Acc@1 75.781 (75.781)	Acc@5 96.484 (96.484)
Epoch: [148][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 1.9385 (1.9039)	Acc@1 73.438 (74.716)	Acc@5 97.266 (95.739)
Epoch: [148][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.9874 (1.8878)	Acc@1 70.312 (74.870)	Acc@5 96.484 (95.833)
Epoch: [148][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.9789 (1.9042)	Acc@1 72.266 (74.458)	Acc@5 94.531 (95.539)
Epoch: [148][40/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 1.8523 (1.8935)	Acc@1 75.000 (74.590)	Acc@5 95.703 (95.817)
Epoch: [148][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8672 (1.8893)	Acc@1 74.219 (74.510)	Acc@5 96.875 (95.856)
Epoch: [148][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9303 (1.8996)	Acc@1 74.609 (74.244)	Acc@5 94.922 (95.806)
Epoch: [148][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.8776 (1.8990)	Acc@1 75.000 (74.268)	Acc@5 98.438 (95.885)
Epoch: [148][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.9514 (1.9020)	Acc@1 73.047 (74.310)	Acc@5 94.531 (95.766)
Epoch: [148][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1487 (1.9189)	Acc@1 65.625 (73.897)	Acc@5 92.578 (95.536)
Epoch: [148][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1470 (1.9291)	Acc@1 67.969 (73.627)	Acc@5 92.578 (95.382)
Epoch: [148][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1029 (1.9351)	Acc@1 69.141 (73.357)	Acc@5 91.016 (95.369)
Epoch: [148][120/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 1.9746 (1.9411)	Acc@1 72.656 (73.250)	Acc@5 95.312 (95.296)
Epoch: [148][130/196]	Time 0.018 (0.016)	Data 0.002 (0.003)	Loss 2.0254 (1.9495)	Acc@1 68.750 (73.056)	Acc@5 96.875 (95.208)
Epoch: [148][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9436 (1.9571)	Acc@1 75.781 (72.828)	Acc@5 94.531 (95.096)
Epoch: [148][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0921 (1.9640)	Acc@1 70.312 (72.659)	Acc@5 93.359 (95.041)
Epoch: [148][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1295 (1.9697)	Acc@1 68.359 (72.528)	Acc@5 93.750 (94.995)
Epoch: [148][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0917 (1.9763)	Acc@1 71.094 (72.368)	Acc@5 92.578 (94.888)
Epoch: [148][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0626 (1.9813)	Acc@1 69.531 (72.216)	Acc@5 94.141 (94.818)
Epoch: [148][190/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.0846 (1.9880)	Acc@1 71.094 (72.096)	Acc@5 96.484 (94.728)
num momentum params: 26
[0.1, 1.9908999977874755, 2.0250120782852172, 72.012, 51.93, tensor(0.5332, device='cuda:0', grad_fn=<DivBackward0>), 3.15111494064331, 0.39925312995910645]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [149 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [149][0/196]	Time 0.061 (0.061)	Data 0.190 (0.190)	Loss 1.9128 (1.9128)	Acc@1 75.781 (75.781)	Acc@5 92.969 (92.969)
Epoch: [149][10/196]	Time 0.016 (0.022)	Data 0.002 (0.019)	Loss 2.0024 (1.9185)	Acc@1 73.047 (73.935)	Acc@5 94.141 (95.348)
Epoch: [149][20/196]	Time 0.018 (0.019)	Data 0.002 (0.011)	Loss 1.7636 (1.9040)	Acc@1 78.906 (74.163)	Acc@5 96.875 (95.592)
Epoch: [149][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.9191 (1.8951)	Acc@1 75.391 (74.735)	Acc@5 96.484 (95.892)
Epoch: [149][40/196]	Time 0.015 (0.018)	Data 0.002 (0.007)	Loss 1.7777 (1.8953)	Acc@1 77.344 (74.829)	Acc@5 97.266 (95.684)
Epoch: [149][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8408 (1.8904)	Acc@1 75.391 (74.931)	Acc@5 96.875 (95.718)
Epoch: [149][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.9466 (1.8954)	Acc@1 75.000 (74.769)	Acc@5 94.922 (95.722)
Epoch: [149][70/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.8820 (1.8994)	Acc@1 74.609 (74.774)	Acc@5 96.484 (95.736)
Epoch: [149][80/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 1.8822 (1.9102)	Acc@1 76.172 (74.595)	Acc@5 96.875 (95.626)
Epoch: [149][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8957 (1.9168)	Acc@1 75.391 (74.463)	Acc@5 94.141 (95.566)
Epoch: [149][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9827 (1.9209)	Acc@1 75.000 (74.408)	Acc@5 93.750 (95.471)
Epoch: [149][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0482 (1.9259)	Acc@1 66.797 (74.134)	Acc@5 94.922 (95.390)
Epoch: [149][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9945 (1.9314)	Acc@1 73.438 (74.006)	Acc@5 93.750 (95.325)
Epoch: [149][130/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1479 (1.9383)	Acc@1 67.188 (73.810)	Acc@5 94.141 (95.238)
Epoch: [149][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.9410 (1.9408)	Acc@1 71.484 (73.673)	Acc@5 95.703 (95.210)
Epoch: [149][150/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0271 (1.9474)	Acc@1 69.141 (73.523)	Acc@5 93.359 (95.134)
Epoch: [149][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1928 (1.9570)	Acc@1 68.359 (73.234)	Acc@5 93.359 (95.041)
Epoch: [149][170/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 1.9466 (1.9653)	Acc@1 71.484 (72.987)	Acc@5 94.531 (94.936)
Epoch: [149][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2025 (1.9701)	Acc@1 68.750 (72.846)	Acc@5 92.188 (94.866)
Epoch: [149][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1099 (1.9738)	Acc@1 67.969 (72.730)	Acc@5 92.188 (94.822)
num momentum params: 26
[0.1, 1.9749706199645995, 1.8850186634063721, 72.668, 53.15, tensor(0.5377, device='cuda:0', grad_fn=<DivBackward0>), 3.0944647789001465, 0.4037749767303467]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [150 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [150][0/196]	Time 0.071 (0.071)	Data 0.188 (0.188)	Loss 1.8669 (1.8669)	Acc@1 76.953 (76.953)	Acc@5 96.094 (96.094)
Epoch: [150][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 1.7960 (1.8351)	Acc@1 75.781 (76.491)	Acc@5 96.875 (96.591)
Epoch: [150][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.6413 (1.7715)	Acc@1 80.469 (78.516)	Acc@5 98.438 (96.931)
Epoch: [150][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.6705 (1.7259)	Acc@1 81.641 (80.129)	Acc@5 96.875 (97.177)
Epoch: [150][40/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 1.5400 (1.6965)	Acc@1 87.500 (81.183)	Acc@5 98.047 (97.447)
Epoch: [150][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.5906 (1.6698)	Acc@1 86.328 (81.932)	Acc@5 98.438 (97.664)
Epoch: [150][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.5447 (1.6515)	Acc@1 87.109 (82.550)	Acc@5 98.438 (97.804)
Epoch: [150][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.5663 (1.6384)	Acc@1 87.500 (82.978)	Acc@5 97.266 (97.920)
Epoch: [150][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.4767 (1.6242)	Acc@1 87.891 (83.367)	Acc@5 98.828 (97.970)
Epoch: [150][90/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.4756 (1.6106)	Acc@1 86.328 (83.667)	Acc@5 97.266 (98.038)
Epoch: [150][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.4485 (1.6004)	Acc@1 87.500 (83.907)	Acc@5 98.438 (98.097)
Epoch: [150][110/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 1.5150 (1.5920)	Acc@1 85.547 (84.160)	Acc@5 98.438 (98.114)
Epoch: [150][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.4929 (1.5834)	Acc@1 87.500 (84.427)	Acc@5 98.828 (98.170)
Epoch: [150][130/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 1.4451 (1.5742)	Acc@1 89.062 (84.703)	Acc@5 98.828 (98.199)
Epoch: [150][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.5445 (1.5668)	Acc@1 85.938 (84.943)	Acc@5 98.828 (98.238)
Epoch: [150][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.4350 (1.5594)	Acc@1 89.453 (85.151)	Acc@5 98.828 (98.282)
Epoch: [150][160/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 1.5729 (1.5542)	Acc@1 83.984 (85.268)	Acc@5 98.047 (98.299)
Epoch: [150][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.4272 (1.5468)	Acc@1 89.453 (85.499)	Acc@5 98.438 (98.328)
Epoch: [150][180/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.4187 (1.5404)	Acc@1 89.844 (85.689)	Acc@5 99.609 (98.364)
Epoch: [150][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.4390 (1.5344)	Acc@1 90.625 (85.862)	Acc@5 98.828 (98.407)
num momentum params: 26
[0.010000000000000002, 1.533116986694336, 1.1877774798870087, 85.894, 68.22, tensor(0.6905, device='cuda:0', grad_fn=<DivBackward0>), 3.0648465156555176, 0.39911079406738276]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [151 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [151][0/196]	Time 0.066 (0.066)	Data 0.186 (0.186)	Loss 1.3938 (1.3938)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [151][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 1.3532 (1.3739)	Acc@1 92.578 (90.732)	Acc@5 99.609 (99.432)
Epoch: [151][20/196]	Time 0.019 (0.020)	Data 0.002 (0.010)	Loss 1.3450 (1.3687)	Acc@1 92.969 (90.978)	Acc@5 100.000 (99.442)
Epoch: [151][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 1.4004 (1.3791)	Acc@1 89.453 (90.562)	Acc@5 98.828 (99.332)
Epoch: [151][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 1.3707 (1.3771)	Acc@1 90.625 (90.568)	Acc@5 98.828 (99.324)
Epoch: [151][50/196]	Time 0.018 (0.017)	Data 0.002 (0.005)	Loss 1.2994 (1.3713)	Acc@1 92.188 (90.656)	Acc@5 100.000 (99.318)
Epoch: [151][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.3388 (1.3694)	Acc@1 91.016 (90.779)	Acc@5 99.219 (99.353)
Epoch: [151][70/196]	Time 0.017 (0.017)	Data 0.002 (0.004)	Loss 1.3847 (1.3685)	Acc@1 91.016 (90.785)	Acc@5 98.438 (99.340)
Epoch: [151][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.3383 (1.3677)	Acc@1 92.188 (90.779)	Acc@5 99.219 (99.363)
Epoch: [151][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.3647 (1.3672)	Acc@1 90.625 (90.758)	Acc@5 99.219 (99.365)
Epoch: [151][100/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.4242 (1.3676)	Acc@1 89.062 (90.791)	Acc@5 99.219 (99.366)
Epoch: [151][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.3301 (1.3659)	Acc@1 92.188 (90.826)	Acc@5 99.219 (99.356)
Epoch: [151][120/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 1.3579 (1.3629)	Acc@1 89.453 (90.886)	Acc@5 99.609 (99.370)
Epoch: [151][130/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.3622 (1.3616)	Acc@1 90.234 (90.902)	Acc@5 99.609 (99.374)
Epoch: [151][140/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.4200 (1.3606)	Acc@1 90.234 (90.908)	Acc@5 98.047 (99.357)
Epoch: [151][150/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 1.2923 (1.3597)	Acc@1 91.406 (90.894)	Acc@5 99.609 (99.356)
Epoch: [151][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.3107 (1.3572)	Acc@1 90.625 (90.936)	Acc@5 99.219 (99.372)
Epoch: [151][170/196]	Time 0.014 (0.016)	Data 0.005 (0.003)	Loss 1.3007 (1.3551)	Acc@1 94.141 (90.997)	Acc@5 99.219 (99.370)
Epoch: [151][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.3189 (1.3539)	Acc@1 91.797 (91.037)	Acc@5 98.828 (99.372)
Epoch: [151][190/196]	Time 0.013 (0.016)	Data 0.003 (0.003)	Loss 1.2846 (1.3523)	Acc@1 92.188 (91.073)	Acc@5 99.219 (99.362)
num momentum params: 26
[0.010000000000000002, 1.3511355559158325, 1.1764513099193572, 91.096, 68.64, tensor(0.7698, device='cuda:0', grad_fn=<DivBackward0>), 3.139619588851929, 0.39557480812072754]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [152 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [152][0/196]	Time 0.064 (0.064)	Data 0.200 (0.200)	Loss 1.3244 (1.3244)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)
Epoch: [152][10/196]	Time 0.016 (0.022)	Data 0.002 (0.020)	Loss 1.2931 (1.2949)	Acc@1 92.578 (93.111)	Acc@5 99.609 (99.467)
Epoch: [152][20/196]	Time 0.018 (0.020)	Data 0.002 (0.011)	Loss 1.3087 (1.2858)	Acc@1 91.016 (93.229)	Acc@5 100.000 (99.554)
Epoch: [152][30/196]	Time 0.018 (0.019)	Data 0.002 (0.008)	Loss 1.2654 (1.2826)	Acc@1 94.141 (93.385)	Acc@5 100.000 (99.584)
Epoch: [152][40/196]	Time 0.016 (0.018)	Data 0.002 (0.007)	Loss 1.3078 (1.2797)	Acc@1 92.188 (93.436)	Acc@5 100.000 (99.619)
Epoch: [152][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 1.2541 (1.2767)	Acc@1 95.312 (93.551)	Acc@5 99.609 (99.609)
Epoch: [152][60/196]	Time 0.016 (0.017)	Data 0.003 (0.005)	Loss 1.3080 (1.2784)	Acc@1 93.359 (93.455)	Acc@5 98.438 (99.577)
Epoch: [152][70/196]	Time 0.017 (0.017)	Data 0.002 (0.005)	Loss 1.2664 (1.2778)	Acc@1 94.531 (93.436)	Acc@5 100.000 (99.587)
Epoch: [152][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.3025 (1.2779)	Acc@1 91.016 (93.364)	Acc@5 99.219 (99.576)
Epoch: [152][90/196]	Time 0.017 (0.017)	Data 0.002 (0.004)	Loss 1.3065 (1.2792)	Acc@1 93.359 (93.269)	Acc@5 99.219 (99.562)
Epoch: [152][100/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.2187 (1.2783)	Acc@1 95.703 (93.274)	Acc@5 99.609 (99.551)
Epoch: [152][110/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.2457 (1.2775)	Acc@1 94.141 (93.278)	Acc@5 99.219 (99.546)
Epoch: [152][120/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.2293 (1.2789)	Acc@1 95.312 (93.224)	Acc@5 99.609 (99.542)
Epoch: [152][130/196]	Time 0.016 (0.017)	Data 0.002 (0.003)	Loss 1.2530 (1.2776)	Acc@1 92.578 (93.195)	Acc@5 99.219 (99.553)
Epoch: [152][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.2204 (1.2779)	Acc@1 95.703 (93.171)	Acc@5 100.000 (99.560)
Epoch: [152][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.2073 (1.2773)	Acc@1 96.094 (93.145)	Acc@5 100.000 (99.571)
Epoch: [152][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.2731 (1.2775)	Acc@1 91.797 (93.083)	Acc@5 100.000 (99.558)
Epoch: [152][170/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 1.2403 (1.2773)	Acc@1 93.750 (93.090)	Acc@5 100.000 (99.564)
Epoch: [152][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.3258 (1.2764)	Acc@1 90.234 (93.064)	Acc@5 99.219 (99.575)
Epoch: [152][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 1.2335 (1.2749)	Acc@1 94.531 (93.098)	Acc@5 99.609 (99.577)
num momentum params: 26
[0.010000000000000002, 1.274346927947998, 1.1808611863851548, 93.126, 69.22, tensor(0.8047, device='cuda:0', grad_fn=<DivBackward0>), 3.16841983795166, 0.40561485290527344]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [153 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [153][0/196]	Time 0.070 (0.070)	Data 0.207 (0.207)	Loss 1.2168 (1.2168)	Acc@1 94.922 (94.922)	Acc@5 99.609 (99.609)
Epoch: [153][10/196]	Time 0.019 (0.022)	Data 0.002 (0.020)	Loss 1.2479 (1.2374)	Acc@1 92.969 (93.999)	Acc@5 99.609 (99.680)
Epoch: [153][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.2539 (1.2426)	Acc@1 93.359 (93.397)	Acc@5 98.828 (99.702)
Epoch: [153][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 1.2025 (1.2407)	Acc@1 94.922 (93.649)	Acc@5 100.000 (99.723)
Epoch: [153][40/196]	Time 0.017 (0.018)	Data 0.002 (0.007)	Loss 1.2055 (1.2352)	Acc@1 94.922 (94.026)	Acc@5 100.000 (99.733)
Epoch: [153][50/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 1.2045 (1.2344)	Acc@1 96.484 (94.079)	Acc@5 99.609 (99.740)
Epoch: [153][60/196]	Time 0.016 (0.018)	Data 0.002 (0.005)	Loss 1.2412 (1.2345)	Acc@1 93.359 (94.045)	Acc@5 99.609 (99.712)
Epoch: [153][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.2617 (1.2342)	Acc@1 91.406 (93.959)	Acc@5 100.000 (99.697)
Epoch: [153][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.2781 (1.2334)	Acc@1 92.969 (94.006)	Acc@5 98.828 (99.691)
Epoch: [153][90/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.1868 (1.2305)	Acc@1 94.531 (94.081)	Acc@5 100.000 (99.691)
Epoch: [153][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.2232 (1.2299)	Acc@1 94.922 (94.071)	Acc@5 100.000 (99.706)
Epoch: [153][110/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.1618 (1.2280)	Acc@1 96.094 (94.137)	Acc@5 99.609 (99.694)
Epoch: [153][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.1792 (1.2253)	Acc@1 94.922 (94.228)	Acc@5 100.000 (99.709)
Epoch: [153][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.2376 (1.2243)	Acc@1 94.141 (94.260)	Acc@5 99.609 (99.720)
Epoch: [153][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.2098 (1.2237)	Acc@1 94.922 (94.257)	Acc@5 100.000 (99.720)
Epoch: [153][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.2832 (1.2240)	Acc@1 92.969 (94.257)	Acc@5 100.000 (99.715)
Epoch: [153][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.1720 (1.2238)	Acc@1 95.312 (94.252)	Acc@5 100.000 (99.709)
Epoch: [153][170/196]	Time 0.014 (0.016)	Data 0.002 (0.003)	Loss 1.1761 (1.2236)	Acc@1 95.312 (94.255)	Acc@5 100.000 (99.705)
Epoch: [153][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.1987 (1.2226)	Acc@1 94.922 (94.281)	Acc@5 99.609 (99.715)
Epoch: [153][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.2134 (1.2215)	Acc@1 93.359 (94.284)	Acc@5 99.609 (99.722)
num momentum params: 26
[0.010000000000000002, 1.2209072620391845, 1.1892985278367996, 94.29, 69.03, tensor(0.8278, device='cuda:0', grad_fn=<DivBackward0>), 3.1210036277770996, 0.401357889175415]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [154 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [154][0/196]	Time 0.066 (0.066)	Data 0.193 (0.193)	Loss 1.1624 (1.1624)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [154][10/196]	Time 0.016 (0.023)	Data 0.002 (0.019)	Loss 1.2271 (1.1799)	Acc@1 92.969 (95.241)	Acc@5 100.000 (99.787)
Epoch: [154][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.2294 (1.1743)	Acc@1 94.141 (95.554)	Acc@5 100.000 (99.870)
Epoch: [154][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 1.1863 (1.1759)	Acc@1 95.312 (95.527)	Acc@5 99.609 (99.836)
Epoch: [154][40/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 1.1978 (1.1769)	Acc@1 93.750 (95.408)	Acc@5 99.219 (99.848)
Epoch: [154][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 1.1462 (1.1749)	Acc@1 96.875 (95.535)	Acc@5 100.000 (99.854)
Epoch: [154][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.1417 (1.1727)	Acc@1 98.047 (95.639)	Acc@5 100.000 (99.859)
Epoch: [154][70/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 1.1515 (1.1719)	Acc@1 96.094 (95.676)	Acc@5 99.219 (99.829)
Epoch: [154][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.2011 (1.1742)	Acc@1 92.578 (95.549)	Acc@5 100.000 (99.822)
Epoch: [154][90/196]	Time 0.017 (0.017)	Data 0.001 (0.004)	Loss 1.1316 (1.1748)	Acc@1 96.484 (95.519)	Acc@5 100.000 (99.820)
Epoch: [154][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.2113 (1.1756)	Acc@1 94.922 (95.467)	Acc@5 99.609 (99.814)
Epoch: [154][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 1.1442 (1.1748)	Acc@1 96.484 (95.478)	Acc@5 100.000 (99.824)
Epoch: [154][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.2010 (1.1747)	Acc@1 96.094 (95.464)	Acc@5 99.219 (99.816)
Epoch: [154][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.2189 (1.1743)	Acc@1 92.578 (95.456)	Acc@5 99.609 (99.812)
Epoch: [154][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.1770 (1.1748)	Acc@1 94.922 (95.440)	Acc@5 99.609 (99.809)
Epoch: [154][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.1642 (1.1734)	Acc@1 94.531 (95.457)	Acc@5 99.609 (99.816)
Epoch: [154][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.1311 (1.1738)	Acc@1 96.484 (95.436)	Acc@5 100.000 (99.816)
Epoch: [154][170/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 1.1493 (1.1733)	Acc@1 96.094 (95.408)	Acc@5 100.000 (99.820)
Epoch: [154][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.1100 (1.1732)	Acc@1 96.875 (95.371)	Acc@5 100.000 (99.808)
Epoch: [154][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 1.1557 (1.1728)	Acc@1 96.094 (95.366)	Acc@5 99.609 (99.806)
num momentum params: 26
[0.010000000000000002, 1.1734909395599364, 1.2054899495840072, 95.332, 69.54, tensor(0.8490, device='cuda:0', grad_fn=<DivBackward0>), 3.0895462036132812, 0.40857577323913574]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [155 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [155][0/196]	Time 0.069 (0.069)	Data 0.191 (0.191)	Loss 1.1463 (1.1463)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [155][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 1.1315 (1.1259)	Acc@1 95.312 (96.591)	Acc@5 100.000 (99.929)
Epoch: [155][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 1.1492 (1.1306)	Acc@1 94.141 (96.354)	Acc@5 100.000 (99.944)
Epoch: [155][30/196]	Time 0.015 (0.018)	Data 0.003 (0.008)	Loss 1.1377 (1.1337)	Acc@1 97.656 (96.283)	Acc@5 100.000 (99.912)
Epoch: [155][40/196]	Time 0.017 (0.018)	Data 0.002 (0.007)	Loss 1.1137 (1.1322)	Acc@1 96.094 (96.303)	Acc@5 100.000 (99.924)
Epoch: [155][50/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.1183 (1.1328)	Acc@1 96.484 (96.324)	Acc@5 100.000 (99.908)
Epoch: [155][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.1183 (1.1343)	Acc@1 96.484 (96.273)	Acc@5 100.000 (99.904)
Epoch: [155][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.1460 (1.1340)	Acc@1 94.531 (96.259)	Acc@5 99.219 (99.884)
Epoch: [155][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.1290 (1.1339)	Acc@1 96.094 (96.209)	Acc@5 99.609 (99.879)
Epoch: [155][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.1160 (1.1335)	Acc@1 95.703 (96.205)	Acc@5 100.000 (99.884)
Epoch: [155][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.1324 (1.1329)	Acc@1 96.875 (96.233)	Acc@5 100.000 (99.896)
Epoch: [155][110/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.1620 (1.1335)	Acc@1 94.531 (96.171)	Acc@5 100.000 (99.894)
Epoch: [155][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.1348 (1.1324)	Acc@1 95.703 (96.191)	Acc@5 100.000 (99.900)
Epoch: [155][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.1688 (1.1333)	Acc@1 96.484 (96.147)	Acc@5 98.828 (99.890)
Epoch: [155][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.1484 (1.1337)	Acc@1 94.922 (96.102)	Acc@5 100.000 (99.892)
Epoch: [155][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.0812 (1.1326)	Acc@1 98.047 (96.151)	Acc@5 100.000 (99.889)
Epoch: [155][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.1361 (1.1325)	Acc@1 94.531 (96.159)	Acc@5 100.000 (99.888)
Epoch: [155][170/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 1.1149 (1.1316)	Acc@1 95.703 (96.158)	Acc@5 100.000 (99.893)
Epoch: [155][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.1090 (1.1311)	Acc@1 97.266 (96.139)	Acc@5 99.219 (99.890)
Epoch: [155][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.1077 (1.1302)	Acc@1 96.875 (96.165)	Acc@5 99.609 (99.892)
num momentum params: 26
[0.010000000000000002, 1.130389253387451, 1.208077877163887, 96.148, 69.5, tensor(0.8684, device='cuda:0', grad_fn=<DivBackward0>), 3.0861456394195557, 0.4007391929626465]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [156 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [156][0/196]	Time 0.057 (0.057)	Data 0.194 (0.194)	Loss 1.1270 (1.1270)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [156][10/196]	Time 0.015 (0.022)	Data 0.002 (0.019)	Loss 1.0915 (1.1036)	Acc@1 97.656 (97.230)	Acc@5 100.000 (100.000)
Epoch: [156][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.0679 (1.1089)	Acc@1 97.266 (96.856)	Acc@5 100.000 (99.888)
Epoch: [156][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.1374 (1.1087)	Acc@1 94.922 (96.812)	Acc@5 100.000 (99.912)
Epoch: [156][40/196]	Time 0.015 (0.018)	Data 0.002 (0.007)	Loss 1.1027 (1.1080)	Acc@1 95.703 (96.732)	Acc@5 100.000 (99.914)
Epoch: [156][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.1112 (1.1060)	Acc@1 95.703 (96.668)	Acc@5 100.000 (99.931)
Epoch: [156][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.0790 (1.1052)	Acc@1 97.656 (96.715)	Acc@5 100.000 (99.923)
Epoch: [156][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.0906 (1.1063)	Acc@1 97.266 (96.644)	Acc@5 100.000 (99.906)
Epoch: [156][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.0822 (1.1052)	Acc@1 96.484 (96.624)	Acc@5 100.000 (99.908)
Epoch: [156][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.0666 (1.1037)	Acc@1 97.656 (96.647)	Acc@5 100.000 (99.901)
Epoch: [156][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.0844 (1.1041)	Acc@1 96.875 (96.616)	Acc@5 100.000 (99.911)
Epoch: [156][110/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 1.0775 (1.1044)	Acc@1 97.656 (96.579)	Acc@5 100.000 (99.909)
Epoch: [156][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.1081 (1.1034)	Acc@1 96.094 (96.539)	Acc@5 99.609 (99.913)
Epoch: [156][130/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.1116 (1.1021)	Acc@1 95.312 (96.589)	Acc@5 100.000 (99.919)
Epoch: [156][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.0995 (1.1012)	Acc@1 95.312 (96.598)	Acc@5 99.609 (99.920)
Epoch: [156][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.1014 (1.1018)	Acc@1 96.484 (96.534)	Acc@5 99.609 (99.915)
Epoch: [156][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.1739 (1.1018)	Acc@1 94.531 (96.538)	Acc@5 100.000 (99.918)
Epoch: [156][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.0568 (1.1013)	Acc@1 97.656 (96.548)	Acc@5 100.000 (99.915)
Epoch: [156][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0794 (1.1002)	Acc@1 97.266 (96.573)	Acc@5 100.000 (99.920)
Epoch: [156][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.1286 (1.1000)	Acc@1 94.922 (96.556)	Acc@5 100.000 (99.916)
num momentum params: 26
[0.010000000000000002, 1.0999619944000245, 1.2179757249355316, 96.556, 69.67, tensor(0.8796, device='cuda:0', grad_fn=<DivBackward0>), 3.1357097625732426, 0.4016478061676026]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [157 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [157][0/196]	Time 0.075 (0.075)	Data 0.185 (0.185)	Loss 1.0935 (1.0935)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)
Epoch: [157][10/196]	Time 0.015 (0.022)	Data 0.002 (0.019)	Loss 1.0667 (1.0681)	Acc@1 97.266 (97.585)	Acc@5 99.609 (99.929)
Epoch: [157][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.0593 (1.0687)	Acc@1 97.266 (97.582)	Acc@5 100.000 (99.870)
Epoch: [157][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 1.0611 (1.0706)	Acc@1 97.266 (97.404)	Acc@5 99.609 (99.874)
Epoch: [157][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 1.0891 (1.0711)	Acc@1 97.266 (97.342)	Acc@5 100.000 (99.905)
Epoch: [157][50/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 1.0628 (1.0703)	Acc@1 97.656 (97.350)	Acc@5 100.000 (99.916)
Epoch: [157][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.0855 (1.0693)	Acc@1 96.094 (97.362)	Acc@5 100.000 (99.930)
Epoch: [157][70/196]	Time 0.017 (0.017)	Data 0.002 (0.004)	Loss 1.0931 (1.0689)	Acc@1 96.484 (97.376)	Acc@5 100.000 (99.928)
Epoch: [157][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.0646 (1.0687)	Acc@1 97.656 (97.405)	Acc@5 100.000 (99.932)
Epoch: [157][90/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 1.0336 (1.0675)	Acc@1 98.438 (97.386)	Acc@5 100.000 (99.931)
Epoch: [157][100/196]	Time 0.018 (0.017)	Data 0.003 (0.004)	Loss 1.0710 (1.0669)	Acc@1 97.656 (97.393)	Acc@5 100.000 (99.930)
Epoch: [157][110/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.0481 (1.0663)	Acc@1 98.438 (97.392)	Acc@5 100.000 (99.933)
Epoch: [157][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.0858 (1.0671)	Acc@1 96.094 (97.366)	Acc@5 100.000 (99.932)
Epoch: [157][130/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.0727 (1.0670)	Acc@1 96.875 (97.382)	Acc@5 100.000 (99.931)
Epoch: [157][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0721 (1.0665)	Acc@1 96.484 (97.368)	Acc@5 100.000 (99.928)
Epoch: [157][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0529 (1.0666)	Acc@1 97.656 (97.323)	Acc@5 100.000 (99.933)
Epoch: [157][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.1019 (1.0670)	Acc@1 94.141 (97.292)	Acc@5 100.000 (99.932)
Epoch: [157][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0856 (1.0668)	Acc@1 95.703 (97.275)	Acc@5 100.000 (99.934)
Epoch: [157][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.0553 (1.0672)	Acc@1 97.656 (97.248)	Acc@5 100.000 (99.933)
Epoch: [157][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0965 (1.0673)	Acc@1 96.094 (97.223)	Acc@5 99.609 (99.935)
num momentum params: 26
[0.010000000000000002, 1.067397885131836, 1.2231843715906143, 97.198, 69.31, tensor(0.8928, device='cuda:0', grad_fn=<DivBackward0>), 3.151179075241089, 0.39502477645874023]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [158 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [158][0/196]	Time 0.070 (0.070)	Data 0.193 (0.193)	Loss 1.0631 (1.0631)	Acc@1 98.438 (98.438)	Acc@5 99.609 (99.609)
Epoch: [158][10/196]	Time 0.018 (0.023)	Data 0.002 (0.019)	Loss 1.0363 (1.0522)	Acc@1 98.047 (97.550)	Acc@5 100.000 (99.929)
Epoch: [158][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 1.0417 (1.0511)	Acc@1 97.656 (97.619)	Acc@5 100.000 (99.963)
Epoch: [158][30/196]	Time 0.017 (0.019)	Data 0.002 (0.008)	Loss 1.0397 (1.0504)	Acc@1 97.266 (97.631)	Acc@5 100.000 (99.962)
Epoch: [158][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 1.0544 (1.0486)	Acc@1 98.047 (97.647)	Acc@5 100.000 (99.971)
Epoch: [158][50/196]	Time 0.016 (0.018)	Data 0.002 (0.005)	Loss 1.0530 (1.0460)	Acc@1 97.266 (97.718)	Acc@5 100.000 (99.977)
Epoch: [158][60/196]	Time 0.016 (0.018)	Data 0.002 (0.005)	Loss 1.0459 (1.0459)	Acc@1 98.047 (97.765)	Acc@5 100.000 (99.968)
Epoch: [158][70/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.0146 (1.0459)	Acc@1 99.219 (97.728)	Acc@5 100.000 (99.961)
Epoch: [158][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.0471 (1.0455)	Acc@1 96.094 (97.695)	Acc@5 100.000 (99.966)
Epoch: [158][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 1.0279 (1.0449)	Acc@1 98.047 (97.738)	Acc@5 100.000 (99.966)
Epoch: [158][100/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 1.0409 (1.0446)	Acc@1 96.875 (97.695)	Acc@5 100.000 (99.969)
Epoch: [158][110/196]	Time 0.022 (0.017)	Data 0.002 (0.004)	Loss 1.0325 (1.0441)	Acc@1 98.438 (97.716)	Acc@5 100.000 (99.972)
Epoch: [158][120/196]	Time 0.015 (0.017)	Data 0.003 (0.003)	Loss 1.0168 (1.0446)	Acc@1 98.047 (97.643)	Acc@5 100.000 (99.968)
Epoch: [158][130/196]	Time 0.015 (0.017)	Data 0.002 (0.003)	Loss 1.0828 (1.0442)	Acc@1 95.703 (97.647)	Acc@5 100.000 (99.964)
Epoch: [158][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0498 (1.0437)	Acc@1 98.047 (97.648)	Acc@5 100.000 (99.967)
Epoch: [158][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0460 (1.0432)	Acc@1 97.656 (97.659)	Acc@5 100.000 (99.969)
Epoch: [158][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0219 (1.0436)	Acc@1 98.047 (97.615)	Acc@5 100.000 (99.959)
Epoch: [158][170/196]	Time 0.014 (0.016)	Data 0.002 (0.003)	Loss 1.0861 (1.0438)	Acc@1 94.531 (97.572)	Acc@5 100.000 (99.961)
Epoch: [158][180/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 1.0692 (1.0437)	Acc@1 96.094 (97.570)	Acc@5 99.609 (99.959)
Epoch: [158][190/196]	Time 0.014 (0.016)	Data 0.002 (0.003)	Loss 1.0110 (1.0434)	Acc@1 98.438 (97.552)	Acc@5 100.000 (99.961)
num momentum params: 26
[0.010000000000000002, 1.0433957661819457, 1.2300954085588456, 97.544, 69.74, tensor(0.8999, device='cuda:0', grad_fn=<DivBackward0>), 3.154276132583618, 0.39981937408447266]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [159 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [159][0/196]	Time 0.060 (0.060)	Data 0.186 (0.186)	Loss 1.0339 (1.0339)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [159][10/196]	Time 0.015 (0.022)	Data 0.003 (0.019)	Loss 1.0078 (1.0199)	Acc@1 98.438 (98.153)	Acc@5 100.000 (100.000)
Epoch: [159][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 1.0099 (1.0191)	Acc@1 98.828 (98.233)	Acc@5 100.000 (99.944)
Epoch: [159][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 1.0213 (1.0187)	Acc@1 99.219 (98.274)	Acc@5 100.000 (99.962)
Epoch: [159][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.9954 (1.0194)	Acc@1 98.828 (98.152)	Acc@5 100.000 (99.971)
Epoch: [159][50/196]	Time 0.017 (0.018)	Data 0.002 (0.005)	Loss 1.0334 (1.0194)	Acc@1 97.656 (98.100)	Acc@5 100.000 (99.977)
Epoch: [159][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.9929 (1.0184)	Acc@1 99.609 (98.105)	Acc@5 100.000 (99.974)
Epoch: [159][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.0222 (1.0183)	Acc@1 99.219 (98.118)	Acc@5 100.000 (99.978)
Epoch: [159][80/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.9840 (1.0165)	Acc@1 98.438 (98.100)	Acc@5 100.000 (99.981)
Epoch: [159][90/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.9791 (1.0156)	Acc@1 99.219 (98.111)	Acc@5 100.000 (99.974)
Epoch: [159][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.0157 (1.0159)	Acc@1 98.047 (98.078)	Acc@5 100.000 (99.973)
Epoch: [159][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.9930 (1.0156)	Acc@1 98.828 (98.082)	Acc@5 100.000 (99.972)
Epoch: [159][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.0102 (1.0148)	Acc@1 98.047 (98.115)	Acc@5 100.000 (99.974)
Epoch: [159][130/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 1.0276 (1.0149)	Acc@1 98.438 (98.109)	Acc@5 100.000 (99.973)
Epoch: [159][140/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.0136 (1.0148)	Acc@1 96.875 (98.086)	Acc@5 100.000 (99.972)
Epoch: [159][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.0086 (1.0143)	Acc@1 96.875 (98.083)	Acc@5 100.000 (99.974)
Epoch: [159][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.0139 (1.0145)	Acc@1 97.266 (98.059)	Acc@5 100.000 (99.973)
Epoch: [159][170/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 1.0005 (1.0144)	Acc@1 98.438 (98.061)	Acc@5 100.000 (99.970)
Epoch: [159][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.0091 (1.0144)	Acc@1 98.047 (98.053)	Acc@5 100.000 (99.970)
Epoch: [159][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 1.0030 (1.0149)	Acc@1 98.047 (98.024)	Acc@5 100.000 (99.971)
num momentum params: 26
[0.010000000000000002, 1.0145619414901734, 1.2462074536085128, 98.042, 69.47, tensor(0.9116, device='cuda:0', grad_fn=<DivBackward0>), 3.104295015335083, 0.4012932777404785]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [160 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [160][0/196]	Time 0.065 (0.065)	Data 0.200 (0.200)	Loss 1.0154 (1.0154)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [160][10/196]	Time 0.016 (0.022)	Data 0.002 (0.020)	Loss 0.9896 (0.9980)	Acc@1 99.219 (98.438)	Acc@5 100.000 (100.000)
Epoch: [160][20/196]	Time 0.014 (0.020)	Data 0.002 (0.011)	Loss 0.9841 (0.9973)	Acc@1 98.438 (98.382)	Acc@5 100.000 (99.963)
Epoch: [160][30/196]	Time 0.015 (0.019)	Data 0.002 (0.008)	Loss 1.0110 (0.9950)	Acc@1 97.266 (98.412)	Acc@5 100.000 (99.975)
Epoch: [160][40/196]	Time 0.016 (0.018)	Data 0.002 (0.007)	Loss 1.0044 (0.9950)	Acc@1 97.656 (98.333)	Acc@5 100.000 (99.981)
Epoch: [160][50/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 1.0260 (0.9956)	Acc@1 97.656 (98.269)	Acc@5 100.000 (99.985)
Epoch: [160][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.9839 (0.9961)	Acc@1 98.828 (98.245)	Acc@5 100.000 (99.987)
Epoch: [160][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.9834 (0.9948)	Acc@1 98.828 (98.272)	Acc@5 100.000 (99.983)
Epoch: [160][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9963 (0.9940)	Acc@1 98.047 (98.264)	Acc@5 100.000 (99.981)
Epoch: [160][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9940 (0.9932)	Acc@1 98.828 (98.287)	Acc@5 100.000 (99.983)
Epoch: [160][100/196]	Time 0.017 (0.017)	Data 0.001 (0.004)	Loss 0.9680 (0.9926)	Acc@1 99.219 (98.294)	Acc@5 100.000 (99.985)
Epoch: [160][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.9596 (0.9938)	Acc@1 98.828 (98.255)	Acc@5 100.000 (99.982)
Epoch: [160][120/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 0.9734 (0.9932)	Acc@1 98.828 (98.270)	Acc@5 100.000 (99.984)
Epoch: [160][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.9965 (0.9929)	Acc@1 98.047 (98.268)	Acc@5 100.000 (99.985)
Epoch: [160][140/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 0.9900 (0.9928)	Acc@1 97.656 (98.266)	Acc@5 100.000 (99.983)
Epoch: [160][150/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 1.0291 (0.9928)	Acc@1 96.094 (98.267)	Acc@5 99.609 (99.982)
Epoch: [160][160/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 1.0238 (0.9930)	Acc@1 98.047 (98.248)	Acc@5 100.000 (99.983)
Epoch: [160][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9824 (0.9929)	Acc@1 98.047 (98.232)	Acc@5 100.000 (99.979)
Epoch: [160][180/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 0.9841 (0.9924)	Acc@1 98.047 (98.241)	Acc@5 100.000 (99.978)
Epoch: [160][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9600 (0.9917)	Acc@1 99.609 (98.270)	Acc@5 100.000 (99.980)
num momentum params: 26
[0.010000000000000002, 0.9917834248161316, 1.2428876411914827, 98.278, 69.68, tensor(0.9185, device='cuda:0', grad_fn=<DivBackward0>), 3.1343488693237305, 0.3887450695037842]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [161 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [161][0/196]	Time 0.064 (0.064)	Data 0.192 (0.192)	Loss 0.9591 (0.9591)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [161][10/196]	Time 0.017 (0.022)	Data 0.002 (0.019)	Loss 0.9840 (0.9799)	Acc@1 97.266 (97.940)	Acc@5 100.000 (100.000)
Epoch: [161][20/196]	Time 0.017 (0.019)	Data 0.002 (0.011)	Loss 0.9725 (0.9788)	Acc@1 98.828 (98.103)	Acc@5 100.000 (100.000)
Epoch: [161][30/196]	Time 0.018 (0.019)	Data 0.002 (0.008)	Loss 0.9792 (0.9757)	Acc@1 97.656 (98.261)	Acc@5 100.000 (100.000)
Epoch: [161][40/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 0.9657 (0.9755)	Acc@1 98.828 (98.371)	Acc@5 100.000 (100.000)
Epoch: [161][50/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 0.9726 (0.9747)	Acc@1 98.438 (98.399)	Acc@5 100.000 (100.000)
Epoch: [161][60/196]	Time 0.017 (0.017)	Data 0.002 (0.005)	Loss 0.9894 (0.9742)	Acc@1 96.875 (98.399)	Acc@5 100.000 (100.000)
Epoch: [161][70/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9722 (0.9738)	Acc@1 98.047 (98.426)	Acc@5 100.000 (100.000)
Epoch: [161][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9566 (0.9727)	Acc@1 98.047 (98.462)	Acc@5 100.000 (99.995)
Epoch: [161][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9800 (0.9725)	Acc@1 96.875 (98.472)	Acc@5 100.000 (99.991)
Epoch: [161][100/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 0.9516 (0.9726)	Acc@1 99.609 (98.476)	Acc@5 100.000 (99.992)
Epoch: [161][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.9652 (0.9729)	Acc@1 98.828 (98.455)	Acc@5 100.000 (99.989)
Epoch: [161][120/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9846 (0.9726)	Acc@1 96.875 (98.434)	Acc@5 100.000 (99.990)
Epoch: [161][130/196]	Time 0.013 (0.016)	Data 0.004 (0.003)	Loss 0.9604 (0.9722)	Acc@1 98.438 (98.443)	Acc@5 100.000 (99.988)
Epoch: [161][140/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 0.9515 (0.9717)	Acc@1 98.047 (98.429)	Acc@5 100.000 (99.986)
Epoch: [161][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.9786 (0.9717)	Acc@1 96.094 (98.427)	Acc@5 100.000 (99.987)
Epoch: [161][160/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.9775 (0.9713)	Acc@1 98.047 (98.423)	Acc@5 100.000 (99.983)
Epoch: [161][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.9556 (0.9714)	Acc@1 99.609 (98.422)	Acc@5 100.000 (99.984)
Epoch: [161][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9685 (0.9714)	Acc@1 98.438 (98.425)	Acc@5 100.000 (99.985)
Epoch: [161][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9624 (0.9716)	Acc@1 98.047 (98.409)	Acc@5 100.000 (99.986)
num momentum params: 26
[0.010000000000000002, 0.9714082198524475, 1.2556732565164566, 98.414, 69.84, tensor(0.9234, device='cuda:0', grad_fn=<DivBackward0>), 3.1017262935638428, 0.4031965732574463]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [162 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [162][0/196]	Time 0.067 (0.067)	Data 0.198 (0.198)	Loss 0.9476 (0.9476)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [162][10/196]	Time 0.017 (0.023)	Data 0.002 (0.019)	Loss 0.9501 (0.9478)	Acc@1 98.438 (98.757)	Acc@5 100.000 (100.000)
Epoch: [162][20/196]	Time 0.015 (0.020)	Data 0.002 (0.011)	Loss 0.9418 (0.9509)	Acc@1 99.219 (98.772)	Acc@5 100.000 (100.000)
Epoch: [162][30/196]	Time 0.017 (0.018)	Data 0.002 (0.008)	Loss 0.9516 (0.9510)	Acc@1 97.656 (98.753)	Acc@5 100.000 (100.000)
Epoch: [162][40/196]	Time 0.017 (0.018)	Data 0.002 (0.007)	Loss 0.9374 (0.9509)	Acc@1 98.828 (98.666)	Acc@5 100.000 (100.000)
Epoch: [162][50/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 0.9613 (0.9512)	Acc@1 98.438 (98.660)	Acc@5 100.000 (100.000)
Epoch: [162][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.9442 (0.9510)	Acc@1 99.219 (98.642)	Acc@5 100.000 (99.994)
Epoch: [162][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.9475 (0.9511)	Acc@1 99.219 (98.685)	Acc@5 100.000 (99.994)
Epoch: [162][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9685 (0.9512)	Acc@1 97.656 (98.683)	Acc@5 100.000 (99.995)
Epoch: [162][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9556 (0.9512)	Acc@1 99.219 (98.682)	Acc@5 100.000 (99.996)
Epoch: [162][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.9740 (0.9523)	Acc@1 97.266 (98.623)	Acc@5 100.000 (99.996)
Epoch: [162][110/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 0.9647 (0.9523)	Acc@1 98.828 (98.624)	Acc@5 99.609 (99.993)
Epoch: [162][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.9310 (0.9519)	Acc@1 99.609 (98.634)	Acc@5 100.000 (99.994)
Epoch: [162][130/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.9383 (0.9513)	Acc@1 98.438 (98.658)	Acc@5 100.000 (99.994)
Epoch: [162][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 0.9529 (0.9512)	Acc@1 98.438 (98.645)	Acc@5 100.000 (99.994)
Epoch: [162][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.9518 (0.9510)	Acc@1 98.047 (98.626)	Acc@5 100.000 (99.995)
Epoch: [162][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.9333 (0.9508)	Acc@1 99.219 (98.624)	Acc@5 100.000 (99.995)
Epoch: [162][170/196]	Time 0.012 (0.016)	Data 0.010 (0.003)	Loss 0.9438 (0.9504)	Acc@1 98.438 (98.639)	Acc@5 100.000 (99.995)
Epoch: [162][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9341 (0.9504)	Acc@1 99.219 (98.636)	Acc@5 100.000 (99.991)
Epoch: [162][190/196]	Time 0.013 (0.016)	Data 0.003 (0.003)	Loss 0.9368 (0.9501)	Acc@1 98.828 (98.634)	Acc@5 100.000 (99.992)
num momentum params: 26
[0.010000000000000002, 0.9500983135986328, 1.25643596470356, 98.624, 69.57, tensor(0.9297, device='cuda:0', grad_fn=<DivBackward0>), 3.098383903503418, 0.40166592597961426]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [163 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [163][0/196]	Time 0.068 (0.068)	Data 0.203 (0.203)	Loss 0.9233 (0.9233)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [163][10/196]	Time 0.018 (0.023)	Data 0.002 (0.020)	Loss 0.9445 (0.9334)	Acc@1 98.828 (99.112)	Acc@5 100.000 (100.000)
Epoch: [163][20/196]	Time 0.016 (0.020)	Data 0.003 (0.011)	Loss 0.9228 (0.9303)	Acc@1 99.609 (99.256)	Acc@5 100.000 (100.000)
Epoch: [163][30/196]	Time 0.017 (0.019)	Data 0.002 (0.008)	Loss 0.9410 (0.9314)	Acc@1 97.656 (99.105)	Acc@5 100.000 (100.000)
Epoch: [163][40/196]	Time 0.016 (0.018)	Data 0.002 (0.007)	Loss 0.9405 (0.9316)	Acc@1 98.438 (99.123)	Acc@5 100.000 (100.000)
Epoch: [163][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.9274 (0.9308)	Acc@1 99.609 (99.165)	Acc@5 100.000 (100.000)
Epoch: [163][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.9340 (0.9298)	Acc@1 99.219 (99.161)	Acc@5 100.000 (100.000)
Epoch: [163][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.9191 (0.9297)	Acc@1 98.828 (99.153)	Acc@5 100.000 (100.000)
Epoch: [163][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9270 (0.9286)	Acc@1 99.219 (99.166)	Acc@5 100.000 (100.000)
Epoch: [163][90/196]	Time 0.014 (0.017)	Data 0.002 (0.004)	Loss 0.9353 (0.9286)	Acc@1 98.047 (99.129)	Acc@5 100.000 (100.000)
Epoch: [163][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9063 (0.9282)	Acc@1 99.219 (99.095)	Acc@5 100.000 (100.000)
Epoch: [163][110/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.9076 (0.9283)	Acc@1 100.000 (99.071)	Acc@5 100.000 (100.000)
Epoch: [163][120/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.9104 (0.9283)	Acc@1 100.000 (99.054)	Acc@5 100.000 (100.000)
Epoch: [163][130/196]	Time 0.016 (0.017)	Data 0.002 (0.003)	Loss 0.9281 (0.9281)	Acc@1 98.438 (99.046)	Acc@5 100.000 (100.000)
Epoch: [163][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.9374 (0.9287)	Acc@1 98.828 (98.997)	Acc@5 100.000 (99.997)
Epoch: [163][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9148 (0.9284)	Acc@1 99.609 (98.991)	Acc@5 100.000 (99.997)
Epoch: [163][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9401 (0.9284)	Acc@1 98.438 (98.981)	Acc@5 100.000 (99.998)
Epoch: [163][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.9085 (0.9281)	Acc@1 99.219 (98.981)	Acc@5 100.000 (99.995)
Epoch: [163][180/196]	Time 0.017 (0.016)	Data 0.004 (0.003)	Loss 0.9317 (0.9286)	Acc@1 98.828 (98.951)	Acc@5 100.000 (99.994)
Epoch: [163][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9253 (0.9287)	Acc@1 98.438 (98.922)	Acc@5 100.000 (99.994)
num momentum params: 26
[0.010000000000000002, 0.9285476014900208, 1.2646778452396392, 98.926, 69.68, tensor(0.9367, device='cuda:0', grad_fn=<DivBackward0>), 3.1785016059875493, 0.4025747776031494]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [164 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [164][0/196]	Time 0.067 (0.067)	Data 0.195 (0.195)	Loss 0.9189 (0.9189)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [164][10/196]	Time 0.018 (0.023)	Data 0.002 (0.020)	Loss 0.9060 (0.9225)	Acc@1 100.000 (99.112)	Acc@5 100.000 (100.000)
Epoch: [164][20/196]	Time 0.023 (0.020)	Data 0.002 (0.011)	Loss 0.8929 (0.9153)	Acc@1 100.000 (99.237)	Acc@5 100.000 (100.000)
Epoch: [164][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.9056 (0.9135)	Acc@1 99.609 (99.320)	Acc@5 100.000 (100.000)
Epoch: [164][40/196]	Time 0.017 (0.018)	Data 0.002 (0.007)	Loss 0.9178 (0.9152)	Acc@1 99.609 (99.266)	Acc@5 100.000 (100.000)
Epoch: [164][50/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 0.9127 (0.9141)	Acc@1 98.828 (99.257)	Acc@5 100.000 (100.000)
Epoch: [164][60/196]	Time 0.014 (0.017)	Data 0.002 (0.005)	Loss 0.9176 (0.9136)	Acc@1 98.438 (99.232)	Acc@5 100.000 (100.000)
Epoch: [164][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.9008 (0.9134)	Acc@1 99.609 (99.191)	Acc@5 100.000 (100.000)
Epoch: [164][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9246 (0.9136)	Acc@1 99.609 (99.180)	Acc@5 100.000 (100.000)
Epoch: [164][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9116 (0.9138)	Acc@1 99.609 (99.154)	Acc@5 100.000 (100.000)
Epoch: [164][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.9058 (0.9133)	Acc@1 99.219 (99.157)	Acc@5 100.000 (100.000)
Epoch: [164][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.9157 (0.9134)	Acc@1 97.656 (99.127)	Acc@5 100.000 (100.000)
Epoch: [164][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 0.8921 (0.9136)	Acc@1 100.000 (99.096)	Acc@5 100.000 (99.997)
Epoch: [164][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.9118 (0.9137)	Acc@1 99.219 (99.070)	Acc@5 100.000 (99.994)
Epoch: [164][140/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 0.8981 (0.9133)	Acc@1 98.828 (99.066)	Acc@5 100.000 (99.994)
Epoch: [164][150/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.8895 (0.9127)	Acc@1 100.000 (99.064)	Acc@5 100.000 (99.992)
Epoch: [164][160/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.9292 (0.9130)	Acc@1 97.656 (99.027)	Acc@5 100.000 (99.993)
Epoch: [164][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.8956 (0.9125)	Acc@1 99.219 (99.029)	Acc@5 100.000 (99.993)
Epoch: [164][180/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.9042 (0.9124)	Acc@1 99.609 (99.009)	Acc@5 100.000 (99.994)
Epoch: [164][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9411 (0.9125)	Acc@1 97.266 (98.992)	Acc@5 100.000 (99.994)
num momentum params: 26
[0.010000000000000002, 0.9123756548118591, 1.2654389214515687, 98.992, 69.79, tensor(0.9385, device='cuda:0', grad_fn=<DivBackward0>), 3.098965883255005, 0.40785527229309076]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [165 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [165][0/196]	Time 0.065 (0.065)	Data 0.196 (0.196)	Loss 0.9020 (0.9020)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [165][10/196]	Time 0.018 (0.023)	Data 0.002 (0.019)	Loss 0.9231 (0.9027)	Acc@1 98.828 (98.864)	Acc@5 100.000 (100.000)
Epoch: [165][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.8904 (0.9006)	Acc@1 99.219 (98.940)	Acc@5 100.000 (100.000)
Epoch: [165][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.8784 (0.9009)	Acc@1 100.000 (98.942)	Acc@5 100.000 (100.000)
Epoch: [165][40/196]	Time 0.016 (0.018)	Data 0.002 (0.007)	Loss 0.8824 (0.8994)	Acc@1 100.000 (99.038)	Acc@5 100.000 (100.000)
Epoch: [165][50/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 0.9044 (0.8977)	Acc@1 99.609 (99.119)	Acc@5 100.000 (100.000)
Epoch: [165][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.8881 (0.8972)	Acc@1 99.219 (99.110)	Acc@5 100.000 (100.000)
Epoch: [165][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.8976 (0.8967)	Acc@1 98.828 (99.120)	Acc@5 100.000 (100.000)
Epoch: [165][80/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.8880 (0.8969)	Acc@1 99.609 (99.098)	Acc@5 100.000 (100.000)
Epoch: [165][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.9022 (0.8966)	Acc@1 98.438 (99.094)	Acc@5 100.000 (100.000)
Epoch: [165][100/196]	Time 0.017 (0.017)	Data 0.000 (0.004)	Loss 0.8891 (0.8961)	Acc@1 99.609 (99.130)	Acc@5 100.000 (100.000)
Epoch: [165][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8902 (0.8961)	Acc@1 99.609 (99.106)	Acc@5 100.000 (100.000)
Epoch: [165][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8804 (0.8958)	Acc@1 99.609 (99.103)	Acc@5 100.000 (99.997)
Epoch: [165][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8881 (0.8957)	Acc@1 99.219 (99.096)	Acc@5 100.000 (99.997)
Epoch: [165][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8737 (0.8953)	Acc@1 99.609 (99.097)	Acc@5 100.000 (99.997)
Epoch: [165][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8833 (0.8955)	Acc@1 99.609 (99.097)	Acc@5 100.000 (99.995)
Epoch: [165][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8913 (0.8954)	Acc@1 98.828 (99.085)	Acc@5 100.000 (99.995)
Epoch: [165][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.9062 (0.8956)	Acc@1 97.266 (99.052)	Acc@5 100.000 (99.995)
Epoch: [165][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8871 (0.8950)	Acc@1 99.219 (99.055)	Acc@5 100.000 (99.996)
Epoch: [165][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 0.9066 (0.8947)	Acc@1 98.828 (99.071)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.8945915884780884, 1.270807722210884, 99.066, 69.66, tensor(0.9422, device='cuda:0', grad_fn=<DivBackward0>), 3.1078007221221924, 0.40145659446716314]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [166 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [166][0/196]	Time 0.094 (0.094)	Data 0.189 (0.189)	Loss 0.8815 (0.8815)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [166][10/196]	Time 0.017 (0.025)	Data 0.001 (0.019)	Loss 0.8856 (0.8822)	Acc@1 98.438 (99.325)	Acc@5 100.000 (100.000)
Epoch: [166][20/196]	Time 0.018 (0.021)	Data 0.002 (0.011)	Loss 0.9043 (0.8838)	Acc@1 98.828 (99.200)	Acc@5 100.000 (99.981)
Epoch: [166][30/196]	Time 0.017 (0.019)	Data 0.002 (0.008)	Loss 0.8828 (0.8838)	Acc@1 98.828 (99.181)	Acc@5 100.000 (99.987)
Epoch: [166][40/196]	Time 0.017 (0.019)	Data 0.002 (0.006)	Loss 0.8690 (0.8824)	Acc@1 99.609 (99.209)	Acc@5 100.000 (99.990)
Epoch: [166][50/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 0.8786 (0.8826)	Acc@1 99.609 (99.196)	Acc@5 100.000 (99.992)
Epoch: [166][60/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 0.8676 (0.8812)	Acc@1 99.609 (99.212)	Acc@5 100.000 (99.994)
Epoch: [166][70/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8701 (0.8806)	Acc@1 100.000 (99.230)	Acc@5 100.000 (99.994)
Epoch: [166][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8856 (0.8800)	Acc@1 98.828 (99.243)	Acc@5 100.000 (99.995)
Epoch: [166][90/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.8660 (0.8792)	Acc@1 99.609 (99.270)	Acc@5 100.000 (99.996)
Epoch: [166][100/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.8794 (0.8790)	Acc@1 98.828 (99.277)	Acc@5 100.000 (99.996)
Epoch: [166][110/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8769 (0.8788)	Acc@1 99.219 (99.236)	Acc@5 100.000 (99.996)
Epoch: [166][120/196]	Time 0.014 (0.017)	Data 0.003 (0.004)	Loss 0.8613 (0.8788)	Acc@1 100.000 (99.228)	Acc@5 100.000 (99.997)
Epoch: [166][130/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 0.8798 (0.8788)	Acc@1 98.828 (99.210)	Acc@5 100.000 (99.997)
Epoch: [166][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8633 (0.8791)	Acc@1 100.000 (99.188)	Acc@5 100.000 (99.997)
Epoch: [166][150/196]	Time 0.012 (0.016)	Data 0.004 (0.003)	Loss 0.8717 (0.8787)	Acc@1 99.609 (99.195)	Acc@5 100.000 (99.997)
Epoch: [166][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8611 (0.8785)	Acc@1 99.609 (99.199)	Acc@5 100.000 (99.998)
Epoch: [166][170/196]	Time 0.015 (0.016)	Data 0.006 (0.003)	Loss 0.8722 (0.8784)	Acc@1 98.828 (99.189)	Acc@5 100.000 (99.998)
Epoch: [166][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8583 (0.8780)	Acc@1 99.609 (99.186)	Acc@5 100.000 (99.998)
Epoch: [166][190/196]	Time 0.015 (0.016)	Data 0.004 (0.003)	Loss 0.8924 (0.8779)	Acc@1 98.047 (99.188)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8779670482826233, 1.2721056020259858, 99.176, 69.61, tensor(0.9449, device='cuda:0', grad_fn=<DivBackward0>), 3.144712209701538, 0.4107506275177002]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [167 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [167][0/196]	Time 0.072 (0.072)	Data 0.192 (0.192)	Loss 0.8623 (0.8623)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [167][10/196]	Time 0.017 (0.023)	Data 0.002 (0.019)	Loss 0.8850 (0.8654)	Acc@1 98.047 (99.183)	Acc@5 100.000 (100.000)
Epoch: [167][20/196]	Time 0.015 (0.020)	Data 0.002 (0.011)	Loss 0.8661 (0.8643)	Acc@1 99.609 (99.219)	Acc@5 100.000 (100.000)
Epoch: [167][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.8549 (0.8635)	Acc@1 99.609 (99.332)	Acc@5 100.000 (100.000)
Epoch: [167][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.8571 (0.8640)	Acc@1 100.000 (99.285)	Acc@5 100.000 (99.990)
Epoch: [167][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.8757 (0.8636)	Acc@1 99.219 (99.303)	Acc@5 100.000 (99.992)
Epoch: [167][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.8708 (0.8630)	Acc@1 99.219 (99.315)	Acc@5 100.000 (99.994)
Epoch: [167][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.8622 (0.8619)	Acc@1 99.219 (99.334)	Acc@5 100.000 (99.994)
Epoch: [167][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8507 (0.8615)	Acc@1 100.000 (99.344)	Acc@5 100.000 (99.995)
Epoch: [167][90/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.8555 (0.8619)	Acc@1 100.000 (99.326)	Acc@5 100.000 (99.996)
Epoch: [167][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8537 (0.8621)	Acc@1 100.000 (99.304)	Acc@5 100.000 (99.996)
Epoch: [167][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8561 (0.8621)	Acc@1 99.609 (99.289)	Acc@5 100.000 (99.996)
Epoch: [167][120/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.8436 (0.8619)	Acc@1 100.000 (99.283)	Acc@5 100.000 (99.997)
Epoch: [167][130/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 0.8534 (0.8615)	Acc@1 100.000 (99.287)	Acc@5 100.000 (99.997)
Epoch: [167][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8812 (0.8613)	Acc@1 99.219 (99.285)	Acc@5 100.000 (99.997)
Epoch: [167][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8528 (0.8607)	Acc@1 99.219 (99.299)	Acc@5 100.000 (99.997)
Epoch: [167][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8473 (0.8607)	Acc@1 99.609 (99.284)	Acc@5 100.000 (99.998)
Epoch: [167][170/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 0.8457 (0.8609)	Acc@1 99.609 (99.260)	Acc@5 100.000 (99.998)
Epoch: [167][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8555 (0.8608)	Acc@1 99.609 (99.258)	Acc@5 100.000 (99.998)
Epoch: [167][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8624 (0.8607)	Acc@1 99.219 (99.245)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8605539736175537, 1.272286149263382, 99.248, 69.7, tensor(0.9487, device='cuda:0', grad_fn=<DivBackward0>), 3.1335721015930176, 0.407916784286499]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [168 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [168][0/196]	Time 0.073 (0.073)	Data 0.193 (0.193)	Loss 0.8412 (0.8412)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [168][10/196]	Time 0.016 (0.023)	Data 0.002 (0.019)	Loss 0.8531 (0.8449)	Acc@1 98.828 (99.574)	Acc@5 100.000 (100.000)
Epoch: [168][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.8375 (0.8447)	Acc@1 99.609 (99.554)	Acc@5 100.000 (100.000)
Epoch: [168][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.8392 (0.8456)	Acc@1 99.609 (99.559)	Acc@5 100.000 (100.000)
Epoch: [168][40/196]	Time 0.018 (0.018)	Data 0.002 (0.007)	Loss 0.8592 (0.8470)	Acc@1 99.609 (99.543)	Acc@5 100.000 (100.000)
Epoch: [168][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.8504 (0.8472)	Acc@1 99.219 (99.510)	Acc@5 100.000 (100.000)
Epoch: [168][60/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 0.8373 (0.8469)	Acc@1 99.609 (99.494)	Acc@5 100.000 (100.000)
Epoch: [168][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.8410 (0.8471)	Acc@1 99.609 (99.450)	Acc@5 100.000 (100.000)
Epoch: [168][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8618 (0.8466)	Acc@1 99.219 (99.455)	Acc@5 100.000 (100.000)
Epoch: [168][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8429 (0.8467)	Acc@1 99.219 (99.416)	Acc@5 100.000 (100.000)
Epoch: [168][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 0.8422 (0.8463)	Acc@1 99.609 (99.420)	Acc@5 100.000 (100.000)
Epoch: [168][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.8739 (0.8462)	Acc@1 97.266 (99.398)	Acc@5 100.000 (100.000)
Epoch: [168][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8368 (0.8458)	Acc@1 100.000 (99.406)	Acc@5 100.000 (100.000)
Epoch: [168][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8504 (0.8455)	Acc@1 98.438 (99.410)	Acc@5 100.000 (100.000)
Epoch: [168][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8291 (0.8454)	Acc@1 100.000 (99.377)	Acc@5 100.000 (99.997)
Epoch: [168][150/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.8258 (0.8452)	Acc@1 99.609 (99.377)	Acc@5 100.000 (99.997)
Epoch: [168][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8593 (0.8450)	Acc@1 99.219 (99.372)	Acc@5 100.000 (99.998)
Epoch: [168][170/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.8330 (0.8450)	Acc@1 100.000 (99.363)	Acc@5 100.000 (99.998)
Epoch: [168][180/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 0.8272 (0.8446)	Acc@1 100.000 (99.370)	Acc@5 100.000 (99.998)
Epoch: [168][190/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 0.8345 (0.8444)	Acc@1 99.609 (99.366)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8445959424209595, 1.2698853564262391, 99.352, 69.79, tensor(0.9512, device='cuda:0', grad_fn=<DivBackward0>), 3.1071364879608154, 0.4012618064880371]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [169 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [169][0/196]	Time 0.067 (0.067)	Data 0.200 (0.200)	Loss 0.8179 (0.8179)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [169][10/196]	Time 0.019 (0.023)	Data 0.002 (0.020)	Loss 0.8330 (0.8306)	Acc@1 99.609 (99.432)	Acc@5 100.000 (100.000)
Epoch: [169][20/196]	Time 0.015 (0.020)	Data 0.002 (0.011)	Loss 0.8318 (0.8323)	Acc@1 99.219 (99.349)	Acc@5 100.000 (100.000)
Epoch: [169][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.8469 (0.8337)	Acc@1 99.219 (99.307)	Acc@5 100.000 (100.000)
Epoch: [169][40/196]	Time 0.017 (0.018)	Data 0.002 (0.007)	Loss 0.8204 (0.8325)	Acc@1 100.000 (99.362)	Acc@5 100.000 (100.000)
Epoch: [169][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.8494 (0.8331)	Acc@1 99.219 (99.349)	Acc@5 100.000 (100.000)
Epoch: [169][60/196]	Time 0.014 (0.017)	Data 0.002 (0.005)	Loss 0.8282 (0.8329)	Acc@1 100.000 (99.353)	Acc@5 100.000 (100.000)
Epoch: [169][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.8377 (0.8322)	Acc@1 99.609 (99.389)	Acc@5 100.000 (100.000)
Epoch: [169][80/196]	Time 0.014 (0.017)	Data 0.003 (0.004)	Loss 0.8192 (0.8311)	Acc@1 100.000 (99.421)	Acc@5 100.000 (100.000)
Epoch: [169][90/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.8188 (0.8313)	Acc@1 100.000 (99.395)	Acc@5 100.000 (100.000)
Epoch: [169][100/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8400 (0.8312)	Acc@1 99.219 (99.401)	Acc@5 100.000 (100.000)
Epoch: [169][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.8470 (0.8316)	Acc@1 99.219 (99.377)	Acc@5 100.000 (100.000)
Epoch: [169][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 0.8345 (0.8311)	Acc@1 99.219 (99.383)	Acc@5 100.000 (100.000)
Epoch: [169][130/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 0.8179 (0.8306)	Acc@1 99.609 (99.392)	Acc@5 100.000 (100.000)
Epoch: [169][140/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 0.8241 (0.8307)	Acc@1 99.609 (99.371)	Acc@5 100.000 (100.000)
Epoch: [169][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.8145 (0.8307)	Acc@1 99.609 (99.371)	Acc@5 100.000 (100.000)
Epoch: [169][160/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 0.8291 (0.8302)	Acc@1 99.219 (99.376)	Acc@5 100.000 (100.000)
Epoch: [169][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8242 (0.8301)	Acc@1 98.828 (99.372)	Acc@5 100.000 (100.000)
Epoch: [169][180/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 0.8204 (0.8297)	Acc@1 100.000 (99.374)	Acc@5 100.000 (100.000)
Epoch: [169][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8227 (0.8295)	Acc@1 99.609 (99.376)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.8293017184638977, 1.2765099412202836, 99.382, 69.68, tensor(0.9532, device='cuda:0', grad_fn=<DivBackward0>), 3.0734968185424805, 0.39756441116333013]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [170 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [170][0/196]	Time 0.064 (0.064)	Data 0.192 (0.192)	Loss 0.8078 (0.8078)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [170][10/196]	Time 0.018 (0.023)	Data 0.002 (0.019)	Loss 0.8119 (0.8178)	Acc@1 100.000 (99.645)	Acc@5 100.000 (100.000)
Epoch: [170][20/196]	Time 0.016 (0.019)	Data 0.002 (0.011)	Loss 0.8331 (0.8189)	Acc@1 98.438 (99.535)	Acc@5 100.000 (100.000)
Epoch: [170][30/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 0.8252 (0.8183)	Acc@1 99.609 (99.534)	Acc@5 100.000 (100.000)
Epoch: [170][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.8205 (0.8186)	Acc@1 99.219 (99.495)	Acc@5 100.000 (100.000)
Epoch: [170][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.8164 (0.8186)	Acc@1 100.000 (99.487)	Acc@5 100.000 (100.000)
Epoch: [170][60/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.8160 (0.8178)	Acc@1 99.609 (99.501)	Acc@5 100.000 (100.000)
Epoch: [170][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.8095 (0.8177)	Acc@1 100.000 (99.510)	Acc@5 100.000 (100.000)
Epoch: [170][80/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.8118 (0.8173)	Acc@1 99.609 (99.498)	Acc@5 100.000 (100.000)
Epoch: [170][90/196]	Time 0.016 (0.017)	Data 0.003 (0.004)	Loss 0.8029 (0.8170)	Acc@1 99.609 (99.506)	Acc@5 100.000 (100.000)
Epoch: [170][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8291 (0.8171)	Acc@1 98.828 (99.486)	Acc@5 100.000 (99.996)
Epoch: [170][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.8016 (0.8166)	Acc@1 100.000 (99.497)	Acc@5 100.000 (99.996)
Epoch: [170][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8207 (0.8167)	Acc@1 98.438 (99.467)	Acc@5 100.000 (99.997)
Epoch: [170][130/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8126 (0.8167)	Acc@1 99.609 (99.445)	Acc@5 100.000 (99.997)
Epoch: [170][140/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.8025 (0.8165)	Acc@1 100.000 (99.432)	Acc@5 100.000 (99.997)
Epoch: [170][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.8225 (0.8163)	Acc@1 98.828 (99.433)	Acc@5 100.000 (99.997)
Epoch: [170][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.8138 (0.8160)	Acc@1 99.609 (99.435)	Acc@5 100.000 (99.998)
Epoch: [170][170/196]	Time 0.014 (0.016)	Data 0.002 (0.003)	Loss 0.8144 (0.8156)	Acc@1 99.219 (99.424)	Acc@5 100.000 (99.998)
Epoch: [170][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8030 (0.8152)	Acc@1 99.609 (99.426)	Acc@5 100.000 (99.998)
Epoch: [170][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.8185 (0.8152)	Acc@1 98.438 (99.415)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.8150415275001526, 1.2735934793949126, 99.418, 69.43, tensor(0.9541, device='cuda:0', grad_fn=<DivBackward0>), 3.1251180171966553, 0.3951592445373535]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [171 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [171][0/196]	Time 0.067 (0.067)	Data 0.206 (0.206)	Loss 0.7913 (0.7913)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [171][10/196]	Time 0.015 (0.023)	Data 0.002 (0.020)	Loss 0.7925 (0.8011)	Acc@1 100.000 (99.680)	Acc@5 100.000 (100.000)
Epoch: [171][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.7971 (0.8021)	Acc@1 99.609 (99.572)	Acc@5 100.000 (100.000)
Epoch: [171][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.8071 (0.8025)	Acc@1 99.219 (99.534)	Acc@5 100.000 (100.000)
Epoch: [171][40/196]	Time 0.015 (0.018)	Data 0.002 (0.007)	Loss 0.8338 (0.8031)	Acc@1 98.438 (99.505)	Acc@5 99.609 (99.990)
Epoch: [171][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.7881 (0.8017)	Acc@1 100.000 (99.556)	Acc@5 100.000 (99.992)
Epoch: [171][60/196]	Time 0.014 (0.017)	Data 0.002 (0.005)	Loss 0.7966 (0.8018)	Acc@1 100.000 (99.545)	Acc@5 100.000 (99.994)
Epoch: [171][70/196]	Time 0.014 (0.017)	Data 0.003 (0.005)	Loss 0.7943 (0.8013)	Acc@1 99.609 (99.565)	Acc@5 100.000 (99.994)
Epoch: [171][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7970 (0.8006)	Acc@1 99.609 (99.585)	Acc@5 100.000 (99.995)
Epoch: [171][90/196]	Time 0.013 (0.017)	Data 0.005 (0.004)	Loss 0.8150 (0.8008)	Acc@1 99.219 (99.579)	Acc@5 100.000 (99.996)
Epoch: [171][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.8040 (0.8008)	Acc@1 99.219 (99.571)	Acc@5 100.000 (99.996)
Epoch: [171][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.7892 (0.8005)	Acc@1 100.000 (99.578)	Acc@5 100.000 (99.996)
Epoch: [171][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.7922 (0.8009)	Acc@1 100.000 (99.554)	Acc@5 100.000 (99.997)
Epoch: [171][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.7923 (0.8007)	Acc@1 99.609 (99.547)	Acc@5 100.000 (99.997)
Epoch: [171][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.8112 (0.8009)	Acc@1 98.828 (99.515)	Acc@5 99.609 (99.994)
Epoch: [171][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.7935 (0.8003)	Acc@1 99.609 (99.532)	Acc@5 100.000 (99.995)
Epoch: [171][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.7923 (0.7998)	Acc@1 100.000 (99.537)	Acc@5 100.000 (99.995)
Epoch: [171][170/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.7966 (0.7998)	Acc@1 99.609 (99.520)	Acc@5 100.000 (99.995)
Epoch: [171][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7929 (0.7995)	Acc@1 100.000 (99.527)	Acc@5 100.000 (99.996)
Epoch: [171][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.7908 (0.7994)	Acc@1 99.609 (99.519)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.7993396370697021, 1.2895361763238906, 99.518, 69.56, tensor(0.9570, device='cuda:0', grad_fn=<DivBackward0>), 3.1240274906158447, 0.39769744873046875]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [172 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [172][0/196]	Time 0.071 (0.071)	Data 0.201 (0.201)	Loss 0.7789 (0.7789)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [172][10/196]	Time 0.016 (0.023)	Data 0.002 (0.020)	Loss 0.7914 (0.7861)	Acc@1 98.828 (99.645)	Acc@5 100.000 (100.000)
Epoch: [172][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.7760 (0.7869)	Acc@1 100.000 (99.684)	Acc@5 100.000 (100.000)
Epoch: [172][30/196]	Time 0.017 (0.019)	Data 0.001 (0.008)	Loss 0.7871 (0.7868)	Acc@1 99.609 (99.635)	Acc@5 100.000 (100.000)
Epoch: [172][40/196]	Time 0.016 (0.018)	Data 0.002 (0.007)	Loss 0.7818 (0.7873)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [172][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.7850 (0.7867)	Acc@1 99.609 (99.625)	Acc@5 100.000 (100.000)
Epoch: [172][60/196]	Time 0.012 (0.017)	Data 0.008 (0.005)	Loss 0.7854 (0.7863)	Acc@1 99.609 (99.635)	Acc@5 100.000 (100.000)
Epoch: [172][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.7942 (0.7872)	Acc@1 99.219 (99.565)	Acc@5 100.000 (100.000)
Epoch: [172][80/196]	Time 0.017 (0.017)	Data 0.003 (0.004)	Loss 0.7866 (0.7868)	Acc@1 99.219 (99.576)	Acc@5 100.000 (100.000)
Epoch: [172][90/196]	Time 0.017 (0.017)	Data 0.002 (0.004)	Loss 0.7936 (0.7865)	Acc@1 98.828 (99.575)	Acc@5 100.000 (100.000)
Epoch: [172][100/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7829 (0.7863)	Acc@1 99.609 (99.575)	Acc@5 100.000 (100.000)
Epoch: [172][110/196]	Time 0.017 (0.017)	Data 0.002 (0.004)	Loss 0.7783 (0.7860)	Acc@1 100.000 (99.588)	Acc@5 100.000 (100.000)
Epoch: [172][120/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.7858 (0.7858)	Acc@1 99.219 (99.590)	Acc@5 100.000 (100.000)
Epoch: [172][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.7833 (0.7858)	Acc@1 99.609 (99.586)	Acc@5 100.000 (100.000)
Epoch: [172][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.7858 (0.7855)	Acc@1 99.219 (99.582)	Acc@5 100.000 (100.000)
Epoch: [172][150/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 0.7743 (0.7853)	Acc@1 100.000 (99.589)	Acc@5 100.000 (100.000)
Epoch: [172][160/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.7710 (0.7850)	Acc@1 99.609 (99.585)	Acc@5 100.000 (100.000)
Epoch: [172][170/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 0.7782 (0.7848)	Acc@1 99.219 (99.582)	Acc@5 100.000 (100.000)
Epoch: [172][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7652 (0.7844)	Acc@1 100.000 (99.592)	Acc@5 100.000 (100.000)
Epoch: [172][190/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 0.8083 (0.7843)	Acc@1 98.828 (99.581)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7841996121788025, 1.280383014678955, 99.582, 69.61, tensor(0.9594, device='cuda:0', grad_fn=<DivBackward0>), 3.15325665473938, 0.40200138092041016]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [173 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [173][0/196]	Time 0.074 (0.074)	Data 0.199 (0.199)	Loss 0.7728 (0.7728)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [173][10/196]	Time 0.017 (0.024)	Data 0.002 (0.020)	Loss 0.7619 (0.7726)	Acc@1 100.000 (99.716)	Acc@5 100.000 (100.000)
Epoch: [173][20/196]	Time 0.017 (0.021)	Data 0.002 (0.011)	Loss 0.7735 (0.7746)	Acc@1 100.000 (99.684)	Acc@5 100.000 (100.000)
Epoch: [173][30/196]	Time 0.016 (0.020)	Data 0.002 (0.008)	Loss 0.7747 (0.7744)	Acc@1 99.609 (99.672)	Acc@5 100.000 (100.000)
Epoch: [173][40/196]	Time 0.015 (0.019)	Data 0.002 (0.007)	Loss 0.7868 (0.7739)	Acc@1 98.828 (99.628)	Acc@5 100.000 (100.000)
Epoch: [173][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.7757 (0.7736)	Acc@1 99.609 (99.625)	Acc@5 100.000 (100.000)
Epoch: [173][60/196]	Time 0.016 (0.018)	Data 0.002 (0.005)	Loss 0.7784 (0.7738)	Acc@1 100.000 (99.616)	Acc@5 100.000 (100.000)
Epoch: [173][70/196]	Time 0.015 (0.018)	Data 0.003 (0.005)	Loss 0.7604 (0.7737)	Acc@1 100.000 (99.615)	Acc@5 100.000 (100.000)
Epoch: [173][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7696 (0.7738)	Acc@1 99.219 (99.590)	Acc@5 100.000 (100.000)
Epoch: [173][90/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.7744 (0.7734)	Acc@1 99.609 (99.601)	Acc@5 100.000 (100.000)
Epoch: [173][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7706 (0.7731)	Acc@1 99.609 (99.606)	Acc@5 100.000 (100.000)
Epoch: [173][110/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7909 (0.7729)	Acc@1 98.047 (99.602)	Acc@5 100.000 (100.000)
Epoch: [173][120/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7692 (0.7725)	Acc@1 100.000 (99.609)	Acc@5 100.000 (100.000)
Epoch: [173][130/196]	Time 0.014 (0.017)	Data 0.003 (0.004)	Loss 0.7639 (0.7721)	Acc@1 100.000 (99.606)	Acc@5 100.000 (100.000)
Epoch: [173][140/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.7684 (0.7721)	Acc@1 99.609 (99.590)	Acc@5 100.000 (100.000)
Epoch: [173][150/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 0.7720 (0.7720)	Acc@1 99.219 (99.591)	Acc@5 100.000 (100.000)
Epoch: [173][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7669 (0.7715)	Acc@1 100.000 (99.605)	Acc@5 100.000 (100.000)
Epoch: [173][170/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 0.7612 (0.7712)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [173][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7604 (0.7712)	Acc@1 100.000 (99.596)	Acc@5 100.000 (100.000)
Epoch: [173][190/196]	Time 0.019 (0.016)	Data 0.000 (0.003)	Loss 0.7520 (0.7707)	Acc@1 100.000 (99.605)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7708956862449646, 1.2799605768918991, 99.596, 69.38, tensor(0.9598, device='cuda:0', grad_fn=<DivBackward0>), 3.1553211212158203, 0.4018716812133789]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [174 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [174][0/196]	Time 0.075 (0.075)	Data 0.195 (0.195)	Loss 0.7569 (0.7569)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [174][10/196]	Time 0.018 (0.024)	Data 0.002 (0.019)	Loss 0.7734 (0.7627)	Acc@1 98.828 (99.574)	Acc@5 100.000 (100.000)
Epoch: [174][20/196]	Time 0.017 (0.020)	Data 0.002 (0.011)	Loss 0.7572 (0.7617)	Acc@1 100.000 (99.647)	Acc@5 100.000 (100.000)
Epoch: [174][30/196]	Time 0.017 (0.019)	Data 0.002 (0.008)	Loss 0.7559 (0.7628)	Acc@1 100.000 (99.622)	Acc@5 100.000 (100.000)
Epoch: [174][40/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 0.7523 (0.7620)	Acc@1 100.000 (99.657)	Acc@5 100.000 (100.000)
Epoch: [174][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.7709 (0.7625)	Acc@1 99.219 (99.632)	Acc@5 100.000 (100.000)
Epoch: [174][60/196]	Time 0.018 (0.018)	Data 0.002 (0.005)	Loss 0.7585 (0.7616)	Acc@1 99.609 (99.654)	Acc@5 100.000 (100.000)
Epoch: [174][70/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7532 (0.7617)	Acc@1 100.000 (99.626)	Acc@5 100.000 (100.000)
Epoch: [174][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7596 (0.7617)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [174][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7539 (0.7608)	Acc@1 99.609 (99.618)	Acc@5 100.000 (100.000)
Epoch: [174][100/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.7603 (0.7610)	Acc@1 99.219 (99.590)	Acc@5 100.000 (100.000)
Epoch: [174][110/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7571 (0.7605)	Acc@1 99.609 (99.585)	Acc@5 100.000 (100.000)
Epoch: [174][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 0.7559 (0.7600)	Acc@1 99.609 (99.590)	Acc@5 100.000 (100.000)
Epoch: [174][130/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7503 (0.7597)	Acc@1 100.000 (99.606)	Acc@5 100.000 (100.000)
Epoch: [174][140/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.7472 (0.7593)	Acc@1 100.000 (99.620)	Acc@5 100.000 (100.000)
Epoch: [174][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7489 (0.7591)	Acc@1 100.000 (99.617)	Acc@5 100.000 (100.000)
Epoch: [174][160/196]	Time 0.014 (0.016)	Data 0.002 (0.003)	Loss 0.7441 (0.7587)	Acc@1 100.000 (99.624)	Acc@5 100.000 (100.000)
Epoch: [174][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7595 (0.7583)	Acc@1 98.828 (99.619)	Acc@5 100.000 (100.000)
Epoch: [174][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7463 (0.7583)	Acc@1 100.000 (99.609)	Acc@5 100.000 (100.000)
Epoch: [174][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7525 (0.7579)	Acc@1 99.609 (99.616)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7581790784454345, 1.2753795099258423, 99.6, 69.68, tensor(0.9597, device='cuda:0', grad_fn=<DivBackward0>), 3.133077621459961, 0.3998739719390869]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [175 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [175][0/196]	Time 0.071 (0.071)	Data 0.192 (0.192)	Loss 0.7503 (0.7503)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [175][10/196]	Time 0.019 (0.023)	Data 0.002 (0.019)	Loss 0.7462 (0.7475)	Acc@1 100.000 (99.645)	Acc@5 100.000 (100.000)
Epoch: [175][20/196]	Time 0.018 (0.020)	Data 0.002 (0.011)	Loss 0.7505 (0.7480)	Acc@1 100.000 (99.647)	Acc@5 100.000 (100.000)
Epoch: [175][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.7399 (0.7487)	Acc@1 100.000 (99.622)	Acc@5 100.000 (100.000)
Epoch: [175][40/196]	Time 0.015 (0.018)	Data 0.002 (0.006)	Loss 0.7406 (0.7482)	Acc@1 99.609 (99.638)	Acc@5 100.000 (100.000)
Epoch: [175][50/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 0.7415 (0.7483)	Acc@1 100.000 (99.602)	Acc@5 100.000 (100.000)
Epoch: [175][60/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 0.7352 (0.7480)	Acc@1 100.000 (99.584)	Acc@5 100.000 (100.000)
Epoch: [175][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.7704 (0.7485)	Acc@1 98.828 (99.560)	Acc@5 100.000 (100.000)
Epoch: [175][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7444 (0.7484)	Acc@1 100.000 (99.571)	Acc@5 100.000 (100.000)
Epoch: [175][90/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.7701 (0.7487)	Acc@1 98.828 (99.558)	Acc@5 100.000 (100.000)
Epoch: [175][100/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7487 (0.7487)	Acc@1 99.609 (99.544)	Acc@5 99.609 (99.996)
Epoch: [175][110/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7432 (0.7485)	Acc@1 99.609 (99.546)	Acc@5 100.000 (99.996)
Epoch: [175][120/196]	Time 0.015 (0.017)	Data 0.002 (0.003)	Loss 0.7401 (0.7483)	Acc@1 100.000 (99.548)	Acc@5 100.000 (99.997)
Epoch: [175][130/196]	Time 0.015 (0.017)	Data 0.002 (0.003)	Loss 0.7351 (0.7479)	Acc@1 100.000 (99.559)	Acc@5 100.000 (99.997)
Epoch: [175][140/196]	Time 0.016 (0.017)	Data 0.002 (0.003)	Loss 0.7733 (0.7478)	Acc@1 99.609 (99.571)	Acc@5 100.000 (99.997)
Epoch: [175][150/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.7402 (0.7477)	Acc@1 100.000 (99.571)	Acc@5 100.000 (99.997)
Epoch: [175][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7635 (0.7474)	Acc@1 98.828 (99.573)	Acc@5 100.000 (99.998)
Epoch: [175][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7348 (0.7472)	Acc@1 100.000 (99.564)	Acc@5 100.000 (99.998)
Epoch: [175][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.7353 (0.7470)	Acc@1 100.000 (99.555)	Acc@5 100.000 (99.998)
Epoch: [175][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.7458 (0.7468)	Acc@1 99.219 (99.552)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.7467805540084839, 1.284384434223175, 99.554, 69.51, tensor(0.9580, device='cuda:0', grad_fn=<DivBackward0>), 3.1698656082153325, 0.40428924560546875]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [176 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [176][0/196]	Time 0.078 (0.078)	Data 0.197 (0.197)	Loss 0.7270 (0.7270)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [176][10/196]	Time 0.016 (0.023)	Data 0.002 (0.019)	Loss 0.7403 (0.7365)	Acc@1 99.609 (99.680)	Acc@5 100.000 (100.000)
Epoch: [176][20/196]	Time 0.017 (0.020)	Data 0.002 (0.011)	Loss 0.7326 (0.7372)	Acc@1 99.609 (99.591)	Acc@5 100.000 (100.000)
Epoch: [176][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.7599 (0.7380)	Acc@1 98.438 (99.534)	Acc@5 99.609 (99.987)
Epoch: [176][40/196]	Time 0.016 (0.019)	Data 0.002 (0.006)	Loss 0.7399 (0.7380)	Acc@1 99.609 (99.543)	Acc@5 100.000 (99.990)
Epoch: [176][50/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 0.7318 (0.7368)	Acc@1 100.000 (99.586)	Acc@5 100.000 (99.992)
Epoch: [176][60/196]	Time 0.018 (0.018)	Data 0.002 (0.005)	Loss 0.7428 (0.7368)	Acc@1 99.609 (99.590)	Acc@5 100.000 (99.994)
Epoch: [176][70/196]	Time 0.017 (0.018)	Data 0.002 (0.005)	Loss 0.7307 (0.7363)	Acc@1 100.000 (99.604)	Acc@5 100.000 (99.994)
Epoch: [176][80/196]	Time 0.017 (0.017)	Data 0.002 (0.004)	Loss 0.7492 (0.7365)	Acc@1 99.219 (99.600)	Acc@5 100.000 (99.995)
Epoch: [176][90/196]	Time 0.020 (0.017)	Data 0.002 (0.004)	Loss 0.7322 (0.7361)	Acc@1 99.219 (99.584)	Acc@5 100.000 (99.996)
Epoch: [176][100/196]	Time 0.014 (0.017)	Data 0.004 (0.004)	Loss 0.7285 (0.7360)	Acc@1 100.000 (99.594)	Acc@5 100.000 (99.996)
Epoch: [176][110/196]	Time 0.013 (0.017)	Data 0.004 (0.004)	Loss 0.7280 (0.7351)	Acc@1 100.000 (99.606)	Acc@5 100.000 (99.996)
Epoch: [176][120/196]	Time 0.012 (0.017)	Data 0.004 (0.004)	Loss 0.7221 (0.7345)	Acc@1 100.000 (99.619)	Acc@5 100.000 (99.997)
Epoch: [176][130/196]	Time 0.013 (0.017)	Data 0.005 (0.004)	Loss 0.7301 (0.7340)	Acc@1 99.609 (99.618)	Acc@5 100.000 (99.997)
Epoch: [176][140/196]	Time 0.014 (0.017)	Data 0.004 (0.004)	Loss 0.7311 (0.7337)	Acc@1 99.609 (99.612)	Acc@5 100.000 (99.997)
Epoch: [176][150/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.7323 (0.7332)	Acc@1 99.219 (99.627)	Acc@5 100.000 (99.997)
Epoch: [176][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.7201 (0.7329)	Acc@1 100.000 (99.634)	Acc@5 100.000 (99.998)
Epoch: [176][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.7246 (0.7330)	Acc@1 99.609 (99.612)	Acc@5 100.000 (99.998)
Epoch: [176][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.7261 (0.7328)	Acc@1 99.219 (99.612)	Acc@5 100.000 (99.998)
Epoch: [176][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7229 (0.7327)	Acc@1 100.000 (99.607)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.7324389227294922, 1.2812604808807373, 99.61, 69.71, tensor(0.9603, device='cuda:0', grad_fn=<DivBackward0>), 3.1581826210021973, 0.4084405899047852]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [177 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [177][0/196]	Time 0.074 (0.074)	Data 0.195 (0.195)	Loss 0.7275 (0.7275)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [177][10/196]	Time 0.016 (0.024)	Data 0.002 (0.019)	Loss 0.7136 (0.7250)	Acc@1 100.000 (99.645)	Acc@5 100.000 (100.000)
Epoch: [177][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.7125 (0.7228)	Acc@1 100.000 (99.665)	Acc@5 100.000 (100.000)
Epoch: [177][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.7161 (0.7226)	Acc@1 100.000 (99.672)	Acc@5 100.000 (100.000)
Epoch: [177][40/196]	Time 0.016 (0.018)	Data 0.002 (0.007)	Loss 0.7250 (0.7221)	Acc@1 99.609 (99.667)	Acc@5 100.000 (100.000)
Epoch: [177][50/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 0.7189 (0.7217)	Acc@1 99.609 (99.655)	Acc@5 100.000 (100.000)
Epoch: [177][60/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 0.7217 (0.7220)	Acc@1 100.000 (99.661)	Acc@5 100.000 (100.000)
Epoch: [177][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.7192 (0.7215)	Acc@1 100.000 (99.686)	Acc@5 100.000 (100.000)
Epoch: [177][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7147 (0.7209)	Acc@1 100.000 (99.715)	Acc@5 100.000 (100.000)
Epoch: [177][90/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.7189 (0.7208)	Acc@1 99.609 (99.708)	Acc@5 100.000 (99.996)
Epoch: [177][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7409 (0.7209)	Acc@1 98.828 (99.691)	Acc@5 100.000 (99.996)
Epoch: [177][110/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.7119 (0.7204)	Acc@1 100.000 (99.690)	Acc@5 100.000 (99.996)
Epoch: [177][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.7071 (0.7201)	Acc@1 100.000 (99.690)	Acc@5 100.000 (99.997)
Epoch: [177][130/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 0.7044 (0.7198)	Acc@1 100.000 (99.693)	Acc@5 100.000 (99.997)
Epoch: [177][140/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7301 (0.7198)	Acc@1 99.219 (99.681)	Acc@5 100.000 (99.997)
Epoch: [177][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7145 (0.7198)	Acc@1 99.609 (99.679)	Acc@5 100.000 (99.997)
Epoch: [177][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.7038 (0.7194)	Acc@1 100.000 (99.680)	Acc@5 100.000 (99.998)
Epoch: [177][170/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.7198 (0.7192)	Acc@1 99.219 (99.680)	Acc@5 100.000 (99.998)
Epoch: [177][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.7073 (0.7191)	Acc@1 99.609 (99.661)	Acc@5 100.000 (99.998)
Epoch: [177][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 0.7065 (0.7186)	Acc@1 100.000 (99.671)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.7187321739578247, 1.2791645121574402, 99.664, 69.93, tensor(0.9620, device='cuda:0', grad_fn=<DivBackward0>), 3.126150846481323, 0.40709900856018066]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [178 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [178][0/196]	Time 0.077 (0.077)	Data 0.192 (0.192)	Loss 0.7141 (0.7141)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [178][10/196]	Time 0.016 (0.024)	Data 0.002 (0.019)	Loss 0.7131 (0.7111)	Acc@1 99.609 (99.751)	Acc@5 100.000 (100.000)
Epoch: [178][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.7080 (0.7103)	Acc@1 100.000 (99.684)	Acc@5 100.000 (100.000)
Epoch: [178][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.7235 (0.7103)	Acc@1 99.219 (99.672)	Acc@5 100.000 (100.000)
Epoch: [178][40/196]	Time 0.014 (0.018)	Data 0.003 (0.006)	Loss 0.7130 (0.7106)	Acc@1 99.609 (99.667)	Acc@5 100.000 (100.000)
Epoch: [178][50/196]	Time 0.018 (0.018)	Data 0.002 (0.005)	Loss 0.7047 (0.7104)	Acc@1 100.000 (99.640)	Acc@5 100.000 (100.000)
Epoch: [178][60/196]	Time 0.017 (0.018)	Data 0.002 (0.005)	Loss 0.6997 (0.7102)	Acc@1 100.000 (99.648)	Acc@5 100.000 (100.000)
Epoch: [178][70/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 0.7036 (0.7101)	Acc@1 100.000 (99.637)	Acc@5 100.000 (100.000)
Epoch: [178][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7071 (0.7097)	Acc@1 99.609 (99.653)	Acc@5 100.000 (100.000)
Epoch: [178][90/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.7130 (0.7093)	Acc@1 99.609 (99.648)	Acc@5 100.000 (100.000)
Epoch: [178][100/196]	Time 0.018 (0.017)	Data 0.002 (0.004)	Loss 0.7107 (0.7091)	Acc@1 98.828 (99.656)	Acc@5 100.000 (100.000)
Epoch: [178][110/196]	Time 0.015 (0.017)	Data 0.003 (0.004)	Loss 0.7095 (0.7087)	Acc@1 99.609 (99.655)	Acc@5 100.000 (100.000)
Epoch: [178][120/196]	Time 0.016 (0.017)	Data 0.002 (0.003)	Loss 0.6951 (0.7083)	Acc@1 100.000 (99.658)	Acc@5 100.000 (100.000)
Epoch: [178][130/196]	Time 0.016 (0.017)	Data 0.002 (0.003)	Loss 0.7084 (0.7080)	Acc@1 99.609 (99.663)	Acc@5 100.000 (100.000)
Epoch: [178][140/196]	Time 0.016 (0.017)	Data 0.002 (0.003)	Loss 0.6953 (0.7080)	Acc@1 100.000 (99.651)	Acc@5 100.000 (100.000)
Epoch: [178][150/196]	Time 0.015 (0.017)	Data 0.002 (0.003)	Loss 0.7059 (0.7077)	Acc@1 99.609 (99.653)	Acc@5 100.000 (100.000)
Epoch: [178][160/196]	Time 0.015 (0.017)	Data 0.003 (0.003)	Loss 0.6954 (0.7076)	Acc@1 100.000 (99.648)	Acc@5 100.000 (100.000)
Epoch: [178][170/196]	Time 0.015 (0.017)	Data 0.002 (0.003)	Loss 0.7126 (0.7070)	Acc@1 98.828 (99.655)	Acc@5 100.000 (100.000)
Epoch: [178][180/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.6942 (0.7067)	Acc@1 100.000 (99.657)	Acc@5 100.000 (100.000)
Epoch: [178][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.7045 (0.7064)	Acc@1 99.609 (99.661)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7064118389892579, 1.293785472512245, 99.65, 70.01, tensor(0.9620, device='cuda:0', grad_fn=<DivBackward0>), 3.2183299064636235, 0.40687561035156244]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [179 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [179][0/196]	Time 0.074 (0.074)	Data 0.197 (0.197)	Loss 0.6924 (0.6924)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [179][10/196]	Time 0.021 (0.023)	Data 0.002 (0.019)	Loss 0.6970 (0.6941)	Acc@1 100.000 (99.787)	Acc@5 100.000 (100.000)
Epoch: [179][20/196]	Time 0.018 (0.020)	Data 0.002 (0.011)	Loss 0.6930 (0.6945)	Acc@1 100.000 (99.777)	Acc@5 100.000 (100.000)
Epoch: [179][30/196]	Time 0.016 (0.019)	Data 0.002 (0.008)	Loss 0.7016 (0.6958)	Acc@1 99.219 (99.723)	Acc@5 100.000 (100.000)
Epoch: [179][40/196]	Time 0.016 (0.018)	Data 0.002 (0.007)	Loss 0.6996 (0.6953)	Acc@1 99.609 (99.743)	Acc@5 100.000 (100.000)
Epoch: [179][50/196]	Time 0.017 (0.018)	Data 0.002 (0.006)	Loss 0.6839 (0.6950)	Acc@1 100.000 (99.755)	Acc@5 100.000 (100.000)
Epoch: [179][60/196]	Time 0.017 (0.018)	Data 0.002 (0.005)	Loss 0.6899 (0.6946)	Acc@1 100.000 (99.757)	Acc@5 100.000 (100.000)
Epoch: [179][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.7164 (0.6954)	Acc@1 99.219 (99.719)	Acc@5 100.000 (100.000)
Epoch: [179][80/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.6978 (0.6951)	Acc@1 99.219 (99.711)	Acc@5 100.000 (100.000)
Epoch: [179][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.6844 (0.6946)	Acc@1 100.000 (99.730)	Acc@5 100.000 (100.000)
Epoch: [179][100/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.6908 (0.6944)	Acc@1 99.609 (99.737)	Acc@5 100.000 (100.000)
Epoch: [179][110/196]	Time 0.016 (0.017)	Data 0.003 (0.004)	Loss 0.6809 (0.6942)	Acc@1 100.000 (99.726)	Acc@5 100.000 (100.000)
Epoch: [179][120/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.6865 (0.6939)	Acc@1 100.000 (99.722)	Acc@5 100.000 (100.000)
Epoch: [179][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.7038 (0.6936)	Acc@1 99.609 (99.726)	Acc@5 100.000 (100.000)
Epoch: [179][140/196]	Time 0.011 (0.016)	Data 0.010 (0.003)	Loss 0.6921 (0.6935)	Acc@1 100.000 (99.720)	Acc@5 100.000 (100.000)
Epoch: [179][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.6837 (0.6933)	Acc@1 100.000 (99.721)	Acc@5 100.000 (100.000)
Epoch: [179][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.6966 (0.6930)	Acc@1 99.609 (99.719)	Acc@5 100.000 (100.000)
Epoch: [179][170/196]	Time 0.019 (0.016)	Data 0.002 (0.003)	Loss 0.6936 (0.6928)	Acc@1 99.219 (99.714)	Acc@5 100.000 (100.000)
Epoch: [179][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 0.6979 (0.6930)	Acc@1 98.828 (99.696)	Acc@5 100.000 (100.000)
Epoch: [179][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.6820 (0.6930)	Acc@1 100.000 (99.681)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.6929287243270874, 1.295705981850624, 99.678, 69.66, tensor(0.9638, device='cuda:0', grad_fn=<DivBackward0>), 3.1585259437561035, 0.39639282226562494]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [180 | 180] LR: 0.010000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [180][0/196]	Time 0.068 (0.068)	Data 0.189 (0.189)	Loss 0.6806 (0.6806)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [180][10/196]	Time 0.017 (0.024)	Data 0.002 (0.019)	Loss 0.6861 (0.6880)	Acc@1 99.609 (99.716)	Acc@5 100.000 (100.000)
Epoch: [180][20/196]	Time 0.016 (0.020)	Data 0.002 (0.011)	Loss 0.6761 (0.6860)	Acc@1 100.000 (99.758)	Acc@5 100.000 (100.000)
Epoch: [180][30/196]	Time 0.015 (0.019)	Data 0.002 (0.008)	Loss 0.6774 (0.6840)	Acc@1 100.000 (99.811)	Acc@5 100.000 (100.000)
Epoch: [180][40/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.6833 (0.6839)	Acc@1 100.000 (99.781)	Acc@5 100.000 (100.000)
Epoch: [180][50/196]	Time 0.016 (0.018)	Data 0.002 (0.006)	Loss 0.6846 (0.6843)	Acc@1 99.609 (99.770)	Acc@5 100.000 (100.000)
Epoch: [180][60/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.6814 (0.6837)	Acc@1 99.609 (99.782)	Acc@5 100.000 (100.000)
Epoch: [180][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.6782 (0.6835)	Acc@1 100.000 (99.774)	Acc@5 100.000 (100.000)
Epoch: [180][80/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.6740 (0.6826)	Acc@1 100.000 (99.783)	Acc@5 100.000 (100.000)
Epoch: [180][90/196]	Time 0.015 (0.017)	Data 0.002 (0.004)	Loss 0.6744 (0.6822)	Acc@1 100.000 (99.772)	Acc@5 100.000 (100.000)
Epoch: [180][100/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.6889 (0.6823)	Acc@1 99.609 (99.752)	Acc@5 100.000 (100.000)
Epoch: [180][110/196]	Time 0.016 (0.017)	Data 0.002 (0.004)	Loss 0.6731 (0.6822)	Acc@1 100.000 (99.747)	Acc@5 100.000 (100.000)
Epoch: [180][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.6864 (0.6821)	Acc@1 99.219 (99.735)	Acc@5 100.000 (100.000)
Epoch: [180][130/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.6939 (0.6824)	Acc@1 99.609 (99.726)	Acc@5 100.000 (100.000)
Epoch: [180][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.6974 (0.6825)	Acc@1 99.219 (99.723)	Acc@5 100.000 (100.000)
Epoch: [180][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.6783 (0.6821)	Acc@1 100.000 (99.723)	Acc@5 100.000 (100.000)
Epoch: [180][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.6695 (0.6821)	Acc@1 100.000 (99.719)	Acc@5 100.000 (100.000)
Epoch: [180][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 0.6848 (0.6818)	Acc@1 99.219 (99.710)	Acc@5 100.000 (100.000)
Epoch: [180][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.6769 (0.6817)	Acc@1 99.219 (99.698)	Acc@5 100.000 (100.000)
Epoch: [180][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 0.6889 (0.6816)	Acc@1 99.219 (99.691)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.6814898565864563, 1.2861462920904159, 99.69, 69.88, tensor(0.9629, device='cuda:0', grad_fn=<DivBackward0>), 3.161736249923706, 0.40398144721984863]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...
Best acc:
70.01

Total time:
848.080987
[2021-06-20T03:28:32.074428] Command finished with return code 0


[2021-06-20T03:28:32.075550] The experiment completed successfully. Finalizing run...
Cleaning up all outstanding Run operations, waiting 900.0 seconds
1 items cleaning up...
Cleanup took 0.061728477478027344 seconds
[2021-06-20T03:28:32.300685] Finished context manager injector.
2021/06/20 03:28:33 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status
2021/06/20 03:28:33 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 2
FilteredData: 0.
2021/06/20 03:28:33 Process Exiting with Code:  0
2021/06/20 03:28:33 All App Insights Logs was send successfully
