bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 04:45:15 Starting App Insight Logger for task:  runTaskLet
2021/06/20 04:45:15 Version: 3.0.01622.0001 Branch: .SourceBranch Commit: 1141612
2021/06/20 04:45:15 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info
bash: /azureml-envs/azureml_fd71f2370c59f1f45a36b18b907af511/lib/libtinfo.so.5: no version information available (required by bash)
2021/06/20 04:45:15 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status
[2021-06-20T04:45:15.788235] Entering context manager injector.
[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15'])
Script type = COMMAND
[2021-06-20T04:45:16.413530] Command=python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
[2021-06-20T04:45:16.413833] Entering Run History Context Manager.
[2021-06-20T04:45:16.991641] Command Working Directory=/mnt/batch/tasks/shared/LS_root/jobs/prunetrain/azureml/onthefly-tensor5/wd/azureml/OnTheFly-Tensor5
[2021-06-20T04:45:16.991892] Starting Linux command : python cifar.py --arch vgg11_bn_flat --dataset cifar100 --sparse_interval 10 --var_group_lasso_coeff 0.2 --threshold 0.0001 --epochs 180 --learning-rate 0.1 --train_batch 128 --test_batch 100 --workers 4 --manualSeed 15
no display found. Using non-interactive Agg backend
==> Preparing dataset cifar100
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/data/torch/cifar-100-python.tar.gz
0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%100.0%2021/06/20 04:45:20 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 1
FilteredData: 0.
Extracting ./dataset/data/torch/cifar-100-python.tar.gz to ./dataset/data/torch
==> creating model 'vgg11_bn_flat'
    Total params: 9.27M
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375

Epoch: [1 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
cifar.py:337: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
Epoch: [1][0/196]	Time 0.600 (0.600)	Data 0.162 (0.162)	Loss 6.0768 (6.0768)	Acc@1 0.781 (0.781)	Acc@5 2.734 (2.734)
Epoch: [1][10/196]	Time 0.015 (0.069)	Data 0.002 (0.016)	Loss 6.5304 (6.4500)	Acc@1 4.297 (2.273)	Acc@5 15.625 (9.766)
Epoch: [1][20/196]	Time 0.013 (0.043)	Data 0.004 (0.010)	Loss 6.4085 (6.5651)	Acc@1 1.953 (2.344)	Acc@5 10.938 (10.026)
Epoch: [1][30/196]	Time 0.012 (0.034)	Data 0.034 (0.008)	Loss 5.8436 (6.3737)	Acc@1 1.953 (2.558)	Acc@5 11.719 (10.698)
Epoch: [1][40/196]	Time 0.016 (0.029)	Data 0.003 (0.007)	Loss 5.6829 (6.2059)	Acc@1 3.906 (2.772)	Acc@5 16.016 (11.604)
Epoch: [1][50/196]	Time 0.015 (0.027)	Data 0.002 (0.006)	Loss 5.5487 (6.0903)	Acc@1 3.125 (3.018)	Acc@5 17.969 (12.707)
Epoch: [1][60/196]	Time 0.015 (0.025)	Data 0.003 (0.005)	Loss 5.5691 (5.9915)	Acc@1 3.516 (3.304)	Acc@5 17.578 (13.749)
Epoch: [1][70/196]	Time 0.013 (0.023)	Data 0.004 (0.005)	Loss 5.3184 (5.9096)	Acc@1 7.422 (3.527)	Acc@5 22.656 (14.530)
Epoch: [1][80/196]	Time 0.015 (0.022)	Data 0.002 (0.005)	Loss 5.2933 (5.8449)	Acc@1 8.203 (3.791)	Acc@5 20.703 (15.123)
Epoch: [1][90/196]	Time 0.015 (0.022)	Data 0.003 (0.004)	Loss 5.3139 (5.7869)	Acc@1 4.297 (3.971)	Acc@5 21.875 (16.011)
Epoch: [1][100/196]	Time 0.015 (0.021)	Data 0.002 (0.004)	Loss 5.1113 (5.7319)	Acc@1 7.422 (4.297)	Acc@5 25.000 (16.836)
Epoch: [1][110/196]	Time 0.016 (0.020)	Data 0.004 (0.004)	Loss 5.2384 (5.6818)	Acc@1 5.469 (4.515)	Acc@5 25.391 (17.652)
Epoch: [1][120/196]	Time 0.016 (0.020)	Data 0.000 (0.004)	Loss 5.0923 (5.6313)	Acc@1 9.375 (4.852)	Acc@5 31.250 (18.530)
Epoch: [1][130/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 5.0954 (5.5886)	Acc@1 7.812 (5.203)	Acc@5 30.469 (19.287)
Epoch: [1][140/196]	Time 0.018 (0.019)	Data 0.000 (0.004)	Loss 4.9981 (5.5510)	Acc@1 9.766 (5.527)	Acc@5 30.859 (19.997)
Epoch: [1][150/196]	Time 0.015 (0.019)	Data 0.003 (0.004)	Loss 4.9960 (5.5175)	Acc@1 10.547 (5.802)	Acc@5 30.859 (20.584)
Epoch: [1][160/196]	Time 0.013 (0.019)	Data 0.003 (0.004)	Loss 4.8412 (5.4793)	Acc@1 13.672 (6.075)	Acc@5 35.938 (21.339)
Epoch: [1][170/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 4.9076 (5.4458)	Acc@1 8.984 (6.357)	Acc@5 32.812 (21.998)
Epoch: [1][180/196]	Time 0.012 (0.018)	Data 0.011 (0.004)	Loss 4.8833 (5.4196)	Acc@1 12.500 (6.550)	Acc@5 33.984 (22.427)
Epoch: [1][190/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 4.9175 (5.3906)	Acc@1 8.594 (6.792)	Acc@5 30.859 (23.010)
cifar.py:424: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)
num momentum params: 26
[0.1, 5.377558330841064, 3.787208631038666, 6.918, 10.3, tensor(0.2191, device='cuda:0', grad_fn=<DivBackward0>), 3.7563633918762207, 0.46873497962951655]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [2 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [2][0/196]	Time 0.031 (0.031)	Data 0.162 (0.162)	Loss 4.7992 (4.7992)	Acc@1 11.328 (11.328)	Acc@5 36.328 (36.328)
Epoch: [2][10/196]	Time 0.015 (0.017)	Data 0.003 (0.016)	Loss 4.7922 (4.7989)	Acc@1 11.719 (11.115)	Acc@5 33.984 (35.795)
Epoch: [2][20/196]	Time 0.015 (0.016)	Data 0.002 (0.010)	Loss 4.6886 (4.7842)	Acc@1 12.891 (11.514)	Acc@5 37.891 (35.993)
Epoch: [2][30/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 4.7861 (4.7752)	Acc@1 11.719 (11.832)	Acc@5 33.203 (35.887)
Epoch: [2][40/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 4.6738 (4.7637)	Acc@1 10.938 (11.862)	Acc@5 36.328 (36.090)
Epoch: [2][50/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 4.6873 (4.7506)	Acc@1 10.547 (11.918)	Acc@5 38.281 (36.252)
Epoch: [2][60/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 4.8353 (4.7354)	Acc@1 10.547 (12.154)	Acc@5 30.469 (36.431)
Epoch: [2][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 4.5911 (4.7210)	Acc@1 14.844 (12.357)	Acc@5 37.891 (36.834)
Epoch: [2][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 4.6319 (4.7055)	Acc@1 14.844 (12.582)	Acc@5 41.016 (37.119)
Epoch: [2][90/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 4.7409 (4.6971)	Acc@1 13.672 (12.693)	Acc@5 39.844 (37.290)
Epoch: [2][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 4.4292 (4.6821)	Acc@1 13.672 (12.902)	Acc@5 41.406 (37.705)
Epoch: [2][110/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 4.5474 (4.6660)	Acc@1 15.625 (13.095)	Acc@5 41.406 (38.077)
Epoch: [2][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 4.3876 (4.6504)	Acc@1 16.016 (13.275)	Acc@5 46.484 (38.449)
Epoch: [2][130/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 4.4309 (4.6345)	Acc@1 18.750 (13.550)	Acc@5 42.188 (38.809)
Epoch: [2][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 4.3094 (4.6173)	Acc@1 18.750 (13.821)	Acc@5 50.000 (39.220)
Epoch: [2][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 4.3102 (4.6011)	Acc@1 22.266 (14.096)	Acc@5 44.141 (39.546)
Epoch: [2][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 4.3414 (4.5876)	Acc@1 19.922 (14.293)	Acc@5 46.875 (39.921)
Epoch: [2][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 4.3117 (4.5744)	Acc@1 17.188 (14.460)	Acc@5 42.578 (40.154)
Epoch: [2][180/196]	Time 0.012 (0.015)	Data 0.021 (0.004)	Loss 4.2410 (4.5553)	Acc@1 21.094 (14.760)	Acc@5 49.219 (40.612)
Epoch: [2][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 4.2812 (4.5390)	Acc@1 17.969 (14.977)	Acc@5 46.094 (41.020)
num momentum params: 26
[0.1, 4.530528220825195, 3.2621169662475586, 15.066, 19.69, tensor(0.2272, device='cuda:0', grad_fn=<DivBackward0>), 2.932129144668579, 0.3682088851928711]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [3 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [3][0/196]	Time 0.030 (0.030)	Data 0.156 (0.156)	Loss 4.1205 (4.1205)	Acc@1 19.922 (19.922)	Acc@5 50.000 (50.000)
Epoch: [3][10/196]	Time 0.016 (0.016)	Data 0.000 (0.017)	Loss 3.9418 (4.1860)	Acc@1 24.219 (19.496)	Acc@5 55.859 (49.219)
Epoch: [3][20/196]	Time 0.015 (0.015)	Data 0.002 (0.011)	Loss 4.0976 (4.1332)	Acc@1 22.656 (20.350)	Acc@5 54.297 (50.242)
Epoch: [3][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 4.0879 (4.1253)	Acc@1 22.266 (20.829)	Acc@5 55.859 (50.391)
Epoch: [3][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 4.1534 (4.1301)	Acc@1 21.094 (20.522)	Acc@5 48.828 (50.314)
Epoch: [3][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 4.1835 (4.1217)	Acc@1 19.531 (20.810)	Acc@5 50.000 (50.444)
Epoch: [3][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 4.1425 (4.1148)	Acc@1 17.578 (20.748)	Acc@5 48.438 (50.442)
Epoch: [3][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 3.9863 (4.1034)	Acc@1 22.266 (20.813)	Acc@5 55.469 (50.935)
Epoch: [3][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 3.9618 (4.0826)	Acc@1 27.734 (21.373)	Acc@5 54.297 (51.495)
Epoch: [3][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 3.8975 (4.0620)	Acc@1 21.484 (21.643)	Acc@5 55.078 (51.932)
Epoch: [3][100/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 3.7898 (4.0574)	Acc@1 26.172 (21.670)	Acc@5 58.594 (52.081)
Epoch: [3][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 4.1059 (4.0455)	Acc@1 21.094 (21.875)	Acc@5 51.562 (52.259)
Epoch: [3][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.9301 (4.0395)	Acc@1 24.219 (22.017)	Acc@5 56.641 (52.347)
Epoch: [3][130/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 3.9041 (4.0300)	Acc@1 22.656 (22.134)	Acc@5 51.953 (52.484)
Epoch: [3][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 3.7522 (4.0147)	Acc@1 29.688 (22.410)	Acc@5 57.422 (52.812)
Epoch: [3][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.7144 (3.9979)	Acc@1 25.000 (22.602)	Acc@5 61.328 (53.159)
Epoch: [3][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.7453 (3.9835)	Acc@1 26.953 (22.790)	Acc@5 57.422 (53.438)
Epoch: [3][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 3.7363 (3.9718)	Acc@1 28.906 (23.015)	Acc@5 55.469 (53.625)
Epoch: [3][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.6757 (3.9573)	Acc@1 29.688 (23.211)	Acc@5 58.594 (53.941)
Epoch: [3][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.6154 (3.9451)	Acc@1 28.125 (23.423)	Acc@5 61.328 (54.166)
num momentum params: 26
[0.1, 3.9397716361999513, 2.9060904812812804, 23.486, 26.46, tensor(0.2324, device='cuda:0', grad_fn=<DivBackward0>), 2.995802640914917, 0.3682425022125244]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [4 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [4][0/196]	Time 0.030 (0.030)	Data 0.152 (0.152)	Loss 3.5080 (3.5080)	Acc@1 27.344 (27.344)	Acc@5 64.844 (64.844)
Epoch: [4][10/196]	Time 0.015 (0.016)	Data 0.003 (0.016)	Loss 3.7465 (3.6248)	Acc@1 22.266 (27.947)	Acc@5 60.938 (61.825)
Epoch: [4][20/196]	Time 0.014 (0.016)	Data 0.004 (0.010)	Loss 3.6159 (3.6511)	Acc@1 28.125 (27.530)	Acc@5 62.109 (61.347)
Epoch: [4][30/196]	Time 0.016 (0.015)	Data 0.002 (0.007)	Loss 3.6923 (3.6638)	Acc@1 25.391 (27.482)	Acc@5 61.328 (61.051)
Epoch: [4][40/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 3.6018 (3.6352)	Acc@1 30.859 (28.239)	Acc@5 58.984 (61.462)
Epoch: [4][50/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 3.7241 (3.6288)	Acc@1 28.516 (28.546)	Acc@5 58.594 (61.512)
Epoch: [4][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 3.3492 (3.6173)	Acc@1 36.719 (28.695)	Acc@5 66.797 (61.578)
Epoch: [4][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 3.5527 (3.6108)	Acc@1 28.125 (28.736)	Acc@5 66.016 (61.592)
Epoch: [4][80/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 3.6466 (3.6040)	Acc@1 25.391 (28.766)	Acc@5 67.188 (61.781)
Epoch: [4][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 3.5521 (3.5975)	Acc@1 27.734 (28.739)	Acc@5 63.281 (61.839)
Epoch: [4][100/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 3.3418 (3.5843)	Acc@1 30.859 (28.949)	Acc@5 67.578 (62.028)
Epoch: [4][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.6567 (3.5779)	Acc@1 23.047 (28.948)	Acc@5 64.844 (62.197)
Epoch: [4][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 3.6601 (3.5710)	Acc@1 25.000 (29.116)	Acc@5 60.547 (62.293)
Epoch: [4][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 3.4572 (3.5615)	Acc@1 31.250 (29.306)	Acc@5 63.672 (62.494)
Epoch: [4][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 3.4472 (3.5548)	Acc@1 33.594 (29.502)	Acc@5 60.547 (62.555)
Epoch: [4][150/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 3.3878 (3.5486)	Acc@1 35.156 (29.625)	Acc@5 68.359 (62.710)
Epoch: [4][160/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 3.1978 (3.5386)	Acc@1 36.328 (29.768)	Acc@5 72.266 (62.915)
Epoch: [4][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.2718 (3.5325)	Acc@1 33.984 (29.788)	Acc@5 67.188 (63.019)
Epoch: [4][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 3.3659 (3.5221)	Acc@1 29.688 (29.860)	Acc@5 64.844 (63.191)
Epoch: [4][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 3.4772 (3.5141)	Acc@1 29.297 (29.994)	Acc@5 62.109 (63.294)
num momentum params: 26
[0.1, 3.51336284614563, 2.857135753631592, 30.046, 28.02, tensor(0.2358, device='cuda:0', grad_fn=<DivBackward0>), 2.891206741333008, 0.3664224147796631]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [5 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [5][0/196]	Time 0.031 (0.031)	Data 0.167 (0.167)	Loss 3.3143 (3.3143)	Acc@1 32.031 (32.031)	Acc@5 65.625 (65.625)
Epoch: [5][10/196]	Time 0.015 (0.017)	Data 0.002 (0.017)	Loss 3.2277 (3.3130)	Acc@1 38.281 (33.700)	Acc@5 69.141 (66.371)
Epoch: [5][20/196]	Time 0.011 (0.016)	Data 0.008 (0.010)	Loss 3.1058 (3.2880)	Acc@1 37.500 (33.836)	Acc@5 71.484 (67.299)
Epoch: [5][30/196]	Time 0.011 (0.015)	Data 0.007 (0.008)	Loss 3.2618 (3.2906)	Acc@1 32.812 (33.380)	Acc@5 66.406 (67.490)
Epoch: [5][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 3.4705 (3.2993)	Acc@1 29.297 (33.337)	Acc@5 66.406 (67.407)
Epoch: [5][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 3.2632 (3.2895)	Acc@1 37.109 (33.647)	Acc@5 64.844 (67.410)
Epoch: [5][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 3.2863 (3.2884)	Acc@1 33.984 (33.523)	Acc@5 63.672 (67.418)
Epoch: [5][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 3.0683 (3.2757)	Acc@1 39.062 (33.869)	Acc@5 73.047 (67.617)
Epoch: [5][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 3.3108 (3.2660)	Acc@1 34.766 (34.129)	Acc@5 68.359 (67.776)
Epoch: [5][90/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 3.1574 (3.2602)	Acc@1 33.203 (34.010)	Acc@5 70.703 (67.844)
Epoch: [5][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 3.1646 (3.2506)	Acc@1 32.812 (34.216)	Acc@5 69.141 (67.988)
Epoch: [5][110/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 3.2493 (3.2486)	Acc@1 35.156 (34.361)	Acc@5 67.578 (68.007)
Epoch: [5][120/196]	Time 0.010 (0.015)	Data 0.009 (0.004)	Loss 3.2920 (3.2401)	Acc@1 34.766 (34.527)	Acc@5 67.188 (68.185)
Epoch: [5][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.1257 (3.2348)	Acc@1 39.062 (34.593)	Acc@5 71.484 (68.318)
Epoch: [5][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 3.0087 (3.2268)	Acc@1 37.891 (34.730)	Acc@5 73.047 (68.467)
Epoch: [5][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.1054 (3.2170)	Acc@1 36.328 (34.851)	Acc@5 72.266 (68.691)
Epoch: [5][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 3.2565 (3.2098)	Acc@1 33.594 (35.042)	Acc@5 66.016 (68.755)
Epoch: [5][170/196]	Time 0.012 (0.015)	Data 0.001 (0.004)	Loss 3.2455 (3.2036)	Acc@1 33.984 (35.275)	Acc@5 67.969 (68.910)
Epoch: [5][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 3.1740 (3.1975)	Acc@1 38.672 (35.432)	Acc@5 69.531 (69.033)
Epoch: [5][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.9063 (3.1908)	Acc@1 39.844 (35.547)	Acc@5 72.656 (69.141)
num momentum params: 26
[0.1, 3.1892784590148926, 2.6592898750305176, 35.55, 32.01, tensor(0.2397, device='cuda:0', grad_fn=<DivBackward0>), 2.9349827766418457, 0.36768150329589844]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [6 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [6][0/196]	Time 0.029 (0.029)	Data 0.168 (0.168)	Loss 3.0785 (3.0785)	Acc@1 41.016 (41.016)	Acc@5 70.703 (70.703)
Epoch: [6][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 2.8418 (3.0606)	Acc@1 41.016 (38.423)	Acc@5 76.172 (71.165)
Epoch: [6][20/196]	Time 0.011 (0.015)	Data 0.009 (0.011)	Loss 3.0242 (3.0287)	Acc@1 40.234 (38.560)	Acc@5 73.828 (71.875)
Epoch: [6][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.8222 (3.0162)	Acc@1 43.750 (38.697)	Acc@5 77.734 (72.152)
Epoch: [6][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 2.9566 (3.0007)	Acc@1 38.672 (39.205)	Acc@5 71.875 (72.437)
Epoch: [6][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 3.0311 (2.9945)	Acc@1 39.062 (39.331)	Acc@5 73.047 (72.541)
Epoch: [6][60/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.9341 (2.9918)	Acc@1 38.672 (39.421)	Acc@5 72.266 (72.611)
Epoch: [6][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.9751 (2.9924)	Acc@1 42.578 (39.530)	Acc@5 72.266 (72.574)
Epoch: [6][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.8049 (2.9844)	Acc@1 46.094 (39.685)	Acc@5 76.562 (72.714)
Epoch: [6][90/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.9365 (2.9800)	Acc@1 39.453 (39.839)	Acc@5 71.875 (72.888)
Epoch: [6][100/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.8116 (2.9742)	Acc@1 42.578 (39.964)	Acc@5 74.609 (72.958)
Epoch: [6][110/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 3.0856 (2.9726)	Acc@1 37.109 (39.907)	Acc@5 69.922 (72.959)
Epoch: [6][120/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 2.8789 (2.9663)	Acc@1 42.578 (40.031)	Acc@5 70.312 (73.053)
Epoch: [6][130/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.9340 (2.9643)	Acc@1 41.016 (40.097)	Acc@5 73.438 (72.987)
Epoch: [6][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 3.0757 (2.9613)	Acc@1 39.453 (40.110)	Acc@5 68.359 (73.030)
Epoch: [6][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 3.0024 (2.9637)	Acc@1 40.234 (40.165)	Acc@5 70.703 (72.969)
Epoch: [6][160/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 2.8369 (2.9587)	Acc@1 44.141 (40.213)	Acc@5 74.219 (73.091)
Epoch: [6][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.6616 (2.9511)	Acc@1 46.875 (40.296)	Acc@5 79.297 (73.223)
Epoch: [6][180/196]	Time 0.013 (0.015)	Data 0.011 (0.004)	Loss 2.7812 (2.9477)	Acc@1 42.188 (40.290)	Acc@5 79.688 (73.261)
Epoch: [6][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.8155 (2.9406)	Acc@1 40.234 (40.412)	Acc@5 76.562 (73.358)
num momentum params: 26
[0.1, 2.9392550825500487, 2.4449926686286925, 40.42, 37.44, tensor(0.2447, device='cuda:0', grad_fn=<DivBackward0>), 2.9825448989868164, 0.37235593795776367]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [7 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [7][0/196]	Time 0.031 (0.031)	Data 0.168 (0.168)	Loss 2.7584 (2.7584)	Acc@1 38.672 (38.672)	Acc@5 77.344 (77.344)
Epoch: [7][10/196]	Time 0.016 (0.017)	Data 0.000 (0.018)	Loss 2.8661 (2.8499)	Acc@1 40.625 (41.158)	Acc@5 75.391 (74.609)
Epoch: [7][20/196]	Time 0.012 (0.016)	Data 0.006 (0.011)	Loss 2.9009 (2.8077)	Acc@1 43.750 (42.504)	Acc@5 70.312 (75.000)
Epoch: [7][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.7631 (2.7853)	Acc@1 43.359 (42.956)	Acc@5 78.906 (75.769)
Epoch: [7][40/196]	Time 0.012 (0.015)	Data 0.006 (0.007)	Loss 2.9296 (2.7881)	Acc@1 38.672 (42.883)	Acc@5 73.438 (75.857)
Epoch: [7][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.7643 (2.7773)	Acc@1 42.578 (42.884)	Acc@5 76.953 (76.088)
Epoch: [7][60/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.7879 (2.7832)	Acc@1 45.312 (42.905)	Acc@5 77.344 (75.897)
Epoch: [7][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.6495 (2.7853)	Acc@1 44.922 (42.936)	Acc@5 79.688 (75.979)
Epoch: [7][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.7368 (2.7795)	Acc@1 44.531 (43.123)	Acc@5 76.953 (76.042)
Epoch: [7][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.7697 (2.7764)	Acc@1 44.141 (43.153)	Acc@5 75.781 (76.086)
Epoch: [7][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.7442 (2.7721)	Acc@1 42.578 (43.185)	Acc@5 76.172 (76.122)
Epoch: [7][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.7578 (2.7701)	Acc@1 46.094 (43.257)	Acc@5 76.953 (76.161)
Epoch: [7][120/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.7657 (2.7697)	Acc@1 46.094 (43.340)	Acc@5 74.219 (76.191)
Epoch: [7][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.7901 (2.7648)	Acc@1 39.844 (43.491)	Acc@5 79.688 (76.312)
Epoch: [7][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.7493 (2.7629)	Acc@1 44.141 (43.564)	Acc@5 76.953 (76.335)
Epoch: [7][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.5285 (2.7594)	Acc@1 48.828 (43.564)	Acc@5 81.641 (76.438)
Epoch: [7][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.6660 (2.7579)	Acc@1 42.578 (43.505)	Acc@5 79.688 (76.456)
Epoch: [7][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.6752 (2.7550)	Acc@1 45.703 (43.560)	Acc@5 76.953 (76.515)
Epoch: [7][180/196]	Time 0.012 (0.015)	Data 0.011 (0.004)	Loss 2.8146 (2.7526)	Acc@1 42.188 (43.674)	Acc@5 74.609 (76.543)
Epoch: [7][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.9165 (2.7506)	Acc@1 39.844 (43.707)	Acc@5 73.828 (76.632)
num momentum params: 26
[0.1, 2.7510866918945314, 2.399077260494232, 43.662, 38.52, tensor(0.2505, device='cuda:0', grad_fn=<DivBackward0>), 2.9530255794525146, 0.3684816360473633]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [8 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [8][0/196]	Time 0.029 (0.029)	Data 0.172 (0.172)	Loss 2.6331 (2.6331)	Acc@1 45.312 (45.312)	Acc@5 79.688 (79.688)
Epoch: [8][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 2.7136 (2.6469)	Acc@1 48.438 (46.982)	Acc@5 76.172 (78.551)
Epoch: [8][20/196]	Time 0.011 (0.015)	Data 0.006 (0.011)	Loss 2.7196 (2.6408)	Acc@1 42.188 (46.354)	Acc@5 77.734 (78.757)
Epoch: [8][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.5114 (2.6319)	Acc@1 48.438 (46.560)	Acc@5 80.078 (78.805)
Epoch: [8][40/196]	Time 0.013 (0.015)	Data 0.005 (0.007)	Loss 2.4849 (2.6342)	Acc@1 48.438 (46.361)	Acc@5 80.469 (78.830)
Epoch: [8][50/196]	Time 0.023 (0.015)	Data 0.001 (0.006)	Loss 2.4354 (2.6252)	Acc@1 51.562 (46.515)	Acc@5 83.203 (78.914)
Epoch: [8][60/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.5337 (2.6248)	Acc@1 48.438 (46.580)	Acc@5 81.641 (78.797)
Epoch: [8][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.6554 (2.6222)	Acc@1 44.922 (46.457)	Acc@5 76.562 (78.714)
Epoch: [8][80/196]	Time 0.014 (0.015)	Data 0.007 (0.005)	Loss 2.5563 (2.6305)	Acc@1 49.609 (46.436)	Acc@5 77.734 (78.443)
Epoch: [8][90/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.6175 (2.6351)	Acc@1 46.875 (46.347)	Acc@5 78.906 (78.413)
Epoch: [8][100/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.6138 (2.6314)	Acc@1 50.781 (46.426)	Acc@5 79.297 (78.504)
Epoch: [8][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.5614 (2.6283)	Acc@1 51.562 (46.597)	Acc@5 80.078 (78.572)
Epoch: [8][120/196]	Time 0.011 (0.015)	Data 0.017 (0.004)	Loss 2.8546 (2.6270)	Acc@1 40.234 (46.510)	Acc@5 75.391 (78.616)
Epoch: [8][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.6675 (2.6292)	Acc@1 45.703 (46.434)	Acc@5 77.344 (78.578)
Epoch: [8][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.7038 (2.6307)	Acc@1 43.750 (46.401)	Acc@5 77.344 (78.552)
Epoch: [8][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.6557 (2.6322)	Acc@1 44.531 (46.337)	Acc@5 77.734 (78.565)
Epoch: [8][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.4675 (2.6319)	Acc@1 51.562 (46.380)	Acc@5 81.250 (78.591)
Epoch: [8][170/196]	Time 0.015 (0.015)	Data 0.001 (0.004)	Loss 2.4701 (2.6287)	Acc@1 53.516 (46.491)	Acc@5 82.422 (78.646)
Epoch: [8][180/196]	Time 0.013 (0.015)	Data 0.008 (0.004)	Loss 2.5769 (2.6250)	Acc@1 46.484 (46.562)	Acc@5 78.906 (78.660)
Epoch: [8][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.7576 (2.6232)	Acc@1 41.406 (46.599)	Acc@5 75.000 (78.708)
num momentum params: 26
[0.1, 2.621932630157471, 2.2618967485427857, 46.63, 40.52, tensor(0.2569, device='cuda:0', grad_fn=<DivBackward0>), 3.0151822566986084, 0.36835670471191406]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [9 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [9][0/196]	Time 0.031 (0.031)	Data 0.174 (0.174)	Loss 2.4449 (2.4449)	Acc@1 50.000 (50.000)	Acc@5 80.078 (80.078)
Epoch: [9][10/196]	Time 0.016 (0.016)	Data 0.001 (0.018)	Loss 2.4651 (2.5238)	Acc@1 48.828 (49.538)	Acc@5 80.078 (79.830)
Epoch: [9][20/196]	Time 0.011 (0.016)	Data 0.007 (0.011)	Loss 2.4082 (2.5156)	Acc@1 52.734 (49.498)	Acc@5 83.203 (80.041)
Epoch: [9][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 2.5799 (2.5110)	Acc@1 46.484 (49.118)	Acc@5 83.203 (80.481)
Epoch: [9][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 2.5713 (2.5132)	Acc@1 43.750 (48.790)	Acc@5 80.078 (80.402)
Epoch: [9][50/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 2.4786 (2.4981)	Acc@1 51.562 (49.219)	Acc@5 82.031 (80.744)
Epoch: [9][60/196]	Time 0.014 (0.015)	Data 0.007 (0.006)	Loss 2.6115 (2.5114)	Acc@1 45.703 (48.745)	Acc@5 76.953 (80.616)
Epoch: [9][70/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.3085 (2.5085)	Acc@1 50.391 (48.856)	Acc@5 84.766 (80.700)
Epoch: [9][80/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.6240 (2.5126)	Acc@1 50.000 (48.857)	Acc@5 76.562 (80.633)
Epoch: [9][90/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.5553 (2.5155)	Acc@1 50.781 (48.832)	Acc@5 80.078 (80.550)
Epoch: [9][100/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 2.5029 (2.5125)	Acc@1 48.438 (48.979)	Acc@5 82.422 (80.550)
Epoch: [9][110/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3712 (2.5164)	Acc@1 50.781 (48.793)	Acc@5 83.203 (80.514)
Epoch: [9][120/196]	Time 0.014 (0.015)	Data 0.010 (0.004)	Loss 2.7380 (2.5195)	Acc@1 44.531 (48.764)	Acc@5 77.734 (80.456)
Epoch: [9][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.5513 (2.5210)	Acc@1 45.312 (48.724)	Acc@5 80.859 (80.516)
Epoch: [9][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.5058 (2.5231)	Acc@1 50.391 (48.762)	Acc@5 79.688 (80.494)
Epoch: [9][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.4674 (2.5241)	Acc@1 50.391 (48.707)	Acc@5 80.078 (80.461)
Epoch: [9][160/196]	Time 0.016 (0.015)	Data 0.011 (0.004)	Loss 2.5953 (2.5243)	Acc@1 47.266 (48.748)	Acc@5 78.516 (80.478)
Epoch: [9][170/196]	Time 0.026 (0.016)	Data 0.001 (0.004)	Loss 2.5631 (2.5221)	Acc@1 48.438 (48.899)	Acc@5 79.297 (80.512)
Epoch: [9][180/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.4502 (2.5233)	Acc@1 50.391 (48.912)	Acc@5 80.859 (80.451)
Epoch: [9][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.7371 (2.5270)	Acc@1 43.750 (48.802)	Acc@5 76.562 (80.393)
num momentum params: 26
[0.1, 2.528828989715576, 2.114456431865692, 48.794, 43.62, tensor(0.2642, device='cuda:0', grad_fn=<DivBackward0>), 3.0696356296539307, 0.45304703712463373]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [10 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [10][0/196]	Time 0.030 (0.030)	Data 0.189 (0.189)	Loss 2.4134 (2.4134)	Acc@1 52.344 (52.344)	Acc@5 82.422 (82.422)
Epoch: [10][10/196]	Time 0.019 (0.017)	Data 0.001 (0.019)	Loss 2.3914 (2.4605)	Acc@1 47.266 (49.716)	Acc@5 86.328 (81.463)
Epoch: [10][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 2.3541 (2.4439)	Acc@1 50.391 (50.539)	Acc@5 84.766 (81.734)
Epoch: [10][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.3245 (2.4247)	Acc@1 49.219 (50.731)	Acc@5 86.719 (82.220)
Epoch: [10][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 2.3160 (2.4194)	Acc@1 53.516 (50.781)	Acc@5 83.984 (82.298)
Epoch: [10][50/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.4043 (2.4104)	Acc@1 50.391 (51.164)	Acc@5 84.766 (82.583)
Epoch: [10][60/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 2.5199 (2.4098)	Acc@1 46.875 (51.262)	Acc@5 78.906 (82.499)
Epoch: [10][70/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2415 (2.4116)	Acc@1 52.344 (51.375)	Acc@5 85.938 (82.466)
Epoch: [10][80/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.4933 (2.4112)	Acc@1 49.219 (51.379)	Acc@5 79.688 (82.412)
Epoch: [10][90/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.4928 (2.4139)	Acc@1 50.391 (51.236)	Acc@5 78.516 (82.396)
Epoch: [10][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.5894 (2.4130)	Acc@1 48.047 (51.211)	Acc@5 83.594 (82.453)
Epoch: [10][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4916 (2.4223)	Acc@1 50.000 (51.112)	Acc@5 80.469 (82.264)
Epoch: [10][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.4536 (2.4196)	Acc@1 49.609 (51.311)	Acc@5 81.250 (82.312)
Epoch: [10][130/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.3960 (2.4196)	Acc@1 54.297 (51.354)	Acc@5 83.594 (82.267)
Epoch: [10][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.4210 (2.4246)	Acc@1 52.344 (51.266)	Acc@5 83.594 (82.167)
Epoch: [10][150/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.5517 (2.4313)	Acc@1 50.000 (51.193)	Acc@5 79.297 (82.096)
Epoch: [10][160/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.5548 (2.4325)	Acc@1 48.438 (51.167)	Acc@5 81.641 (82.036)
Epoch: [10][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4772 (2.4340)	Acc@1 52.344 (51.099)	Acc@5 83.594 (82.011)
Epoch: [10][180/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.5491 (2.4378)	Acc@1 48.438 (51.021)	Acc@5 78.125 (81.902)
Epoch: [10][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.5497 (2.4408)	Acc@1 49.219 (51.021)	Acc@5 82.031 (81.896)
num momentum params: 26
[0.1, 2.4425445823669434, 2.212093657255173, 50.982, 42.83, tensor(0.2752, device='cuda:0', grad_fn=<DivBackward0>), 2.8969171047210693, 0.37079548835754395]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [512, 512, 3, 3]
Before - module.bn8.weight: [512]
Before - module.bn8.bias: [512]
Before - module.fc.weight: [100, 512]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [64, 3, 3, 3]
After - module.bn1.weight: [64]
After - module.bn1.bias: [64]
After - module.conv2.weight: [128, 64, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [512, 512, 3, 3]
After - module.bn7.weight: [512]
After - module.bn7.bias: [512]
After - module.conv8.weight: [512, 512, 3, 3]
After - module.bn8.weight: [512]
After - module.bn8.bias: [512]
After - module.fc.weight: [100, 512]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375
[INFO] Storing checkpoint...

Epoch: [11 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [11][0/196]	Time 0.031 (0.031)	Data 0.180 (0.180)	Loss 2.4274 (2.4274)	Acc@1 51.562 (51.562)	Acc@5 82.812 (82.812)
Epoch: [11][10/196]	Time 0.012 (0.016)	Data 0.018 (0.020)	Loss 2.4191 (2.4271)	Acc@1 55.078 (51.882)	Acc@5 81.641 (82.564)
Epoch: [11][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 2.3333 (2.3925)	Acc@1 53.516 (51.879)	Acc@5 84.766 (83.166)
Epoch: [11][30/196]	Time 0.017 (0.015)	Data 0.000 (0.008)	Loss 2.2275 (2.3795)	Acc@1 55.469 (52.382)	Acc@5 85.156 (83.443)
Epoch: [11][40/196]	Time 0.011 (0.015)	Data 0.008 (0.007)	Loss 2.3697 (2.3710)	Acc@1 53.516 (52.534)	Acc@5 82.031 (83.451)
Epoch: [11][50/196]	Time 0.015 (0.015)	Data 0.003 (0.006)	Loss 2.1266 (2.3515)	Acc@1 57.031 (53.041)	Acc@5 89.453 (83.663)
Epoch: [11][60/196]	Time 0.013 (0.015)	Data 0.004 (0.006)	Loss 2.4205 (2.3545)	Acc@1 51.953 (52.965)	Acc@5 83.594 (83.498)
Epoch: [11][70/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.3825 (2.3604)	Acc@1 52.734 (52.899)	Acc@5 83.203 (83.363)
Epoch: [11][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.4241 (2.3609)	Acc@1 50.781 (52.797)	Acc@5 80.859 (83.377)
Epoch: [11][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.4273 (2.3621)	Acc@1 49.609 (52.867)	Acc@5 82.812 (83.392)
Epoch: [11][100/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 2.4954 (2.3658)	Acc@1 50.000 (52.847)	Acc@5 80.859 (83.296)
Epoch: [11][110/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.2953 (2.3713)	Acc@1 53.906 (52.685)	Acc@5 87.109 (83.238)
Epoch: [11][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3527 (2.3740)	Acc@1 51.172 (52.628)	Acc@5 81.250 (83.119)
Epoch: [11][130/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.4589 (2.3811)	Acc@1 46.484 (52.496)	Acc@5 83.594 (83.012)
Epoch: [11][140/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.4292 (2.3864)	Acc@1 48.828 (52.358)	Acc@5 81.250 (82.929)
Epoch: [11][150/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.3763 (2.3856)	Acc@1 47.266 (52.434)	Acc@5 85.938 (82.970)
Epoch: [11][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1515 (2.3869)	Acc@1 57.422 (52.395)	Acc@5 89.844 (83.043)
Epoch: [11][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2662 (2.3859)	Acc@1 55.078 (52.440)	Acc@5 84.766 (83.036)
Epoch: [11][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3546 (2.3857)	Acc@1 54.688 (52.497)	Acc@5 83.984 (83.035)
Epoch: [11][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.5657 (2.3887)	Acc@1 48.438 (52.421)	Acc@5 76.953 (82.935)
num momentum params: 26
[0.1, 2.390628006286621, 2.2097716557979585, 52.36, 42.69, tensor(0.2856, device='cuda:0', grad_fn=<DivBackward0>), 2.9117660522460938, 0.3695645332336426]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [12 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [12][0/196]	Time 0.031 (0.031)	Data 0.181 (0.181)	Loss 2.2760 (2.2760)	Acc@1 55.469 (55.469)	Acc@5 83.594 (83.594)
Epoch: [12][10/196]	Time 0.017 (0.017)	Data 0.000 (0.019)	Loss 2.1554 (2.3736)	Acc@1 56.250 (52.770)	Acc@5 88.281 (83.274)
Epoch: [12][20/196]	Time 0.016 (0.016)	Data 0.003 (0.011)	Loss 2.4026 (2.3483)	Acc@1 52.344 (53.311)	Acc@5 84.766 (84.003)
Epoch: [12][30/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 2.1772 (2.3351)	Acc@1 57.422 (53.642)	Acc@5 86.328 (84.577)
Epoch: [12][40/196]	Time 0.015 (0.015)	Data 0.002 (0.007)	Loss 2.4764 (2.3348)	Acc@1 51.172 (53.859)	Acc@5 82.422 (84.404)
Epoch: [12][50/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 2.2895 (2.3486)	Acc@1 58.984 (53.945)	Acc@5 82.422 (84.084)
Epoch: [12][60/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.3097 (2.3403)	Acc@1 55.469 (53.919)	Acc@5 83.203 (84.157)
Epoch: [12][70/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.3039 (2.3391)	Acc@1 56.250 (53.945)	Acc@5 84.375 (84.111)
Epoch: [12][80/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.3674 (2.3426)	Acc@1 53.516 (53.863)	Acc@5 81.250 (83.984)
Epoch: [12][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.3319 (2.3423)	Acc@1 52.734 (53.808)	Acc@5 84.766 (83.916)
Epoch: [12][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2915 (2.3465)	Acc@1 54.688 (53.748)	Acc@5 85.938 (83.818)
Epoch: [12][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3169 (2.3472)	Acc@1 51.172 (53.737)	Acc@5 86.328 (83.805)
Epoch: [12][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.3778 (2.3478)	Acc@1 51.172 (53.683)	Acc@5 82.422 (83.820)
Epoch: [12][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1677 (2.3465)	Acc@1 58.203 (53.769)	Acc@5 83.984 (83.805)
Epoch: [12][140/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.2793 (2.3477)	Acc@1 57.812 (53.859)	Acc@5 88.672 (83.766)
Epoch: [12][150/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.3937 (2.3513)	Acc@1 53.906 (53.818)	Acc@5 83.594 (83.700)
Epoch: [12][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3272 (2.3561)	Acc@1 51.953 (53.625)	Acc@5 83.594 (83.618)
Epoch: [12][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.3628 (2.3557)	Acc@1 53.906 (53.628)	Acc@5 82.422 (83.621)
Epoch: [12][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.3251 (2.3572)	Acc@1 53.125 (53.608)	Acc@5 87.109 (83.583)
Epoch: [12][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3677 (2.3586)	Acc@1 55.078 (53.612)	Acc@5 84.766 (83.551)
num momentum params: 26
[0.1, 2.359405549697876, 2.1931312084198, 53.57, 43.12, tensor(0.2956, device='cuda:0', grad_fn=<DivBackward0>), 2.906764507293701, 0.36849498748779297]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [13 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [13][0/196]	Time 0.030 (0.030)	Data 0.184 (0.184)	Loss 2.2961 (2.2961)	Acc@1 56.641 (56.641)	Acc@5 83.203 (83.203)
Epoch: [13][10/196]	Time 0.018 (0.017)	Data 0.001 (0.019)	Loss 2.2460 (2.2767)	Acc@1 59.375 (55.753)	Acc@5 85.938 (84.837)
Epoch: [13][20/196]	Time 0.011 (0.016)	Data 0.007 (0.011)	Loss 2.1300 (2.2590)	Acc@1 57.422 (55.859)	Acc@5 85.547 (85.751)
Epoch: [13][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.2458 (2.2601)	Acc@1 56.641 (56.187)	Acc@5 85.938 (85.635)
Epoch: [13][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 2.3989 (2.2777)	Acc@1 50.781 (55.393)	Acc@5 85.938 (85.709)
Epoch: [13][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.4253 (2.2752)	Acc@1 51.172 (55.400)	Acc@5 87.500 (85.815)
Epoch: [13][60/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.2228 (2.2797)	Acc@1 58.594 (55.289)	Acc@5 83.594 (85.669)
Epoch: [13][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.2865 (2.2797)	Acc@1 55.859 (55.309)	Acc@5 85.156 (85.679)
Epoch: [13][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.3323 (2.2897)	Acc@1 52.344 (55.160)	Acc@5 83.594 (85.421)
Epoch: [13][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.3519 (2.3025)	Acc@1 53.906 (54.954)	Acc@5 84.766 (85.139)
Epoch: [13][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1756 (2.3021)	Acc@1 57.812 (55.036)	Acc@5 87.500 (85.052)
Epoch: [13][110/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2158 (2.3016)	Acc@1 54.688 (55.050)	Acc@5 88.672 (85.037)
Epoch: [13][120/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.2748 (2.3059)	Acc@1 54.688 (55.004)	Acc@5 84.766 (84.998)
Epoch: [13][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.5075 (2.3074)	Acc@1 52.344 (55.054)	Acc@5 81.641 (85.022)
Epoch: [13][140/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.5846 (2.3119)	Acc@1 44.531 (54.962)	Acc@5 82.422 (84.951)
Epoch: [13][150/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1809 (2.3141)	Acc@1 57.031 (54.962)	Acc@5 89.844 (84.898)
Epoch: [13][160/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.4952 (2.3173)	Acc@1 48.438 (54.819)	Acc@5 82.422 (84.841)
Epoch: [13][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.6101 (2.3235)	Acc@1 47.656 (54.713)	Acc@5 80.469 (84.718)
Epoch: [13][180/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.4161 (2.3241)	Acc@1 52.734 (54.716)	Acc@5 84.766 (84.727)
Epoch: [13][190/196]	Time 0.013 (0.015)	Data 0.006 (0.004)	Loss 2.5464 (2.3238)	Acc@1 50.391 (54.743)	Acc@5 83.984 (84.719)
num momentum params: 26
[0.1, 2.3259951097869873, 1.9932167375087737, 54.686, 47.7, tensor(0.3073, device='cuda:0', grad_fn=<DivBackward0>), 2.8893558979034424, 0.37319326400756836]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [14 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [14][0/196]	Time 0.034 (0.034)	Data 0.173 (0.173)	Loss 2.1400 (2.1400)	Acc@1 59.766 (59.766)	Acc@5 84.766 (84.766)
Epoch: [14][10/196]	Time 0.016 (0.016)	Data 0.000 (0.018)	Loss 2.2290 (2.2246)	Acc@1 54.688 (56.428)	Acc@5 85.938 (86.293)
Epoch: [14][20/196]	Time 0.011 (0.016)	Data 0.009 (0.011)	Loss 2.2950 (2.2201)	Acc@1 53.906 (57.199)	Acc@5 84.766 (86.440)
Epoch: [14][30/196]	Time 0.016 (0.016)	Data 0.000 (0.008)	Loss 2.2551 (2.2282)	Acc@1 58.203 (57.107)	Acc@5 85.547 (86.164)
Epoch: [14][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 2.3931 (2.2619)	Acc@1 52.344 (56.383)	Acc@5 84.375 (85.633)
Epoch: [14][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.2214 (2.2649)	Acc@1 58.594 (56.204)	Acc@5 85.156 (85.685)
Epoch: [14][60/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.1606 (2.2611)	Acc@1 58.203 (56.384)	Acc@5 87.109 (85.784)
Epoch: [14][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.4137 (2.2710)	Acc@1 53.516 (56.333)	Acc@5 82.812 (85.536)
Epoch: [14][80/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.1256 (2.2771)	Acc@1 59.375 (56.187)	Acc@5 87.500 (85.402)
Epoch: [14][90/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1737 (2.2822)	Acc@1 60.547 (56.065)	Acc@5 87.109 (85.379)
Epoch: [14][100/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.3690 (2.2851)	Acc@1 55.078 (56.049)	Acc@5 84.766 (85.326)
Epoch: [14][110/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0402 (2.2841)	Acc@1 63.281 (56.067)	Acc@5 91.016 (85.353)
Epoch: [14][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.3646 (2.2876)	Acc@1 55.469 (56.072)	Acc@5 82.422 (85.308)
Epoch: [14][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2697 (2.2891)	Acc@1 55.469 (56.044)	Acc@5 87.500 (85.273)
Epoch: [14][140/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.4673 (2.2969)	Acc@1 51.562 (55.901)	Acc@5 83.984 (85.156)
Epoch: [14][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1571 (2.2982)	Acc@1 59.375 (55.844)	Acc@5 87.891 (85.151)
Epoch: [14][160/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.3028 (2.2997)	Acc@1 57.422 (55.852)	Acc@5 85.156 (85.151)
Epoch: [14][170/196]	Time 0.020 (0.015)	Data 0.001 (0.004)	Loss 2.2906 (2.3045)	Acc@1 51.953 (55.697)	Acc@5 88.281 (85.113)
Epoch: [14][180/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.3910 (2.3081)	Acc@1 55.469 (55.609)	Acc@5 84.375 (85.098)
Epoch: [14][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.4565 (2.3106)	Acc@1 51.562 (55.551)	Acc@5 84.766 (85.050)
num momentum params: 26
[0.1, 2.310916075134277, 2.3711742150783537, 55.538, 41.89, tensor(0.3172, device='cuda:0', grad_fn=<DivBackward0>), 2.979670286178589, 0.3735475540161133]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [15 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [15][0/196]	Time 0.030 (0.030)	Data 0.173 (0.173)	Loss 2.2244 (2.2244)	Acc@1 58.594 (58.594)	Acc@5 85.938 (85.938)
Epoch: [15][10/196]	Time 0.015 (0.016)	Data 0.002 (0.018)	Loss 2.2920 (2.2660)	Acc@1 55.859 (57.528)	Acc@5 85.938 (86.009)
Epoch: [15][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 2.3277 (2.2772)	Acc@1 51.953 (56.808)	Acc@5 86.719 (86.068)
Epoch: [15][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 2.2137 (2.2722)	Acc@1 56.641 (56.918)	Acc@5 87.891 (86.341)
Epoch: [15][40/196]	Time 0.011 (0.015)	Data 0.006 (0.007)	Loss 2.4444 (2.2608)	Acc@1 53.516 (57.193)	Acc@5 81.250 (86.328)
Epoch: [15][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.2814 (2.2624)	Acc@1 57.422 (57.238)	Acc@5 89.844 (86.389)
Epoch: [15][60/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.2473 (2.2576)	Acc@1 55.078 (57.313)	Acc@5 88.672 (86.456)
Epoch: [15][70/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.2725 (2.2534)	Acc@1 58.984 (57.537)	Acc@5 84.375 (86.543)
Epoch: [15][80/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.2817 (2.2595)	Acc@1 57.812 (57.436)	Acc@5 84.766 (86.405)
Epoch: [15][90/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.4173 (2.2624)	Acc@1 55.859 (57.353)	Acc@5 84.375 (86.337)
Epoch: [15][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1662 (2.2596)	Acc@1 59.766 (57.391)	Acc@5 86.719 (86.378)
Epoch: [15][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.4385 (2.2621)	Acc@1 53.906 (57.401)	Acc@5 82.031 (86.296)
Epoch: [15][120/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.3298 (2.2637)	Acc@1 57.422 (57.357)	Acc@5 83.594 (86.199)
Epoch: [15][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3413 (2.2674)	Acc@1 54.688 (57.228)	Acc@5 83.984 (86.110)
Epoch: [15][140/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.1195 (2.2703)	Acc@1 61.719 (57.189)	Acc@5 87.109 (86.023)
Epoch: [15][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4136 (2.2735)	Acc@1 57.031 (57.047)	Acc@5 82.422 (85.976)
Epoch: [15][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.3769 (2.2781)	Acc@1 54.688 (56.988)	Acc@5 85.547 (85.952)
Epoch: [15][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1115 (2.2776)	Acc@1 61.328 (56.995)	Acc@5 88.281 (85.965)
Epoch: [15][180/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.2029 (2.2808)	Acc@1 59.375 (56.928)	Acc@5 87.109 (85.920)
Epoch: [15][190/196]	Time 0.017 (0.015)	Data 0.000 (0.003)	Loss 2.3193 (2.2843)	Acc@1 52.734 (56.839)	Acc@5 86.719 (85.854)
num momentum params: 26
[0.1, 2.2850934507751464, 1.9367580473423005, 56.806, 49.38, tensor(0.3289, device='cuda:0', grad_fn=<DivBackward0>), 2.9853570461273193, 0.37793564796447754]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [16 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [16][0/196]	Time 0.035 (0.035)	Data 0.186 (0.186)	Loss 2.1744 (2.1744)	Acc@1 58.203 (58.203)	Acc@5 88.281 (88.281)
Epoch: [16][10/196]	Time 0.015 (0.017)	Data 0.002 (0.019)	Loss 2.2097 (2.2078)	Acc@1 57.812 (58.026)	Acc@5 87.500 (86.932)
Epoch: [16][20/196]	Time 0.012 (0.016)	Data 0.008 (0.011)	Loss 2.3314 (2.1987)	Acc@1 53.516 (58.371)	Acc@5 85.547 (87.240)
Epoch: [16][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.1817 (2.2093)	Acc@1 57.031 (58.317)	Acc@5 88.281 (87.059)
Epoch: [16][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.0743 (2.2066)	Acc@1 64.844 (58.375)	Acc@5 87.500 (87.119)
Epoch: [16][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.1905 (2.2171)	Acc@1 58.594 (58.310)	Acc@5 86.719 (86.972)
Epoch: [16][60/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.3155 (2.2321)	Acc@1 58.594 (58.069)	Acc@5 85.938 (86.764)
Epoch: [16][70/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.2931 (2.2368)	Acc@1 55.469 (57.972)	Acc@5 85.156 (86.691)
Epoch: [16][80/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.3614 (2.2402)	Acc@1 54.297 (57.856)	Acc@5 85.156 (86.622)
Epoch: [16][90/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1430 (2.2416)	Acc@1 57.422 (57.898)	Acc@5 89.062 (86.706)
Epoch: [16][100/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.3816 (2.2483)	Acc@1 58.984 (57.805)	Acc@5 83.203 (86.560)
Epoch: [16][110/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1463 (2.2508)	Acc@1 62.109 (57.753)	Acc@5 87.500 (86.539)
Epoch: [16][120/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.2797 (2.2524)	Acc@1 57.812 (57.667)	Acc@5 85.547 (86.512)
Epoch: [16][130/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.3377 (2.2527)	Acc@1 56.250 (57.708)	Acc@5 82.422 (86.525)
Epoch: [16][140/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.3078 (2.2543)	Acc@1 56.250 (57.635)	Acc@5 86.328 (86.519)
Epoch: [16][150/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.1619 (2.2564)	Acc@1 60.156 (57.606)	Acc@5 88.281 (86.520)
Epoch: [16][160/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2651 (2.2613)	Acc@1 60.156 (57.536)	Acc@5 85.547 (86.483)
Epoch: [16][170/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.2163 (2.2618)	Acc@1 57.422 (57.532)	Acc@5 86.719 (86.483)
Epoch: [16][180/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.2054 (2.2632)	Acc@1 61.719 (57.523)	Acc@5 87.891 (86.475)
Epoch: [16][190/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.0994 (2.2654)	Acc@1 60.547 (57.502)	Acc@5 90.625 (86.457)
num momentum params: 26
[0.1, 2.263817858734131, 2.12288537979126, 57.558, 46.13, tensor(0.3396, device='cuda:0', grad_fn=<DivBackward0>), 2.9590373039245605, 0.3801708221435547]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [17 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [17][0/196]	Time 0.032 (0.032)	Data 0.171 (0.171)	Loss 2.1096 (2.1096)	Acc@1 62.891 (62.891)	Acc@5 85.156 (85.156)
Epoch: [17][10/196]	Time 0.016 (0.016)	Data 0.001 (0.018)	Loss 2.1650 (2.1709)	Acc@1 62.109 (60.795)	Acc@5 89.062 (88.352)
Epoch: [17][20/196]	Time 0.015 (0.016)	Data 0.003 (0.011)	Loss 2.2466 (2.1942)	Acc@1 55.469 (60.398)	Acc@5 87.891 (87.630)
Epoch: [17][30/196]	Time 0.017 (0.015)	Data 0.001 (0.008)	Loss 2.2415 (2.2087)	Acc@1 58.594 (60.093)	Acc@5 87.891 (87.462)
Epoch: [17][40/196]	Time 0.012 (0.015)	Data 0.008 (0.007)	Loss 2.0577 (2.1908)	Acc@1 66.016 (60.547)	Acc@5 88.672 (87.719)
Epoch: [17][50/196]	Time 0.017 (0.015)	Data 0.000 (0.006)	Loss 2.0970 (2.1816)	Acc@1 60.547 (60.639)	Acc@5 89.453 (87.860)
Epoch: [17][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.2896 (2.1924)	Acc@1 54.688 (60.182)	Acc@5 84.375 (87.769)
Epoch: [17][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.3540 (2.1994)	Acc@1 60.156 (59.953)	Acc@5 84.375 (87.649)
Epoch: [17][80/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.1835 (2.2075)	Acc@1 61.328 (59.775)	Acc@5 87.109 (87.524)
Epoch: [17][90/196]	Time 0.011 (0.015)	Data 0.014 (0.005)	Loss 2.0576 (2.2111)	Acc@1 66.016 (59.744)	Acc@5 91.016 (87.534)
Epoch: [17][100/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.4022 (2.2159)	Acc@1 55.859 (59.634)	Acc@5 86.328 (87.481)
Epoch: [17][110/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.1959 (2.2219)	Acc@1 60.547 (59.540)	Acc@5 87.891 (87.359)
Epoch: [17][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.3876 (2.2237)	Acc@1 57.812 (59.417)	Acc@5 84.766 (87.377)
Epoch: [17][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2834 (2.2261)	Acc@1 57.031 (59.378)	Acc@5 84.766 (87.300)
Epoch: [17][140/196]	Time 0.012 (0.015)	Data 0.009 (0.004)	Loss 2.3047 (2.2299)	Acc@1 55.078 (59.253)	Acc@5 85.547 (87.181)
Epoch: [17][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3700 (2.2339)	Acc@1 56.250 (59.111)	Acc@5 83.203 (87.135)
Epoch: [17][160/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.2280 (2.2379)	Acc@1 58.984 (59.052)	Acc@5 88.281 (87.095)
Epoch: [17][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2771 (2.2431)	Acc@1 61.328 (58.934)	Acc@5 84.375 (86.997)
Epoch: [17][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.3610 (2.2469)	Acc@1 57.031 (58.823)	Acc@5 82.422 (86.922)
Epoch: [17][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2430 (2.2507)	Acc@1 56.641 (58.733)	Acc@5 88.672 (86.882)
num momentum params: 26
[0.1, 2.250932565536499, 2.115626609325409, 58.726, 46.11, tensor(0.3488, device='cuda:0', grad_fn=<DivBackward0>), 2.982462167739868, 0.3832833766937256]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [18 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [18][0/196]	Time 0.033 (0.033)	Data 0.178 (0.178)	Loss 2.2476 (2.2476)	Acc@1 55.469 (55.469)	Acc@5 87.500 (87.500)
Epoch: [18][10/196]	Time 0.017 (0.016)	Data 0.000 (0.018)	Loss 2.1638 (2.2226)	Acc@1 66.016 (59.553)	Acc@5 88.672 (87.500)
Epoch: [18][20/196]	Time 0.011 (0.016)	Data 0.007 (0.011)	Loss 2.1975 (2.2019)	Acc@1 58.594 (59.431)	Acc@5 89.453 (87.798)
Epoch: [18][30/196]	Time 0.017 (0.015)	Data 0.001 (0.008)	Loss 2.3321 (2.1854)	Acc@1 57.812 (59.803)	Acc@5 86.719 (88.256)
Epoch: [18][40/196]	Time 0.011 (0.015)	Data 0.018 (0.007)	Loss 2.1093 (2.1763)	Acc@1 62.891 (60.375)	Acc@5 89.062 (88.319)
Epoch: [18][50/196]	Time 0.017 (0.015)	Data 0.001 (0.007)	Loss 2.3018 (2.1818)	Acc@1 59.766 (60.371)	Acc@5 85.156 (88.166)
Epoch: [18][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.2698 (2.1873)	Acc@1 58.984 (60.041)	Acc@5 85.938 (88.115)
Epoch: [18][70/196]	Time 0.017 (0.015)	Data 0.001 (0.006)	Loss 2.2846 (2.1969)	Acc@1 56.641 (59.782)	Acc@5 87.500 (87.885)
Epoch: [18][80/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.1423 (2.2021)	Acc@1 63.281 (59.809)	Acc@5 87.500 (87.823)
Epoch: [18][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2256 (2.2038)	Acc@1 58.203 (59.860)	Acc@5 87.891 (87.800)
Epoch: [18][100/196]	Time 0.020 (0.015)	Data 0.009 (0.005)	Loss 2.1625 (2.2038)	Acc@1 59.766 (59.816)	Acc@5 88.281 (87.817)
Epoch: [18][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3154 (2.2062)	Acc@1 53.516 (59.695)	Acc@5 87.891 (87.820)
Epoch: [18][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.4958 (2.2127)	Acc@1 54.297 (59.511)	Acc@5 83.594 (87.726)
Epoch: [18][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2123 (2.2184)	Acc@1 60.156 (59.342)	Acc@5 87.500 (87.670)
Epoch: [18][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1442 (2.2180)	Acc@1 62.500 (59.292)	Acc@5 87.891 (87.686)
Epoch: [18][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1098 (2.2247)	Acc@1 60.938 (59.165)	Acc@5 88.281 (87.614)
Epoch: [18][160/196]	Time 0.013 (0.015)	Data 0.006 (0.004)	Loss 2.1218 (2.2279)	Acc@1 64.062 (59.137)	Acc@5 86.719 (87.529)
Epoch: [18][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2505 (2.2269)	Acc@1 56.641 (59.213)	Acc@5 86.719 (87.541)
Epoch: [18][180/196]	Time 0.012 (0.016)	Data 0.010 (0.004)	Loss 2.2402 (2.2287)	Acc@1 58.594 (59.187)	Acc@5 86.328 (87.522)
Epoch: [18][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.4708 (2.2336)	Acc@1 54.688 (59.109)	Acc@5 85.547 (87.502)
num momentum params: 26
[0.1, 2.2347805784606933, 1.8829645383358002, 59.086, 50.11, tensor(0.3582, device='cuda:0', grad_fn=<DivBackward0>), 3.0463507175445557, 0.37398838996887207]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [19 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [19][0/196]	Time 0.034 (0.034)	Data 0.171 (0.171)	Loss 2.0705 (2.0705)	Acc@1 62.109 (62.109)	Acc@5 91.016 (91.016)
Epoch: [19][10/196]	Time 0.016 (0.016)	Data 0.001 (0.018)	Loss 2.1580 (2.2156)	Acc@1 62.109 (59.766)	Acc@5 89.062 (88.388)
Epoch: [19][20/196]	Time 0.014 (0.016)	Data 0.003 (0.011)	Loss 1.9574 (2.2011)	Acc@1 65.234 (60.026)	Acc@5 89.844 (88.318)
Epoch: [19][30/196]	Time 0.016 (0.015)	Data 0.000 (0.008)	Loss 2.1020 (2.1760)	Acc@1 63.281 (60.811)	Acc@5 89.062 (88.773)
Epoch: [19][40/196]	Time 0.011 (0.015)	Data 0.007 (0.007)	Loss 2.1313 (2.1607)	Acc@1 60.156 (61.309)	Acc@5 89.844 (89.015)
Epoch: [19][50/196]	Time 0.014 (0.015)	Data 0.004 (0.006)	Loss 2.1647 (2.1655)	Acc@1 60.156 (61.144)	Acc@5 90.625 (88.940)
Epoch: [19][60/196]	Time 0.011 (0.015)	Data 0.012 (0.006)	Loss 2.1710 (2.1619)	Acc@1 63.672 (61.405)	Acc@5 87.891 (88.960)
Epoch: [19][70/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.2658 (2.1653)	Acc@1 60.547 (61.394)	Acc@5 88.281 (88.870)
Epoch: [19][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.3597 (2.1685)	Acc@1 58.984 (61.429)	Acc@5 84.766 (88.841)
Epoch: [19][90/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.2121 (2.1787)	Acc@1 59.766 (61.131)	Acc@5 88.672 (88.672)
Epoch: [19][100/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1980 (2.1830)	Acc@1 58.984 (60.976)	Acc@5 88.281 (88.571)
Epoch: [19][110/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.1111 (2.1864)	Acc@1 65.625 (60.860)	Acc@5 91.016 (88.563)
Epoch: [19][120/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1871 (2.1923)	Acc@1 60.156 (60.753)	Acc@5 87.500 (88.407)
Epoch: [19][130/196]	Time 0.011 (0.014)	Data 0.008 (0.005)	Loss 2.3635 (2.1952)	Acc@1 55.469 (60.639)	Acc@5 85.547 (88.341)
Epoch: [19][140/196]	Time 0.013 (0.014)	Data 0.003 (0.005)	Loss 2.3294 (2.2012)	Acc@1 57.812 (60.436)	Acc@5 86.328 (88.218)
Epoch: [19][150/196]	Time 0.012 (0.014)	Data 0.008 (0.005)	Loss 2.2383 (2.2037)	Acc@1 58.203 (60.423)	Acc@5 89.062 (88.152)
Epoch: [19][160/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 2.1869 (2.2048)	Acc@1 59.766 (60.411)	Acc@5 89.062 (88.133)
Epoch: [19][170/196]	Time 0.012 (0.014)	Data 0.007 (0.005)	Loss 2.2456 (2.2056)	Acc@1 58.203 (60.305)	Acc@5 87.109 (88.089)
Epoch: [19][180/196]	Time 0.015 (0.014)	Data 0.002 (0.004)	Loss 2.3868 (2.2088)	Acc@1 56.250 (60.258)	Acc@5 85.156 (88.050)
Epoch: [19][190/196]	Time 0.011 (0.014)	Data 0.007 (0.004)	Loss 2.1661 (2.2135)	Acc@1 60.547 (60.169)	Acc@5 88.281 (88.019)
num momentum params: 26
[0.1, 2.217390535583496, 2.2427455163002015, 60.1, 43.69, tensor(0.3670, device='cuda:0', grad_fn=<DivBackward0>), 2.812757968902588, 0.3759336471557617]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [20 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [20][0/196]	Time 0.037 (0.037)	Data 0.179 (0.179)	Loss 2.0471 (2.0471)	Acc@1 66.016 (66.016)	Acc@5 88.281 (88.281)
Epoch: [20][10/196]	Time 0.018 (0.018)	Data 0.001 (0.018)	Loss 2.2503 (2.1611)	Acc@1 58.203 (62.322)	Acc@5 87.109 (88.139)
Epoch: [20][20/196]	Time 0.013 (0.016)	Data 0.006 (0.011)	Loss 2.2356 (2.1697)	Acc@1 62.891 (61.868)	Acc@5 83.594 (88.300)
Epoch: [20][30/196]	Time 0.018 (0.016)	Data 0.000 (0.008)	Loss 2.1482 (2.1508)	Acc@1 58.203 (62.172)	Acc@5 90.234 (88.710)
Epoch: [20][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 1.9605 (2.1582)	Acc@1 64.453 (61.928)	Acc@5 92.188 (88.748)
Epoch: [20][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.1564 (2.1471)	Acc@1 61.328 (62.331)	Acc@5 87.500 (88.925)
Epoch: [20][60/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.3850 (2.1502)	Acc@1 54.297 (62.129)	Acc@5 84.375 (88.877)
Epoch: [20][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1844 (2.1563)	Acc@1 63.281 (61.911)	Acc@5 89.844 (88.919)
Epoch: [20][80/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 2.2148 (2.1618)	Acc@1 58.594 (61.724)	Acc@5 90.625 (88.966)
Epoch: [20][90/196]	Time 0.019 (0.016)	Data 0.000 (0.004)	Loss 2.0097 (2.1655)	Acc@1 61.719 (61.581)	Acc@5 93.750 (88.977)
Epoch: [20][100/196]	Time 0.013 (0.016)	Data 0.007 (0.004)	Loss 2.5148 (2.1718)	Acc@1 54.688 (61.502)	Acc@5 82.422 (88.819)
Epoch: [20][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2685 (2.1759)	Acc@1 57.812 (61.437)	Acc@5 89.844 (88.763)
Epoch: [20][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.2557 (2.1827)	Acc@1 58.984 (61.225)	Acc@5 87.109 (88.711)
Epoch: [20][130/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1426 (2.1859)	Acc@1 58.594 (61.075)	Acc@5 88.672 (88.678)
Epoch: [20][140/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.1714 (2.1896)	Acc@1 59.766 (60.935)	Acc@5 88.281 (88.622)
Epoch: [20][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4052 (2.1948)	Acc@1 54.297 (60.790)	Acc@5 85.547 (88.524)
Epoch: [20][160/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.3363 (2.2012)	Acc@1 60.938 (60.700)	Acc@5 84.766 (88.383)
Epoch: [20][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2510 (2.2044)	Acc@1 56.641 (60.654)	Acc@5 88.672 (88.322)
Epoch: [20][180/196]	Time 0.012 (0.015)	Data 0.007 (0.003)	Loss 2.2370 (2.2087)	Acc@1 59.375 (60.538)	Acc@5 89.844 (88.249)
Epoch: [20][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.2342 (2.2120)	Acc@1 62.109 (60.504)	Acc@5 85.547 (88.156)
num momentum params: 26
[0.1, 2.2128006583404543, 2.024786604642868, 60.478, 47.29, tensor(0.3739, device='cuda:0', grad_fn=<DivBackward0>), 3.0172462463378906, 0.37368297576904297]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [512, 512, 3, 3]
Before - module.bn8.weight: [512]
Before - module.bn8.bias: [512]
Before - module.fc.weight: [100, 512]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [64, 3, 3, 3]
After - module.bn1.weight: [64]
After - module.bn1.bias: [64]
After - module.conv2.weight: [128, 64, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [512, 512, 3, 3]
After - module.bn7.weight: [512]
After - module.bn7.bias: [512]
After - module.conv8.weight: [512, 512, 3, 3]
After - module.bn8.weight: [512]
After - module.bn8.bias: [512]
After - module.fc.weight: [100, 512]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [64, 3, 3, 3]
conv2 --> [128, 64, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [512, 512, 3, 3]
fc --> [512, 100]
1, 708673536, 1769472, 64
2, 7889485824, 18874368, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7247757312, 9437184, 512
fc, 19660800, 51200, 0
===================
FLOP REPORT: 31147046400000.0 60620800000.0 152815616 151552 2752 17.685302734375
[INFO] Storing checkpoint...

Epoch: [21 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [21][0/196]	Time 0.036 (0.036)	Data 0.168 (0.168)	Loss 1.9947 (1.9947)	Acc@1 65.625 (65.625)	Acc@5 92.969 (92.969)
Epoch: [21][10/196]	Time 0.018 (0.017)	Data 0.001 (0.018)	Loss 2.1655 (2.1576)	Acc@1 60.156 (60.298)	Acc@5 88.281 (90.128)
Epoch: [21][20/196]	Time 0.012 (0.016)	Data 0.008 (0.011)	Loss 2.2400 (2.1608)	Acc@1 61.328 (61.291)	Acc@5 88.672 (89.565)
Epoch: [21][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0668 (2.1424)	Acc@1 66.797 (61.845)	Acc@5 89.062 (89.693)
Epoch: [21][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 2.0304 (2.1223)	Acc@1 67.969 (62.767)	Acc@5 89.453 (89.920)
Epoch: [21][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0356 (2.1243)	Acc@1 70.703 (62.722)	Acc@5 89.062 (89.805)
Epoch: [21][60/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.2787 (2.1317)	Acc@1 58.203 (62.660)	Acc@5 86.328 (89.703)
Epoch: [21][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2092 (2.1317)	Acc@1 60.547 (62.698)	Acc@5 89.062 (89.739)
Epoch: [21][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9741 (2.1331)	Acc@1 69.531 (62.698)	Acc@5 94.141 (89.733)
Epoch: [21][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.4211 (2.1415)	Acc@1 51.172 (62.470)	Acc@5 85.156 (89.582)
Epoch: [21][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1099 (2.1518)	Acc@1 65.234 (62.322)	Acc@5 88.281 (89.387)
Epoch: [21][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2690 (2.1596)	Acc@1 62.891 (62.060)	Acc@5 85.938 (89.228)
Epoch: [21][120/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.2894 (2.1644)	Acc@1 55.859 (61.912)	Acc@5 89.844 (89.179)
Epoch: [21][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.2096 (2.1691)	Acc@1 58.984 (61.853)	Acc@5 89.062 (89.116)
Epoch: [21][140/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.2478 (2.1753)	Acc@1 58.594 (61.705)	Acc@5 87.891 (88.977)
Epoch: [21][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 1.9833 (2.1790)	Acc@1 65.234 (61.706)	Acc@5 92.578 (88.866)
Epoch: [21][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.4055 (2.1833)	Acc@1 51.172 (61.515)	Acc@5 85.938 (88.803)
Epoch: [21][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1239 (2.1869)	Acc@1 62.109 (61.463)	Acc@5 89.844 (88.759)
Epoch: [21][180/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.2553 (2.1883)	Acc@1 60.938 (61.402)	Acc@5 85.156 (88.730)
Epoch: [21][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2480 (2.1907)	Acc@1 61.328 (61.357)	Acc@5 86.328 (88.690)
num momentum params: 26
[0.1, 2.194326447753906, 1.8094927167892456, 61.304, 51.21, tensor(0.3827, device='cuda:0', grad_fn=<DivBackward0>), 2.9952902793884277, 0.3954172134399414]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [22 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [22][0/196]	Time 0.038 (0.038)	Data 0.187 (0.187)	Loss 2.0812 (2.0812)	Acc@1 61.328 (61.328)	Acc@5 89.062 (89.062)
Epoch: [22][10/196]	Time 0.016 (0.017)	Data 0.003 (0.019)	Loss 2.1297 (2.1524)	Acc@1 65.625 (62.287)	Acc@5 90.234 (89.773)
Epoch: [22][20/196]	Time 0.015 (0.016)	Data 0.002 (0.011)	Loss 2.1385 (2.1348)	Acc@1 61.719 (62.537)	Acc@5 88.281 (89.769)
Epoch: [22][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 2.0614 (2.1076)	Acc@1 64.062 (63.243)	Acc@5 90.625 (89.982)
Epoch: [22][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 2.2728 (2.1130)	Acc@1 58.984 (63.348)	Acc@5 90.234 (90.044)
Epoch: [22][50/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 2.1716 (2.1063)	Acc@1 62.891 (63.312)	Acc@5 86.719 (90.204)
Epoch: [22][60/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.2653 (2.1185)	Acc@1 61.719 (63.102)	Acc@5 87.109 (89.997)
Epoch: [22][70/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 2.2087 (2.1287)	Acc@1 61.328 (62.819)	Acc@5 86.328 (89.750)
Epoch: [22][80/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0939 (2.1308)	Acc@1 64.062 (62.809)	Acc@5 91.016 (89.709)
Epoch: [22][90/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9826 (2.1295)	Acc@1 67.578 (62.839)	Acc@5 92.578 (89.702)
Epoch: [22][100/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0587 (2.1379)	Acc@1 66.797 (62.682)	Acc@5 90.234 (89.619)
Epoch: [22][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1008 (2.1450)	Acc@1 64.453 (62.496)	Acc@5 89.844 (89.502)
Epoch: [22][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1626 (2.1509)	Acc@1 59.766 (62.413)	Acc@5 88.281 (89.389)
Epoch: [22][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1537 (2.1558)	Acc@1 61.328 (62.241)	Acc@5 89.062 (89.322)
Epoch: [22][140/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.4566 (2.1635)	Acc@1 55.078 (62.090)	Acc@5 83.984 (89.168)
Epoch: [22][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2169 (2.1668)	Acc@1 58.594 (62.047)	Acc@5 88.672 (89.135)
Epoch: [22][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1464 (2.1735)	Acc@1 64.062 (61.898)	Acc@5 88.672 (89.065)
Epoch: [22][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0922 (2.1742)	Acc@1 65.234 (61.908)	Acc@5 91.016 (89.042)
Epoch: [22][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.3563 (2.1774)	Acc@1 57.422 (61.816)	Acc@5 85.938 (89.006)
Epoch: [22][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2794 (2.1828)	Acc@1 60.547 (61.706)	Acc@5 86.719 (88.932)
num momentum params: 26
[0.1, 2.1855485809326174, 1.839332138299942, 61.684, 51.46, tensor(0.3892, device='cuda:0', grad_fn=<DivBackward0>), 2.9362380504608154, 0.37613558769226074]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [23 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [23][0/196]	Time 0.037 (0.037)	Data 0.171 (0.171)	Loss 2.0233 (2.0233)	Acc@1 66.406 (66.406)	Acc@5 90.234 (90.234)
Epoch: [23][10/196]	Time 0.016 (0.017)	Data 0.002 (0.018)	Loss 2.2395 (2.1166)	Acc@1 61.719 (63.849)	Acc@5 89.062 (90.447)
Epoch: [23][20/196]	Time 0.013 (0.016)	Data 0.004 (0.010)	Loss 2.0884 (2.1233)	Acc@1 65.625 (63.914)	Acc@5 90.234 (90.123)
Epoch: [23][30/196]	Time 0.013 (0.016)	Data 0.005 (0.008)	Loss 2.1719 (2.1233)	Acc@1 62.109 (63.697)	Acc@5 91.797 (90.146)
Epoch: [23][40/196]	Time 0.011 (0.016)	Data 0.006 (0.007)	Loss 2.2108 (2.1130)	Acc@1 62.500 (64.072)	Acc@5 87.891 (90.168)
Epoch: [23][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.2182 (2.1099)	Acc@1 61.719 (64.170)	Acc@5 85.547 (90.188)
Epoch: [23][60/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.1343 (2.1037)	Acc@1 64.453 (64.203)	Acc@5 87.500 (90.177)
Epoch: [23][70/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1964 (2.1138)	Acc@1 61.328 (63.881)	Acc@5 88.672 (90.009)
Epoch: [23][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1704 (2.1231)	Acc@1 64.453 (63.628)	Acc@5 88.281 (89.906)
Epoch: [23][90/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.1991 (2.1290)	Acc@1 62.109 (63.487)	Acc@5 87.500 (89.775)
Epoch: [23][100/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.2670 (2.1356)	Acc@1 59.766 (63.324)	Acc@5 88.281 (89.716)
Epoch: [23][110/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1169 (2.1369)	Acc@1 60.156 (63.246)	Acc@5 92.578 (89.738)
Epoch: [23][120/196]	Time 0.012 (0.015)	Data 0.004 (0.004)	Loss 2.2995 (2.1407)	Acc@1 58.594 (63.246)	Acc@5 86.719 (89.673)
Epoch: [23][130/196]	Time 0.011 (0.015)	Data 0.010 (0.004)	Loss 2.2353 (2.1469)	Acc@1 59.375 (63.108)	Acc@5 87.500 (89.614)
Epoch: [23][140/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.3035 (2.1554)	Acc@1 61.719 (62.907)	Acc@5 87.500 (89.439)
Epoch: [23][150/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1288 (2.1608)	Acc@1 60.156 (62.640)	Acc@5 88.281 (89.401)
Epoch: [23][160/196]	Time 0.015 (0.015)	Data 0.008 (0.004)	Loss 2.1893 (2.1650)	Acc@1 57.422 (62.459)	Acc@5 91.016 (89.409)
Epoch: [23][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2143 (2.1718)	Acc@1 61.719 (62.235)	Acc@5 87.500 (89.307)
Epoch: [23][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.4384 (2.1769)	Acc@1 57.422 (62.153)	Acc@5 83.594 (89.203)
Epoch: [23][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.3638 (2.1821)	Acc@1 55.859 (62.019)	Acc@5 84.375 (89.142)
num momentum params: 26
[0.1, 2.18348466545105, 2.0382832074165345, 62.038, 47.26, tensor(0.3948, device='cuda:0', grad_fn=<DivBackward0>), 2.9390223026275635, 0.37903428077697754]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [24 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [24][0/196]	Time 0.038 (0.038)	Data 0.162 (0.162)	Loss 2.1135 (2.1135)	Acc@1 66.797 (66.797)	Acc@5 89.844 (89.844)
Epoch: [24][10/196]	Time 0.018 (0.017)	Data 0.001 (0.017)	Loss 2.1565 (2.1347)	Acc@1 63.281 (63.956)	Acc@5 90.234 (90.199)
Epoch: [24][20/196]	Time 0.012 (0.016)	Data 0.007 (0.010)	Loss 2.2054 (2.1294)	Acc@1 63.672 (64.249)	Acc@5 88.281 (90.179)
Epoch: [24][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 2.0367 (2.1060)	Acc@1 64.453 (64.844)	Acc@5 91.406 (90.323)
Epoch: [24][40/196]	Time 0.013 (0.016)	Data 0.007 (0.007)	Loss 2.1841 (2.0916)	Acc@1 62.500 (65.149)	Acc@5 92.969 (90.587)
Epoch: [24][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9998 (2.0814)	Acc@1 64.453 (65.035)	Acc@5 92.969 (90.786)
Epoch: [24][60/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.1582 (2.0887)	Acc@1 62.109 (64.831)	Acc@5 89.844 (90.644)
Epoch: [24][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1795 (2.0909)	Acc@1 63.281 (64.690)	Acc@5 89.844 (90.664)
Epoch: [24][80/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.1056 (2.0998)	Acc@1 65.234 (64.492)	Acc@5 89.844 (90.447)
Epoch: [24][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1556 (2.1085)	Acc@1 64.062 (64.273)	Acc@5 88.281 (90.234)
Epoch: [24][100/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1195 (2.1180)	Acc@1 63.281 (63.977)	Acc@5 89.844 (90.215)
Epoch: [24][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1644 (2.1247)	Acc@1 64.062 (63.795)	Acc@5 88.672 (90.129)
Epoch: [24][120/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.4326 (2.1352)	Acc@1 58.203 (63.578)	Acc@5 85.938 (89.950)
Epoch: [24][130/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.3486 (2.1429)	Acc@1 58.203 (63.418)	Acc@5 85.156 (89.835)
Epoch: [24][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.2161 (2.1474)	Acc@1 62.500 (63.303)	Acc@5 90.234 (89.786)
Epoch: [24][150/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.4246 (2.1534)	Acc@1 51.953 (63.121)	Acc@5 85.156 (89.707)
Epoch: [24][160/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 1.9791 (2.1590)	Acc@1 67.969 (63.007)	Acc@5 91.016 (89.621)
Epoch: [24][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0524 (2.1633)	Acc@1 66.797 (62.875)	Acc@5 89.453 (89.567)
Epoch: [24][180/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1873 (2.1658)	Acc@1 65.625 (62.841)	Acc@5 90.234 (89.537)
Epoch: [24][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.2633 (2.1719)	Acc@1 62.109 (62.711)	Acc@5 87.891 (89.469)
num momentum params: 26
[0.1, 2.1743705976867678, 1.9031091010570527, 62.644, 50.68, tensor(0.4015, device='cuda:0', grad_fn=<DivBackward0>), 2.9212918281555176, 0.37877392768859863]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [25 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [25][0/196]	Time 0.037 (0.037)	Data 0.166 (0.166)	Loss 1.9929 (1.9929)	Acc@1 67.578 (67.578)	Acc@5 92.969 (92.969)
Epoch: [25][10/196]	Time 0.018 (0.017)	Data 0.000 (0.017)	Loss 2.1009 (2.0498)	Acc@1 66.797 (66.300)	Acc@5 91.016 (91.335)
Epoch: [25][20/196]	Time 0.011 (0.016)	Data 0.007 (0.010)	Loss 2.2488 (2.0744)	Acc@1 61.719 (65.699)	Acc@5 89.844 (91.071)
Epoch: [25][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 2.0515 (2.0962)	Acc@1 66.016 (65.008)	Acc@5 89.453 (90.587)
Epoch: [25][40/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0877 (2.0939)	Acc@1 66.406 (64.901)	Acc@5 90.625 (90.635)
Epoch: [25][50/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.1632 (2.0917)	Acc@1 64.844 (64.844)	Acc@5 89.062 (90.709)
Epoch: [25][60/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 2.1383 (2.1030)	Acc@1 61.719 (64.524)	Acc@5 92.188 (90.587)
Epoch: [25][70/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0937 (2.1052)	Acc@1 65.234 (64.514)	Acc@5 88.672 (90.526)
Epoch: [25][80/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1780 (2.1063)	Acc@1 63.672 (64.564)	Acc@5 88.672 (90.374)
Epoch: [25][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0953 (2.1129)	Acc@1 63.281 (64.350)	Acc@5 90.625 (90.243)
Epoch: [25][100/196]	Time 0.016 (0.015)	Data 0.016 (0.004)	Loss 2.1740 (2.1162)	Acc@1 60.547 (64.179)	Acc@5 90.234 (90.242)
Epoch: [25][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3903 (2.1247)	Acc@1 55.859 (63.904)	Acc@5 86.328 (90.136)
Epoch: [25][120/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.0015 (2.1247)	Acc@1 62.109 (63.853)	Acc@5 92.969 (90.112)
Epoch: [25][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1109 (2.1279)	Acc@1 62.500 (63.702)	Acc@5 90.234 (90.115)
Epoch: [25][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1582 (2.1318)	Acc@1 64.844 (63.592)	Acc@5 90.234 (90.107)
Epoch: [25][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3834 (2.1392)	Acc@1 60.547 (63.470)	Acc@5 86.719 (90.030)
Epoch: [25][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1830 (2.1424)	Acc@1 64.062 (63.400)	Acc@5 88.281 (89.931)
Epoch: [25][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1978 (2.1473)	Acc@1 59.375 (63.343)	Acc@5 89.453 (89.855)
Epoch: [25][180/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1674 (2.1492)	Acc@1 61.719 (63.318)	Acc@5 89.453 (89.805)
Epoch: [25][190/196]	Time 0.019 (0.015)	Data 0.000 (0.004)	Loss 2.2681 (2.1544)	Acc@1 58.203 (63.156)	Acc@5 87.109 (89.721)
num momentum params: 26
[0.1, 2.1574691052246093, 1.740614857673645, 63.088, 53.31, tensor(0.4080, device='cuda:0', grad_fn=<DivBackward0>), 2.9947078227996826, 0.3727898597717285]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [26 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [26][0/196]	Time 0.037 (0.037)	Data 0.179 (0.179)	Loss 2.1234 (2.1234)	Acc@1 63.281 (63.281)	Acc@5 89.844 (89.844)
Epoch: [26][10/196]	Time 0.017 (0.017)	Data 0.000 (0.018)	Loss 2.0829 (2.1214)	Acc@1 63.281 (63.885)	Acc@5 91.797 (90.270)
Epoch: [26][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 1.9928 (2.1116)	Acc@1 67.969 (63.876)	Acc@5 92.969 (90.644)
Epoch: [26][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.1359 (2.1044)	Acc@1 64.453 (64.390)	Acc@5 90.625 (90.776)
Epoch: [26][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 2.1534 (2.1129)	Acc@1 63.672 (64.272)	Acc@5 90.625 (90.730)
Epoch: [26][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0986 (2.1067)	Acc@1 69.531 (64.415)	Acc@5 91.406 (90.771)
Epoch: [26][60/196]	Time 0.011 (0.015)	Data 0.008 (0.006)	Loss 2.0396 (2.1048)	Acc@1 66.406 (64.568)	Acc@5 93.359 (90.772)
Epoch: [26][70/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.1082 (2.1117)	Acc@1 62.891 (64.404)	Acc@5 91.406 (90.631)
Epoch: [26][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.3287 (2.1129)	Acc@1 55.469 (64.333)	Acc@5 85.547 (90.519)
Epoch: [26][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0274 (2.1108)	Acc@1 68.750 (64.518)	Acc@5 92.188 (90.604)
Epoch: [26][100/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2322 (2.1169)	Acc@1 60.156 (64.403)	Acc@5 89.453 (90.490)
Epoch: [26][110/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0221 (2.1162)	Acc@1 70.312 (64.348)	Acc@5 89.453 (90.502)
Epoch: [26][120/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.1345 (2.1204)	Acc@1 65.234 (64.337)	Acc@5 89.844 (90.402)
Epoch: [26][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1030 (2.1230)	Acc@1 68.750 (64.382)	Acc@5 89.844 (90.318)
Epoch: [26][140/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.2207 (2.1298)	Acc@1 63.672 (64.179)	Acc@5 87.891 (90.218)
Epoch: [26][150/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0995 (2.1339)	Acc@1 64.453 (64.055)	Acc@5 89.453 (90.206)
Epoch: [26][160/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.3011 (2.1362)	Acc@1 61.719 (64.038)	Acc@5 86.719 (90.152)
Epoch: [26][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9716 (2.1372)	Acc@1 72.266 (63.976)	Acc@5 92.578 (90.205)
Epoch: [26][180/196]	Time 0.011 (0.015)	Data 0.009 (0.004)	Loss 2.1079 (2.1410)	Acc@1 64.844 (63.909)	Acc@5 89.844 (90.096)
Epoch: [26][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3281 (2.1458)	Acc@1 59.766 (63.819)	Acc@5 90.234 (90.056)
num momentum params: 26
[0.1, 2.147710595855713, 2.0793008077144624, 63.794, 47.36, tensor(0.4135, device='cuda:0', grad_fn=<DivBackward0>), 2.9958863258361816, 0.37560009956359863]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [27 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [27][0/196]	Time 0.037 (0.037)	Data 0.172 (0.172)	Loss 2.0887 (2.0887)	Acc@1 66.797 (66.797)	Acc@5 91.797 (91.797)
Epoch: [27][10/196]	Time 0.018 (0.018)	Data 0.000 (0.017)	Loss 2.0202 (2.0972)	Acc@1 67.188 (65.696)	Acc@5 91.406 (91.584)
Epoch: [27][20/196]	Time 0.013 (0.016)	Data 0.005 (0.010)	Loss 2.1756 (2.1041)	Acc@1 64.453 (64.825)	Acc@5 87.500 (91.462)
Epoch: [27][30/196]	Time 0.017 (0.016)	Data 0.000 (0.008)	Loss 1.9124 (2.0965)	Acc@1 69.141 (64.831)	Acc@5 91.406 (91.494)
Epoch: [27][40/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0279 (2.0914)	Acc@1 64.062 (65.082)	Acc@5 92.188 (91.568)
Epoch: [27][50/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.1782 (2.0970)	Acc@1 64.844 (64.905)	Acc@5 91.406 (91.460)
Epoch: [27][60/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.1331 (2.0944)	Acc@1 64.062 (65.042)	Acc@5 92.578 (91.368)
Epoch: [27][70/196]	Time 0.014 (0.015)	Data 0.006 (0.005)	Loss 2.1913 (2.0975)	Acc@1 61.328 (65.102)	Acc@5 92.188 (91.258)
Epoch: [27][80/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.1000 (2.1032)	Acc@1 63.281 (64.897)	Acc@5 92.969 (91.155)
Epoch: [27][90/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.2293 (2.1042)	Acc@1 61.328 (64.818)	Acc@5 88.672 (91.097)
Epoch: [27][100/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.1816 (2.1050)	Acc@1 60.547 (64.708)	Acc@5 89.062 (91.062)
Epoch: [27][110/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 2.2579 (2.1157)	Acc@1 57.812 (64.446)	Acc@5 89.453 (90.892)
Epoch: [27][120/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0783 (2.1183)	Acc@1 65.234 (64.398)	Acc@5 89.844 (90.825)
Epoch: [27][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.2517 (2.1268)	Acc@1 62.109 (64.197)	Acc@5 87.109 (90.703)
Epoch: [27][140/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.3580 (2.1337)	Acc@1 57.031 (64.007)	Acc@5 85.938 (90.611)
Epoch: [27][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.1577 (2.1395)	Acc@1 63.672 (63.876)	Acc@5 88.672 (90.519)
Epoch: [27][160/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.1389 (2.1453)	Acc@1 64.844 (63.737)	Acc@5 91.016 (90.399)
Epoch: [27][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0949 (2.1499)	Acc@1 63.672 (63.688)	Acc@5 92.578 (90.367)
Epoch: [27][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1647 (2.1533)	Acc@1 64.844 (63.594)	Acc@5 89.453 (90.310)
Epoch: [27][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1993 (2.1574)	Acc@1 62.891 (63.527)	Acc@5 87.891 (90.247)
num momentum params: 26
[0.1, 2.1581126944732665, 1.794041930437088, 63.514, 53.38, tensor(0.4157, device='cuda:0', grad_fn=<DivBackward0>), 2.8678553104400635, 0.378589391708374]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [28 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [28][0/196]	Time 0.039 (0.039)	Data 0.183 (0.183)	Loss 1.9153 (1.9153)	Acc@1 69.141 (69.141)	Acc@5 92.578 (92.578)
Epoch: [28][10/196]	Time 0.017 (0.018)	Data 0.001 (0.018)	Loss 2.2869 (2.0913)	Acc@1 58.203 (64.986)	Acc@5 89.453 (91.335)
Epoch: [28][20/196]	Time 0.013 (0.016)	Data 0.004 (0.011)	Loss 2.1413 (2.0919)	Acc@1 63.281 (65.253)	Acc@5 92.969 (91.332)
Epoch: [28][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.1365 (2.0819)	Acc@1 62.891 (65.134)	Acc@5 93.750 (91.494)
Epoch: [28][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.1119 (2.0699)	Acc@1 64.453 (65.482)	Acc@5 91.406 (91.625)
Epoch: [28][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9263 (2.0638)	Acc@1 68.359 (65.702)	Acc@5 92.578 (91.628)
Epoch: [28][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1480 (2.0694)	Acc@1 62.891 (65.727)	Acc@5 93.359 (91.541)
Epoch: [28][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 1.9443 (2.0769)	Acc@1 69.141 (65.421)	Acc@5 92.188 (91.478)
Epoch: [28][80/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 1.9639 (2.0834)	Acc@1 71.484 (65.432)	Acc@5 91.797 (91.281)
Epoch: [28][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2566 (2.0955)	Acc@1 64.062 (65.273)	Acc@5 91.016 (91.093)
Epoch: [28][100/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1419 (2.1025)	Acc@1 65.625 (65.161)	Acc@5 91.797 (91.004)
Epoch: [28][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1029 (2.1057)	Acc@1 64.062 (65.062)	Acc@5 91.406 (90.935)
Epoch: [28][120/196]	Time 0.017 (0.015)	Data 0.004 (0.004)	Loss 2.0539 (2.1060)	Acc@1 67.969 (65.083)	Acc@5 91.797 (90.967)
Epoch: [28][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2388 (2.1167)	Acc@1 63.672 (64.784)	Acc@5 89.062 (90.807)
Epoch: [28][140/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.2165 (2.1246)	Acc@1 61.328 (64.614)	Acc@5 89.453 (90.719)
Epoch: [28][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.4969 (2.1334)	Acc@1 52.344 (64.357)	Acc@5 84.766 (90.635)
Epoch: [28][160/196]	Time 0.012 (0.015)	Data 0.003 (0.004)	Loss 2.1248 (2.1392)	Acc@1 60.938 (64.196)	Acc@5 89.062 (90.526)
Epoch: [28][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9509 (2.1396)	Acc@1 67.578 (64.188)	Acc@5 94.141 (90.493)
Epoch: [28][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.2519 (2.1431)	Acc@1 61.328 (64.142)	Acc@5 88.672 (90.470)
Epoch: [28][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1078 (2.1479)	Acc@1 66.016 (64.013)	Acc@5 89.844 (90.380)
num momentum params: 26
[0.1, 2.150996654663086, 1.8663304221630097, 63.938, 50.9, tensor(0.4212, device='cuda:0', grad_fn=<DivBackward0>), 2.9408161640167236, 0.3745088577270508]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [29 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [29][0/196]	Time 0.037 (0.037)	Data 0.175 (0.175)	Loss 2.1469 (2.1469)	Acc@1 62.500 (62.500)	Acc@5 89.844 (89.844)
Epoch: [29][10/196]	Time 0.015 (0.017)	Data 0.002 (0.018)	Loss 2.1071 (2.1029)	Acc@1 63.672 (64.311)	Acc@5 91.016 (91.229)
Epoch: [29][20/196]	Time 0.014 (0.016)	Data 0.002 (0.011)	Loss 2.0356 (2.0677)	Acc@1 67.578 (65.253)	Acc@5 92.188 (91.853)
Epoch: [29][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9864 (2.0450)	Acc@1 68.359 (66.142)	Acc@5 91.016 (92.074)
Epoch: [29][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9641 (2.0557)	Acc@1 67.578 (66.044)	Acc@5 94.141 (91.864)
Epoch: [29][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1756 (2.0634)	Acc@1 64.062 (65.893)	Acc@5 91.016 (91.697)
Epoch: [29][60/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.1375 (2.0670)	Acc@1 64.062 (65.881)	Acc@5 90.234 (91.560)
Epoch: [29][70/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0890 (2.0732)	Acc@1 67.578 (65.735)	Acc@5 90.234 (91.483)
Epoch: [29][80/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.1931 (2.0819)	Acc@1 64.844 (65.606)	Acc@5 91.797 (91.440)
Epoch: [29][90/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0592 (2.0821)	Acc@1 63.281 (65.522)	Acc@5 90.625 (91.466)
Epoch: [29][100/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0038 (2.0884)	Acc@1 68.359 (65.265)	Acc@5 93.359 (91.422)
Epoch: [29][110/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0799 (2.1005)	Acc@1 67.969 (65.023)	Acc@5 92.578 (91.287)
Epoch: [29][120/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0491 (2.1071)	Acc@1 65.234 (64.815)	Acc@5 94.531 (91.142)
Epoch: [29][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3099 (2.1164)	Acc@1 57.031 (64.525)	Acc@5 88.672 (90.953)
Epoch: [29][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2967 (2.1229)	Acc@1 60.938 (64.428)	Acc@5 86.328 (90.752)
Epoch: [29][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1647 (2.1263)	Acc@1 61.328 (64.326)	Acc@5 90.234 (90.752)
Epoch: [29][160/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.1691 (2.1299)	Acc@1 59.766 (64.286)	Acc@5 90.234 (90.720)
Epoch: [29][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0645 (2.1350)	Acc@1 67.578 (64.158)	Acc@5 90.234 (90.641)
Epoch: [29][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.3260 (2.1420)	Acc@1 58.203 (64.019)	Acc@5 87.891 (90.500)
Epoch: [29][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3318 (2.1442)	Acc@1 59.375 (63.981)	Acc@5 90.234 (90.447)
num momentum params: 26
[0.1, 2.1450008023071288, 1.7821823000907897, 63.994, 53.16, tensor(0.4252, device='cuda:0', grad_fn=<DivBackward0>), 2.981308698654175, 0.37866950035095215]
Non Pruning Epoch - module.conv1.weight: [64, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [64]
Non Pruning Epoch - module.bn1.bias: [64]
Non Pruning Epoch - module.conv2.weight: [128, 64, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [512]
Non Pruning Epoch - module.bn8.bias: [512]
Non Pruning Epoch - module.fc.weight: [100, 512]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [30 | 180] LR: 0.100000
module.conv1.weight [64, 3, 3, 3]
module.conv2.weight [128, 64, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [512, 512, 3, 3]
Epoch: [30][0/196]	Time 0.039 (0.039)	Data 0.175 (0.175)	Loss 1.7528 (1.7528)	Acc@1 76.953 (76.953)	Acc@5 96.484 (96.484)
Epoch: [30][10/196]	Time 0.015 (0.018)	Data 0.003 (0.018)	Loss 2.0540 (2.0168)	Acc@1 64.844 (68.288)	Acc@5 91.797 (92.578)
Epoch: [30][20/196]	Time 0.015 (0.016)	Data 0.004 (0.010)	Loss 2.3316 (2.0350)	Acc@1 58.594 (67.336)	Acc@5 87.500 (92.001)
Epoch: [30][30/196]	Time 0.016 (0.016)	Data 0.003 (0.008)	Loss 2.0694 (2.0297)	Acc@1 63.281 (67.087)	Acc@5 92.969 (92.137)
Epoch: [30][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 1.9856 (2.0276)	Acc@1 70.312 (67.102)	Acc@5 91.797 (92.026)
Epoch: [30][50/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 1.8204 (2.0332)	Acc@1 71.875 (67.065)	Acc@5 95.703 (91.950)
Epoch: [30][60/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 2.0497 (2.0353)	Acc@1 68.750 (67.047)	Acc@5 92.578 (92.021)
Epoch: [30][70/196]	Time 0.014 (0.015)	Data 0.004 (0.005)	Loss 1.9212 (2.0412)	Acc@1 68.359 (66.835)	Acc@5 92.578 (91.863)
Epoch: [30][80/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.1244 (2.0419)	Acc@1 66.016 (66.821)	Acc@5 91.016 (91.922)
Epoch: [30][90/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0290 (2.0555)	Acc@1 67.969 (66.501)	Acc@5 93.359 (91.784)
Epoch: [30][100/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.3828 (2.0680)	Acc@1 59.375 (66.201)	Acc@5 87.109 (91.542)
Epoch: [30][110/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1175 (2.0739)	Acc@1 64.453 (66.026)	Acc@5 93.359 (91.543)
Epoch: [30][120/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.0602 (2.0808)	Acc@1 65.625 (65.854)	Acc@5 89.844 (91.426)
Epoch: [30][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3099 (2.0882)	Acc@1 63.281 (65.777)	Acc@5 87.109 (91.278)
Epoch: [30][140/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.4342 (2.0996)	Acc@1 56.250 (65.437)	Acc@5 87.891 (91.146)
Epoch: [30][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.3224 (2.1068)	Acc@1 61.328 (65.278)	Acc@5 86.719 (91.013)
Epoch: [30][160/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.0816 (2.1123)	Acc@1 69.922 (65.198)	Acc@5 90.234 (90.938)
Epoch: [30][170/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.2744 (2.1169)	Acc@1 60.156 (65.095)	Acc@5 86.328 (90.915)
Epoch: [30][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1623 (2.1219)	Acc@1 64.844 (64.993)	Acc@5 91.016 (90.830)
Epoch: [30][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.4100 (2.1293)	Acc@1 61.719 (64.844)	Acc@5 87.500 (90.754)
num momentum params: 26
[0.1, 2.1313061743927, 1.6981084334850312, 64.798, 54.64, tensor(0.4310, device='cuda:0', grad_fn=<DivBackward0>), 2.8853235244750977, 0.37496495246887207]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [64, 3, 3, 3]
Before - module.bn1.weight: [64]
Before - module.bn1.bias: [64]
Before - module.conv2.weight: [128, 64, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [512, 512, 3, 3]
Before - module.bn8.weight: [512]
Before - module.bn8.bias: [512]
Before - module.fc.weight: [100, 512]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [64, 3, 3, 3] >> [63, 3, 3, 3]
[module.bn1.weight]: 64 >> 63
running_mean [63]
running_var [63]
num_batches_tracked []
[module.conv2.weight]: [128, 64, 3, 3] >> [128, 63, 3, 3]
[module.conv8.weight]: [512, 512, 3, 3] >> [500, 512, 3, 3]
[module.bn8.weight]: 512 >> 500
running_mean [500]
running_var [500]
num_batches_tracked []
[module.fc.weight]: [100, 512] >> [100, 500]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [63, 3, 3, 3]
After - module.bn1.weight: [63]
After - module.bn1.bias: [63]
After - module.conv2.weight: [128, 63, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [512, 512, 3, 3]
After - module.bn7.weight: [512]
After - module.bn7.bias: [512]
After - module.conv8.weight: [500, 512, 3, 3]
After - module.bn8.weight: [500]
After - module.bn8.bias: [500]
After - module.fc.weight: [100, 500]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [63, 3, 3, 3]
conv2 --> [128, 63, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [512, 512, 3, 3]
conv8 --> [500, 512, 3, 3]
fc --> [500, 100]
1, 697600512, 1741824, 63
2, 7766212608, 18579456, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7247757312, 9437184, 512
8, 7077888000, 9216000, 500
fc, 19200000, 50000, 0
===================
FLOP REPORT: 31028032200000.0 60192000000.0 152270672 150480 2739 17.57524871826172
[INFO] Storing checkpoint...

Epoch: [31 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [31][0/196]	Time 0.307 (0.307)	Data 0.177 (0.177)	Loss 1.9578 (1.9578)	Acc@1 66.406 (66.406)	Acc@5 93.750 (93.750)
Epoch: [31][10/196]	Time 0.016 (0.043)	Data 0.002 (0.018)	Loss 1.9943 (2.0588)	Acc@1 68.359 (65.447)	Acc@5 93.750 (92.294)
Epoch: [31][20/196]	Time 0.016 (0.030)	Data 0.002 (0.010)	Loss 1.9900 (2.0346)	Acc@1 68.750 (66.629)	Acc@5 93.750 (92.746)
Epoch: [31][30/196]	Time 0.017 (0.026)	Data 0.002 (0.007)	Loss 2.0724 (2.0314)	Acc@1 64.062 (66.910)	Acc@5 92.578 (92.540)
Epoch: [31][40/196]	Time 0.017 (0.023)	Data 0.002 (0.006)	Loss 2.0812 (2.0293)	Acc@1 66.406 (67.302)	Acc@5 91.016 (92.397)
Epoch: [31][50/196]	Time 0.016 (0.022)	Data 0.002 (0.005)	Loss 2.0806 (2.0472)	Acc@1 66.797 (66.774)	Acc@5 93.359 (92.318)
Epoch: [31][60/196]	Time 0.017 (0.021)	Data 0.002 (0.005)	Loss 2.1410 (2.0572)	Acc@1 66.406 (66.560)	Acc@5 91.406 (92.130)
Epoch: [31][70/196]	Time 0.016 (0.020)	Data 0.002 (0.004)	Loss 2.0456 (2.0668)	Acc@1 68.359 (66.417)	Acc@5 92.188 (91.890)
Epoch: [31][80/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 2.0984 (2.0683)	Acc@1 67.578 (66.479)	Acc@5 89.453 (91.811)
Epoch: [31][90/196]	Time 0.016 (0.019)	Data 0.002 (0.004)	Loss 2.0017 (2.0697)	Acc@1 70.312 (66.488)	Acc@5 91.797 (91.780)
Epoch: [31][100/196]	Time 0.016 (0.019)	Data 0.002 (0.004)	Loss 2.1328 (2.0752)	Acc@1 66.016 (66.391)	Acc@5 90.625 (91.600)
Epoch: [31][110/196]	Time 0.016 (0.019)	Data 0.002 (0.003)	Loss 2.1323 (2.0833)	Acc@1 62.891 (66.100)	Acc@5 91.406 (91.543)
Epoch: [31][120/196]	Time 0.016 (0.019)	Data 0.002 (0.003)	Loss 2.0666 (2.0853)	Acc@1 66.797 (66.016)	Acc@5 91.406 (91.542)
Epoch: [31][130/196]	Time 0.015 (0.018)	Data 0.002 (0.003)	Loss 1.9666 (2.0873)	Acc@1 69.531 (65.986)	Acc@5 93.359 (91.529)
Epoch: [31][140/196]	Time 0.017 (0.018)	Data 0.002 (0.003)	Loss 2.1913 (2.0952)	Acc@1 65.625 (65.836)	Acc@5 87.109 (91.367)
Epoch: [31][150/196]	Time 0.016 (0.018)	Data 0.002 (0.003)	Loss 2.1068 (2.0990)	Acc@1 65.234 (65.760)	Acc@5 91.016 (91.274)
Epoch: [31][160/196]	Time 0.018 (0.018)	Data 0.002 (0.003)	Loss 2.2084 (2.1062)	Acc@1 62.891 (65.586)	Acc@5 92.578 (91.168)
Epoch: [31][170/196]	Time 0.016 (0.018)	Data 0.002 (0.003)	Loss 2.0508 (2.1113)	Acc@1 69.922 (65.534)	Acc@5 91.406 (91.098)
Epoch: [31][180/196]	Time 0.015 (0.018)	Data 0.003 (0.003)	Loss 2.1014 (2.1142)	Acc@1 66.797 (65.455)	Acc@5 90.625 (91.035)
Epoch: [31][190/196]	Time 0.016 (0.018)	Data 0.001 (0.003)	Loss 2.3381 (2.1176)	Acc@1 58.594 (65.386)	Acc@5 90.625 (90.997)
num momentum params: 26
[0.1, 2.119582784423828, 2.061989952325821, 65.386, 48.22, tensor(0.4359, device='cuda:0', grad_fn=<DivBackward0>), 3.57204008102417, 0.41471219062805176]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [32 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [32][0/196]	Time 0.037 (0.037)	Data 0.171 (0.171)	Loss 1.9764 (1.9764)	Acc@1 66.406 (66.406)	Acc@5 94.531 (94.531)
Epoch: [32][10/196]	Time 0.018 (0.017)	Data 0.000 (0.018)	Loss 1.9360 (2.0193)	Acc@1 71.484 (68.395)	Acc@5 94.141 (92.791)
Epoch: [32][20/196]	Time 0.012 (0.016)	Data 0.006 (0.011)	Loss 2.0153 (2.0476)	Acc@1 66.406 (67.392)	Acc@5 93.750 (92.411)
Epoch: [32][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.0462 (2.0387)	Acc@1 69.531 (67.591)	Acc@5 91.797 (92.553)
Epoch: [32][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9932 (2.0321)	Acc@1 66.797 (67.597)	Acc@5 91.406 (92.530)
Epoch: [32][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1422 (2.0328)	Acc@1 64.453 (67.555)	Acc@5 90.234 (92.471)
Epoch: [32][60/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 2.1518 (2.0369)	Acc@1 66.016 (67.501)	Acc@5 91.406 (92.335)
Epoch: [32][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1961 (2.0400)	Acc@1 65.234 (67.331)	Acc@5 89.844 (92.270)
Epoch: [32][80/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1511 (2.0439)	Acc@1 65.234 (67.318)	Acc@5 91.406 (92.048)
Epoch: [32][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1349 (2.0501)	Acc@1 67.578 (67.222)	Acc@5 90.234 (91.926)
Epoch: [32][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1044 (2.0552)	Acc@1 63.672 (67.095)	Acc@5 91.406 (91.882)
Epoch: [32][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2275 (2.0654)	Acc@1 65.234 (66.909)	Acc@5 89.062 (91.737)
Epoch: [32][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1447 (2.0695)	Acc@1 65.234 (66.800)	Acc@5 92.578 (91.726)
Epoch: [32][130/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.1729 (2.0756)	Acc@1 61.719 (66.627)	Acc@5 91.797 (91.672)
Epoch: [32][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1135 (2.0866)	Acc@1 65.625 (66.304)	Acc@5 91.406 (91.531)
Epoch: [32][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3241 (2.0967)	Acc@1 60.547 (65.961)	Acc@5 85.547 (91.370)
Epoch: [32][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0091 (2.0985)	Acc@1 68.359 (65.848)	Acc@5 91.797 (91.377)
Epoch: [32][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1869 (2.1037)	Acc@1 65.234 (65.680)	Acc@5 90.625 (91.317)
Epoch: [32][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2650 (2.1117)	Acc@1 60.547 (65.498)	Acc@5 87.891 (91.253)
Epoch: [32][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2266 (2.1144)	Acc@1 59.766 (65.468)	Acc@5 89.453 (91.177)
num momentum params: 26
[0.1, 2.116058282623291, 1.9199686217308045, 65.408, 50.05, tensor(0.4397, device='cuda:0', grad_fn=<DivBackward0>), 3.1670293807983403, 0.3727409839630127]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [33 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [33][0/196]	Time 0.036 (0.036)	Data 0.169 (0.169)	Loss 1.9236 (1.9236)	Acc@1 69.531 (69.531)	Acc@5 93.359 (93.359)
Epoch: [33][10/196]	Time 0.018 (0.017)	Data 0.001 (0.018)	Loss 2.0253 (2.0856)	Acc@1 67.188 (66.939)	Acc@5 92.188 (91.513)
Epoch: [33][20/196]	Time 0.011 (0.016)	Data 0.008 (0.011)	Loss 2.1174 (2.0812)	Acc@1 64.062 (66.574)	Acc@5 92.969 (91.797)
Epoch: [33][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.0338 (2.0735)	Acc@1 64.844 (66.595)	Acc@5 92.188 (91.885)
Epoch: [33][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 1.9581 (2.0582)	Acc@1 68.359 (66.978)	Acc@5 93.359 (92.130)
Epoch: [33][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.0048 (2.0509)	Acc@1 70.312 (67.126)	Acc@5 90.625 (92.188)
Epoch: [33][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9877 (2.0565)	Acc@1 70.312 (66.925)	Acc@5 91.797 (92.143)
Epoch: [33][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0128 (2.0625)	Acc@1 70.312 (66.835)	Acc@5 90.234 (92.033)
Epoch: [33][80/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0069 (2.0716)	Acc@1 70.703 (66.667)	Acc@5 89.844 (91.942)
Epoch: [33][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1451 (2.0805)	Acc@1 66.797 (66.531)	Acc@5 91.016 (91.818)
Epoch: [33][100/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0131 (2.0838)	Acc@1 66.406 (66.538)	Acc@5 92.969 (91.708)
Epoch: [33][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1061 (2.0873)	Acc@1 63.281 (66.434)	Acc@5 91.016 (91.639)
Epoch: [33][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2990 (2.0887)	Acc@1 62.109 (66.477)	Acc@5 87.891 (91.568)
Epoch: [33][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1873 (2.0923)	Acc@1 65.234 (66.397)	Acc@5 89.453 (91.523)
Epoch: [33][140/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2771 (2.0952)	Acc@1 61.719 (66.218)	Acc@5 86.719 (91.520)
Epoch: [33][150/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.0949 (2.1004)	Acc@1 66.797 (66.088)	Acc@5 90.625 (91.430)
Epoch: [33][160/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.2071 (2.1055)	Acc@1 65.625 (65.957)	Acc@5 89.062 (91.338)
Epoch: [33][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2778 (2.1107)	Acc@1 60.547 (65.867)	Acc@5 90.234 (91.276)
Epoch: [33][180/196]	Time 0.014 (0.016)	Data 0.005 (0.003)	Loss 2.1521 (2.1129)	Acc@1 66.406 (65.746)	Acc@5 92.969 (91.255)
Epoch: [33][190/196]	Time 0.017 (0.016)	Data 0.004 (0.003)	Loss 2.1925 (2.1159)	Acc@1 64.844 (65.707)	Acc@5 89.844 (91.189)
num momentum params: 26
[0.1, 2.1174829697418214, 1.968823742866516, 65.67, 51.76, tensor(0.4424, device='cuda:0', grad_fn=<DivBackward0>), 3.1255552768707275, 0.3794252872467041]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [34 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [34][0/196]	Time 0.038 (0.038)	Data 0.173 (0.173)	Loss 1.8472 (1.8472)	Acc@1 73.047 (73.047)	Acc@5 94.922 (94.922)
Epoch: [34][10/196]	Time 0.018 (0.018)	Data 0.000 (0.017)	Loss 1.9882 (2.0344)	Acc@1 68.359 (68.288)	Acc@5 91.797 (92.010)
Epoch: [34][20/196]	Time 0.012 (0.017)	Data 0.006 (0.010)	Loss 1.9112 (2.0271)	Acc@1 71.094 (68.118)	Acc@5 94.922 (92.150)
Epoch: [34][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.0981 (2.0444)	Acc@1 67.188 (67.477)	Acc@5 91.797 (92.036)
Epoch: [34][40/196]	Time 0.015 (0.016)	Data 0.004 (0.006)	Loss 2.0769 (2.0489)	Acc@1 66.406 (67.407)	Acc@5 93.359 (92.130)
Epoch: [34][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.1198 (2.0481)	Acc@1 67.578 (67.325)	Acc@5 91.406 (92.165)
Epoch: [34][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1789 (2.0562)	Acc@1 64.453 (67.200)	Acc@5 92.188 (92.079)
Epoch: [34][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.1586 (2.0608)	Acc@1 65.625 (67.127)	Acc@5 90.625 (92.088)
Epoch: [34][80/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.2852 (2.0617)	Acc@1 65.625 (67.183)	Acc@5 90.234 (92.014)
Epoch: [34][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0052 (2.0719)	Acc@1 67.188 (66.964)	Acc@5 93.359 (91.917)
Epoch: [34][100/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 2.2573 (2.0785)	Acc@1 64.062 (66.913)	Acc@5 89.844 (91.843)
Epoch: [34][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1431 (2.0826)	Acc@1 64.844 (66.793)	Acc@5 90.234 (91.807)
Epoch: [34][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1790 (2.0836)	Acc@1 65.234 (66.758)	Acc@5 89.062 (91.778)
Epoch: [34][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9271 (2.0863)	Acc@1 71.875 (66.594)	Acc@5 93.750 (91.776)
Epoch: [34][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9909 (2.0916)	Acc@1 68.750 (66.495)	Acc@5 92.188 (91.686)
Epoch: [34][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0864 (2.0951)	Acc@1 66.797 (66.453)	Acc@5 89.453 (91.587)
Epoch: [34][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1796 (2.1010)	Acc@1 65.625 (66.353)	Acc@5 90.625 (91.513)
Epoch: [34][170/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1174 (2.1038)	Acc@1 67.969 (66.329)	Acc@5 91.797 (91.454)
Epoch: [34][180/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2602 (2.1099)	Acc@1 62.891 (66.206)	Acc@5 88.281 (91.372)
Epoch: [34][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0058 (2.1111)	Acc@1 69.141 (66.208)	Acc@5 93.750 (91.361)
num momentum params: 26
[0.1, 2.1128934999847413, 1.9129599511623383, 66.146, 50.79, tensor(0.4463, device='cuda:0', grad_fn=<DivBackward0>), 3.163685321807861, 0.3777751922607422]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [35 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [35][0/196]	Time 0.037 (0.037)	Data 0.173 (0.173)	Loss 1.9534 (1.9534)	Acc@1 69.531 (69.531)	Acc@5 93.359 (93.359)
Epoch: [35][10/196]	Time 0.017 (0.018)	Data 0.000 (0.017)	Loss 2.1129 (2.0413)	Acc@1 66.016 (67.578)	Acc@5 91.016 (92.188)
Epoch: [35][20/196]	Time 0.013 (0.017)	Data 0.005 (0.010)	Loss 1.9137 (2.0208)	Acc@1 69.922 (68.062)	Acc@5 92.969 (92.448)
Epoch: [35][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 1.9008 (1.9975)	Acc@1 73.047 (69.002)	Acc@5 93.359 (92.729)
Epoch: [35][40/196]	Time 0.011 (0.016)	Data 0.009 (0.007)	Loss 1.8893 (1.9873)	Acc@1 70.703 (69.245)	Acc@5 94.141 (92.931)
Epoch: [35][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1162 (2.0011)	Acc@1 66.016 (69.056)	Acc@5 91.797 (92.670)
Epoch: [35][60/196]	Time 0.016 (0.016)	Data 0.006 (0.005)	Loss 2.0377 (2.0002)	Acc@1 67.578 (68.974)	Acc@5 91.406 (92.751)
Epoch: [35][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1036 (2.0122)	Acc@1 66.406 (68.590)	Acc@5 92.188 (92.650)
Epoch: [35][80/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1431 (2.0241)	Acc@1 66.406 (68.248)	Acc@5 91.406 (92.573)
Epoch: [35][90/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0825 (2.0305)	Acc@1 66.406 (68.136)	Acc@5 92.969 (92.428)
Epoch: [35][100/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1171 (2.0456)	Acc@1 65.625 (67.795)	Acc@5 92.188 (92.211)
Epoch: [35][110/196]	Time 0.016 (0.016)	Data 0.004 (0.004)	Loss 2.1615 (2.0522)	Acc@1 68.359 (67.575)	Acc@5 92.578 (92.205)
Epoch: [35][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0126 (2.0555)	Acc@1 69.141 (67.455)	Acc@5 93.359 (92.204)
Epoch: [35][130/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.1681 (2.0617)	Acc@1 63.672 (67.292)	Acc@5 87.500 (92.119)
Epoch: [35][140/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1986 (2.0672)	Acc@1 61.328 (67.165)	Acc@5 91.797 (92.046)
Epoch: [35][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.2402 (2.0703)	Acc@1 63.672 (67.012)	Acc@5 86.328 (92.001)
Epoch: [35][160/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 2.1145 (2.0774)	Acc@1 64.453 (66.836)	Acc@5 90.625 (91.899)
Epoch: [35][170/196]	Time 0.012 (0.016)	Data 0.018 (0.003)	Loss 2.2319 (2.0870)	Acc@1 62.109 (66.603)	Acc@5 93.750 (91.817)
Epoch: [35][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2055 (2.0923)	Acc@1 62.109 (66.477)	Acc@5 88.281 (91.756)
Epoch: [35][190/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1041 (2.0955)	Acc@1 66.406 (66.396)	Acc@5 91.797 (91.721)
num momentum params: 26
[0.1, 2.0981080713653566, 1.8881924057006836, 66.358, 51.52, tensor(0.4515, device='cuda:0', grad_fn=<DivBackward0>), 3.1352810859680176, 0.37900209426879883]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [36 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [36][0/196]	Time 0.036 (0.036)	Data 0.181 (0.181)	Loss 2.3160 (2.3160)	Acc@1 58.984 (58.984)	Acc@5 89.844 (89.844)
Epoch: [36][10/196]	Time 0.018 (0.018)	Data 0.001 (0.018)	Loss 2.1091 (2.0688)	Acc@1 65.234 (66.726)	Acc@5 92.188 (92.436)
Epoch: [36][20/196]	Time 0.012 (0.017)	Data 0.007 (0.011)	Loss 1.9168 (2.0112)	Acc@1 74.219 (68.787)	Acc@5 92.188 (92.746)
Epoch: [36][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 1.9789 (1.9987)	Acc@1 66.797 (68.926)	Acc@5 92.578 (92.843)
Epoch: [36][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 1.9983 (2.0007)	Acc@1 69.141 (68.893)	Acc@5 91.797 (92.816)
Epoch: [36][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.0127 (2.0184)	Acc@1 70.312 (68.474)	Acc@5 94.141 (92.616)
Epoch: [36][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1182 (2.0302)	Acc@1 69.922 (68.327)	Acc@5 91.797 (92.341)
Epoch: [36][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0629 (2.0355)	Acc@1 67.969 (68.222)	Acc@5 93.750 (92.314)
Epoch: [36][80/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0783 (2.0439)	Acc@1 67.188 (67.930)	Acc@5 91.016 (92.250)
Epoch: [36][90/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0943 (2.0480)	Acc@1 66.406 (67.758)	Acc@5 93.359 (92.192)
Epoch: [36][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2200 (2.0562)	Acc@1 64.453 (67.559)	Acc@5 89.844 (92.129)
Epoch: [36][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2011 (2.0592)	Acc@1 60.938 (67.546)	Acc@5 91.016 (92.110)
Epoch: [36][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1496 (2.0636)	Acc@1 63.672 (67.407)	Acc@5 91.406 (92.062)
Epoch: [36][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0642 (2.0694)	Acc@1 67.188 (67.295)	Acc@5 91.406 (92.003)
Epoch: [36][140/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 2.1171 (2.0721)	Acc@1 62.500 (67.221)	Acc@5 92.188 (91.969)
Epoch: [36][150/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0003 (2.0730)	Acc@1 67.188 (67.288)	Acc@5 93.359 (91.957)
Epoch: [36][160/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.0246 (2.0756)	Acc@1 69.531 (67.183)	Acc@5 91.406 (91.959)
Epoch: [36][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1509 (2.0791)	Acc@1 64.844 (67.048)	Acc@5 88.281 (91.875)
Epoch: [36][180/196]	Time 0.013 (0.016)	Data 0.006 (0.003)	Loss 2.3376 (2.0840)	Acc@1 61.328 (66.959)	Acc@5 91.406 (91.825)
Epoch: [36][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2090 (2.0856)	Acc@1 64.844 (66.922)	Acc@5 89.062 (91.809)
num momentum params: 26
[0.1, 2.0853295661926268, 2.1649321150779723, 66.948, 47.53, tensor(0.4560, device='cuda:0', grad_fn=<DivBackward0>), 3.1143219470977783, 0.3768744468688965]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [37 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [37][0/196]	Time 0.035 (0.035)	Data 0.183 (0.183)	Loss 2.0220 (2.0220)	Acc@1 69.531 (69.531)	Acc@5 92.188 (92.188)
Epoch: [37][10/196]	Time 0.017 (0.019)	Data 0.002 (0.018)	Loss 2.1010 (2.1049)	Acc@1 66.406 (65.767)	Acc@5 91.797 (91.619)
Epoch: [37][20/196]	Time 0.015 (0.017)	Data 0.004 (0.011)	Loss 2.0202 (2.0438)	Acc@1 66.406 (67.392)	Acc@5 95.312 (92.671)
Epoch: [37][30/196]	Time 0.017 (0.017)	Data 0.002 (0.008)	Loss 2.0336 (2.0437)	Acc@1 64.062 (67.503)	Acc@5 92.578 (92.528)
Epoch: [37][40/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 2.0215 (2.0411)	Acc@1 67.188 (67.864)	Acc@5 93.750 (92.492)
Epoch: [37][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 2.1362 (2.0469)	Acc@1 67.969 (67.923)	Acc@5 93.359 (92.440)
Epoch: [37][60/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0532 (2.0502)	Acc@1 68.359 (67.930)	Acc@5 92.969 (92.335)
Epoch: [37][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1817 (2.0485)	Acc@1 62.500 (67.870)	Acc@5 91.797 (92.441)
Epoch: [37][80/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1025 (2.0538)	Acc@1 66.406 (67.747)	Acc@5 91.406 (92.395)
Epoch: [37][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0702 (2.0562)	Acc@1 67.578 (67.711)	Acc@5 92.969 (92.338)
Epoch: [37][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2689 (2.0602)	Acc@1 61.719 (67.555)	Acc@5 91.797 (92.331)
Epoch: [37][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0919 (2.0631)	Acc@1 66.016 (67.585)	Acc@5 89.453 (92.216)
Epoch: [37][120/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1840 (2.0727)	Acc@1 62.500 (67.378)	Acc@5 90.234 (92.123)
Epoch: [37][130/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9371 (2.0798)	Acc@1 70.312 (67.182)	Acc@5 94.141 (92.035)
Epoch: [37][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1261 (2.0830)	Acc@1 64.062 (67.121)	Acc@5 91.406 (91.994)
Epoch: [37][150/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.3970 (2.0884)	Acc@1 59.375 (66.947)	Acc@5 89.062 (91.934)
Epoch: [37][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2234 (2.0961)	Acc@1 60.547 (66.758)	Acc@5 90.234 (91.782)
Epoch: [37][170/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.0580 (2.1000)	Acc@1 70.312 (66.717)	Acc@5 91.016 (91.767)
Epoch: [37][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1834 (2.1062)	Acc@1 66.797 (66.557)	Acc@5 90.234 (91.719)
Epoch: [37][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3261 (2.1099)	Acc@1 60.547 (66.443)	Acc@5 90.625 (91.652)
num momentum params: 26
[0.1, 2.112114324760437, 1.8579314661026, 66.392, 51.62, tensor(0.4531, device='cuda:0', grad_fn=<DivBackward0>), 3.1852939128875732, 0.3754751682281494]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [38 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [38][0/196]	Time 0.036 (0.036)	Data 0.173 (0.173)	Loss 2.0652 (2.0652)	Acc@1 67.188 (67.188)	Acc@5 91.406 (91.406)
Epoch: [38][10/196]	Time 0.018 (0.018)	Data 0.001 (0.018)	Loss 2.0005 (2.0300)	Acc@1 68.750 (69.531)	Acc@5 94.141 (92.685)
Epoch: [38][20/196]	Time 0.020 (0.017)	Data 0.006 (0.011)	Loss 2.0184 (2.0359)	Acc@1 71.875 (69.271)	Acc@5 92.188 (92.578)
Epoch: [38][30/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 2.0592 (2.0411)	Acc@1 68.750 (68.914)	Acc@5 93.750 (92.666)
Epoch: [38][40/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 2.0580 (2.0278)	Acc@1 72.266 (69.179)	Acc@5 88.281 (92.702)
Epoch: [38][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9958 (2.0310)	Acc@1 69.141 (69.064)	Acc@5 90.625 (92.693)
Epoch: [38][60/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9290 (2.0297)	Acc@1 67.969 (68.949)	Acc@5 93.750 (92.719)
Epoch: [38][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0931 (2.0354)	Acc@1 66.797 (68.844)	Acc@5 91.797 (92.633)
Epoch: [38][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1560 (2.0405)	Acc@1 69.922 (68.702)	Acc@5 89.453 (92.520)
Epoch: [38][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0777 (2.0428)	Acc@1 66.016 (68.570)	Acc@5 91.797 (92.497)
Epoch: [38][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0512 (2.0477)	Acc@1 65.234 (68.414)	Acc@5 93.359 (92.454)
Epoch: [38][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2509 (2.0574)	Acc@1 62.500 (68.180)	Acc@5 89.062 (92.332)
Epoch: [38][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0280 (2.0645)	Acc@1 69.141 (68.001)	Acc@5 92.969 (92.233)
Epoch: [38][130/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2562 (2.0740)	Acc@1 61.719 (67.754)	Acc@5 89.844 (92.146)
Epoch: [38][140/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1656 (2.0748)	Acc@1 65.625 (67.667)	Acc@5 90.625 (92.165)
Epoch: [38][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1965 (2.0785)	Acc@1 67.188 (67.565)	Acc@5 90.234 (92.151)
Epoch: [38][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.3502 (2.0832)	Acc@1 62.500 (67.389)	Acc@5 90.625 (92.129)
Epoch: [38][170/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.1647 (2.0920)	Acc@1 62.891 (67.176)	Acc@5 91.406 (92.018)
Epoch: [38][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0844 (2.0968)	Acc@1 67.969 (67.034)	Acc@5 89.062 (91.935)
Epoch: [38][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0325 (2.0995)	Acc@1 69.922 (66.940)	Acc@5 92.578 (91.883)
num momentum params: 26
[0.1, 2.1040148777770997, 1.870179318189621, 66.822, 52.14, tensor(0.4573, device='cuda:0', grad_fn=<DivBackward0>), 3.160828113555908, 0.38120579719543457]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [39 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [39][0/196]	Time 0.038 (0.038)	Data 0.177 (0.177)	Loss 1.9447 (1.9447)	Acc@1 68.359 (68.359)	Acc@5 94.531 (94.531)
Epoch: [39][10/196]	Time 0.018 (0.017)	Data 0.000 (0.018)	Loss 1.8723 (2.0242)	Acc@1 74.219 (68.217)	Acc@5 94.141 (92.614)
Epoch: [39][20/196]	Time 0.015 (0.017)	Data 0.003 (0.011)	Loss 2.0000 (2.0087)	Acc@1 69.922 (69.327)	Acc@5 93.750 (93.025)
Epoch: [39][30/196]	Time 0.016 (0.016)	Data 0.003 (0.008)	Loss 2.0618 (2.0094)	Acc@1 65.625 (69.380)	Acc@5 92.969 (92.893)
Epoch: [39][40/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 2.0107 (2.0121)	Acc@1 67.578 (69.236)	Acc@5 92.578 (92.845)
Epoch: [39][50/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 2.0377 (2.0151)	Acc@1 67.969 (69.179)	Acc@5 90.625 (92.747)
Epoch: [39][60/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0130 (2.0296)	Acc@1 67.578 (68.808)	Acc@5 92.578 (92.604)
Epoch: [39][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9880 (2.0344)	Acc@1 69.922 (68.557)	Acc@5 92.578 (92.606)
Epoch: [39][80/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1618 (2.0412)	Acc@1 66.016 (68.490)	Acc@5 91.016 (92.501)
Epoch: [39][90/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9952 (2.0411)	Acc@1 71.094 (68.501)	Acc@5 94.141 (92.518)
Epoch: [39][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0677 (2.0466)	Acc@1 67.188 (68.352)	Acc@5 92.188 (92.423)
Epoch: [39][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2507 (2.0511)	Acc@1 62.891 (68.293)	Acc@5 91.406 (92.381)
Epoch: [39][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3236 (2.0596)	Acc@1 61.328 (68.066)	Acc@5 88.672 (92.281)
Epoch: [39][130/196]	Time 0.015 (0.016)	Data 0.004 (0.003)	Loss 2.0409 (2.0639)	Acc@1 68.750 (67.933)	Acc@5 92.578 (92.205)
Epoch: [39][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2080 (2.0667)	Acc@1 66.797 (67.902)	Acc@5 88.672 (92.193)
Epoch: [39][150/196]	Time 0.018 (0.016)	Data 0.001 (0.003)	Loss 2.2108 (2.0713)	Acc@1 67.188 (67.782)	Acc@5 89.844 (92.087)
Epoch: [39][160/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.1479 (2.0749)	Acc@1 66.016 (67.736)	Acc@5 89.453 (92.013)
Epoch: [39][170/196]	Time 0.017 (0.016)	Data 0.000 (0.003)	Loss 2.1812 (2.0778)	Acc@1 65.625 (67.622)	Acc@5 91.016 (91.982)
Epoch: [39][180/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1182 (2.0826)	Acc@1 66.406 (67.531)	Acc@5 90.625 (91.903)
Epoch: [39][190/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 2.3368 (2.0884)	Acc@1 60.938 (67.415)	Acc@5 87.500 (91.795)
num momentum params: 26
[0.1, 2.091840333251953, 1.8917307651042938, 67.33, 51.73, tensor(0.4616, device='cuda:0', grad_fn=<DivBackward0>), 3.156010389328003, 0.3747889995574951]
Non Pruning Epoch - module.conv1.weight: [63, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [63]
Non Pruning Epoch - module.bn1.bias: [63]
Non Pruning Epoch - module.conv2.weight: [128, 63, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [512]
Non Pruning Epoch - module.bn7.bias: [512]
Non Pruning Epoch - module.conv8.weight: [500, 512, 3, 3]
Non Pruning Epoch - module.bn8.weight: [500]
Non Pruning Epoch - module.bn8.bias: [500]
Non Pruning Epoch - module.fc.weight: [100, 500]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [40 | 180] LR: 0.100000
module.conv1.weight [63, 3, 3, 3]
module.conv2.weight [128, 63, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [512, 512, 3, 3]
module.conv8.weight [500, 512, 3, 3]
Epoch: [40][0/196]	Time 0.039 (0.039)	Data 0.177 (0.177)	Loss 1.9151 (1.9151)	Acc@1 71.484 (71.484)	Acc@5 95.312 (95.312)
Epoch: [40][10/196]	Time 0.018 (0.018)	Data 0.000 (0.018)	Loss 1.9952 (2.0017)	Acc@1 66.797 (69.318)	Acc@5 94.922 (93.750)
Epoch: [40][20/196]	Time 0.016 (0.017)	Data 0.002 (0.011)	Loss 2.1227 (2.0154)	Acc@1 69.922 (68.862)	Acc@5 90.234 (93.229)
Epoch: [40][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.0532 (2.0389)	Acc@1 69.922 (68.397)	Acc@5 92.969 (92.956)
Epoch: [40][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9552 (2.0490)	Acc@1 72.266 (68.197)	Acc@5 95.703 (92.854)
Epoch: [40][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9400 (2.0499)	Acc@1 69.922 (68.114)	Acc@5 96.094 (92.762)
Epoch: [40][60/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9788 (2.0519)	Acc@1 70.312 (68.046)	Acc@5 91.797 (92.649)
Epoch: [40][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0005 (2.0567)	Acc@1 70.703 (68.095)	Acc@5 94.141 (92.666)
Epoch: [40][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1422 (2.0637)	Acc@1 66.016 (67.795)	Acc@5 89.844 (92.626)
Epoch: [40][90/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0743 (2.0657)	Acc@1 64.844 (67.758)	Acc@5 93.359 (92.591)
Epoch: [40][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0970 (2.0656)	Acc@1 68.750 (67.864)	Acc@5 91.406 (92.570)
Epoch: [40][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0782 (2.0694)	Acc@1 65.234 (67.733)	Acc@5 93.750 (92.458)
Epoch: [40][120/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 2.1944 (2.0718)	Acc@1 62.109 (67.691)	Acc@5 94.141 (92.401)
Epoch: [40][130/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0854 (2.0708)	Acc@1 69.922 (67.632)	Acc@5 89.844 (92.405)
Epoch: [40][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0405 (2.0762)	Acc@1 67.188 (67.506)	Acc@5 92.578 (92.332)
Epoch: [40][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0264 (2.0802)	Acc@1 68.359 (67.457)	Acc@5 94.531 (92.356)
Epoch: [40][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2908 (2.0840)	Acc@1 61.328 (67.333)	Acc@5 88.672 (92.311)
Epoch: [40][170/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.2000 (2.0882)	Acc@1 62.109 (67.267)	Acc@5 90.234 (92.261)
Epoch: [40][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9540 (2.0939)	Acc@1 68.359 (67.149)	Acc@5 94.531 (92.131)
Epoch: [40][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.3322 (2.0988)	Acc@1 62.109 (67.055)	Acc@5 87.500 (92.048)
num momentum params: 26
[0.1, 2.0995700483703614, 2.072897948026657, 67.06, 49.06, tensor(0.4615, device='cuda:0', grad_fn=<DivBackward0>), 3.1335396766662598, 0.37243008613586426]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [63, 3, 3, 3]
Before - module.bn1.weight: [63]
Before - module.bn1.bias: [63]
Before - module.conv2.weight: [128, 63, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [512, 512, 3, 3]
Before - module.bn7.weight: [512]
Before - module.bn7.bias: [512]
Before - module.conv8.weight: [500, 512, 3, 3]
Before - module.bn8.weight: [500]
Before - module.bn8.bias: [500]
Before - module.fc.weight: [100, 500]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [63, 3, 3, 3] >> [60, 3, 3, 3]
[module.bn1.weight]: 63 >> 60
running_mean [60]
running_var [60]
num_batches_tracked []
[module.conv2.weight]: [128, 63, 3, 3] >> [128, 60, 3, 3]
[module.conv7.weight]: [512, 512, 3, 3] >> [508, 512, 3, 3]
[module.bn7.weight]: 512 >> 508
running_mean [508]
running_var [508]
num_batches_tracked []
[module.conv8.weight]: [500, 512, 3, 3] >> [479, 508, 3, 3]
[module.bn8.weight]: 500 >> 479
running_mean [479]
running_var [479]
num_batches_tracked []
[module.fc.weight]: [100, 500] >> [100, 479]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [60, 3, 3, 3]
After - module.bn1.weight: [60]
After - module.bn1.bias: [60]
After - module.conv2.weight: [128, 60, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [512, 256, 3, 3]
After - module.bn5.weight: [512]
After - module.bn5.bias: [512]
After - module.conv6.weight: [512, 512, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [508, 512, 3, 3]
After - module.bn7.weight: [508]
After - module.bn7.bias: [508]
After - module.conv8.weight: [479, 508, 3, 3]
After - module.bn8.weight: [479]
After - module.bn8.bias: [479]
After - module.fc.weight: [100, 479]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [60, 3, 3, 3]
conv2 --> [128, 60, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [512, 256, 3, 3]
conv6 --> [512, 512, 3, 3]
conv7 --> [508, 512, 3, 3]
conv8 --> [479, 508, 3, 3]
fc --> [479, 100]
1, 664381440, 1658880, 60
2, 7396392960, 17694720, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10267656192, 18874368, 512
6, 20535312384, 37748736, 512
7, 7191134208, 9363456, 508
8, 6727643136, 8759952, 479
fc, 18393600, 47900, 0
===================
FLOP REPORT: 30711347400000.0 58923200000.0 150771116 147308 2711 17.311786651611328
[INFO] Storing checkpoint...

Epoch: [41 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [41][0/196]	Time 0.403 (0.403)	Data 0.175 (0.175)	Loss 2.1970 (2.1970)	Acc@1 65.625 (65.625)	Acc@5 91.016 (91.016)
Epoch: [41][10/196]	Time 0.016 (0.051)	Data 0.002 (0.018)	Loss 1.9968 (2.0919)	Acc@1 68.359 (67.330)	Acc@5 92.188 (92.401)
Epoch: [41][20/196]	Time 0.016 (0.035)	Data 0.002 (0.010)	Loss 1.9381 (2.0272)	Acc@1 72.266 (69.401)	Acc@5 95.312 (93.080)
Epoch: [41][30/196]	Time 0.016 (0.029)	Data 0.002 (0.008)	Loss 1.8927 (2.0105)	Acc@1 72.266 (69.544)	Acc@5 96.484 (93.435)
Epoch: [41][40/196]	Time 0.016 (0.026)	Data 0.002 (0.006)	Loss 2.0645 (2.0008)	Acc@1 67.188 (69.731)	Acc@5 92.188 (93.464)
Epoch: [41][50/196]	Time 0.016 (0.024)	Data 0.002 (0.005)	Loss 2.1060 (2.0062)	Acc@1 68.359 (69.501)	Acc@5 92.578 (93.329)
Epoch: [41][60/196]	Time 0.015 (0.023)	Data 0.002 (0.005)	Loss 2.2556 (2.0157)	Acc@1 61.328 (69.089)	Acc@5 90.234 (93.199)
Epoch: [41][70/196]	Time 0.016 (0.022)	Data 0.002 (0.004)	Loss 2.0661 (2.0214)	Acc@1 70.703 (68.871)	Acc@5 89.062 (93.123)
Epoch: [41][80/196]	Time 0.016 (0.021)	Data 0.002 (0.004)	Loss 2.1655 (2.0312)	Acc@1 64.844 (68.567)	Acc@5 91.406 (93.065)
Epoch: [41][90/196]	Time 0.017 (0.020)	Data 0.002 (0.004)	Loss 1.9965 (2.0336)	Acc@1 69.922 (68.462)	Acc@5 94.141 (93.007)
Epoch: [41][100/196]	Time 0.016 (0.020)	Data 0.002 (0.004)	Loss 2.2809 (2.0453)	Acc@1 62.891 (68.259)	Acc@5 90.234 (92.814)
Epoch: [41][110/196]	Time 0.016 (0.020)	Data 0.002 (0.004)	Loss 2.1901 (2.0493)	Acc@1 65.234 (68.289)	Acc@5 85.938 (92.712)
Epoch: [41][120/196]	Time 0.016 (0.019)	Data 0.002 (0.003)	Loss 2.0781 (2.0569)	Acc@1 67.969 (68.169)	Acc@5 91.016 (92.627)
Epoch: [41][130/196]	Time 0.016 (0.019)	Data 0.002 (0.003)	Loss 2.2759 (2.0635)	Acc@1 64.062 (68.008)	Acc@5 88.281 (92.554)
Epoch: [41][140/196]	Time 0.016 (0.019)	Data 0.002 (0.003)	Loss 2.1727 (2.0648)	Acc@1 65.234 (67.930)	Acc@5 92.188 (92.537)
Epoch: [41][150/196]	Time 0.015 (0.019)	Data 0.002 (0.003)	Loss 2.1683 (2.0696)	Acc@1 65.234 (67.816)	Acc@5 91.016 (92.431)
Epoch: [41][160/196]	Time 0.016 (0.019)	Data 0.002 (0.003)	Loss 2.2505 (2.0778)	Acc@1 61.719 (67.593)	Acc@5 90.625 (92.309)
Epoch: [41][170/196]	Time 0.016 (0.018)	Data 0.002 (0.003)	Loss 2.2175 (2.0871)	Acc@1 62.109 (67.414)	Acc@5 91.406 (92.160)
Epoch: [41][180/196]	Time 0.016 (0.018)	Data 0.002 (0.003)	Loss 2.1653 (2.0900)	Acc@1 66.016 (67.367)	Acc@5 90.625 (92.118)
Epoch: [41][190/196]	Time 0.016 (0.018)	Data 0.001 (0.003)	Loss 2.0902 (2.0931)	Acc@1 66.406 (67.310)	Acc@5 90.625 (92.065)
num momentum params: 26
[0.1, 2.0953928363800047, 1.9108658146858215, 67.252, 52.33, tensor(0.4644, device='cuda:0', grad_fn=<DivBackward0>), 3.696512937545776, 0.44877004623413086]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [42 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [42][0/196]	Time 0.040 (0.040)	Data 0.186 (0.186)	Loss 1.9676 (1.9676)	Acc@1 71.094 (71.094)	Acc@5 94.531 (94.531)
Epoch: [42][10/196]	Time 0.017 (0.018)	Data 0.000 (0.019)	Loss 1.9431 (2.0493)	Acc@1 70.703 (68.040)	Acc@5 93.750 (92.969)
Epoch: [42][20/196]	Time 0.013 (0.016)	Data 0.006 (0.011)	Loss 2.0658 (2.0437)	Acc@1 64.844 (68.341)	Acc@5 93.359 (93.062)
Epoch: [42][30/196]	Time 0.019 (0.016)	Data 0.000 (0.008)	Loss 1.9897 (2.0173)	Acc@1 71.484 (69.166)	Acc@5 93.359 (93.259)
Epoch: [42][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 1.9977 (2.0153)	Acc@1 71.875 (69.312)	Acc@5 92.188 (93.197)
Epoch: [42][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 2.0532 (2.0159)	Acc@1 66.797 (69.363)	Acc@5 92.969 (93.168)
Epoch: [42][60/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9234 (2.0165)	Acc@1 72.656 (69.173)	Acc@5 94.141 (93.186)
Epoch: [42][70/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 2.1243 (2.0282)	Acc@1 67.578 (69.014)	Acc@5 91.016 (92.996)
Epoch: [42][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.8577 (2.0342)	Acc@1 74.609 (68.682)	Acc@5 94.531 (93.003)
Epoch: [42][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9546 (2.0452)	Acc@1 71.484 (68.355)	Acc@5 93.750 (92.857)
Epoch: [42][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0547 (2.0527)	Acc@1 69.531 (68.325)	Acc@5 91.797 (92.694)
Epoch: [42][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0231 (2.0570)	Acc@1 67.188 (68.208)	Acc@5 92.969 (92.631)
Epoch: [42][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.2014 (2.0620)	Acc@1 65.234 (68.030)	Acc@5 91.406 (92.588)
Epoch: [42][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.2334 (2.0680)	Acc@1 64.453 (67.876)	Acc@5 88.281 (92.521)
Epoch: [42][140/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9964 (2.0710)	Acc@1 69.531 (67.875)	Acc@5 95.312 (92.484)
Epoch: [42][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0696 (2.0758)	Acc@1 67.969 (67.749)	Acc@5 91.797 (92.363)
Epoch: [42][160/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.0812 (2.0774)	Acc@1 70.703 (67.758)	Acc@5 89.844 (92.304)
Epoch: [42][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9869 (2.0850)	Acc@1 70.312 (67.562)	Acc@5 92.969 (92.183)
Epoch: [42][180/196]	Time 0.013 (0.016)	Data 0.005 (0.003)	Loss 2.2318 (2.0907)	Acc@1 63.672 (67.429)	Acc@5 90.234 (92.123)
Epoch: [42][190/196]	Time 0.016 (0.016)	Data 0.003 (0.003)	Loss 2.3464 (2.0952)	Acc@1 57.422 (67.298)	Acc@5 89.844 (92.034)
num momentum params: 26
[0.1, 2.095866944580078, 1.7560198557376863, 67.282, 54.31, tensor(0.4658, device='cuda:0', grad_fn=<DivBackward0>), 3.1074705123901363, 0.37580418586730957]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [43 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [43][0/196]	Time 0.042 (0.042)	Data 0.190 (0.190)	Loss 2.0670 (2.0670)	Acc@1 66.797 (66.797)	Acc@5 92.578 (92.578)
Epoch: [43][10/196]	Time 0.019 (0.018)	Data 0.001 (0.019)	Loss 1.9994 (2.0121)	Acc@1 71.094 (69.638)	Acc@5 94.141 (93.111)
Epoch: [43][20/196]	Time 0.012 (0.017)	Data 0.006 (0.011)	Loss 1.9898 (2.0003)	Acc@1 69.531 (69.996)	Acc@5 94.141 (92.969)
Epoch: [43][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 2.0231 (2.0002)	Acc@1 71.484 (70.123)	Acc@5 93.359 (93.208)
Epoch: [43][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 2.0143 (1.9976)	Acc@1 67.969 (70.274)	Acc@5 91.406 (93.150)
Epoch: [43][50/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 1.9508 (1.9952)	Acc@1 70.703 (70.458)	Acc@5 94.141 (93.068)
Epoch: [43][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9218 (2.0080)	Acc@1 69.922 (69.973)	Acc@5 94.922 (93.020)
Epoch: [43][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1604 (2.0120)	Acc@1 64.453 (69.812)	Acc@5 92.188 (93.007)
Epoch: [43][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0622 (2.0161)	Acc@1 66.797 (69.695)	Acc@5 93.750 (92.978)
Epoch: [43][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0555 (2.0323)	Acc@1 67.578 (69.274)	Acc@5 91.016 (92.771)
Epoch: [43][100/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0499 (2.0407)	Acc@1 69.922 (68.963)	Acc@5 90.625 (92.663)
Epoch: [43][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1604 (2.0503)	Acc@1 65.234 (68.725)	Acc@5 90.234 (92.508)
Epoch: [43][120/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1044 (2.0567)	Acc@1 70.312 (68.514)	Acc@5 91.797 (92.413)
Epoch: [43][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.2752 (2.0607)	Acc@1 62.891 (68.386)	Acc@5 91.016 (92.408)
Epoch: [43][140/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.1430 (2.0667)	Acc@1 62.500 (68.251)	Acc@5 92.969 (92.312)
Epoch: [43][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9497 (2.0733)	Acc@1 68.750 (68.064)	Acc@5 94.141 (92.250)
Epoch: [43][160/196]	Time 0.014 (0.016)	Data 0.008 (0.004)	Loss 2.0716 (2.0802)	Acc@1 66.016 (67.923)	Acc@5 91.797 (92.158)
Epoch: [43][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0094 (2.0830)	Acc@1 71.875 (67.820)	Acc@5 91.406 (92.114)
Epoch: [43][180/196]	Time 0.012 (0.016)	Data 0.006 (0.003)	Loss 2.1337 (2.0877)	Acc@1 67.969 (67.723)	Acc@5 91.797 (92.000)
Epoch: [43][190/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 2.2681 (2.0907)	Acc@1 61.719 (67.646)	Acc@5 91.797 (91.991)
num momentum params: 26
[0.1, 2.0913802016448972, 2.040040683746338, 67.648, 49.19, tensor(0.4680, device='cuda:0', grad_fn=<DivBackward0>), 3.10483980178833, 0.37778472900390625]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [44 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [44][0/196]	Time 0.040 (0.040)	Data 0.173 (0.173)	Loss 1.9702 (1.9702)	Acc@1 71.484 (71.484)	Acc@5 93.359 (93.359)
Epoch: [44][10/196]	Time 0.015 (0.018)	Data 0.003 (0.018)	Loss 1.8985 (2.0332)	Acc@1 73.047 (68.928)	Acc@5 94.141 (92.614)
Epoch: [44][20/196]	Time 0.015 (0.017)	Data 0.004 (0.011)	Loss 2.0089 (2.0048)	Acc@1 68.359 (69.382)	Acc@5 93.359 (93.359)
Epoch: [44][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 2.0710 (1.9821)	Acc@1 67.969 (70.123)	Acc@5 89.844 (93.649)
Epoch: [44][40/196]	Time 0.012 (0.016)	Data 0.007 (0.007)	Loss 1.9641 (1.9964)	Acc@1 71.094 (69.588)	Acc@5 92.188 (93.350)
Epoch: [44][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0067 (2.0024)	Acc@1 72.656 (69.470)	Acc@5 92.188 (93.298)
Epoch: [44][60/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0334 (2.0089)	Acc@1 71.484 (69.371)	Acc@5 93.359 (93.295)
Epoch: [44][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0162 (2.0145)	Acc@1 69.922 (69.394)	Acc@5 91.797 (93.167)
Epoch: [44][80/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 2.0436 (2.0183)	Acc@1 67.969 (69.348)	Acc@5 92.188 (93.094)
Epoch: [44][90/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 1.9606 (2.0273)	Acc@1 71.094 (69.115)	Acc@5 93.750 (92.960)
Epoch: [44][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0934 (2.0349)	Acc@1 65.625 (68.963)	Acc@5 91.016 (92.806)
Epoch: [44][110/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1862 (2.0436)	Acc@1 67.188 (68.768)	Acc@5 91.016 (92.698)
Epoch: [44][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2310 (2.0525)	Acc@1 63.672 (68.524)	Acc@5 91.016 (92.604)
Epoch: [44][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1535 (2.0571)	Acc@1 64.844 (68.428)	Acc@5 92.578 (92.551)
Epoch: [44][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9531 (2.0586)	Acc@1 69.141 (68.390)	Acc@5 95.312 (92.611)
Epoch: [44][150/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0349 (2.0616)	Acc@1 70.703 (68.284)	Acc@5 92.969 (92.563)
Epoch: [44][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.0415 (2.0646)	Acc@1 68.750 (68.224)	Acc@5 93.359 (92.544)
Epoch: [44][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2689 (2.0731)	Acc@1 64.453 (68.001)	Acc@5 89.453 (92.414)
Epoch: [44][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1935 (2.0757)	Acc@1 64.062 (67.967)	Acc@5 89.062 (92.364)
Epoch: [44][190/196]	Time 0.016 (0.016)	Data 0.001 (0.003)	Loss 2.1613 (2.0817)	Acc@1 66.406 (67.807)	Acc@5 92.578 (92.314)
num momentum params: 26
[0.1, 2.0853083529663086, 1.82438716173172, 67.688, 52.99, tensor(0.4710, device='cuda:0', grad_fn=<DivBackward0>), 3.124391555786133, 0.3771796226501465]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [45 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [45][0/196]	Time 0.039 (0.039)	Data 0.181 (0.181)	Loss 1.9967 (1.9967)	Acc@1 72.656 (72.656)	Acc@5 94.141 (94.141)
Epoch: [45][10/196]	Time 0.016 (0.018)	Data 0.002 (0.019)	Loss 1.9840 (2.0498)	Acc@1 70.312 (68.359)	Acc@5 92.969 (93.217)
Epoch: [45][20/196]	Time 0.012 (0.017)	Data 0.006 (0.011)	Loss 2.1499 (2.0592)	Acc@1 64.453 (68.173)	Acc@5 91.406 (92.839)
Epoch: [45][30/196]	Time 0.018 (0.016)	Data 0.001 (0.008)	Loss 2.0957 (2.0514)	Acc@1 68.359 (68.448)	Acc@5 91.406 (92.956)
Epoch: [45][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 2.0906 (2.0422)	Acc@1 68.750 (68.893)	Acc@5 92.188 (92.940)
Epoch: [45][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.8770 (2.0315)	Acc@1 73.438 (69.386)	Acc@5 93.750 (93.053)
Epoch: [45][60/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.0839 (2.0361)	Acc@1 67.188 (69.166)	Acc@5 92.578 (92.975)
Epoch: [45][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0150 (2.0385)	Acc@1 66.016 (69.069)	Acc@5 94.531 (92.919)
Epoch: [45][80/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1145 (2.0426)	Acc@1 65.234 (69.010)	Acc@5 91.016 (92.824)
Epoch: [45][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2492 (2.0476)	Acc@1 67.188 (68.990)	Acc@5 90.625 (92.724)
Epoch: [45][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0869 (2.0495)	Acc@1 67.578 (68.936)	Acc@5 89.844 (92.628)
Epoch: [45][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0512 (2.0491)	Acc@1 67.969 (68.975)	Acc@5 92.578 (92.680)
Epoch: [45][120/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1004 (2.0564)	Acc@1 66.797 (68.844)	Acc@5 91.797 (92.594)
Epoch: [45][130/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 2.1338 (2.0642)	Acc@1 65.234 (68.646)	Acc@5 92.578 (92.513)
Epoch: [45][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2637 (2.0680)	Acc@1 67.188 (68.623)	Acc@5 89.062 (92.429)
Epoch: [45][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1089 (2.0725)	Acc@1 66.016 (68.543)	Acc@5 92.188 (92.392)
Epoch: [45][160/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9855 (2.0746)	Acc@1 71.094 (68.532)	Acc@5 94.141 (92.311)
Epoch: [45][170/196]	Time 0.018 (0.016)	Data 0.002 (0.003)	Loss 2.4024 (2.0778)	Acc@1 60.547 (68.435)	Acc@5 87.109 (92.311)
Epoch: [45][180/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.1307 (2.0816)	Acc@1 66.797 (68.308)	Acc@5 90.625 (92.289)
Epoch: [45][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1823 (2.0862)	Acc@1 58.984 (68.116)	Acc@5 93.359 (92.249)
num momentum params: 26
[0.1, 2.0886105699920656, 1.9953591561317443, 68.064, 49.43, tensor(0.4721, device='cuda:0', grad_fn=<DivBackward0>), 3.110119581222534, 0.3847041130065918]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [46 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [46][0/196]	Time 0.040 (0.040)	Data 0.172 (0.172)	Loss 1.9718 (1.9718)	Acc@1 67.188 (67.188)	Acc@5 96.094 (96.094)
Epoch: [46][10/196]	Time 0.016 (0.018)	Data 0.002 (0.018)	Loss 2.0299 (2.0198)	Acc@1 71.484 (70.597)	Acc@5 93.750 (93.395)
Epoch: [46][20/196]	Time 0.015 (0.017)	Data 0.003 (0.010)	Loss 1.9852 (1.9928)	Acc@1 65.625 (71.019)	Acc@5 93.750 (93.750)
Epoch: [46][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 1.9910 (1.9969)	Acc@1 73.047 (70.854)	Acc@5 93.750 (93.662)
Epoch: [46][40/196]	Time 0.013 (0.017)	Data 0.006 (0.006)	Loss 1.9296 (1.9871)	Acc@1 73.438 (71.027)	Acc@5 93.750 (93.607)
Epoch: [46][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0962 (1.9801)	Acc@1 70.312 (71.109)	Acc@5 92.188 (93.735)
Epoch: [46][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0835 (1.9916)	Acc@1 67.969 (70.665)	Acc@5 91.406 (93.590)
Epoch: [46][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0619 (2.0033)	Acc@1 69.141 (70.340)	Acc@5 92.969 (93.431)
Epoch: [46][80/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0972 (2.0121)	Acc@1 67.969 (69.989)	Acc@5 92.969 (93.316)
Epoch: [46][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0650 (2.0127)	Acc@1 66.016 (69.973)	Acc@5 94.141 (93.304)
Epoch: [46][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0579 (2.0216)	Acc@1 71.484 (69.787)	Acc@5 93.750 (93.193)
Epoch: [46][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0843 (2.0259)	Acc@1 68.359 (69.591)	Acc@5 92.969 (93.148)
Epoch: [46][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0398 (2.0361)	Acc@1 68.750 (69.244)	Acc@5 91.016 (93.020)
Epoch: [46][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.3103 (2.0471)	Acc@1 65.625 (68.980)	Acc@5 86.719 (92.849)
Epoch: [46][140/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 1.9401 (2.0541)	Acc@1 69.922 (68.792)	Acc@5 94.922 (92.819)
Epoch: [46][150/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.2277 (2.0583)	Acc@1 65.625 (68.727)	Acc@5 92.188 (92.757)
Epoch: [46][160/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.2497 (2.0666)	Acc@1 64.453 (68.507)	Acc@5 92.188 (92.629)
Epoch: [46][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2049 (2.0716)	Acc@1 63.672 (68.371)	Acc@5 92.969 (92.603)
Epoch: [46][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0545 (2.0764)	Acc@1 70.312 (68.234)	Acc@5 92.969 (92.528)
Epoch: [46][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1427 (2.0804)	Acc@1 66.016 (68.108)	Acc@5 90.625 (92.482)
num momentum params: 26
[0.1, 2.083558588027954, 1.963595793247223, 68.044, 50.79, tensor(0.4744, device='cuda:0', grad_fn=<DivBackward0>), 3.1304001808166504, 0.37474894523620605]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [47 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [47][0/196]	Time 0.041 (0.041)	Data 0.181 (0.181)	Loss 1.9228 (1.9228)	Acc@1 71.094 (71.094)	Acc@5 93.359 (93.359)
Epoch: [47][10/196]	Time 0.019 (0.018)	Data 0.000 (0.019)	Loss 1.8849 (2.0204)	Acc@1 74.219 (70.099)	Acc@5 93.359 (92.862)
Epoch: [47][20/196]	Time 0.011 (0.016)	Data 0.006 (0.011)	Loss 2.0810 (2.0235)	Acc@1 67.188 (69.624)	Acc@5 93.750 (93.006)
Epoch: [47][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.9266 (1.9961)	Acc@1 73.438 (70.464)	Acc@5 94.141 (93.460)
Epoch: [47][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 2.0721 (1.9972)	Acc@1 67.188 (70.303)	Acc@5 91.406 (93.426)
Epoch: [47][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.9376 (2.0011)	Acc@1 70.703 (70.320)	Acc@5 95.312 (93.359)
Epoch: [47][60/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0446 (2.0046)	Acc@1 72.266 (70.293)	Acc@5 91.797 (93.347)
Epoch: [47][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0593 (2.0034)	Acc@1 69.141 (70.312)	Acc@5 91.016 (93.310)
Epoch: [47][80/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 1.9627 (2.0063)	Acc@1 73.438 (70.129)	Acc@5 94.922 (93.282)
Epoch: [47][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2539 (2.0149)	Acc@1 66.797 (69.948)	Acc@5 89.453 (93.132)
Epoch: [47][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.1769 (2.0209)	Acc@1 67.188 (69.868)	Acc@5 91.016 (93.019)
Epoch: [47][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1430 (2.0240)	Acc@1 63.281 (69.827)	Acc@5 94.531 (93.046)
Epoch: [47][120/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.1890 (2.0338)	Acc@1 63.281 (69.531)	Acc@5 90.625 (92.946)
Epoch: [47][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9275 (2.0378)	Acc@1 69.922 (69.436)	Acc@5 92.578 (92.861)
Epoch: [47][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0636 (2.0442)	Acc@1 66.406 (69.207)	Acc@5 92.578 (92.800)
Epoch: [47][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2338 (2.0459)	Acc@1 66.797 (69.128)	Acc@5 89.844 (92.806)
Epoch: [47][160/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0782 (2.0513)	Acc@1 68.359 (69.024)	Acc@5 90.625 (92.702)
Epoch: [47][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0635 (2.0542)	Acc@1 64.453 (68.857)	Acc@5 92.969 (92.685)
Epoch: [47][180/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.2051 (2.0585)	Acc@1 65.625 (68.748)	Acc@5 91.406 (92.645)
Epoch: [47][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0394 (2.0631)	Acc@1 69.922 (68.642)	Acc@5 90.234 (92.554)
num momentum params: 26
[0.1, 2.0648096658706665, 1.8502488481998443, 68.59, 52.96, tensor(0.4794, device='cuda:0', grad_fn=<DivBackward0>), 3.101387739181518, 0.3778505325317383]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [48 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [48][0/196]	Time 0.041 (0.041)	Data 0.179 (0.179)	Loss 1.9689 (1.9689)	Acc@1 69.531 (69.531)	Acc@5 96.094 (96.094)
Epoch: [48][10/196]	Time 0.017 (0.018)	Data 0.001 (0.018)	Loss 2.0001 (2.0095)	Acc@1 68.359 (70.099)	Acc@5 93.359 (93.643)
Epoch: [48][20/196]	Time 0.017 (0.017)	Data 0.003 (0.011)	Loss 2.0165 (2.0046)	Acc@1 71.875 (70.517)	Acc@5 92.188 (93.359)
Epoch: [48][30/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 2.0963 (2.0102)	Acc@1 64.844 (70.237)	Acc@5 95.312 (93.511)
Epoch: [48][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 2.0902 (2.0108)	Acc@1 73.047 (70.036)	Acc@5 91.797 (93.569)
Epoch: [48][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.1459 (2.0071)	Acc@1 67.188 (70.029)	Acc@5 91.406 (93.597)
Epoch: [48][60/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.1093 (2.0119)	Acc@1 64.844 (69.832)	Acc@5 92.188 (93.616)
Epoch: [48][70/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.1651 (2.0123)	Acc@1 67.188 (69.845)	Acc@5 95.312 (93.618)
Epoch: [48][80/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2338 (2.0209)	Acc@1 64.844 (69.618)	Acc@5 91.016 (93.547)
Epoch: [48][90/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2186 (2.0288)	Acc@1 66.406 (69.510)	Acc@5 91.016 (93.402)
Epoch: [48][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0023 (2.0291)	Acc@1 69.922 (69.512)	Acc@5 92.578 (93.359)
Epoch: [48][110/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1082 (2.0362)	Acc@1 70.703 (69.348)	Acc@5 91.016 (93.233)
Epoch: [48][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1768 (2.0415)	Acc@1 68.359 (69.228)	Acc@5 89.453 (93.143)
Epoch: [48][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1022 (2.0470)	Acc@1 68.359 (69.129)	Acc@5 92.578 (93.064)
Epoch: [48][140/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1293 (2.0500)	Acc@1 66.406 (68.988)	Acc@5 90.234 (93.027)
Epoch: [48][150/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.2972 (2.0539)	Acc@1 62.500 (68.874)	Acc@5 90.234 (93.008)
Epoch: [48][160/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.1435 (2.0558)	Acc@1 66.016 (68.835)	Acc@5 91.797 (92.964)
Epoch: [48][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.2087 (2.0627)	Acc@1 60.938 (68.633)	Acc@5 92.188 (92.921)
Epoch: [48][180/196]	Time 0.015 (0.016)	Data 0.003 (0.003)	Loss 2.1338 (2.0686)	Acc@1 67.578 (68.506)	Acc@5 90.625 (92.807)
Epoch: [48][190/196]	Time 0.014 (0.016)	Data 0.003 (0.003)	Loss 2.0123 (2.0722)	Acc@1 67.188 (68.380)	Acc@5 94.141 (92.754)
num momentum params: 26
[0.1, 2.0733902284240724, 2.0897332668304442, 68.34, 48.35, tensor(0.4790, device='cuda:0', grad_fn=<DivBackward0>), 3.124567270278931, 0.3773672580718994]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [49 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [49][0/196]	Time 0.041 (0.041)	Data 0.193 (0.193)	Loss 2.0494 (2.0494)	Acc@1 69.922 (69.922)	Acc@5 93.750 (93.750)
Epoch: [49][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 1.9538 (2.0594)	Acc@1 71.484 (68.679)	Acc@5 94.922 (92.578)
Epoch: [49][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 2.0764 (2.0240)	Acc@1 67.578 (69.475)	Acc@5 92.578 (93.192)
Epoch: [49][30/196]	Time 0.017 (0.017)	Data 0.000 (0.008)	Loss 1.8838 (2.0140)	Acc@1 75.391 (69.897)	Acc@5 94.531 (93.435)
Epoch: [49][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 2.0605 (2.0221)	Acc@1 69.531 (69.684)	Acc@5 95.312 (93.417)
Epoch: [49][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9923 (2.0311)	Acc@1 68.359 (69.493)	Acc@5 92.578 (93.237)
Epoch: [49][60/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.1673 (2.0371)	Acc@1 66.406 (69.397)	Acc@5 91.406 (93.225)
Epoch: [49][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0619 (2.0386)	Acc@1 67.969 (69.339)	Acc@5 94.141 (93.156)
Epoch: [49][80/196]	Time 0.017 (0.016)	Data 0.002 (0.005)	Loss 1.9620 (2.0360)	Acc@1 72.266 (69.579)	Acc@5 96.094 (93.176)
Epoch: [49][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0801 (2.0377)	Acc@1 68.359 (69.510)	Acc@5 92.188 (93.213)
Epoch: [49][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9977 (2.0388)	Acc@1 69.922 (69.496)	Acc@5 92.188 (93.181)
Epoch: [49][110/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1397 (2.0438)	Acc@1 64.062 (69.320)	Acc@5 93.359 (93.127)
Epoch: [49][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2585 (2.0536)	Acc@1 63.672 (69.057)	Acc@5 91.016 (93.030)
Epoch: [49][130/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 2.0579 (2.0573)	Acc@1 68.750 (69.033)	Acc@5 92.188 (92.978)
Epoch: [49][140/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2298 (2.0601)	Acc@1 60.547 (68.900)	Acc@5 91.406 (92.980)
Epoch: [49][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2201 (2.0644)	Acc@1 65.625 (68.809)	Acc@5 88.672 (92.907)
Epoch: [49][160/196]	Time 0.017 (0.016)	Data 0.002 (0.003)	Loss 2.1947 (2.0695)	Acc@1 67.188 (68.668)	Acc@5 89.844 (92.821)
Epoch: [49][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1750 (2.0744)	Acc@1 65.625 (68.554)	Acc@5 90.234 (92.729)
Epoch: [49][180/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9730 (2.0756)	Acc@1 67.969 (68.472)	Acc@5 92.969 (92.708)
Epoch: [49][190/196]	Time 0.015 (0.016)	Data 0.002 (0.003)	Loss 2.0212 (2.0814)	Acc@1 69.141 (68.368)	Acc@5 94.141 (92.617)
num momentum params: 26
[0.1, 2.0822325975418092, 2.049434608221054, 68.396, 49.45, tensor(0.4791, device='cuda:0', grad_fn=<DivBackward0>), 3.1505560874938965, 0.3764464855194092]
Non Pruning Epoch - module.conv1.weight: [60, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [60]
Non Pruning Epoch - module.bn1.bias: [60]
Non Pruning Epoch - module.conv2.weight: [128, 60, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [512, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [512]
Non Pruning Epoch - module.bn5.bias: [512]
Non Pruning Epoch - module.conv6.weight: [512, 512, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [508, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [508]
Non Pruning Epoch - module.bn7.bias: [508]
Non Pruning Epoch - module.conv8.weight: [479, 508, 3, 3]
Non Pruning Epoch - module.bn8.weight: [479]
Non Pruning Epoch - module.bn8.bias: [479]
Non Pruning Epoch - module.fc.weight: [100, 479]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [50 | 180] LR: 0.100000
module.conv1.weight [60, 3, 3, 3]
module.conv2.weight [128, 60, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [512, 256, 3, 3]
module.conv6.weight [512, 512, 3, 3]
module.conv7.weight [508, 512, 3, 3]
module.conv8.weight [479, 508, 3, 3]
Epoch: [50][0/196]	Time 0.038 (0.038)	Data 0.174 (0.174)	Loss 1.9727 (1.9727)	Acc@1 67.578 (67.578)	Acc@5 94.141 (94.141)
Epoch: [50][10/196]	Time 0.016 (0.018)	Data 0.002 (0.018)	Loss 2.1196 (2.0117)	Acc@1 67.188 (69.993)	Acc@5 91.406 (93.182)
Epoch: [50][20/196]	Time 0.016 (0.017)	Data 0.003 (0.010)	Loss 2.1747 (2.0114)	Acc@1 65.234 (69.940)	Acc@5 88.672 (93.285)
Epoch: [50][30/196]	Time 0.016 (0.016)	Data 0.003 (0.008)	Loss 1.8725 (1.9981)	Acc@1 73.828 (70.539)	Acc@5 94.922 (93.599)
Epoch: [50][40/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.1313 (2.0152)	Acc@1 68.359 (70.093)	Acc@5 94.141 (93.521)
Epoch: [50][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9693 (2.0179)	Acc@1 69.922 (69.761)	Acc@5 93.750 (93.536)
Epoch: [50][60/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.2479 (2.0296)	Acc@1 63.672 (69.422)	Acc@5 91.797 (93.462)
Epoch: [50][70/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.8764 (2.0299)	Acc@1 73.828 (69.355)	Acc@5 94.141 (93.464)
Epoch: [50][80/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1327 (2.0304)	Acc@1 67.188 (69.444)	Acc@5 93.359 (93.451)
Epoch: [50][90/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1070 (2.0358)	Acc@1 70.312 (69.188)	Acc@5 92.969 (93.407)
Epoch: [50][100/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1502 (2.0372)	Acc@1 67.578 (69.172)	Acc@5 90.234 (93.340)
Epoch: [50][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2124 (2.0440)	Acc@1 65.234 (69.010)	Acc@5 91.016 (93.180)
Epoch: [50][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0558 (2.0502)	Acc@1 64.453 (68.802)	Acc@5 94.141 (93.108)
Epoch: [50][130/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.0787 (2.0552)	Acc@1 68.750 (68.690)	Acc@5 91.797 (93.040)
Epoch: [50][140/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.2658 (2.0591)	Acc@1 58.203 (68.598)	Acc@5 88.672 (92.960)
Epoch: [50][150/196]	Time 0.017 (0.016)	Data 0.001 (0.003)	Loss 2.1542 (2.0629)	Acc@1 68.750 (68.540)	Acc@5 91.406 (92.868)
Epoch: [50][160/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.0571 (2.0689)	Acc@1 69.141 (68.432)	Acc@5 92.188 (92.758)
Epoch: [50][170/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.0228 (2.0705)	Acc@1 68.359 (68.394)	Acc@5 94.922 (92.697)
Epoch: [50][180/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.0865 (2.0723)	Acc@1 69.531 (68.346)	Acc@5 90.625 (92.671)
Epoch: [50][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 2.1989 (2.0744)	Acc@1 64.844 (68.282)	Acc@5 91.016 (92.635)
num momentum params: 26
[0.1, 2.074821039199829, 1.7717145109176635, 68.26, 54.96, tensor(0.4818, device='cuda:0', grad_fn=<DivBackward0>), 3.085113763809204, 0.38803672790527344]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [60, 3, 3, 3]
Before - module.bn1.weight: [60]
Before - module.bn1.bias: [60]
Before - module.conv2.weight: [128, 60, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [512, 256, 3, 3]
Before - module.bn5.weight: [512]
Before - module.bn5.bias: [512]
Before - module.conv6.weight: [512, 512, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [508, 512, 3, 3]
Before - module.bn7.weight: [508]
Before - module.bn7.bias: [508]
Before - module.conv8.weight: [479, 508, 3, 3]
Before - module.bn8.weight: [479]
Before - module.bn8.bias: [479]
Before - module.fc.weight: [100, 479]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [60, 3, 3, 3] >> [56, 3, 3, 3]
[module.bn1.weight]: 60 >> 56
running_mean [56]
running_var [56]
num_batches_tracked []
[module.conv2.weight]: [128, 60, 3, 3] >> [128, 56, 3, 3]
[module.conv5.weight]: [512, 256, 3, 3] >> [511, 256, 3, 3]
[module.bn5.weight]: 512 >> 511
running_mean [511]
running_var [511]
num_batches_tracked []
[module.conv6.weight]: [512, 512, 3, 3] >> [512, 511, 3, 3]
[module.conv7.weight]: [508, 512, 3, 3] >> [502, 512, 3, 3]
[module.bn7.weight]: 508 >> 502
running_mean [502]
running_var [502]
num_batches_tracked []
[module.conv8.weight]: [479, 508, 3, 3] >> [451, 502, 3, 3]
[module.bn8.weight]: 479 >> 451
running_mean [451]
running_var [451]
num_batches_tracked []
[module.fc.weight]: [100, 479] >> [100, 451]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [56, 3, 3, 3]
After - module.bn1.weight: [56]
After - module.bn1.bias: [56]
After - module.conv2.weight: [128, 56, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [511, 256, 3, 3]
After - module.bn5.weight: [511]
After - module.bn5.bias: [511]
After - module.conv6.weight: [512, 511, 3, 3]
After - module.bn6.weight: [512]
After - module.bn6.bias: [512]
After - module.conv7.weight: [502, 512, 3, 3]
After - module.bn7.weight: [502]
After - module.bn7.bias: [502]
After - module.conv8.weight: [451, 502, 3, 3]
After - module.bn8.weight: [451]
After - module.bn8.bias: [451]
After - module.fc.weight: [100, 451]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [56, 3, 3, 3]
conv2 --> [128, 56, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [511, 256, 3, 3]
conv6 --> [512, 511, 3, 3]
conv7 --> [502, 512, 3, 3]
conv8 --> [451, 502, 3, 3]
fc --> [451, 100]
1, 620089344, 1548288, 56
2, 6903300096, 16515072, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10247602176, 18837504, 511
6, 20495204352, 37675008, 512
7, 7106199552, 9252864, 502
8, 6259562496, 8150472, 451
fc, 17318400, 45100, 0
===================
FLOP REPORT: 30261489000000.0 57224000000.0 148647412 143060 2672 16.94078254699707
[INFO] Storing checkpoint...

Epoch: [51 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [51][0/196]	Time 0.596 (0.596)	Data 0.194 (0.194)	Loss 1.8492 (1.8492)	Acc@1 75.391 (75.391)	Acc@5 97.266 (97.266)
Epoch: [51][10/196]	Time 0.016 (0.069)	Data 0.003 (0.019)	Loss 2.1440 (1.9688)	Acc@1 68.750 (70.881)	Acc@5 89.453 (94.922)
Epoch: [51][20/196]	Time 0.015 (0.043)	Data 0.003 (0.011)	Loss 2.1579 (1.9642)	Acc@1 67.578 (71.001)	Acc@5 92.578 (94.792)
Epoch: [51][30/196]	Time 0.015 (0.034)	Data 0.003 (0.008)	Loss 1.8748 (1.9711)	Acc@1 70.703 (70.653)	Acc@5 93.359 (94.493)
Epoch: [51][40/196]	Time 0.015 (0.029)	Data 0.002 (0.007)	Loss 1.9912 (1.9690)	Acc@1 70.312 (70.989)	Acc@5 91.406 (94.379)
Epoch: [51][50/196]	Time 0.015 (0.027)	Data 0.003 (0.006)	Loss 2.1345 (1.9783)	Acc@1 65.234 (70.726)	Acc@5 91.797 (94.286)
Epoch: [51][60/196]	Time 0.014 (0.025)	Data 0.003 (0.005)	Loss 2.0035 (1.9883)	Acc@1 73.047 (70.581)	Acc@5 94.141 (94.128)
Epoch: [51][70/196]	Time 0.015 (0.023)	Data 0.002 (0.005)	Loss 2.1140 (1.9911)	Acc@1 69.141 (70.516)	Acc@5 92.969 (93.992)
Epoch: [51][80/196]	Time 0.015 (0.022)	Data 0.002 (0.005)	Loss 2.0422 (1.9923)	Acc@1 67.578 (70.549)	Acc@5 94.922 (93.924)
Epoch: [51][90/196]	Time 0.015 (0.022)	Data 0.002 (0.005)	Loss 2.2341 (1.9953)	Acc@1 64.844 (70.416)	Acc@5 90.625 (93.887)
Epoch: [51][100/196]	Time 0.015 (0.021)	Data 0.003 (0.004)	Loss 2.0811 (1.9987)	Acc@1 67.188 (70.301)	Acc@5 92.578 (93.796)
Epoch: [51][110/196]	Time 0.013 (0.020)	Data 0.004 (0.004)	Loss 2.1619 (2.0046)	Acc@1 65.625 (70.147)	Acc@5 90.625 (93.715)
Epoch: [51][120/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 2.1752 (2.0160)	Acc@1 64.062 (69.880)	Acc@5 91.797 (93.495)
Epoch: [51][130/196]	Time 0.017 (0.020)	Data 0.000 (0.004)	Loss 2.0330 (2.0258)	Acc@1 67.578 (69.633)	Acc@5 94.531 (93.371)
Epoch: [51][140/196]	Time 0.016 (0.019)	Data 0.003 (0.004)	Loss 2.1648 (2.0292)	Acc@1 69.531 (69.551)	Acc@5 91.016 (93.329)
Epoch: [51][150/196]	Time 0.013 (0.019)	Data 0.005 (0.004)	Loss 2.0846 (2.0358)	Acc@1 68.359 (69.428)	Acc@5 92.969 (93.235)
Epoch: [51][160/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 2.1554 (2.0445)	Acc@1 64.062 (69.158)	Acc@5 90.625 (93.124)
Epoch: [51][170/196]	Time 0.012 (0.018)	Data 0.006 (0.004)	Loss 2.1296 (2.0505)	Acc@1 64.844 (68.949)	Acc@5 92.578 (93.051)
Epoch: [51][180/196]	Time 0.017 (0.018)	Data 0.001 (0.004)	Loss 2.2121 (2.0599)	Acc@1 64.844 (68.754)	Acc@5 89.844 (92.923)
Epoch: [51][190/196]	Time 0.011 (0.018)	Data 0.006 (0.004)	Loss 2.3197 (2.0675)	Acc@1 60.156 (68.537)	Acc@5 88.672 (92.799)
num momentum params: 26
[0.1, 2.06973146232605, 1.7956501972675323, 68.478, 53.63, tensor(0.4843, device='cuda:0', grad_fn=<DivBackward0>), 3.766475677490234, 0.46701955795288086]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [52 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [52][0/196]	Time 0.036 (0.036)	Data 0.196 (0.196)	Loss 1.9795 (1.9795)	Acc@1 70.312 (70.312)	Acc@5 94.922 (94.922)
Epoch: [52][10/196]	Time 0.017 (0.018)	Data 0.001 (0.020)	Loss 2.1058 (1.9790)	Acc@1 66.797 (71.946)	Acc@5 92.188 (94.070)
Epoch: [52][20/196]	Time 0.013 (0.016)	Data 0.005 (0.012)	Loss 2.0651 (1.9827)	Acc@1 69.922 (71.931)	Acc@5 93.359 (94.048)
Epoch: [52][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 2.1134 (2.0067)	Acc@1 67.578 (70.804)	Acc@5 90.625 (93.574)
Epoch: [52][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9626 (2.0021)	Acc@1 74.609 (70.894)	Acc@5 95.312 (93.740)
Epoch: [52][50/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9982 (2.0010)	Acc@1 69.531 (70.826)	Acc@5 93.359 (93.620)
Epoch: [52][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.1156 (2.0006)	Acc@1 67.188 (70.825)	Acc@5 92.969 (93.596)
Epoch: [52][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0234 (2.0025)	Acc@1 71.875 (70.852)	Acc@5 93.750 (93.502)
Epoch: [52][80/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.0326 (2.0027)	Acc@1 67.969 (70.742)	Acc@5 94.922 (93.538)
Epoch: [52][90/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1081 (2.0065)	Acc@1 66.016 (70.514)	Acc@5 91.797 (93.437)
Epoch: [52][100/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 2.0395 (2.0131)	Acc@1 69.141 (70.386)	Acc@5 91.797 (93.328)
Epoch: [52][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.2703 (2.0188)	Acc@1 64.453 (70.182)	Acc@5 89.844 (93.275)
Epoch: [52][120/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1904 (2.0283)	Acc@1 66.016 (69.925)	Acc@5 91.016 (93.188)
Epoch: [52][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1535 (2.0355)	Acc@1 66.016 (69.761)	Acc@5 91.016 (93.073)
Epoch: [52][140/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.1687 (2.0434)	Acc@1 65.625 (69.481)	Acc@5 91.406 (92.994)
Epoch: [52][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1834 (2.0538)	Acc@1 65.234 (69.216)	Acc@5 91.016 (92.860)
Epoch: [52][160/196]	Time 0.015 (0.015)	Data 0.004 (0.004)	Loss 2.0227 (2.0571)	Acc@1 69.531 (69.131)	Acc@5 92.578 (92.843)
Epoch: [52][170/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0515 (2.0613)	Acc@1 66.016 (68.985)	Acc@5 94.531 (92.820)
Epoch: [52][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0492 (2.0631)	Acc@1 69.922 (68.946)	Acc@5 92.969 (92.798)
Epoch: [52][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1857 (2.0683)	Acc@1 60.547 (68.817)	Acc@5 94.141 (92.766)
num momentum params: 26
[0.1, 2.06990126083374, 2.090711843967438, 68.796, 48.83, tensor(0.4856, device='cuda:0', grad_fn=<DivBackward0>), 2.9501256942749023, 0.37452149391174316]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [53 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [53][0/196]	Time 0.039 (0.039)	Data 0.190 (0.190)	Loss 2.0992 (2.0992)	Acc@1 67.969 (67.969)	Acc@5 93.359 (93.359)
Epoch: [53][10/196]	Time 0.016 (0.018)	Data 0.002 (0.019)	Loss 1.9335 (2.0301)	Acc@1 70.312 (70.561)	Acc@5 94.531 (93.217)
Epoch: [53][20/196]	Time 0.013 (0.017)	Data 0.005 (0.011)	Loss 1.9770 (2.0157)	Acc@1 71.094 (70.703)	Acc@5 94.141 (93.155)
Epoch: [53][30/196]	Time 0.017 (0.016)	Data 0.001 (0.008)	Loss 1.9083 (2.0007)	Acc@1 74.219 (70.993)	Acc@5 94.922 (93.536)
Epoch: [53][40/196]	Time 0.014 (0.016)	Data 0.005 (0.007)	Loss 2.0215 (1.9950)	Acc@1 70.703 (71.008)	Acc@5 93.359 (93.455)
Epoch: [53][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1665 (1.9974)	Acc@1 62.891 (70.910)	Acc@5 93.359 (93.513)
Epoch: [53][60/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0034 (1.9922)	Acc@1 69.141 (70.876)	Acc@5 92.969 (93.609)
Epoch: [53][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2007 (2.0041)	Acc@1 66.797 (70.527)	Acc@5 91.797 (93.508)
Epoch: [53][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9013 (2.0123)	Acc@1 73.828 (70.332)	Acc@5 94.922 (93.494)
Epoch: [53][90/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0488 (2.0180)	Acc@1 69.922 (70.175)	Acc@5 92.969 (93.377)
Epoch: [53][100/196]	Time 0.021 (0.016)	Data 0.012 (0.004)	Loss 2.0626 (2.0232)	Acc@1 69.531 (70.084)	Acc@5 94.141 (93.340)
Epoch: [53][110/196]	Time 0.028 (0.016)	Data 0.005 (0.004)	Loss 2.0053 (2.0282)	Acc@1 68.359 (69.978)	Acc@5 94.531 (93.275)
Epoch: [53][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1134 (2.0360)	Acc@1 64.062 (69.657)	Acc@5 94.531 (93.208)
Epoch: [53][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1341 (2.0440)	Acc@1 67.969 (69.457)	Acc@5 92.969 (93.136)
Epoch: [53][140/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.0721 (2.0465)	Acc@1 70.703 (69.401)	Acc@5 92.969 (93.082)
Epoch: [53][150/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1934 (2.0497)	Acc@1 66.797 (69.311)	Acc@5 91.797 (93.046)
Epoch: [53][160/196]	Time 0.015 (0.016)	Data 0.006 (0.004)	Loss 2.0376 (2.0557)	Acc@1 67.188 (69.126)	Acc@5 94.531 (92.991)
Epoch: [53][170/196]	Time 0.020 (0.016)	Data 0.003 (0.004)	Loss 2.3758 (2.0615)	Acc@1 62.500 (68.978)	Acc@5 88.281 (92.912)
Epoch: [53][180/196]	Time 0.030 (0.016)	Data 0.002 (0.004)	Loss 2.2698 (2.0652)	Acc@1 63.672 (68.927)	Acc@5 89.844 (92.822)
Epoch: [53][190/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1628 (2.0694)	Acc@1 67.969 (68.803)	Acc@5 89.844 (92.764)
num momentum params: 26
[0.1, 2.072002599029541, 2.1110212767124175, 68.738, 48.4, tensor(0.4860, device='cuda:0', grad_fn=<DivBackward0>), 3.0955662727355957, 0.3749730587005615]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [54 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [54][0/196]	Time 0.035 (0.035)	Data 0.195 (0.195)	Loss 1.9471 (1.9471)	Acc@1 69.922 (69.922)	Acc@5 94.922 (94.922)
Epoch: [54][10/196]	Time 0.018 (0.018)	Data 0.001 (0.020)	Loss 1.9405 (2.0108)	Acc@1 73.438 (70.064)	Acc@5 94.531 (94.141)
Epoch: [54][20/196]	Time 0.013 (0.016)	Data 0.006 (0.012)	Loss 2.0625 (2.0176)	Acc@1 68.359 (69.940)	Acc@5 93.750 (94.103)
Epoch: [54][30/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.8751 (2.0004)	Acc@1 75.391 (70.413)	Acc@5 95.312 (94.002)
Epoch: [54][40/196]	Time 0.011 (0.016)	Data 0.017 (0.007)	Loss 2.0650 (1.9986)	Acc@1 66.797 (70.598)	Acc@5 95.312 (93.979)
Epoch: [54][50/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.9067 (2.0017)	Acc@1 73.047 (70.466)	Acc@5 94.531 (93.857)
Epoch: [54][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.1428 (2.0057)	Acc@1 66.406 (70.453)	Acc@5 91.016 (93.705)
Epoch: [54][70/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.2477 (2.0110)	Acc@1 66.016 (70.285)	Acc@5 89.844 (93.640)
Epoch: [54][80/196]	Time 0.035 (0.016)	Data 0.005 (0.005)	Loss 2.1513 (2.0085)	Acc@1 65.625 (70.491)	Acc@5 92.578 (93.625)
Epoch: [54][90/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 2.1641 (2.0101)	Acc@1 65.234 (70.368)	Acc@5 92.578 (93.647)
Epoch: [54][100/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0367 (2.0100)	Acc@1 72.266 (70.444)	Acc@5 92.578 (93.642)
Epoch: [54][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1332 (2.0112)	Acc@1 66.016 (70.372)	Acc@5 91.406 (93.599)
Epoch: [54][120/196]	Time 0.013 (0.016)	Data 0.012 (0.005)	Loss 2.0334 (2.0137)	Acc@1 69.922 (70.309)	Acc@5 92.969 (93.566)
Epoch: [54][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0835 (2.0182)	Acc@1 67.578 (70.220)	Acc@5 94.141 (93.467)
Epoch: [54][140/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.1322 (2.0263)	Acc@1 66.406 (70.055)	Acc@5 92.969 (93.362)
Epoch: [54][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0798 (2.0347)	Acc@1 67.188 (69.821)	Acc@5 93.359 (93.313)
Epoch: [54][160/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0003 (2.0385)	Acc@1 71.875 (69.740)	Acc@5 94.141 (93.272)
Epoch: [54][170/196]	Time 0.021 (0.016)	Data 0.001 (0.004)	Loss 2.0479 (2.0420)	Acc@1 69.531 (69.616)	Acc@5 93.359 (93.243)
Epoch: [54][180/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.2801 (2.0491)	Acc@1 62.500 (69.410)	Acc@5 89.844 (93.178)
Epoch: [54][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1714 (2.0542)	Acc@1 68.359 (69.300)	Acc@5 93.359 (93.167)
num momentum params: 26
[0.1, 2.0568872412109376, 1.8567490220069884, 69.24, 52.71, tensor(0.4904, device='cuda:0', grad_fn=<DivBackward0>), 3.0469343662261963, 0.3851590156555176]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [55 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [55][0/196]	Time 0.039 (0.039)	Data 0.198 (0.198)	Loss 2.0441 (2.0441)	Acc@1 71.875 (71.875)	Acc@5 91.016 (91.016)
Epoch: [55][10/196]	Time 0.018 (0.018)	Data 0.001 (0.020)	Loss 1.9957 (2.0308)	Acc@1 71.094 (70.561)	Acc@5 93.359 (93.004)
Epoch: [55][20/196]	Time 0.012 (0.016)	Data 0.007 (0.012)	Loss 1.9115 (2.0060)	Acc@1 72.266 (70.740)	Acc@5 95.312 (93.397)
Epoch: [55][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 1.8447 (1.9635)	Acc@1 76.953 (72.190)	Acc@5 96.484 (94.141)
Epoch: [55][40/196]	Time 0.011 (0.016)	Data 0.007 (0.007)	Loss 1.8816 (1.9590)	Acc@1 72.656 (72.085)	Acc@5 96.094 (94.284)
Epoch: [55][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 2.1335 (1.9651)	Acc@1 69.531 (71.745)	Acc@5 92.188 (94.256)
Epoch: [55][60/196]	Time 0.011 (0.016)	Data 0.012 (0.006)	Loss 2.0269 (1.9736)	Acc@1 70.703 (71.369)	Acc@5 93.359 (94.166)
Epoch: [55][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0244 (1.9816)	Acc@1 67.578 (71.132)	Acc@5 92.578 (94.003)
Epoch: [55][80/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.2176 (1.9873)	Acc@1 65.625 (70.988)	Acc@5 89.453 (93.996)
Epoch: [55][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1085 (1.9949)	Acc@1 70.703 (70.823)	Acc@5 93.750 (93.866)
Epoch: [55][100/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.1688 (2.0060)	Acc@1 67.578 (70.579)	Acc@5 93.359 (93.773)
Epoch: [55][110/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0692 (2.0171)	Acc@1 67.188 (70.277)	Acc@5 93.359 (93.606)
Epoch: [55][120/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 2.0146 (2.0230)	Acc@1 73.438 (70.087)	Acc@5 94.141 (93.543)
Epoch: [55][130/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1265 (2.0243)	Acc@1 67.188 (70.068)	Acc@5 94.922 (93.520)
Epoch: [55][140/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.0342 (2.0287)	Acc@1 72.656 (69.902)	Acc@5 91.016 (93.492)
Epoch: [55][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0113 (2.0338)	Acc@1 69.531 (69.821)	Acc@5 94.141 (93.375)
Epoch: [55][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.4000 (2.0413)	Acc@1 59.766 (69.602)	Acc@5 86.719 (93.245)
Epoch: [55][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1964 (2.0479)	Acc@1 66.016 (69.440)	Acc@5 92.188 (93.167)
Epoch: [55][180/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 1.9928 (2.0550)	Acc@1 75.000 (69.259)	Acc@5 89.844 (93.055)
Epoch: [55][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1641 (2.0594)	Acc@1 66.016 (69.173)	Acc@5 92.188 (92.983)
num momentum params: 26
[0.1, 2.0601837351989745, 1.8066783249378204, 69.162, 54.02, tensor(0.4899, device='cuda:0', grad_fn=<DivBackward0>), 3.018134355545044, 0.3748483657836914]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [56 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [56][0/196]	Time 0.041 (0.041)	Data 0.208 (0.208)	Loss 1.7744 (1.7744)	Acc@1 78.125 (78.125)	Acc@5 96.094 (96.094)
Epoch: [56][10/196]	Time 0.017 (0.018)	Data 0.001 (0.021)	Loss 2.0482 (1.9687)	Acc@1 67.969 (71.626)	Acc@5 92.969 (94.034)
Epoch: [56][20/196]	Time 0.013 (0.017)	Data 0.005 (0.012)	Loss 1.9164 (1.9775)	Acc@1 74.609 (71.205)	Acc@5 93.750 (94.196)
Epoch: [56][30/196]	Time 0.013 (0.016)	Data 0.005 (0.009)	Loss 1.9946 (1.9839)	Acc@1 70.703 (70.993)	Acc@5 92.969 (93.989)
Epoch: [56][40/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 1.9620 (1.9724)	Acc@1 71.875 (71.218)	Acc@5 93.359 (94.207)
Epoch: [56][50/196]	Time 0.012 (0.016)	Data 0.005 (0.006)	Loss 2.0226 (1.9714)	Acc@1 71.875 (71.500)	Acc@5 91.797 (94.072)
Epoch: [56][60/196]	Time 0.012 (0.016)	Data 0.011 (0.006)	Loss 1.9206 (1.9711)	Acc@1 76.562 (71.452)	Acc@5 93.750 (94.064)
Epoch: [56][70/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9877 (1.9773)	Acc@1 71.484 (71.248)	Acc@5 94.922 (94.086)
Epoch: [56][80/196]	Time 0.011 (0.016)	Data 0.005 (0.005)	Loss 2.1568 (1.9846)	Acc@1 66.797 (70.978)	Acc@5 92.578 (94.088)
Epoch: [56][90/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 1.9650 (1.9927)	Acc@1 71.484 (70.931)	Acc@5 95.312 (93.960)
Epoch: [56][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.2133 (2.0001)	Acc@1 68.750 (70.777)	Acc@5 90.234 (93.808)
Epoch: [56][110/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0124 (2.0043)	Acc@1 72.266 (70.657)	Acc@5 94.141 (93.764)
Epoch: [56][120/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0296 (2.0084)	Acc@1 72.266 (70.477)	Acc@5 93.359 (93.744)
Epoch: [56][130/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.0247 (2.0140)	Acc@1 66.406 (70.312)	Acc@5 95.312 (93.708)
Epoch: [56][140/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.2253 (2.0185)	Acc@1 64.453 (70.227)	Acc@5 92.188 (93.636)
Epoch: [56][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0743 (2.0215)	Acc@1 71.484 (70.111)	Acc@5 92.578 (93.613)
Epoch: [56][160/196]	Time 0.016 (0.015)	Data 0.007 (0.004)	Loss 2.0407 (2.0232)	Acc@1 69.922 (70.070)	Acc@5 92.188 (93.561)
Epoch: [56][170/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.0965 (2.0304)	Acc@1 64.844 (69.881)	Acc@5 91.016 (93.442)
Epoch: [56][180/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.1618 (2.0371)	Acc@1 67.188 (69.721)	Acc@5 91.406 (93.353)
Epoch: [56][190/196]	Time 0.020 (0.016)	Data 0.000 (0.004)	Loss 2.2085 (2.0443)	Acc@1 64.453 (69.531)	Acc@5 90.234 (93.241)
num momentum params: 26
[0.1, 2.047382935028076, 2.018209390640259, 69.452, 50.05, tensor(0.4932, device='cuda:0', grad_fn=<DivBackward0>), 3.0546178817749023, 0.3931210041046142]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [57 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [57][0/196]	Time 0.038 (0.038)	Data 0.190 (0.190)	Loss 2.0461 (2.0461)	Acc@1 69.141 (69.141)	Acc@5 92.969 (92.969)
Epoch: [57][10/196]	Time 0.015 (0.018)	Data 0.003 (0.019)	Loss 2.1686 (2.0509)	Acc@1 62.891 (69.070)	Acc@5 89.844 (92.507)
Epoch: [57][20/196]	Time 0.015 (0.017)	Data 0.002 (0.011)	Loss 2.0623 (2.0454)	Acc@1 65.234 (69.308)	Acc@5 96.094 (93.080)
Epoch: [57][30/196]	Time 0.013 (0.016)	Data 0.004 (0.009)	Loss 1.9726 (2.0231)	Acc@1 68.359 (69.972)	Acc@5 94.922 (93.372)
Epoch: [57][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.9070 (2.0117)	Acc@1 73.047 (70.179)	Acc@5 94.922 (93.626)
Epoch: [57][50/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0149 (2.0120)	Acc@1 70.312 (70.236)	Acc@5 94.531 (93.559)
Epoch: [57][60/196]	Time 0.011 (0.016)	Data 0.006 (0.006)	Loss 2.0437 (2.0102)	Acc@1 70.703 (70.498)	Acc@5 94.922 (93.584)
Epoch: [57][70/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 2.0673 (2.0107)	Acc@1 71.875 (70.533)	Acc@5 91.406 (93.557)
Epoch: [57][80/196]	Time 0.020 (0.016)	Data 0.008 (0.005)	Loss 2.2135 (2.0136)	Acc@1 63.281 (70.419)	Acc@5 93.359 (93.572)
Epoch: [57][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0819 (2.0130)	Acc@1 65.625 (70.458)	Acc@5 94.531 (93.621)
Epoch: [57][100/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.0444 (2.0151)	Acc@1 69.922 (70.351)	Acc@5 92.969 (93.634)
Epoch: [57][110/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.1166 (2.0217)	Acc@1 65.625 (70.126)	Acc@5 93.750 (93.560)
Epoch: [57][120/196]	Time 0.011 (0.016)	Data 0.010 (0.004)	Loss 2.1904 (2.0293)	Acc@1 65.625 (69.977)	Acc@5 90.234 (93.495)
Epoch: [57][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1152 (2.0312)	Acc@1 67.578 (69.904)	Acc@5 93.359 (93.470)
Epoch: [57][140/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 2.0504 (2.0329)	Acc@1 68.750 (69.830)	Acc@5 92.969 (93.429)
Epoch: [57][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1407 (2.0396)	Acc@1 67.969 (69.743)	Acc@5 90.625 (93.321)
Epoch: [57][160/196]	Time 0.011 (0.016)	Data 0.010 (0.004)	Loss 2.1047 (2.0461)	Acc@1 65.234 (69.619)	Acc@5 94.531 (93.209)
Epoch: [57][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0652 (2.0511)	Acc@1 69.531 (69.454)	Acc@5 92.578 (93.154)
Epoch: [57][180/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1067 (2.0549)	Acc@1 68.359 (69.376)	Acc@5 91.406 (93.092)
Epoch: [57][190/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1727 (2.0583)	Acc@1 64.062 (69.231)	Acc@5 92.188 (93.065)
num momentum params: 26
[0.1, 2.060109035949707, 1.8702349066734314, 69.182, 53.07, tensor(0.4913, device='cuda:0', grad_fn=<DivBackward0>), 3.0848875045776367, 0.3727090358734131]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [58 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [58][0/196]	Time 0.043 (0.043)	Data 0.197 (0.197)	Loss 1.9071 (1.9071)	Acc@1 73.828 (73.828)	Acc@5 95.312 (95.312)
Epoch: [58][10/196]	Time 0.015 (0.018)	Data 0.003 (0.020)	Loss 1.8503 (1.9574)	Acc@1 78.906 (72.017)	Acc@5 94.922 (94.425)
Epoch: [58][20/196]	Time 0.013 (0.017)	Data 0.004 (0.012)	Loss 1.9892 (1.9839)	Acc@1 69.922 (71.131)	Acc@5 94.141 (94.196)
Epoch: [58][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 1.9510 (1.9796)	Acc@1 72.656 (70.955)	Acc@5 93.750 (94.418)
Epoch: [58][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 1.9018 (1.9772)	Acc@1 75.391 (70.913)	Acc@5 95.703 (94.274)
Epoch: [58][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9474 (1.9737)	Acc@1 73.828 (71.117)	Acc@5 91.797 (94.217)
Epoch: [58][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 1.9031 (1.9769)	Acc@1 75.391 (71.267)	Acc@5 94.922 (94.217)
Epoch: [58][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1509 (1.9837)	Acc@1 64.453 (71.154)	Acc@5 92.969 (94.069)
Epoch: [58][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0472 (1.9900)	Acc@1 69.141 (71.017)	Acc@5 91.797 (94.001)
Epoch: [58][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0533 (1.9987)	Acc@1 68.750 (70.866)	Acc@5 95.703 (93.909)
Epoch: [58][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9797 (2.0011)	Acc@1 71.484 (70.792)	Acc@5 95.703 (93.816)
Epoch: [58][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1157 (2.0082)	Acc@1 70.312 (70.605)	Acc@5 92.578 (93.732)
Epoch: [58][120/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2256 (2.0147)	Acc@1 64.844 (70.474)	Acc@5 92.188 (93.608)
Epoch: [58][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0660 (2.0215)	Acc@1 69.922 (70.280)	Acc@5 93.359 (93.529)
Epoch: [58][140/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1780 (2.0302)	Acc@1 66.016 (70.008)	Acc@5 92.578 (93.445)
Epoch: [58][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9383 (2.0328)	Acc@1 70.703 (69.896)	Acc@5 94.922 (93.460)
Epoch: [58][160/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 1.9946 (2.0395)	Acc@1 70.703 (69.718)	Acc@5 94.531 (93.376)
Epoch: [58][170/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0356 (2.0436)	Acc@1 71.484 (69.666)	Acc@5 92.969 (93.337)
Epoch: [58][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1432 (2.0472)	Acc@1 67.188 (69.549)	Acc@5 92.188 (93.312)
Epoch: [58][190/196]	Time 0.019 (0.015)	Data 0.000 (0.004)	Loss 2.0498 (2.0496)	Acc@1 66.406 (69.462)	Acc@5 90.625 (93.243)
num momentum params: 26
[0.1, 2.050875410308838, 1.909039009809494, 69.414, 51.99, tensor(0.4947, device='cuda:0', grad_fn=<DivBackward0>), 3.002485513687134, 0.3791365623474121]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [59 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [59][0/196]	Time 0.042 (0.042)	Data 0.190 (0.190)	Loss 1.9708 (1.9708)	Acc@1 71.094 (71.094)	Acc@5 93.750 (93.750)
Epoch: [59][10/196]	Time 0.017 (0.018)	Data 0.001 (0.019)	Loss 2.0984 (2.0179)	Acc@1 67.969 (70.455)	Acc@5 93.750 (94.212)
Epoch: [59][20/196]	Time 0.015 (0.017)	Data 0.003 (0.011)	Loss 1.9293 (1.9881)	Acc@1 69.141 (71.447)	Acc@5 95.703 (94.289)
Epoch: [59][30/196]	Time 0.011 (0.017)	Data 0.009 (0.009)	Loss 1.9026 (1.9730)	Acc@1 75.000 (71.610)	Acc@5 96.094 (94.430)
Epoch: [59][40/196]	Time 0.020 (0.017)	Data 0.003 (0.007)	Loss 1.8507 (1.9574)	Acc@1 74.219 (71.999)	Acc@5 96.094 (94.579)
Epoch: [59][50/196]	Time 0.016 (0.017)	Data 0.003 (0.006)	Loss 1.9319 (1.9585)	Acc@1 73.047 (72.013)	Acc@5 95.312 (94.516)
Epoch: [59][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.0519 (1.9692)	Acc@1 68.359 (71.491)	Acc@5 94.141 (94.365)
Epoch: [59][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9792 (1.9731)	Acc@1 71.875 (71.424)	Acc@5 94.531 (94.273)
Epoch: [59][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9862 (1.9807)	Acc@1 71.484 (71.282)	Acc@5 94.141 (94.223)
Epoch: [59][90/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9344 (1.9873)	Acc@1 71.875 (71.098)	Acc@5 94.141 (94.209)
Epoch: [59][100/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0861 (1.9916)	Acc@1 68.359 (70.904)	Acc@5 94.141 (94.144)
Epoch: [59][110/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0231 (1.9988)	Acc@1 69.141 (70.791)	Acc@5 94.922 (94.042)
Epoch: [59][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0521 (2.0030)	Acc@1 71.094 (70.703)	Acc@5 95.312 (93.957)
Epoch: [59][130/196]	Time 0.013 (0.016)	Data 0.020 (0.005)	Loss 2.1263 (2.0073)	Acc@1 70.312 (70.646)	Acc@5 92.578 (93.875)
Epoch: [59][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2612 (2.0116)	Acc@1 65.234 (70.573)	Acc@5 90.234 (93.772)
Epoch: [59][150/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 2.2640 (2.0159)	Acc@1 65.625 (70.527)	Acc@5 89.844 (93.647)
Epoch: [59][160/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0260 (2.0206)	Acc@1 70.703 (70.407)	Acc@5 93.750 (93.600)
Epoch: [59][170/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 1.9992 (2.0265)	Acc@1 66.797 (70.205)	Acc@5 94.141 (93.540)
Epoch: [59][180/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2440 (2.0362)	Acc@1 64.844 (69.900)	Acc@5 90.625 (93.396)
Epoch: [59][190/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 2.1568 (2.0426)	Acc@1 68.359 (69.740)	Acc@5 91.406 (93.298)
num momentum params: 26
[0.1, 2.0438028700256345, 1.7948321211338043, 69.704, 54.05, tensor(0.4968, device='cuda:0', grad_fn=<DivBackward0>), 3.1019299030303955, 0.38295507431030273]
Non Pruning Epoch - module.conv1.weight: [56, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [56]
Non Pruning Epoch - module.bn1.bias: [56]
Non Pruning Epoch - module.conv2.weight: [128, 56, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [511, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [511]
Non Pruning Epoch - module.bn5.bias: [511]
Non Pruning Epoch - module.conv6.weight: [512, 511, 3, 3]
Non Pruning Epoch - module.bn6.weight: [512]
Non Pruning Epoch - module.bn6.bias: [512]
Non Pruning Epoch - module.conv7.weight: [502, 512, 3, 3]
Non Pruning Epoch - module.bn7.weight: [502]
Non Pruning Epoch - module.bn7.bias: [502]
Non Pruning Epoch - module.conv8.weight: [451, 502, 3, 3]
Non Pruning Epoch - module.bn8.weight: [451]
Non Pruning Epoch - module.bn8.bias: [451]
Non Pruning Epoch - module.fc.weight: [100, 451]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [60 | 180] LR: 0.100000
module.conv1.weight [56, 3, 3, 3]
module.conv2.weight [128, 56, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [511, 256, 3, 3]
module.conv6.weight [512, 511, 3, 3]
module.conv7.weight [502, 512, 3, 3]
module.conv8.weight [451, 502, 3, 3]
Epoch: [60][0/196]	Time 0.041 (0.041)	Data 0.202 (0.202)	Loss 2.0336 (2.0336)	Acc@1 68.359 (68.359)	Acc@5 96.484 (96.484)
Epoch: [60][10/196]	Time 0.017 (0.018)	Data 0.001 (0.021)	Loss 2.0556 (2.0187)	Acc@1 70.312 (70.241)	Acc@5 94.531 (93.821)
Epoch: [60][20/196]	Time 0.011 (0.016)	Data 0.006 (0.012)	Loss 1.9655 (2.0339)	Acc@1 71.094 (70.033)	Acc@5 94.141 (93.378)
Epoch: [60][30/196]	Time 0.017 (0.016)	Data 0.000 (0.009)	Loss 1.9813 (2.0056)	Acc@1 70.703 (70.842)	Acc@5 95.703 (93.838)
Epoch: [60][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 1.9866 (1.9914)	Acc@1 72.656 (71.494)	Acc@5 92.578 (93.979)
Epoch: [60][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 1.8819 (1.9829)	Acc@1 74.609 (71.607)	Acc@5 94.531 (94.095)
Epoch: [60][60/196]	Time 0.012 (0.016)	Data 0.006 (0.006)	Loss 2.0163 (1.9836)	Acc@1 69.922 (71.420)	Acc@5 95.312 (94.089)
Epoch: [60][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0763 (1.9812)	Acc@1 69.141 (71.523)	Acc@5 90.625 (94.064)
Epoch: [60][80/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 1.9748 (1.9871)	Acc@1 73.047 (71.316)	Acc@5 94.141 (93.943)
Epoch: [60][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9753 (1.9937)	Acc@1 69.141 (71.154)	Acc@5 93.359 (93.819)
Epoch: [60][100/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 2.1959 (1.9983)	Acc@1 65.625 (71.047)	Acc@5 93.359 (93.735)
Epoch: [60][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0992 (2.0039)	Acc@1 69.922 (70.932)	Acc@5 92.188 (93.704)
Epoch: [60][120/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0146 (2.0121)	Acc@1 71.094 (70.651)	Acc@5 93.359 (93.643)
Epoch: [60][130/196]	Time 0.018 (0.015)	Data 0.001 (0.004)	Loss 2.1286 (2.0169)	Acc@1 67.578 (70.474)	Acc@5 89.844 (93.592)
Epoch: [60][140/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0328 (2.0239)	Acc@1 70.312 (70.246)	Acc@5 92.188 (93.498)
Epoch: [60][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.3785 (2.0315)	Acc@1 61.328 (70.028)	Acc@5 88.672 (93.370)
Epoch: [60][160/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.0343 (2.0381)	Acc@1 71.484 (69.818)	Acc@5 91.797 (93.270)
Epoch: [60][170/196]	Time 0.016 (0.015)	Data 0.004 (0.004)	Loss 2.2307 (2.0432)	Acc@1 64.453 (69.666)	Acc@5 91.797 (93.259)
Epoch: [60][180/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.3093 (2.0490)	Acc@1 61.328 (69.477)	Acc@5 91.016 (93.239)
Epoch: [60][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1172 (2.0541)	Acc@1 68.750 (69.355)	Acc@5 91.797 (93.126)
num momentum params: 26
[0.1, 2.055398676223755, 1.9197587049007416, 69.33, 52.0, tensor(0.4951, device='cuda:0', grad_fn=<DivBackward0>), 3.0080552101135254, 0.3832082748413086]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [56, 3, 3, 3]
Before - module.bn1.weight: [56]
Before - module.bn1.bias: [56]
Before - module.conv2.weight: [128, 56, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [511, 256, 3, 3]
Before - module.bn5.weight: [511]
Before - module.bn5.bias: [511]
Before - module.conv6.weight: [512, 511, 3, 3]
Before - module.bn6.weight: [512]
Before - module.bn6.bias: [512]
Before - module.conv7.weight: [502, 512, 3, 3]
Before - module.bn7.weight: [502]
Before - module.bn7.bias: [502]
Before - module.conv8.weight: [451, 502, 3, 3]
Before - module.bn8.weight: [451]
Before - module.bn8.bias: [451]
Before - module.fc.weight: [100, 451]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [56, 3, 3, 3] >> [54, 3, 3, 3]
[module.bn1.weight]: 56 >> 54
running_mean [54]
running_var [54]
num_batches_tracked []
[module.conv2.weight]: [128, 56, 3, 3] >> [128, 54, 3, 3]
[module.conv5.weight]: [511, 256, 3, 3] >> [510, 256, 3, 3]
[module.bn5.weight]: 511 >> 510
running_mean [510]
running_var [510]
num_batches_tracked []
[module.conv6.weight]: [512, 511, 3, 3] >> [510, 510, 3, 3]
[module.bn6.weight]: 512 >> 510
running_mean [510]
running_var [510]
num_batches_tracked []
[module.conv7.weight]: [502, 512, 3, 3] >> [498, 510, 3, 3]
[module.bn7.weight]: 502 >> 498
running_mean [498]
running_var [498]
num_batches_tracked []
[module.conv8.weight]: [451, 502, 3, 3] >> [426, 498, 3, 3]
[module.bn8.weight]: 451 >> 426
running_mean [426]
running_var [426]
num_batches_tracked []
[module.fc.weight]: [100, 451] >> [100, 426]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [54, 3, 3, 3]
After - module.bn1.weight: [54]
After - module.bn1.bias: [54]
After - module.conv2.weight: [128, 54, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [256, 128, 3, 3]
After - module.bn3.weight: [256]
After - module.bn3.bias: [256]
After - module.conv4.weight: [256, 256, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [510, 510, 3, 3]
After - module.bn6.weight: [510]
After - module.bn6.bias: [510]
After - module.conv7.weight: [498, 510, 3, 3]
After - module.bn7.weight: [498]
After - module.bn7.bias: [498]
After - module.conv8.weight: [426, 498, 3, 3]
After - module.bn8.weight: [426]
After - module.bn8.bias: [426]
After - module.fc.weight: [100, 426]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [54, 3, 3, 3]
conv2 --> [128, 54, 3, 3]
conv3 --> [256, 128, 3, 3]
conv4 --> [256, 256, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [510, 510, 3, 3]
conv7 --> [498, 510, 3, 3]
conv8 --> [426, 498, 3, 3]
fc --> [426, 100]
1, 597943296, 1492992, 54
2, 6656753664, 15925248, 128
3, 8606711808, 18874368, 256
4, 17213423616, 37748736, 256
5, 10227548160, 18800640, 510
6, 20375193600, 37454400, 510
7, 7022039040, 9143280, 498
8, 5865467904, 7637328, 426
fc, 16358400, 42600, 0
===================
FLOP REPORT: 29914624800000.0 56339200000.0 147119592 140848 2638 16.603771209716797
[INFO] Storing checkpoint...

Epoch: [61 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [61][0/196]	Time 0.628 (0.628)	Data 0.201 (0.201)	Loss 2.1557 (2.1557)	Acc@1 67.578 (67.578)	Acc@5 89.844 (89.844)
Epoch: [61][10/196]	Time 0.016 (0.072)	Data 0.002 (0.020)	Loss 2.0086 (2.0493)	Acc@1 69.531 (68.857)	Acc@5 93.750 (92.756)
Epoch: [61][20/196]	Time 0.016 (0.045)	Data 0.002 (0.011)	Loss 2.0924 (2.0238)	Acc@1 64.453 (69.606)	Acc@5 92.578 (93.397)
Epoch: [61][30/196]	Time 0.016 (0.036)	Data 0.002 (0.008)	Loss 1.9368 (2.0008)	Acc@1 73.438 (70.577)	Acc@5 92.578 (93.662)
Epoch: [61][40/196]	Time 0.015 (0.031)	Data 0.002 (0.007)	Loss 1.9868 (2.0051)	Acc@1 69.531 (70.513)	Acc@5 94.141 (93.617)
Epoch: [61][50/196]	Time 0.015 (0.028)	Data 0.003 (0.006)	Loss 2.0053 (2.0063)	Acc@1 69.922 (70.412)	Acc@5 94.141 (93.604)
Epoch: [61][60/196]	Time 0.015 (0.026)	Data 0.002 (0.006)	Loss 1.9868 (2.0096)	Acc@1 74.219 (70.338)	Acc@5 94.922 (93.577)
Epoch: [61][70/196]	Time 0.016 (0.024)	Data 0.003 (0.005)	Loss 1.9316 (2.0210)	Acc@1 72.266 (70.087)	Acc@5 96.484 (93.524)
Epoch: [61][80/196]	Time 0.013 (0.023)	Data 0.003 (0.005)	Loss 2.0576 (2.0232)	Acc@1 68.359 (70.110)	Acc@5 95.703 (93.528)
Epoch: [61][90/196]	Time 0.015 (0.022)	Data 0.003 (0.005)	Loss 1.8061 (2.0175)	Acc@1 77.344 (70.231)	Acc@5 94.922 (93.647)
Epoch: [61][100/196]	Time 0.015 (0.022)	Data 0.002 (0.004)	Loss 2.0691 (2.0238)	Acc@1 67.969 (70.073)	Acc@5 93.359 (93.588)
Epoch: [61][110/196]	Time 0.015 (0.021)	Data 0.003 (0.004)	Loss 2.1225 (2.0297)	Acc@1 70.312 (69.943)	Acc@5 92.188 (93.514)
Epoch: [61][120/196]	Time 0.015 (0.021)	Data 0.003 (0.004)	Loss 2.0552 (2.0305)	Acc@1 67.969 (69.957)	Acc@5 94.141 (93.595)
Epoch: [61][130/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 1.9544 (2.0335)	Acc@1 71.094 (69.979)	Acc@5 93.359 (93.505)
Epoch: [61][140/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 2.1919 (2.0394)	Acc@1 64.844 (69.861)	Acc@5 94.531 (93.415)
Epoch: [61][150/196]	Time 0.015 (0.020)	Data 0.002 (0.004)	Loss 2.0560 (2.0450)	Acc@1 70.312 (69.689)	Acc@5 92.188 (93.346)
Epoch: [61][160/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 2.0645 (2.0511)	Acc@1 70.312 (69.570)	Acc@5 93.359 (93.274)
Epoch: [61][170/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 2.1690 (2.0562)	Acc@1 66.797 (69.440)	Acc@5 92.578 (93.218)
Epoch: [61][180/196]	Time 0.016 (0.019)	Data 0.003 (0.004)	Loss 2.0691 (2.0596)	Acc@1 70.312 (69.384)	Acc@5 91.797 (93.141)
Epoch: [61][190/196]	Time 0.016 (0.019)	Data 0.002 (0.004)	Loss 2.0208 (2.0635)	Acc@1 67.188 (69.304)	Acc@5 94.922 (93.081)
num momentum params: 26
[0.1, 2.063790436401367, 1.8566920268535614, 69.298, 52.53, tensor(0.4943, device='cuda:0', grad_fn=<DivBackward0>), 3.910761833190918, 0.48473644256591797]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [62 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [62][0/196]	Time 0.051 (0.051)	Data 0.200 (0.200)	Loss 1.8452 (1.8452)	Acc@1 76.562 (76.562)	Acc@5 95.312 (95.312)
Epoch: [62][10/196]	Time 0.016 (0.019)	Data 0.002 (0.020)	Loss 2.0523 (1.9756)	Acc@1 69.141 (72.124)	Acc@5 92.578 (94.993)
Epoch: [62][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 1.9974 (1.9805)	Acc@1 69.531 (71.838)	Acc@5 96.094 (94.792)
Epoch: [62][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.8016 (1.9734)	Acc@1 79.688 (72.051)	Acc@5 95.312 (94.594)
Epoch: [62][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8901 (1.9655)	Acc@1 74.219 (72.123)	Acc@5 94.531 (94.684)
Epoch: [62][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0739 (1.9655)	Acc@1 69.531 (72.181)	Acc@5 94.141 (94.516)
Epoch: [62][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0292 (1.9721)	Acc@1 71.484 (72.022)	Acc@5 92.578 (94.365)
Epoch: [62][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9285 (1.9814)	Acc@1 71.875 (71.693)	Acc@5 94.922 (94.212)
Epoch: [62][80/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.0097 (1.9845)	Acc@1 71.484 (71.590)	Acc@5 91.406 (94.107)
Epoch: [62][90/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.8908 (1.9844)	Acc@1 73.828 (71.566)	Acc@5 95.312 (94.046)
Epoch: [62][100/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.0484 (1.9852)	Acc@1 70.703 (71.488)	Acc@5 91.797 (94.040)
Epoch: [62][110/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9252 (1.9910)	Acc@1 71.094 (71.305)	Acc@5 94.141 (93.944)
Epoch: [62][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0591 (1.9956)	Acc@1 67.578 (71.168)	Acc@5 94.141 (93.902)
Epoch: [62][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9767 (2.0032)	Acc@1 71.484 (70.969)	Acc@5 93.359 (93.842)
Epoch: [62][140/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0592 (2.0091)	Acc@1 66.016 (70.739)	Acc@5 92.188 (93.750)
Epoch: [62][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1299 (2.0132)	Acc@1 69.922 (70.657)	Acc@5 92.578 (93.678)
Epoch: [62][160/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1045 (2.0175)	Acc@1 64.453 (70.521)	Acc@5 93.750 (93.626)
Epoch: [62][170/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.1159 (2.0192)	Acc@1 67.578 (70.472)	Acc@5 92.578 (93.618)
Epoch: [62][180/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1881 (2.0242)	Acc@1 64.453 (70.369)	Acc@5 91.797 (93.564)
Epoch: [62][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0976 (2.0290)	Acc@1 67.578 (70.229)	Acc@5 93.359 (93.531)
num momentum params: 26
[0.1, 2.0336699126434326, 1.883156965970993, 70.1, 51.88, tensor(0.5014, device='cuda:0', grad_fn=<DivBackward0>), 3.1237590312957764, 0.3782222270965576]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [63 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [63][0/196]	Time 0.039 (0.039)	Data 0.201 (0.201)	Loss 2.0316 (2.0316)	Acc@1 70.312 (70.312)	Acc@5 93.359 (93.359)
Epoch: [63][10/196]	Time 0.019 (0.018)	Data 0.001 (0.020)	Loss 2.0179 (1.9861)	Acc@1 69.531 (71.271)	Acc@5 94.922 (94.460)
Epoch: [63][20/196]	Time 0.015 (0.017)	Data 0.002 (0.012)	Loss 2.0102 (1.9563)	Acc@1 71.875 (71.912)	Acc@5 94.922 (94.866)
Epoch: [63][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.9156 (1.9511)	Acc@1 73.828 (72.215)	Acc@5 96.484 (94.771)
Epoch: [63][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 2.1425 (1.9596)	Acc@1 67.578 (72.161)	Acc@5 93.359 (94.503)
Epoch: [63][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.0122 (1.9653)	Acc@1 69.141 (71.867)	Acc@5 92.578 (94.332)
Epoch: [63][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9237 (1.9714)	Acc@1 73.047 (71.644)	Acc@5 93.359 (94.230)
Epoch: [63][70/196]	Time 0.015 (0.016)	Data 0.001 (0.005)	Loss 2.0287 (1.9730)	Acc@1 69.531 (71.671)	Acc@5 94.922 (94.262)
Epoch: [63][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0401 (1.9800)	Acc@1 68.750 (71.398)	Acc@5 92.969 (94.179)
Epoch: [63][90/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9754 (1.9860)	Acc@1 72.656 (71.265)	Acc@5 94.922 (94.123)
Epoch: [63][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.3416 (1.9917)	Acc@1 59.375 (71.094)	Acc@5 89.844 (93.998)
Epoch: [63][110/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 1.9026 (2.0000)	Acc@1 73.438 (70.893)	Acc@5 96.094 (93.884)
Epoch: [63][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1221 (2.0074)	Acc@1 67.969 (70.674)	Acc@5 92.578 (93.798)
Epoch: [63][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1122 (2.0127)	Acc@1 68.359 (70.477)	Acc@5 94.531 (93.738)
Epoch: [63][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1956 (2.0213)	Acc@1 62.891 (70.232)	Acc@5 91.406 (93.695)
Epoch: [63][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1461 (2.0297)	Acc@1 66.406 (70.046)	Acc@5 92.969 (93.597)
Epoch: [63][160/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1426 (2.0331)	Acc@1 67.969 (69.936)	Acc@5 91.016 (93.566)
Epoch: [63][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1269 (2.0405)	Acc@1 66.797 (69.723)	Acc@5 91.797 (93.471)
Epoch: [63][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2326 (2.0431)	Acc@1 67.188 (69.691)	Acc@5 89.062 (93.426)
Epoch: [63][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0562 (2.0482)	Acc@1 71.484 (69.578)	Acc@5 93.750 (93.359)
num momentum params: 26
[0.1, 2.0499904114532472, 1.8933487820625305, 69.546, 53.03, tensor(0.4983, device='cuda:0', grad_fn=<DivBackward0>), 3.0593693256378174, 0.3827676773071289]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [64 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [64][0/196]	Time 0.044 (0.044)	Data 0.210 (0.210)	Loss 1.9886 (1.9886)	Acc@1 69.922 (69.922)	Acc@5 95.703 (95.703)
Epoch: [64][10/196]	Time 0.013 (0.019)	Data 0.005 (0.021)	Loss 1.9674 (1.9980)	Acc@1 71.484 (70.135)	Acc@5 95.312 (94.283)
Epoch: [64][20/196]	Time 0.013 (0.017)	Data 0.005 (0.013)	Loss 2.1031 (2.0035)	Acc@1 67.188 (70.294)	Acc@5 94.531 (94.066)
Epoch: [64][30/196]	Time 0.013 (0.017)	Data 0.004 (0.009)	Loss 1.8987 (1.9869)	Acc@1 71.484 (70.917)	Acc@5 96.094 (94.342)
Epoch: [64][40/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9084 (1.9702)	Acc@1 74.219 (71.589)	Acc@5 93.750 (94.417)
Epoch: [64][50/196]	Time 0.014 (0.016)	Data 0.004 (0.007)	Loss 2.0267 (1.9683)	Acc@1 67.578 (71.638)	Acc@5 93.359 (94.447)
Epoch: [64][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9631 (1.9648)	Acc@1 72.656 (71.689)	Acc@5 94.141 (94.506)
Epoch: [64][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8113 (1.9727)	Acc@1 76.562 (71.627)	Acc@5 94.141 (94.350)
Epoch: [64][80/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0168 (1.9778)	Acc@1 69.531 (71.566)	Acc@5 94.922 (94.285)
Epoch: [64][90/196]	Time 0.018 (0.016)	Data 0.000 (0.005)	Loss 1.9679 (1.9886)	Acc@1 71.875 (71.326)	Acc@5 94.531 (94.149)
Epoch: [64][100/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0379 (1.9964)	Acc@1 71.484 (71.202)	Acc@5 91.797 (94.032)
Epoch: [64][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0188 (2.0009)	Acc@1 70.703 (71.118)	Acc@5 93.750 (93.968)
Epoch: [64][120/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1386 (2.0045)	Acc@1 64.453 (70.916)	Acc@5 93.359 (93.976)
Epoch: [64][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.3319 (2.0130)	Acc@1 63.281 (70.757)	Acc@5 91.406 (93.860)
Epoch: [64][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0854 (2.0182)	Acc@1 69.922 (70.595)	Acc@5 92.969 (93.797)
Epoch: [64][150/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1724 (2.0250)	Acc@1 67.578 (70.452)	Acc@5 91.797 (93.724)
Epoch: [64][160/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0682 (2.0314)	Acc@1 71.094 (70.342)	Acc@5 94.141 (93.672)
Epoch: [64][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0540 (2.0359)	Acc@1 72.266 (70.255)	Acc@5 94.531 (93.661)
Epoch: [64][180/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1230 (2.0386)	Acc@1 65.625 (70.211)	Acc@5 93.750 (93.636)
Epoch: [64][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0312 (2.0434)	Acc@1 74.609 (70.061)	Acc@5 90.625 (93.576)
num momentum params: 26
[0.1, 2.046051667022705, 1.963791205883026, 69.974, 52.42, tensor(0.5005, device='cuda:0', grad_fn=<DivBackward0>), 3.072247743606567, 0.38145875930786133]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [65 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [65][0/196]	Time 0.041 (0.041)	Data 0.203 (0.203)	Loss 1.8804 (1.8804)	Acc@1 76.953 (76.953)	Acc@5 97.266 (97.266)
Epoch: [65][10/196]	Time 0.015 (0.018)	Data 0.002 (0.021)	Loss 1.9686 (1.9618)	Acc@1 70.703 (72.869)	Acc@5 95.312 (94.993)
Epoch: [65][20/196]	Time 0.014 (0.017)	Data 0.004 (0.012)	Loss 1.9660 (1.9692)	Acc@1 73.828 (72.117)	Acc@5 93.359 (94.829)
Epoch: [65][30/196]	Time 0.017 (0.017)	Data 0.000 (0.009)	Loss 1.8645 (1.9709)	Acc@1 74.219 (71.913)	Acc@5 98.047 (94.783)
Epoch: [65][40/196]	Time 0.011 (0.016)	Data 0.007 (0.008)	Loss 1.8715 (1.9680)	Acc@1 75.781 (72.094)	Acc@5 96.094 (94.750)
Epoch: [65][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8201 (1.9573)	Acc@1 75.000 (72.403)	Acc@5 96.094 (94.830)
Epoch: [65][60/196]	Time 0.011 (0.016)	Data 0.009 (0.006)	Loss 1.8699 (1.9580)	Acc@1 73.047 (72.304)	Acc@5 95.312 (94.672)
Epoch: [65][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0618 (1.9624)	Acc@1 70.703 (72.117)	Acc@5 92.969 (94.614)
Epoch: [65][80/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 2.0316 (1.9711)	Acc@1 68.359 (71.812)	Acc@5 94.922 (94.478)
Epoch: [65][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9983 (1.9763)	Acc@1 71.875 (71.656)	Acc@5 94.922 (94.411)
Epoch: [65][100/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 2.1980 (1.9850)	Acc@1 64.453 (71.411)	Acc@5 92.188 (94.276)
Epoch: [65][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0102 (1.9847)	Acc@1 71.484 (71.337)	Acc@5 94.141 (94.299)
Epoch: [65][120/196]	Time 0.021 (0.016)	Data 0.009 (0.004)	Loss 2.0671 (1.9887)	Acc@1 69.531 (71.275)	Acc@5 93.750 (94.302)
Epoch: [65][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0130 (1.9929)	Acc@1 71.094 (71.159)	Acc@5 94.531 (94.263)
Epoch: [65][140/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.0324 (1.9966)	Acc@1 68.359 (71.047)	Acc@5 94.531 (94.182)
Epoch: [65][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0219 (2.0012)	Acc@1 73.438 (70.995)	Acc@5 93.359 (94.117)
Epoch: [65][160/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.1612 (2.0061)	Acc@1 67.578 (70.856)	Acc@5 91.016 (94.044)
Epoch: [65][170/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.1461 (2.0104)	Acc@1 65.625 (70.740)	Acc@5 92.969 (93.969)
Epoch: [65][180/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.2747 (2.0175)	Acc@1 67.578 (70.589)	Acc@5 89.062 (93.879)
Epoch: [65][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0810 (2.0246)	Acc@1 69.922 (70.407)	Acc@5 91.406 (93.779)
num momentum params: 26
[0.1, 2.0283792096710207, 1.8374013769626618, 70.292, 53.8, tensor(0.5055, device='cuda:0', grad_fn=<DivBackward0>), 3.1091592311859126, 0.37933945655822754]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [66 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [66][0/196]	Time 0.044 (0.044)	Data 0.194 (0.194)	Loss 1.8409 (1.8409)	Acc@1 76.953 (76.953)	Acc@5 94.922 (94.922)
Epoch: [66][10/196]	Time 0.016 (0.019)	Data 0.002 (0.020)	Loss 1.8752 (1.9771)	Acc@1 72.266 (71.591)	Acc@5 96.875 (94.283)
Epoch: [66][20/196]	Time 0.015 (0.017)	Data 0.004 (0.011)	Loss 1.9798 (1.9759)	Acc@1 70.312 (71.596)	Acc@5 93.750 (94.196)
Epoch: [66][30/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 2.1725 (1.9792)	Acc@1 64.453 (71.472)	Acc@5 91.406 (94.103)
Epoch: [66][40/196]	Time 0.013 (0.016)	Data 0.006 (0.007)	Loss 2.0114 (1.9764)	Acc@1 72.656 (71.637)	Acc@5 92.188 (94.207)
Epoch: [66][50/196]	Time 0.018 (0.017)	Data 0.001 (0.006)	Loss 1.9362 (1.9788)	Acc@1 74.609 (71.860)	Acc@5 92.188 (94.095)
Epoch: [66][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 2.0086 (1.9740)	Acc@1 71.094 (71.939)	Acc@5 92.969 (94.179)
Epoch: [66][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.2658 (1.9909)	Acc@1 65.234 (71.655)	Acc@5 92.188 (93.976)
Epoch: [66][80/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.1661 (2.0077)	Acc@1 67.578 (71.229)	Acc@5 90.625 (93.822)
Epoch: [66][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2165 (2.0218)	Acc@1 64.062 (70.892)	Acc@5 92.969 (93.647)
Epoch: [66][100/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0262 (2.0325)	Acc@1 72.656 (70.634)	Acc@5 94.141 (93.530)
Epoch: [66][110/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1071 (2.0377)	Acc@1 69.531 (70.492)	Acc@5 89.844 (93.426)
Epoch: [66][120/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0647 (2.0438)	Acc@1 69.922 (70.416)	Acc@5 93.359 (93.311)
Epoch: [66][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9170 (2.0471)	Acc@1 75.000 (70.295)	Acc@5 94.141 (93.273)
Epoch: [66][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1255 (2.0487)	Acc@1 69.141 (70.260)	Acc@5 91.797 (93.237)
Epoch: [66][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2142 (2.0496)	Acc@1 67.578 (70.181)	Acc@5 92.188 (93.295)
Epoch: [66][160/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1316 (2.0551)	Acc@1 67.969 (70.021)	Acc@5 90.625 (93.214)
Epoch: [66][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1134 (2.0577)	Acc@1 67.578 (69.949)	Acc@5 92.969 (93.172)
Epoch: [66][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0119 (2.0630)	Acc@1 69.531 (69.805)	Acc@5 93.750 (93.079)
Epoch: [66][190/196]	Time 0.018 (0.016)	Data 0.000 (0.003)	Loss 2.0514 (2.0629)	Acc@1 72.656 (69.832)	Acc@5 91.406 (93.073)
num momentum params: 26
[0.1, 2.0653479221343995, 1.8776136338710785, 69.782, 52.02, tensor(0.4984, device='cuda:0', grad_fn=<DivBackward0>), 3.094068050384521, 0.3777282238006592]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [67 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [67][0/196]	Time 0.045 (0.045)	Data 0.192 (0.192)	Loss 1.9721 (1.9721)	Acc@1 71.094 (71.094)	Acc@5 94.141 (94.141)
Epoch: [67][10/196]	Time 0.016 (0.018)	Data 0.002 (0.020)	Loss 1.9792 (1.9679)	Acc@1 69.922 (72.266)	Acc@5 94.141 (94.780)
Epoch: [67][20/196]	Time 0.014 (0.017)	Data 0.005 (0.012)	Loss 1.8607 (1.9511)	Acc@1 74.219 (72.749)	Acc@5 95.312 (94.959)
Epoch: [67][30/196]	Time 0.016 (0.017)	Data 0.003 (0.009)	Loss 1.7257 (1.9492)	Acc@1 77.734 (72.480)	Acc@5 97.656 (94.897)
Epoch: [67][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 1.9380 (1.9469)	Acc@1 75.391 (72.675)	Acc@5 94.531 (94.817)
Epoch: [67][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0351 (1.9525)	Acc@1 69.922 (72.365)	Acc@5 94.922 (94.953)
Epoch: [67][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 1.8829 (1.9531)	Acc@1 73.438 (72.477)	Acc@5 96.094 (94.928)
Epoch: [67][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9441 (1.9638)	Acc@1 73.047 (72.249)	Acc@5 94.141 (94.702)
Epoch: [67][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9183 (1.9699)	Acc@1 72.656 (72.160)	Acc@5 95.312 (94.560)
Epoch: [67][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1035 (1.9796)	Acc@1 69.141 (71.866)	Acc@5 91.797 (94.372)
Epoch: [67][100/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9708 (1.9863)	Acc@1 73.438 (71.713)	Acc@5 95.312 (94.288)
Epoch: [67][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0874 (1.9940)	Acc@1 68.359 (71.502)	Acc@5 94.141 (94.200)
Epoch: [67][120/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0760 (1.9978)	Acc@1 69.531 (71.410)	Acc@5 92.578 (94.157)
Epoch: [67][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0650 (2.0031)	Acc@1 69.922 (71.341)	Acc@5 93.750 (94.105)
Epoch: [67][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9047 (2.0101)	Acc@1 73.047 (71.105)	Acc@5 93.359 (94.019)
Epoch: [67][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1407 (2.0144)	Acc@1 66.797 (70.990)	Acc@5 94.531 (93.931)
Epoch: [67][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1329 (2.0216)	Acc@1 70.312 (70.861)	Acc@5 91.797 (93.825)
Epoch: [67][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0296 (2.0233)	Acc@1 69.531 (70.847)	Acc@5 93.750 (93.789)
Epoch: [67][180/196]	Time 0.019 (0.016)	Data 0.003 (0.004)	Loss 1.9928 (2.0262)	Acc@1 71.094 (70.774)	Acc@5 95.703 (93.752)
Epoch: [67][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9399 (2.0298)	Acc@1 70.312 (70.658)	Acc@5 94.531 (93.658)
num momentum params: 26
[0.1, 2.03199093788147, 1.9078594386577605, 70.602, 51.52, tensor(0.5068, device='cuda:0', grad_fn=<DivBackward0>), 3.070461750030518, 0.3856339454650879]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [68 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [68][0/196]	Time 0.047 (0.047)	Data 0.197 (0.197)	Loss 1.9072 (1.9072)	Acc@1 70.703 (70.703)	Acc@5 96.875 (96.875)
Epoch: [68][10/196]	Time 0.018 (0.019)	Data 0.001 (0.020)	Loss 1.9033 (1.9678)	Acc@1 73.828 (72.372)	Acc@5 93.750 (94.354)
Epoch: [68][20/196]	Time 0.012 (0.018)	Data 0.006 (0.013)	Loss 1.8443 (1.9420)	Acc@1 74.219 (72.749)	Acc@5 95.703 (94.903)
Epoch: [68][30/196]	Time 0.017 (0.017)	Data 0.002 (0.009)	Loss 2.0997 (1.9450)	Acc@1 70.703 (72.896)	Acc@5 92.969 (94.657)
Epoch: [68][40/196]	Time 0.013 (0.017)	Data 0.005 (0.008)	Loss 1.8965 (1.9327)	Acc@1 74.219 (73.247)	Acc@5 95.312 (94.750)
Epoch: [68][50/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 2.0391 (1.9249)	Acc@1 68.359 (73.353)	Acc@5 94.531 (94.845)
Epoch: [68][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.0256 (1.9273)	Acc@1 73.047 (73.297)	Acc@5 92.578 (94.807)
Epoch: [68][70/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0742 (1.9308)	Acc@1 69.141 (73.305)	Acc@5 91.016 (94.740)
Epoch: [68][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9121 (1.9392)	Acc@1 73.047 (73.081)	Acc@5 93.750 (94.628)
Epoch: [68][90/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 2.1515 (1.9502)	Acc@1 67.188 (72.841)	Acc@5 94.531 (94.518)
Epoch: [68][100/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 2.1724 (1.9577)	Acc@1 66.797 (72.707)	Acc@5 92.969 (94.407)
Epoch: [68][110/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 2.0290 (1.9631)	Acc@1 70.312 (72.596)	Acc@5 93.359 (94.317)
Epoch: [68][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1640 (1.9699)	Acc@1 70.312 (72.430)	Acc@5 91.797 (94.241)
Epoch: [68][130/196]	Time 0.011 (0.016)	Data 0.002 (0.004)	Loss 2.0783 (1.9791)	Acc@1 70.703 (72.167)	Acc@5 94.531 (94.159)
Epoch: [68][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1207 (1.9862)	Acc@1 67.188 (71.955)	Acc@5 94.141 (94.060)
Epoch: [68][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1767 (1.9976)	Acc@1 66.016 (71.686)	Acc@5 94.531 (93.890)
Epoch: [68][160/196]	Time 0.016 (0.016)	Data 0.005 (0.004)	Loss 1.9827 (2.0065)	Acc@1 73.828 (71.424)	Acc@5 92.188 (93.782)
Epoch: [68][170/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.3586 (2.0167)	Acc@1 60.938 (71.142)	Acc@5 87.891 (93.684)
Epoch: [68][180/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.3056 (2.0222)	Acc@1 62.109 (70.966)	Acc@5 89.453 (93.625)
Epoch: [68][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1223 (2.0286)	Acc@1 66.797 (70.777)	Acc@5 91.797 (93.525)
num momentum params: 26
[0.1, 2.031352925872803, 1.7620315599441527, 70.692, 55.27, tensor(0.5074, device='cuda:0', grad_fn=<DivBackward0>), 3.0828640460968018, 0.38160157203674316]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [69 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [69][0/196]	Time 0.050 (0.050)	Data 0.184 (0.184)	Loss 1.8311 (1.8311)	Acc@1 74.219 (74.219)	Acc@5 96.094 (96.094)
Epoch: [69][10/196]	Time 0.015 (0.019)	Data 0.003 (0.019)	Loss 1.9211 (1.9889)	Acc@1 73.438 (71.804)	Acc@5 94.531 (94.283)
Epoch: [69][20/196]	Time 0.013 (0.017)	Data 0.005 (0.011)	Loss 1.9761 (1.9606)	Acc@1 68.750 (72.135)	Acc@5 96.094 (94.643)
Epoch: [69][30/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 2.0277 (1.9594)	Acc@1 69.922 (72.114)	Acc@5 91.406 (94.556)
Epoch: [69][40/196]	Time 0.013 (0.017)	Data 0.005 (0.007)	Loss 2.0298 (1.9579)	Acc@1 68.359 (72.113)	Acc@5 94.531 (94.655)
Epoch: [69][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 2.1730 (1.9616)	Acc@1 64.453 (72.181)	Acc@5 91.406 (94.485)
Epoch: [69][60/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 1.9452 (1.9669)	Acc@1 75.000 (72.112)	Acc@5 95.312 (94.378)
Epoch: [69][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0237 (1.9737)	Acc@1 73.438 (71.930)	Acc@5 94.531 (94.251)
Epoch: [69][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0382 (1.9820)	Acc@1 74.609 (71.754)	Acc@5 94.141 (94.160)
Epoch: [69][90/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 1.9688 (1.9846)	Acc@1 73.047 (71.682)	Acc@5 93.359 (94.136)
Epoch: [69][100/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9438 (1.9928)	Acc@1 73.828 (71.481)	Acc@5 92.969 (94.083)
Epoch: [69][110/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1975 (2.0026)	Acc@1 64.453 (71.111)	Acc@5 91.797 (93.996)
Epoch: [69][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0927 (2.0129)	Acc@1 69.531 (70.848)	Acc@5 94.141 (93.886)
Epoch: [69][130/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1105 (2.0174)	Acc@1 68.750 (70.757)	Acc@5 91.797 (93.771)
Epoch: [69][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1212 (2.0217)	Acc@1 65.625 (70.692)	Acc@5 92.188 (93.736)
Epoch: [69][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.8865 (2.0255)	Acc@1 78.516 (70.633)	Acc@5 93.750 (93.672)
Epoch: [69][160/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0947 (2.0351)	Acc@1 71.875 (70.368)	Acc@5 91.797 (93.549)
Epoch: [69][170/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0424 (2.0387)	Acc@1 69.531 (70.246)	Acc@5 94.141 (93.501)
Epoch: [69][180/196]	Time 0.014 (0.016)	Data 0.005 (0.003)	Loss 2.0256 (2.0462)	Acc@1 74.219 (70.075)	Acc@5 91.406 (93.403)
Epoch: [69][190/196]	Time 0.014 (0.016)	Data 0.004 (0.003)	Loss 2.0335 (2.0475)	Acc@1 69.922 (70.071)	Acc@5 93.750 (93.355)
num momentum params: 26
[0.1, 2.049352704925537, 1.7193979239463806, 70.006, 55.97, tensor(0.5035, device='cuda:0', grad_fn=<DivBackward0>), 3.0792453289031982, 0.38186097145080566]
Non Pruning Epoch - module.conv1.weight: [54, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [54]
Non Pruning Epoch - module.bn1.bias: [54]
Non Pruning Epoch - module.conv2.weight: [128, 54, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [256, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [256]
Non Pruning Epoch - module.bn3.bias: [256]
Non Pruning Epoch - module.conv4.weight: [256, 256, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [510, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [510]
Non Pruning Epoch - module.bn6.bias: [510]
Non Pruning Epoch - module.conv7.weight: [498, 510, 3, 3]
Non Pruning Epoch - module.bn7.weight: [498]
Non Pruning Epoch - module.bn7.bias: [498]
Non Pruning Epoch - module.conv8.weight: [426, 498, 3, 3]
Non Pruning Epoch - module.bn8.weight: [426]
Non Pruning Epoch - module.bn8.bias: [426]
Non Pruning Epoch - module.fc.weight: [100, 426]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [70 | 180] LR: 0.100000
module.conv1.weight [54, 3, 3, 3]
module.conv2.weight [128, 54, 3, 3]
module.conv3.weight [256, 128, 3, 3]
module.conv4.weight [256, 256, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [510, 510, 3, 3]
module.conv7.weight [498, 510, 3, 3]
module.conv8.weight [426, 498, 3, 3]
Epoch: [70][0/196]	Time 0.048 (0.048)	Data 0.202 (0.202)	Loss 1.7885 (1.7885)	Acc@1 77.734 (77.734)	Acc@5 95.703 (95.703)
Epoch: [70][10/196]	Time 0.017 (0.020)	Data 0.001 (0.020)	Loss 1.9115 (1.9621)	Acc@1 74.219 (73.047)	Acc@5 93.750 (93.999)
Epoch: [70][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 2.0282 (1.9659)	Acc@1 73.438 (72.600)	Acc@5 92.578 (94.234)
Epoch: [70][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.9698 (1.9580)	Acc@1 70.703 (72.618)	Acc@5 92.969 (94.304)
Epoch: [70][40/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 1.9416 (1.9608)	Acc@1 69.531 (72.256)	Acc@5 94.922 (94.388)
Epoch: [70][50/196]	Time 0.017 (0.017)	Data 0.003 (0.006)	Loss 1.8622 (1.9614)	Acc@1 75.781 (72.319)	Acc@5 95.703 (94.524)
Epoch: [70][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0647 (1.9698)	Acc@1 69.922 (72.118)	Acc@5 95.312 (94.454)
Epoch: [70][70/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9489 (1.9818)	Acc@1 72.266 (71.809)	Acc@5 92.969 (94.229)
Epoch: [70][80/196]	Time 0.014 (0.016)	Data 0.005 (0.005)	Loss 1.9883 (1.9872)	Acc@1 69.922 (71.706)	Acc@5 92.578 (94.112)
Epoch: [70][90/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.1457 (1.9973)	Acc@1 66.016 (71.433)	Acc@5 92.578 (93.982)
Epoch: [70][100/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1459 (2.0101)	Acc@1 68.359 (71.071)	Acc@5 93.750 (93.843)
Epoch: [70][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9166 (2.0131)	Acc@1 72.656 (70.985)	Acc@5 96.484 (93.817)
Epoch: [70][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9971 (2.0179)	Acc@1 72.656 (70.858)	Acc@5 93.750 (93.747)
Epoch: [70][130/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0830 (2.0240)	Acc@1 69.922 (70.620)	Acc@5 92.969 (93.705)
Epoch: [70][140/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1221 (2.0271)	Acc@1 69.141 (70.523)	Acc@5 93.750 (93.720)
Epoch: [70][150/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.0550 (2.0287)	Acc@1 67.188 (70.426)	Acc@5 92.969 (93.703)
Epoch: [70][160/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0715 (2.0329)	Acc@1 68.750 (70.322)	Acc@5 92.578 (93.634)
Epoch: [70][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0057 (2.0343)	Acc@1 71.094 (70.367)	Acc@5 92.578 (93.595)
Epoch: [70][180/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9949 (2.0348)	Acc@1 72.266 (70.395)	Acc@5 92.969 (93.560)
Epoch: [70][190/196]	Time 0.016 (0.016)	Data 0.002 (0.003)	Loss 1.9534 (2.0380)	Acc@1 76.172 (70.335)	Acc@5 92.578 (93.513)
num momentum params: 26
[0.1, 2.039478958969116, 1.8056792163848876, 70.302, 54.42, tensor(0.5066, device='cuda:0', grad_fn=<DivBackward0>), 3.102532863616944, 0.38074254989624023]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [54, 3, 3, 3]
Before - module.bn1.weight: [54]
Before - module.bn1.bias: [54]
Before - module.conv2.weight: [128, 54, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [256, 128, 3, 3]
Before - module.bn3.weight: [256]
Before - module.bn3.bias: [256]
Before - module.conv4.weight: [256, 256, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [510, 510, 3, 3]
Before - module.bn6.weight: [510]
Before - module.bn6.bias: [510]
Before - module.conv7.weight: [498, 510, 3, 3]
Before - module.bn7.weight: [498]
Before - module.bn7.bias: [498]
Before - module.conv8.weight: [426, 498, 3, 3]
Before - module.bn8.weight: [426]
Before - module.bn8.bias: [426]
Before - module.fc.weight: [100, 426]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [54, 3, 3, 3] >> [51, 3, 3, 3]
[module.bn1.weight]: 54 >> 51
running_mean [51]
running_var [51]
num_batches_tracked []
[module.conv2.weight]: [128, 54, 3, 3] >> [128, 51, 3, 3]
[module.conv3.weight]: [256, 128, 3, 3] >> [254, 128, 3, 3]
[module.bn3.weight]: 256 >> 254
running_mean [254]
running_var [254]
num_batches_tracked []
[module.conv4.weight]: [256, 256, 3, 3] >> [256, 254, 3, 3]
[module.conv6.weight]: [510, 510, 3, 3] >> [509, 510, 3, 3]
[module.bn6.weight]: 510 >> 509
running_mean [509]
running_var [509]
num_batches_tracked []
[module.conv7.weight]: [498, 510, 3, 3] >> [494, 509, 3, 3]
[module.bn7.weight]: 498 >> 494
running_mean [494]
running_var [494]
num_batches_tracked []
[module.conv8.weight]: [426, 498, 3, 3] >> [398, 494, 3, 3]
[module.bn8.weight]: 426 >> 398
running_mean [398]
running_var [398]
num_batches_tracked []
[module.fc.weight]: [100, 426] >> [100, 398]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [51, 3, 3, 3]
After - module.bn1.weight: [51]
After - module.bn1.bias: [51]
After - module.conv2.weight: [128, 51, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [254, 128, 3, 3]
After - module.bn3.weight: [254]
After - module.bn3.bias: [254]
After - module.conv4.weight: [256, 254, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [509, 510, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [494, 509, 3, 3]
After - module.bn7.weight: [494]
After - module.bn7.bias: [494]
After - module.conv8.weight: [398, 494, 3, 3]
After - module.bn8.weight: [398]
After - module.bn8.bias: [398]
After - module.fc.weight: [100, 398]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [51, 3, 3, 3]
conv2 --> [128, 51, 3, 3]
conv3 --> [254, 128, 3, 3]
conv4 --> [256, 254, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [509, 510, 3, 3]
conv7 --> [494, 509, 3, 3]
conv8 --> [398, 494, 3, 3]
fc --> [398, 100]
1, 564724224, 1410048, 51
2, 6286934016, 15040512, 128
3, 8539471872, 18726912, 254
4, 17078943744, 37453824, 256
5, 10227548160, 18800640, 510
6, 20335242240, 37380960, 509
7, 6951979008, 9052056, 494
8, 5435928576, 7078032, 398
fc, 15283200, 39800, 0
===================
FLOP REPORT: 29467209000000.0 55001600000.0 144982784 137504 2600 16.259428024291992
[INFO] Storing checkpoint...

Epoch: [71 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [71][0/196]	Time 0.660 (0.660)	Data 0.197 (0.197)	Loss 1.7161 (1.7161)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)
Epoch: [71][10/196]	Time 0.016 (0.076)	Data 0.002 (0.020)	Loss 2.0831 (1.9775)	Acc@1 71.484 (71.875)	Acc@5 94.141 (94.673)
Epoch: [71][20/196]	Time 0.015 (0.047)	Data 0.002 (0.011)	Loss 1.9852 (1.9566)	Acc@1 73.047 (72.824)	Acc@5 94.531 (94.717)
Epoch: [71][30/196]	Time 0.015 (0.037)	Data 0.002 (0.008)	Loss 2.0072 (1.9435)	Acc@1 70.703 (73.299)	Acc@5 94.922 (94.682)
Epoch: [71][40/196]	Time 0.015 (0.031)	Data 0.002 (0.007)	Loss 1.9106 (1.9498)	Acc@1 72.656 (72.961)	Acc@5 94.922 (94.693)
Epoch: [71][50/196]	Time 0.015 (0.028)	Data 0.003 (0.006)	Loss 1.9429 (1.9475)	Acc@1 73.438 (73.078)	Acc@5 94.141 (94.715)
Epoch: [71][60/196]	Time 0.015 (0.026)	Data 0.002 (0.005)	Loss 1.9390 (1.9544)	Acc@1 70.703 (72.932)	Acc@5 96.875 (94.602)
Epoch: [71][70/196]	Time 0.015 (0.024)	Data 0.002 (0.005)	Loss 2.0381 (1.9594)	Acc@1 71.875 (72.739)	Acc@5 93.359 (94.531)
Epoch: [71][80/196]	Time 0.017 (0.023)	Data 0.000 (0.005)	Loss 1.9362 (1.9694)	Acc@1 73.438 (72.410)	Acc@5 94.531 (94.401)
Epoch: [71][90/196]	Time 0.014 (0.022)	Data 0.003 (0.005)	Loss 1.9027 (1.9768)	Acc@1 73.438 (72.206)	Acc@5 94.531 (94.252)
Epoch: [71][100/196]	Time 0.016 (0.022)	Data 0.000 (0.004)	Loss 2.0149 (1.9813)	Acc@1 70.312 (72.003)	Acc@5 94.531 (94.230)
Epoch: [71][110/196]	Time 0.016 (0.021)	Data 0.000 (0.004)	Loss 2.1408 (1.9829)	Acc@1 66.406 (71.931)	Acc@5 91.797 (94.243)
Epoch: [71][120/196]	Time 0.017 (0.020)	Data 0.000 (0.004)	Loss 1.9612 (1.9815)	Acc@1 73.828 (72.007)	Acc@5 94.922 (94.218)
Epoch: [71][130/196]	Time 0.015 (0.020)	Data 0.003 (0.004)	Loss 2.0453 (1.9877)	Acc@1 67.578 (71.860)	Acc@5 93.750 (94.135)
Epoch: [71][140/196]	Time 0.018 (0.020)	Data 0.001 (0.004)	Loss 1.9481 (1.9916)	Acc@1 71.484 (71.695)	Acc@5 95.703 (94.102)
Epoch: [71][150/196]	Time 0.015 (0.019)	Data 0.002 (0.004)	Loss 2.0774 (1.9956)	Acc@1 66.797 (71.575)	Acc@5 92.188 (94.076)
Epoch: [71][160/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 2.0243 (2.0038)	Acc@1 69.922 (71.290)	Acc@5 94.141 (94.017)
Epoch: [71][170/196]	Time 0.016 (0.019)	Data 0.003 (0.004)	Loss 2.1169 (2.0120)	Acc@1 69.141 (71.032)	Acc@5 91.797 (93.917)
Epoch: [71][180/196]	Time 0.017 (0.018)	Data 0.000 (0.004)	Loss 2.0504 (2.0177)	Acc@1 73.828 (70.887)	Acc@5 91.406 (93.815)
Epoch: [71][190/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 2.0963 (2.0223)	Acc@1 67.188 (70.769)	Acc@5 92.969 (93.768)
num momentum params: 26
[0.1, 2.023777359466553, 1.866796671152115, 70.748, 52.87, tensor(0.5102, device='cuda:0', grad_fn=<DivBackward0>), 3.8050129413604736, 0.4777967929840088]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [72 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [72][0/196]	Time 0.047 (0.047)	Data 0.198 (0.198)	Loss 2.0084 (2.0084)	Acc@1 70.703 (70.703)	Acc@5 94.531 (94.531)
Epoch: [72][10/196]	Time 0.018 (0.018)	Data 0.000 (0.020)	Loss 1.8467 (1.9687)	Acc@1 75.000 (72.053)	Acc@5 94.922 (94.780)
Epoch: [72][20/196]	Time 0.015 (0.017)	Data 0.002 (0.012)	Loss 1.8566 (1.9480)	Acc@1 77.344 (72.321)	Acc@5 96.094 (95.015)
Epoch: [72][30/196]	Time 0.013 (0.016)	Data 0.004 (0.009)	Loss 1.9810 (1.9444)	Acc@1 70.312 (72.505)	Acc@5 94.922 (94.985)
Epoch: [72][40/196]	Time 0.011 (0.016)	Data 0.008 (0.007)	Loss 1.8751 (1.9348)	Acc@1 73.438 (72.847)	Acc@5 95.703 (95.027)
Epoch: [72][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9397 (1.9384)	Acc@1 73.828 (72.878)	Acc@5 95.312 (94.922)
Epoch: [72][60/196]	Time 0.012 (0.016)	Data 0.009 (0.006)	Loss 1.8984 (1.9459)	Acc@1 73.438 (72.701)	Acc@5 96.094 (94.851)
Epoch: [72][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0404 (1.9556)	Acc@1 69.531 (72.546)	Acc@5 93.750 (94.680)
Epoch: [72][80/196]	Time 0.011 (0.016)	Data 0.006 (0.005)	Loss 2.0240 (1.9630)	Acc@1 71.484 (72.405)	Acc@5 94.141 (94.517)
Epoch: [72][90/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0877 (1.9705)	Acc@1 66.406 (72.171)	Acc@5 93.750 (94.394)
Epoch: [72][100/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 1.9987 (1.9737)	Acc@1 68.359 (72.006)	Acc@5 95.312 (94.435)
Epoch: [72][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9927 (1.9756)	Acc@1 73.438 (72.030)	Acc@5 94.922 (94.394)
Epoch: [72][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0245 (1.9823)	Acc@1 70.703 (71.862)	Acc@5 94.531 (94.267)
Epoch: [72][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0630 (1.9927)	Acc@1 66.406 (71.520)	Acc@5 92.578 (94.132)
Epoch: [72][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0439 (2.0000)	Acc@1 71.094 (71.343)	Acc@5 94.531 (94.055)
Epoch: [72][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1332 (2.0066)	Acc@1 66.016 (71.138)	Acc@5 94.141 (93.965)
Epoch: [72][160/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0970 (2.0119)	Acc@1 69.141 (70.968)	Acc@5 93.750 (93.922)
Epoch: [72][170/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.1361 (2.0174)	Acc@1 66.016 (70.838)	Acc@5 91.797 (93.901)
Epoch: [72][180/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.2266 (2.0220)	Acc@1 67.969 (70.718)	Acc@5 91.406 (93.830)
Epoch: [72][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0264 (2.0251)	Acc@1 70.703 (70.662)	Acc@5 94.531 (93.803)
num momentum params: 26
[0.1, 2.027354028930664, 1.9423807847499848, 70.6, 51.41, tensor(0.5100, device='cuda:0', grad_fn=<DivBackward0>), 3.02290678024292, 0.38566017150878906]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [73 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [73][0/196]	Time 0.050 (0.050)	Data 0.199 (0.199)	Loss 1.9060 (1.9060)	Acc@1 76.562 (76.562)	Acc@5 93.750 (93.750)
Epoch: [73][10/196]	Time 0.018 (0.018)	Data 0.001 (0.020)	Loss 2.0037 (1.9741)	Acc@1 69.922 (72.266)	Acc@5 93.750 (94.105)
Epoch: [73][20/196]	Time 0.014 (0.017)	Data 0.003 (0.012)	Loss 2.0145 (1.9676)	Acc@1 70.703 (71.987)	Acc@5 94.531 (94.457)
Epoch: [73][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 1.9420 (1.9434)	Acc@1 73.438 (72.732)	Acc@5 94.922 (94.657)
Epoch: [73][40/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 2.0231 (1.9512)	Acc@1 70.312 (72.752)	Acc@5 92.578 (94.446)
Epoch: [73][50/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.8473 (1.9590)	Acc@1 75.781 (72.526)	Acc@5 93.359 (94.271)
Epoch: [73][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9848 (1.9648)	Acc@1 71.875 (72.259)	Acc@5 92.578 (94.179)
Epoch: [73][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0076 (1.9710)	Acc@1 69.922 (72.018)	Acc@5 94.922 (94.179)
Epoch: [73][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8837 (1.9749)	Acc@1 75.781 (71.986)	Acc@5 94.922 (94.170)
Epoch: [73][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9669 (1.9771)	Acc@1 71.484 (71.905)	Acc@5 93.750 (94.171)
Epoch: [73][100/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1648 (1.9877)	Acc@1 67.969 (71.720)	Acc@5 91.406 (94.075)
Epoch: [73][110/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0640 (1.9885)	Acc@1 71.094 (71.699)	Acc@5 92.969 (94.053)
Epoch: [73][120/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.0748 (1.9966)	Acc@1 69.922 (71.562)	Acc@5 94.141 (93.989)
Epoch: [73][130/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.1170 (2.0016)	Acc@1 69.141 (71.380)	Acc@5 92.969 (93.956)
Epoch: [73][140/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 1.9968 (2.0055)	Acc@1 71.094 (71.266)	Acc@5 95.703 (93.936)
Epoch: [73][150/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2070 (2.0104)	Acc@1 70.312 (71.130)	Acc@5 92.969 (93.908)
Epoch: [73][160/196]	Time 0.018 (0.015)	Data 0.002 (0.004)	Loss 1.9534 (2.0125)	Acc@1 73.047 (71.045)	Acc@5 95.312 (93.871)
Epoch: [73][170/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 1.9254 (2.0159)	Acc@1 71.875 (71.002)	Acc@5 95.312 (93.823)
Epoch: [73][180/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0214 (2.0201)	Acc@1 72.656 (70.882)	Acc@5 94.922 (93.800)
Epoch: [73][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 2.3966 (2.0265)	Acc@1 63.672 (70.717)	Acc@5 88.281 (93.744)
num momentum params: 26
[0.1, 2.028439884262085, 2.0075821447372437, 70.65, 50.95, tensor(0.5104, device='cuda:0', grad_fn=<DivBackward0>), 2.9639203548431396, 0.3858656883239746]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [74 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [74][0/196]	Time 0.051 (0.051)	Data 0.200 (0.200)	Loss 1.9438 (1.9438)	Acc@1 70.703 (70.703)	Acc@5 95.312 (95.312)
Epoch: [74][10/196]	Time 0.017 (0.019)	Data 0.001 (0.020)	Loss 2.0265 (2.0205)	Acc@1 70.312 (71.378)	Acc@5 92.188 (93.928)
Epoch: [74][20/196]	Time 0.017 (0.017)	Data 0.002 (0.012)	Loss 2.0226 (2.0046)	Acc@1 70.703 (71.410)	Acc@5 92.969 (94.066)
Epoch: [74][30/196]	Time 0.020 (0.017)	Data 0.001 (0.009)	Loss 1.8765 (1.9848)	Acc@1 73.047 (72.001)	Acc@5 97.656 (94.367)
Epoch: [74][40/196]	Time 0.012 (0.016)	Data 0.005 (0.007)	Loss 1.8329 (1.9597)	Acc@1 74.609 (72.713)	Acc@5 96.094 (94.550)
Epoch: [74][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.8912 (1.9570)	Acc@1 74.609 (72.786)	Acc@5 93.750 (94.570)
Epoch: [74][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 1.9718 (1.9607)	Acc@1 70.312 (72.554)	Acc@5 95.703 (94.627)
Epoch: [74][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0925 (1.9667)	Acc@1 70.312 (72.376)	Acc@5 93.750 (94.559)
Epoch: [74][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.8124 (1.9701)	Acc@1 78.516 (72.270)	Acc@5 96.484 (94.584)
Epoch: [74][90/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0184 (1.9777)	Acc@1 73.438 (72.081)	Acc@5 94.141 (94.518)
Epoch: [74][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1706 (1.9872)	Acc@1 66.797 (71.856)	Acc@5 93.750 (94.469)
Epoch: [74][110/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.2053 (2.0000)	Acc@1 67.969 (71.604)	Acc@5 90.625 (94.218)
Epoch: [74][120/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9364 (2.0001)	Acc@1 77.344 (71.597)	Acc@5 94.141 (94.157)
Epoch: [74][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1603 (2.0068)	Acc@1 67.188 (71.437)	Acc@5 92.578 (94.096)
Epoch: [74][140/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0973 (2.0106)	Acc@1 66.406 (71.290)	Acc@5 92.969 (94.030)
Epoch: [74][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0885 (2.0141)	Acc@1 67.969 (71.184)	Acc@5 93.750 (93.983)
Epoch: [74][160/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.1059 (2.0177)	Acc@1 64.844 (71.094)	Acc@5 92.188 (93.944)
Epoch: [74][170/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0467 (2.0264)	Acc@1 71.484 (70.868)	Acc@5 93.359 (93.812)
Epoch: [74][180/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 2.1245 (2.0347)	Acc@1 69.531 (70.643)	Acc@5 91.797 (93.715)
Epoch: [74][190/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.0573 (2.0363)	Acc@1 67.969 (70.609)	Acc@5 92.969 (93.674)
num momentum params: 26
[0.1, 2.038019981842041, 1.849160785675049, 70.53, 53.97, tensor(0.5091, device='cuda:0', grad_fn=<DivBackward0>), 3.0345263481140137, 0.38560986518859863]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [75 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [75][0/196]	Time 0.045 (0.045)	Data 0.199 (0.199)	Loss 1.9725 (1.9725)	Acc@1 71.875 (71.875)	Acc@5 94.531 (94.531)
Epoch: [75][10/196]	Time 0.017 (0.018)	Data 0.001 (0.021)	Loss 1.9409 (2.0015)	Acc@1 74.609 (71.236)	Acc@5 93.750 (93.537)
Epoch: [75][20/196]	Time 0.011 (0.017)	Data 0.006 (0.012)	Loss 1.9705 (1.9697)	Acc@1 69.141 (71.987)	Acc@5 94.922 (94.382)
Epoch: [75][30/196]	Time 0.018 (0.016)	Data 0.000 (0.009)	Loss 1.8792 (1.9599)	Acc@1 74.609 (72.253)	Acc@5 94.141 (94.380)
Epoch: [75][40/196]	Time 0.012 (0.016)	Data 0.007 (0.007)	Loss 1.8813 (1.9472)	Acc@1 74.219 (72.894)	Acc@5 96.875 (94.607)
Epoch: [75][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.9228 (1.9474)	Acc@1 72.266 (72.947)	Acc@5 96.094 (94.608)
Epoch: [75][60/196]	Time 0.012 (0.016)	Data 0.006 (0.006)	Loss 2.0712 (1.9443)	Acc@1 70.703 (73.060)	Acc@5 90.625 (94.602)
Epoch: [75][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0507 (1.9470)	Acc@1 74.219 (72.992)	Acc@5 93.750 (94.559)
Epoch: [75][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.8887 (1.9506)	Acc@1 73.828 (72.724)	Acc@5 96.484 (94.579)
Epoch: [75][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0953 (1.9624)	Acc@1 71.484 (72.489)	Acc@5 90.234 (94.385)
Epoch: [75][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1146 (1.9748)	Acc@1 66.406 (72.115)	Acc@5 91.406 (94.249)
Epoch: [75][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2400 (1.9786)	Acc@1 65.625 (72.083)	Acc@5 91.406 (94.257)
Epoch: [75][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 1.9427 (1.9805)	Acc@1 73.047 (72.072)	Acc@5 95.703 (94.231)
Epoch: [75][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0514 (1.9842)	Acc@1 68.750 (71.935)	Acc@5 92.188 (94.224)
Epoch: [75][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1153 (1.9878)	Acc@1 68.359 (71.809)	Acc@5 89.453 (94.118)
Epoch: [75][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0838 (1.9922)	Acc@1 68.359 (71.691)	Acc@5 92.188 (94.019)
Epoch: [75][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1645 (1.9972)	Acc@1 66.797 (71.564)	Acc@5 92.969 (93.964)
Epoch: [75][170/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9563 (2.0032)	Acc@1 75.391 (71.450)	Acc@5 92.969 (93.873)
Epoch: [75][180/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.1332 (2.0101)	Acc@1 69.531 (71.238)	Acc@5 91.797 (93.832)
Epoch: [75][190/196]	Time 0.016 (0.015)	Data 0.003 (0.004)	Loss 2.2935 (2.0186)	Acc@1 64.062 (71.000)	Acc@5 91.016 (93.772)
num momentum params: 26
[0.1, 2.020463104095459, 1.8215007770061493, 70.938, 53.91, tensor(0.5137, device='cuda:0', grad_fn=<DivBackward0>), 3.015109062194824, 0.37850069999694824]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [76 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [76][0/196]	Time 0.046 (0.046)	Data 0.218 (0.218)	Loss 1.8727 (1.8727)	Acc@1 75.781 (75.781)	Acc@5 96.484 (96.484)
Epoch: [76][10/196]	Time 0.016 (0.019)	Data 0.002 (0.022)	Loss 2.0443 (1.9890)	Acc@1 69.922 (71.804)	Acc@5 92.969 (93.999)
Epoch: [76][20/196]	Time 0.012 (0.017)	Data 0.006 (0.013)	Loss 1.8480 (1.9695)	Acc@1 75.000 (72.266)	Acc@5 94.141 (94.513)
Epoch: [76][30/196]	Time 0.015 (0.017)	Data 0.003 (0.010)	Loss 1.7492 (1.9464)	Acc@1 77.344 (72.606)	Acc@5 95.703 (94.834)
Epoch: [76][40/196]	Time 0.013 (0.016)	Data 0.004 (0.008)	Loss 1.8703 (1.9443)	Acc@1 76.953 (72.713)	Acc@5 96.875 (94.960)
Epoch: [76][50/196]	Time 0.015 (0.016)	Data 0.003 (0.007)	Loss 1.9149 (1.9490)	Acc@1 73.438 (72.511)	Acc@5 95.703 (94.907)
Epoch: [76][60/196]	Time 0.012 (0.016)	Data 0.006 (0.006)	Loss 1.9576 (1.9659)	Acc@1 70.703 (71.907)	Acc@5 95.312 (94.787)
Epoch: [76][70/196]	Time 0.014 (0.016)	Data 0.005 (0.006)	Loss 1.9452 (1.9722)	Acc@1 73.828 (71.963)	Acc@5 95.312 (94.647)
Epoch: [76][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.1112 (1.9843)	Acc@1 69.922 (71.726)	Acc@5 93.750 (94.488)
Epoch: [76][90/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9911 (1.9892)	Acc@1 67.969 (71.549)	Acc@5 96.484 (94.450)
Epoch: [76][100/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 1.9586 (1.9891)	Acc@1 72.266 (71.527)	Acc@5 94.531 (94.454)
Epoch: [76][110/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 1.9729 (1.9890)	Acc@1 74.609 (71.576)	Acc@5 94.531 (94.422)
Epoch: [76][120/196]	Time 0.015 (0.015)	Data 0.005 (0.005)	Loss 2.0500 (1.9903)	Acc@1 70.312 (71.572)	Acc@5 92.578 (94.421)
Epoch: [76][130/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0761 (1.9907)	Acc@1 69.141 (71.565)	Acc@5 91.797 (94.379)
Epoch: [76][140/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.0910 (1.9935)	Acc@1 69.531 (71.520)	Acc@5 92.578 (94.307)
Epoch: [76][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 1.8676 (1.9972)	Acc@1 74.219 (71.412)	Acc@5 96.875 (94.270)
Epoch: [76][160/196]	Time 0.012 (0.015)	Data 0.011 (0.004)	Loss 1.9876 (2.0042)	Acc@1 70.703 (71.242)	Acc@5 96.484 (94.199)
Epoch: [76][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9976 (2.0106)	Acc@1 69.141 (71.071)	Acc@5 92.969 (94.077)
Epoch: [76][180/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 2.2226 (2.0173)	Acc@1 67.969 (70.871)	Acc@5 88.672 (94.003)
Epoch: [76][190/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1389 (2.0218)	Acc@1 66.406 (70.717)	Acc@5 91.797 (93.957)
num momentum params: 26
[0.1, 2.022913428955078, 2.0969675755500794, 70.656, 49.94, tensor(0.5130, device='cuda:0', grad_fn=<DivBackward0>), 2.9983606338500977, 0.38783907890319824]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [77 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [77][0/196]	Time 0.047 (0.047)	Data 0.216 (0.216)	Loss 1.9448 (1.9448)	Acc@1 74.219 (74.219)	Acc@5 96.875 (96.875)
Epoch: [77][10/196]	Time 0.013 (0.019)	Data 0.004 (0.022)	Loss 1.9504 (1.9389)	Acc@1 70.703 (73.580)	Acc@5 93.750 (95.170)
Epoch: [77][20/196]	Time 0.014 (0.017)	Data 0.004 (0.013)	Loss 1.8060 (1.9326)	Acc@1 76.953 (73.214)	Acc@5 97.266 (95.238)
Epoch: [77][30/196]	Time 0.013 (0.016)	Data 0.005 (0.009)	Loss 1.9087 (1.9144)	Acc@1 72.656 (73.727)	Acc@5 95.703 (95.287)
Epoch: [77][40/196]	Time 0.014 (0.016)	Data 0.005 (0.008)	Loss 1.9575 (1.9176)	Acc@1 75.000 (73.523)	Acc@5 94.922 (95.265)
Epoch: [77][50/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 1.8660 (1.9226)	Acc@1 75.781 (73.407)	Acc@5 95.312 (95.244)
Epoch: [77][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.0252 (1.9301)	Acc@1 69.922 (73.149)	Acc@5 93.359 (95.178)
Epoch: [77][70/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.9266 (1.9367)	Acc@1 73.047 (73.041)	Acc@5 94.141 (94.982)
Epoch: [77][80/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9302 (1.9430)	Acc@1 72.656 (72.888)	Acc@5 93.750 (94.883)
Epoch: [77][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.2515 (1.9467)	Acc@1 61.719 (72.669)	Acc@5 91.016 (94.870)
Epoch: [77][100/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.1044 (1.9560)	Acc@1 67.188 (72.478)	Acc@5 92.578 (94.767)
Epoch: [77][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0873 (1.9634)	Acc@1 67.969 (72.234)	Acc@5 92.188 (94.651)
Epoch: [77][120/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0911 (1.9736)	Acc@1 67.188 (71.988)	Acc@5 93.359 (94.544)
Epoch: [77][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0951 (1.9824)	Acc@1 68.750 (71.774)	Acc@5 93.750 (94.436)
Epoch: [77][140/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.0493 (1.9915)	Acc@1 69.922 (71.507)	Acc@5 91.797 (94.326)
Epoch: [77][150/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.3426 (1.9984)	Acc@1 66.406 (71.352)	Acc@5 87.891 (94.252)
Epoch: [77][160/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.1039 (2.0033)	Acc@1 71.094 (71.242)	Acc@5 96.094 (94.226)
Epoch: [77][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.3323 (2.0069)	Acc@1 61.719 (71.137)	Acc@5 91.406 (94.195)
Epoch: [77][180/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.1790 (2.0135)	Acc@1 64.844 (71.010)	Acc@5 91.797 (94.089)
Epoch: [77][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1974 (2.0177)	Acc@1 66.016 (70.908)	Acc@5 92.188 (94.024)
num momentum params: 26
[0.1, 2.019024531402588, 1.8476964986324311, 70.874, 53.31, tensor(0.5147, device='cuda:0', grad_fn=<DivBackward0>), 3.0556507110595703, 0.38567614555358887]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [78 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [78][0/196]	Time 0.071 (0.071)	Data 0.202 (0.202)	Loss 2.0116 (2.0116)	Acc@1 73.438 (73.438)	Acc@5 95.312 (95.312)
Epoch: [78][10/196]	Time 0.025 (0.022)	Data 0.002 (0.020)	Loss 1.9155 (1.9582)	Acc@1 74.609 (73.260)	Acc@5 96.094 (95.206)
Epoch: [78][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 1.8390 (1.9285)	Acc@1 71.875 (73.679)	Acc@5 96.484 (95.089)
Epoch: [78][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 1.9171 (1.9288)	Acc@1 78.516 (73.664)	Acc@5 96.484 (95.060)
Epoch: [78][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8207 (1.9295)	Acc@1 75.000 (73.390)	Acc@5 97.266 (94.998)
Epoch: [78][50/196]	Time 0.014 (0.017)	Data 0.004 (0.006)	Loss 2.0545 (1.9302)	Acc@1 71.875 (73.361)	Acc@5 94.141 (95.014)
Epoch: [78][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.8697 (1.9341)	Acc@1 75.781 (73.297)	Acc@5 94.141 (94.947)
Epoch: [78][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9275 (1.9346)	Acc@1 73.438 (73.190)	Acc@5 93.750 (94.916)
Epoch: [78][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1470 (1.9448)	Acc@1 65.625 (72.849)	Acc@5 93.750 (94.748)
Epoch: [78][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1164 (1.9449)	Acc@1 66.406 (72.742)	Acc@5 94.141 (94.802)
Epoch: [78][100/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9973 (1.9526)	Acc@1 70.703 (72.575)	Acc@5 94.922 (94.705)
Epoch: [78][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0831 (1.9585)	Acc@1 70.312 (72.449)	Acc@5 94.141 (94.616)
Epoch: [78][120/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0236 (1.9666)	Acc@1 71.484 (72.237)	Acc@5 94.922 (94.528)
Epoch: [78][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9309 (1.9744)	Acc@1 71.484 (72.030)	Acc@5 96.484 (94.457)
Epoch: [78][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1347 (1.9823)	Acc@1 65.625 (71.797)	Acc@5 92.578 (94.357)
Epoch: [78][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1792 (1.9941)	Acc@1 65.234 (71.495)	Acc@5 91.797 (94.221)
Epoch: [78][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1226 (1.9971)	Acc@1 69.922 (71.385)	Acc@5 93.750 (94.192)
Epoch: [78][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2538 (2.0038)	Acc@1 64.453 (71.242)	Acc@5 91.797 (94.106)
Epoch: [78][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1857 (2.0151)	Acc@1 67.969 (70.988)	Acc@5 94.531 (93.961)
Epoch: [78][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0601 (2.0197)	Acc@1 69.141 (70.881)	Acc@5 93.359 (93.883)
num momentum params: 26
[0.1, 2.0221520098876953, 1.9606414341926575, 70.842, 52.79, tensor(0.5134, device='cuda:0', grad_fn=<DivBackward0>), 3.0429069995880127, 0.38989901542663574]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [79 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [79][0/196]	Time 0.048 (0.048)	Data 0.201 (0.201)	Loss 1.8823 (1.8823)	Acc@1 77.734 (77.734)	Acc@5 94.141 (94.141)
Epoch: [79][10/196]	Time 0.018 (0.019)	Data 0.001 (0.021)	Loss 1.8313 (1.9389)	Acc@1 74.609 (73.473)	Acc@5 95.703 (94.744)
Epoch: [79][20/196]	Time 0.011 (0.017)	Data 0.007 (0.012)	Loss 1.8228 (1.9187)	Acc@1 75.781 (73.847)	Acc@5 95.312 (94.959)
Epoch: [79][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 1.8962 (1.9001)	Acc@1 73.438 (73.967)	Acc@5 94.141 (95.174)
Epoch: [79][40/196]	Time 0.011 (0.016)	Data 0.008 (0.008)	Loss 1.8606 (1.8952)	Acc@1 68.750 (74.076)	Acc@5 98.047 (95.284)
Epoch: [79][50/196]	Time 0.021 (0.016)	Data 0.000 (0.006)	Loss 1.9129 (1.9002)	Acc@1 74.609 (73.851)	Acc@5 94.922 (95.343)
Epoch: [79][60/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 1.9736 (1.9079)	Acc@1 73.047 (73.770)	Acc@5 93.750 (95.178)
Epoch: [79][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9516 (1.9074)	Acc@1 75.781 (73.751)	Acc@5 94.141 (95.208)
Epoch: [79][80/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.9358 (1.9179)	Acc@1 73.438 (73.568)	Acc@5 94.141 (95.105)
Epoch: [79][90/196]	Time 0.020 (0.016)	Data 0.001 (0.005)	Loss 1.9691 (1.9263)	Acc@1 72.656 (73.433)	Acc@5 94.141 (94.978)
Epoch: [79][100/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.2295 (1.9401)	Acc@1 62.891 (73.089)	Acc@5 91.797 (94.806)
Epoch: [79][110/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1209 (1.9553)	Acc@1 69.531 (72.653)	Acc@5 92.188 (94.616)
Epoch: [79][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1650 (1.9659)	Acc@1 66.797 (72.401)	Acc@5 92.188 (94.467)
Epoch: [79][130/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9407 (1.9773)	Acc@1 71.484 (72.102)	Acc@5 93.750 (94.296)
Epoch: [79][140/196]	Time 0.018 (0.016)	Data 0.002 (0.004)	Loss 1.9867 (1.9831)	Acc@1 68.359 (71.905)	Acc@5 94.531 (94.232)
Epoch: [79][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0808 (1.9902)	Acc@1 68.359 (71.707)	Acc@5 91.406 (94.135)
Epoch: [79][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1022 (1.9971)	Acc@1 70.312 (71.547)	Acc@5 91.016 (94.051)
Epoch: [79][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.3032 (2.0055)	Acc@1 64.453 (71.324)	Acc@5 89.453 (93.958)
Epoch: [79][180/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1584 (2.0107)	Acc@1 69.141 (71.206)	Acc@5 92.188 (93.899)
Epoch: [79][190/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1801 (2.0143)	Acc@1 65.234 (71.116)	Acc@5 91.016 (93.860)
num momentum params: 26
[0.1, 2.0161888850402834, 1.9364416468143464, 71.094, 52.13, tensor(0.5152, device='cuda:0', grad_fn=<DivBackward0>), 3.0842638015747066, 0.3943469524383545]
Non Pruning Epoch - module.conv1.weight: [51, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [51]
Non Pruning Epoch - module.bn1.bias: [51]
Non Pruning Epoch - module.conv2.weight: [128, 51, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [254, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [254]
Non Pruning Epoch - module.bn3.bias: [254]
Non Pruning Epoch - module.conv4.weight: [256, 254, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [494, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [494]
Non Pruning Epoch - module.bn7.bias: [494]
Non Pruning Epoch - module.conv8.weight: [398, 494, 3, 3]
Non Pruning Epoch - module.bn8.weight: [398]
Non Pruning Epoch - module.bn8.bias: [398]
Non Pruning Epoch - module.fc.weight: [100, 398]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [80 | 180] LR: 0.100000
module.conv1.weight [51, 3, 3, 3]
module.conv2.weight [128, 51, 3, 3]
module.conv3.weight [254, 128, 3, 3]
module.conv4.weight [256, 254, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [494, 509, 3, 3]
module.conv8.weight [398, 494, 3, 3]
Epoch: [80][0/196]	Time 0.060 (0.060)	Data 0.192 (0.192)	Loss 1.9477 (1.9477)	Acc@1 74.609 (74.609)	Acc@5 92.969 (92.969)
Epoch: [80][10/196]	Time 0.016 (0.020)	Data 0.002 (0.019)	Loss 1.9497 (1.9450)	Acc@1 73.047 (72.727)	Acc@5 97.266 (95.597)
Epoch: [80][20/196]	Time 0.015 (0.018)	Data 0.002 (0.011)	Loss 1.9234 (1.9327)	Acc@1 74.219 (73.438)	Acc@5 93.359 (95.275)
Epoch: [80][30/196]	Time 0.015 (0.017)	Data 0.003 (0.009)	Loss 1.9411 (1.9273)	Acc@1 76.172 (73.576)	Acc@5 96.094 (95.300)
Epoch: [80][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.8658 (1.9159)	Acc@1 71.875 (73.561)	Acc@5 97.266 (95.427)
Epoch: [80][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 1.8851 (1.9204)	Acc@1 73.438 (73.545)	Acc@5 95.703 (95.244)
Epoch: [80][60/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.0169 (1.9174)	Acc@1 69.141 (73.540)	Acc@5 92.969 (95.293)
Epoch: [80][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8395 (1.9228)	Acc@1 76.562 (73.509)	Acc@5 97.266 (95.219)
Epoch: [80][80/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 2.0722 (1.9264)	Acc@1 71.484 (73.370)	Acc@5 91.406 (95.197)
Epoch: [80][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8207 (1.9333)	Acc@1 78.125 (73.244)	Acc@5 96.094 (95.081)
Epoch: [80][100/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0725 (1.9393)	Acc@1 72.656 (73.171)	Acc@5 92.188 (94.957)
Epoch: [80][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0681 (1.9525)	Acc@1 70.703 (72.808)	Acc@5 92.578 (94.739)
Epoch: [80][120/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0628 (1.9612)	Acc@1 69.531 (72.579)	Acc@5 92.578 (94.622)
Epoch: [80][130/196]	Time 0.013 (0.015)	Data 0.010 (0.004)	Loss 2.0070 (1.9649)	Acc@1 71.094 (72.498)	Acc@5 93.750 (94.537)
Epoch: [80][140/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0204 (1.9697)	Acc@1 70.703 (72.371)	Acc@5 93.750 (94.487)
Epoch: [80][150/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.0800 (1.9734)	Acc@1 68.359 (72.175)	Acc@5 96.484 (94.425)
Epoch: [80][160/196]	Time 0.018 (0.015)	Data 0.002 (0.004)	Loss 2.0164 (1.9796)	Acc@1 71.484 (72.018)	Acc@5 93.750 (94.323)
Epoch: [80][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0953 (1.9871)	Acc@1 67.969 (71.822)	Acc@5 94.531 (94.255)
Epoch: [80][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1795 (1.9959)	Acc@1 63.281 (71.586)	Acc@5 94.141 (94.132)
Epoch: [80][190/196]	Time 0.012 (0.015)	Data 0.010 (0.004)	Loss 2.1110 (2.0031)	Acc@1 68.750 (71.421)	Acc@5 90.234 (94.051)
num momentum params: 26
[0.1, 2.0054333619689944, 1.9482890772819519, 71.36, 51.83, tensor(0.5178, device='cuda:0', grad_fn=<DivBackward0>), 2.974029541015625, 0.38579344749450684]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [51, 3, 3, 3]
Before - module.bn1.weight: [51]
Before - module.bn1.bias: [51]
Before - module.conv2.weight: [128, 51, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [254, 128, 3, 3]
Before - module.bn3.weight: [254]
Before - module.bn3.bias: [254]
Before - module.conv4.weight: [256, 254, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [509, 510, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [494, 509, 3, 3]
Before - module.bn7.weight: [494]
Before - module.bn7.bias: [494]
Before - module.conv8.weight: [398, 494, 3, 3]
Before - module.bn8.weight: [398]
Before - module.bn8.bias: [398]
Before - module.fc.weight: [100, 398]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [51, 3, 3, 3] >> [48, 3, 3, 3]
[module.bn1.weight]: 51 >> 48
running_mean [48]
running_var [48]
num_batches_tracked []
[module.conv2.weight]: [128, 51, 3, 3] >> [128, 48, 3, 3]
[module.conv3.weight]: [254, 128, 3, 3] >> [253, 128, 3, 3]
[module.bn3.weight]: 254 >> 253
running_mean [253]
running_var [253]
num_batches_tracked []
[module.conv4.weight]: [256, 254, 3, 3] >> [256, 253, 3, 3]
[module.conv7.weight]: [494, 509, 3, 3] >> [490, 509, 3, 3]
[module.bn7.weight]: 494 >> 490
running_mean [490]
running_var [490]
num_batches_tracked []
[module.conv8.weight]: [398, 494, 3, 3] >> [373, 490, 3, 3]
[module.bn8.weight]: 398 >> 373
running_mean [373]
running_var [373]
num_batches_tracked []
[module.fc.weight]: [100, 398] >> [100, 373]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [48, 3, 3, 3]
After - module.bn1.weight: [48]
After - module.bn1.bias: [48]
After - module.conv2.weight: [128, 48, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [253, 128, 3, 3]
After - module.bn3.weight: [253]
After - module.bn3.bias: [253]
After - module.conv4.weight: [256, 253, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [509, 510, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [490, 509, 3, 3]
After - module.bn7.weight: [490]
After - module.bn7.bias: [490]
After - module.conv8.weight: [373, 490, 3, 3]
After - module.bn8.weight: [373]
After - module.bn8.bias: [373]
After - module.fc.weight: [100, 373]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [48, 3, 3, 3]
conv2 --> [128, 48, 3, 3]
conv3 --> [253, 128, 3, 3]
conv4 --> [256, 253, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [509, 510, 3, 3]
conv7 --> [490, 509, 3, 3]
conv8 --> [373, 490, 3, 3]
fc --> [373, 100]
1, 531505152, 1327104, 48
2, 5917114368, 14155776, 128
3, 8505851904, 18653184, 253
4, 17011703808, 37306368, 256
5, 10227548160, 18800640, 510
6, 20335242240, 37380960, 509
7, 6895687680, 8978760, 490
8, 5053224960, 6579720, 373
fc, 14323200, 37300, 0
===================
FLOP REPORT: 29098516200000.0 53700800000.0 143219812 134252 2567 15.968647003173828
[INFO] Storing checkpoint...

Epoch: [81 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [81][0/196]	Time 0.520 (0.520)	Data 0.211 (0.211)	Loss 1.9340 (1.9340)	Acc@1 72.266 (72.266)	Acc@5 95.703 (95.703)
Epoch: [81][10/196]	Time 0.015 (0.062)	Data 0.003 (0.021)	Loss 2.0106 (2.0003)	Acc@1 71.875 (72.088)	Acc@5 93.750 (94.212)
Epoch: [81][20/196]	Time 0.015 (0.040)	Data 0.002 (0.012)	Loss 1.8510 (1.9642)	Acc@1 74.609 (72.768)	Acc@5 95.703 (94.773)
Epoch: [81][30/196]	Time 0.014 (0.031)	Data 0.003 (0.009)	Loss 1.9147 (1.9441)	Acc@1 77.734 (73.085)	Acc@5 93.359 (95.136)
Epoch: [81][40/196]	Time 0.014 (0.028)	Data 0.003 (0.007)	Loss 1.9125 (1.9480)	Acc@1 70.703 (72.847)	Acc@5 94.531 (94.931)
Epoch: [81][50/196]	Time 0.015 (0.025)	Data 0.003 (0.007)	Loss 2.0655 (1.9478)	Acc@1 69.141 (72.786)	Acc@5 92.969 (94.930)
Epoch: [81][60/196]	Time 0.015 (0.023)	Data 0.002 (0.006)	Loss 1.9202 (1.9449)	Acc@1 73.047 (72.925)	Acc@5 96.094 (94.845)
Epoch: [81][70/196]	Time 0.015 (0.022)	Data 0.002 (0.005)	Loss 1.8494 (1.9439)	Acc@1 75.391 (72.832)	Acc@5 96.875 (94.850)
Epoch: [81][80/196]	Time 0.015 (0.021)	Data 0.002 (0.005)	Loss 2.0301 (1.9510)	Acc@1 71.484 (72.671)	Acc@5 94.141 (94.715)
Epoch: [81][90/196]	Time 0.012 (0.020)	Data 0.011 (0.005)	Loss 2.0619 (1.9615)	Acc@1 73.047 (72.424)	Acc@5 93.750 (94.609)
Epoch: [81][100/196]	Time 0.015 (0.020)	Data 0.002 (0.005)	Loss 2.1118 (1.9666)	Acc@1 71.094 (72.347)	Acc@5 90.234 (94.524)
Epoch: [81][110/196]	Time 0.015 (0.019)	Data 0.003 (0.004)	Loss 2.1642 (1.9750)	Acc@1 67.188 (72.065)	Acc@5 94.141 (94.507)
Epoch: [81][120/196]	Time 0.011 (0.019)	Data 0.005 (0.004)	Loss 2.2093 (1.9861)	Acc@1 66.406 (71.781)	Acc@5 90.234 (94.392)
Epoch: [81][130/196]	Time 0.014 (0.019)	Data 0.003 (0.004)	Loss 2.0605 (1.9934)	Acc@1 71.484 (71.645)	Acc@5 94.531 (94.290)
Epoch: [81][140/196]	Time 0.012 (0.018)	Data 0.005 (0.004)	Loss 2.0392 (2.0009)	Acc@1 68.750 (71.446)	Acc@5 93.359 (94.254)
Epoch: [81][150/196]	Time 0.013 (0.018)	Data 0.004 (0.004)	Loss 2.0153 (2.0025)	Acc@1 72.656 (71.456)	Acc@5 92.969 (94.229)
Epoch: [81][160/196]	Time 0.011 (0.017)	Data 0.008 (0.005)	Loss 2.2583 (2.0080)	Acc@1 63.281 (71.334)	Acc@5 90.625 (94.187)
Epoch: [81][170/196]	Time 0.014 (0.017)	Data 0.003 (0.005)	Loss 2.0336 (2.0126)	Acc@1 70.312 (71.206)	Acc@5 92.969 (94.129)
Epoch: [81][180/196]	Time 0.012 (0.017)	Data 0.013 (0.005)	Loss 1.9182 (2.0172)	Acc@1 74.219 (71.068)	Acc@5 94.531 (94.056)
Epoch: [81][190/196]	Time 0.024 (0.017)	Data 0.003 (0.005)	Loss 2.1145 (2.0209)	Acc@1 68.750 (70.959)	Acc@5 92.578 (94.034)
num momentum params: 26
[0.1, 2.024596639099121, 1.8963712859153747, 70.856, 52.72, tensor(0.5143, device='cuda:0', grad_fn=<DivBackward0>), 3.4852828979492183, 0.44444870948791504]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [82 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [82][0/196]	Time 0.043 (0.043)	Data 0.213 (0.213)	Loss 1.9972 (1.9972)	Acc@1 70.312 (70.312)	Acc@5 94.922 (94.922)
Epoch: [82][10/196]	Time 0.017 (0.018)	Data 0.000 (0.022)	Loss 1.9445 (1.9940)	Acc@1 73.047 (71.165)	Acc@5 95.703 (94.318)
Epoch: [82][20/196]	Time 0.011 (0.017)	Data 0.009 (0.013)	Loss 1.8603 (1.9927)	Acc@1 73.828 (71.429)	Acc@5 95.703 (94.178)
Epoch: [82][30/196]	Time 0.018 (0.017)	Data 0.001 (0.009)	Loss 1.8659 (1.9777)	Acc@1 74.219 (72.064)	Acc@5 96.484 (94.380)
Epoch: [82][40/196]	Time 0.011 (0.016)	Data 0.009 (0.008)	Loss 2.0398 (1.9727)	Acc@1 73.047 (72.170)	Acc@5 93.750 (94.512)
Epoch: [82][50/196]	Time 0.017 (0.016)	Data 0.000 (0.007)	Loss 1.8511 (1.9625)	Acc@1 78.516 (72.480)	Acc@5 98.828 (94.799)
Epoch: [82][60/196]	Time 0.013 (0.016)	Data 0.010 (0.006)	Loss 1.9402 (1.9606)	Acc@1 73.047 (72.650)	Acc@5 94.141 (94.787)
Epoch: [82][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9824 (1.9607)	Acc@1 71.484 (72.590)	Acc@5 94.531 (94.856)
Epoch: [82][80/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 1.9673 (1.9612)	Acc@1 74.219 (72.536)	Acc@5 94.531 (94.825)
Epoch: [82][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0606 (1.9675)	Acc@1 71.875 (72.454)	Acc@5 91.406 (94.724)
Epoch: [82][100/196]	Time 0.011 (0.016)	Data 0.010 (0.005)	Loss 2.0862 (1.9737)	Acc@1 67.969 (72.262)	Acc@5 92.969 (94.605)
Epoch: [82][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0082 (1.9802)	Acc@1 71.094 (72.107)	Acc@5 94.531 (94.528)
Epoch: [82][120/196]	Time 0.011 (0.016)	Data 0.009 (0.005)	Loss 1.9443 (1.9862)	Acc@1 71.094 (71.923)	Acc@5 95.703 (94.418)
Epoch: [82][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0404 (1.9937)	Acc@1 70.703 (71.699)	Acc@5 92.969 (94.328)
Epoch: [82][140/196]	Time 0.012 (0.016)	Data 0.019 (0.004)	Loss 2.1229 (1.9989)	Acc@1 69.531 (71.570)	Acc@5 93.750 (94.257)
Epoch: [82][150/196]	Time 0.015 (0.016)	Data 0.001 (0.004)	Loss 2.1145 (2.0039)	Acc@1 67.578 (71.435)	Acc@5 92.578 (94.148)
Epoch: [82][160/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.1205 (2.0113)	Acc@1 65.625 (71.281)	Acc@5 93.750 (94.061)
Epoch: [82][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9557 (2.0160)	Acc@1 69.141 (71.162)	Acc@5 94.922 (94.036)
Epoch: [82][180/196]	Time 0.013 (0.016)	Data 0.009 (0.004)	Loss 2.1050 (2.0208)	Acc@1 70.312 (70.997)	Acc@5 91.406 (93.987)
Epoch: [82][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1970 (2.0277)	Acc@1 68.750 (70.852)	Acc@5 91.406 (93.895)
num momentum params: 26
[0.1, 2.03059608215332, 1.9787492763996124, 70.772, 51.35, tensor(0.5137, device='cuda:0', grad_fn=<DivBackward0>), 3.0736026763916016, 0.38591837882995605]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [83 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [83][0/196]	Time 0.050 (0.050)	Data 0.202 (0.202)	Loss 2.0381 (2.0381)	Acc@1 71.094 (71.094)	Acc@5 92.578 (92.578)
Epoch: [83][10/196]	Time 0.013 (0.019)	Data 0.005 (0.021)	Loss 2.0787 (2.0019)	Acc@1 68.750 (71.520)	Acc@5 95.312 (94.638)
Epoch: [83][20/196]	Time 0.013 (0.017)	Data 0.005 (0.012)	Loss 1.9512 (1.9920)	Acc@1 75.781 (72.080)	Acc@5 95.312 (94.531)
Epoch: [83][30/196]	Time 0.014 (0.016)	Data 0.003 (0.009)	Loss 2.0140 (1.9894)	Acc@1 73.828 (72.114)	Acc@5 94.531 (94.519)
Epoch: [83][40/196]	Time 0.012 (0.016)	Data 0.010 (0.008)	Loss 2.0875 (1.9921)	Acc@1 70.312 (72.161)	Acc@5 93.750 (94.512)
Epoch: [83][50/196]	Time 0.014 (0.015)	Data 0.003 (0.007)	Loss 1.9043 (1.9864)	Acc@1 76.953 (72.358)	Acc@5 94.922 (94.539)
Epoch: [83][60/196]	Time 0.015 (0.015)	Data 0.008 (0.007)	Loss 1.8945 (1.9763)	Acc@1 74.219 (72.592)	Acc@5 92.969 (94.602)
Epoch: [83][70/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 2.1358 (1.9788)	Acc@1 67.969 (72.420)	Acc@5 92.188 (94.575)
Epoch: [83][80/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 1.9843 (1.9776)	Acc@1 73.047 (72.323)	Acc@5 91.797 (94.589)
Epoch: [83][90/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 2.0253 (1.9809)	Acc@1 69.141 (72.253)	Acc@5 93.359 (94.544)
Epoch: [83][100/196]	Time 0.012 (0.015)	Data 0.013 (0.006)	Loss 2.0729 (1.9858)	Acc@1 66.797 (72.084)	Acc@5 94.141 (94.547)
Epoch: [83][110/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 2.2534 (1.9922)	Acc@1 64.844 (71.910)	Acc@5 89.844 (94.429)
Epoch: [83][120/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 1.9990 (1.9932)	Acc@1 68.750 (71.923)	Acc@5 93.750 (94.367)
Epoch: [83][130/196]	Time 0.012 (0.015)	Data 0.014 (0.005)	Loss 2.0678 (1.9950)	Acc@1 69.531 (71.896)	Acc@5 93.359 (94.320)
Epoch: [83][140/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0874 (2.0035)	Acc@1 67.578 (71.689)	Acc@5 92.969 (94.177)
Epoch: [83][150/196]	Time 0.011 (0.015)	Data 0.013 (0.005)	Loss 1.9082 (2.0061)	Acc@1 74.609 (71.611)	Acc@5 94.141 (94.135)
Epoch: [83][160/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 1.9846 (2.0125)	Acc@1 71.484 (71.458)	Acc@5 95.312 (94.068)
Epoch: [83][170/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.1496 (2.0165)	Acc@1 67.969 (71.393)	Acc@5 91.797 (93.992)
Epoch: [83][180/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.1654 (2.0225)	Acc@1 69.141 (71.232)	Acc@5 90.234 (93.933)
Epoch: [83][190/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.2375 (2.0246)	Acc@1 67.188 (71.190)	Acc@5 93.359 (93.899)
num momentum params: 26
[0.1, 2.0247811460876464, 1.8149846088886261, 71.18, 54.52, tensor(0.5163, device='cuda:0', grad_fn=<DivBackward0>), 2.9165120124816895, 0.3797304630279541]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [84 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [84][0/196]	Time 0.046 (0.046)	Data 0.209 (0.209)	Loss 1.9017 (1.9017)	Acc@1 75.000 (75.000)	Acc@5 96.094 (96.094)
Epoch: [84][10/196]	Time 0.018 (0.019)	Data 0.001 (0.021)	Loss 2.0299 (1.9886)	Acc@1 68.750 (71.307)	Acc@5 92.969 (94.070)
Epoch: [84][20/196]	Time 0.015 (0.017)	Data 0.003 (0.012)	Loss 2.0528 (1.9648)	Acc@1 70.312 (72.507)	Acc@5 93.750 (94.550)
Epoch: [84][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 1.9617 (1.9566)	Acc@1 75.781 (72.744)	Acc@5 96.094 (94.582)
Epoch: [84][40/196]	Time 0.015 (0.016)	Data 0.003 (0.008)	Loss 1.9658 (1.9567)	Acc@1 71.094 (72.628)	Acc@5 95.312 (94.684)
Epoch: [84][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.9228 (1.9505)	Acc@1 74.609 (72.771)	Acc@5 94.531 (94.723)
Epoch: [84][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0997 (1.9546)	Acc@1 66.797 (72.586)	Acc@5 92.969 (94.679)
Epoch: [84][70/196]	Time 0.013 (0.015)	Data 0.003 (0.006)	Loss 2.1799 (1.9610)	Acc@1 68.750 (72.453)	Acc@5 91.406 (94.625)
Epoch: [84][80/196]	Time 0.011 (0.015)	Data 0.014 (0.006)	Loss 1.8595 (1.9548)	Acc@1 73.438 (72.569)	Acc@5 96.094 (94.657)
Epoch: [84][90/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 1.9534 (1.9588)	Acc@1 74.219 (72.493)	Acc@5 93.359 (94.621)
Epoch: [84][100/196]	Time 0.011 (0.015)	Data 0.007 (0.006)	Loss 2.0459 (1.9601)	Acc@1 67.969 (72.482)	Acc@5 95.312 (94.667)
Epoch: [84][110/196]	Time 0.017 (0.014)	Data 0.001 (0.005)	Loss 1.9894 (1.9702)	Acc@1 74.219 (72.301)	Acc@5 93.750 (94.559)
Epoch: [84][120/196]	Time 0.011 (0.014)	Data 0.010 (0.005)	Loss 1.8256 (1.9732)	Acc@1 75.000 (72.185)	Acc@5 96.484 (94.528)
Epoch: [84][130/196]	Time 0.015 (0.014)	Data 0.002 (0.005)	Loss 1.9675 (1.9800)	Acc@1 75.391 (72.039)	Acc@5 94.141 (94.439)
Epoch: [84][140/196]	Time 0.012 (0.014)	Data 0.010 (0.005)	Loss 1.9866 (1.9862)	Acc@1 73.828 (71.883)	Acc@5 94.922 (94.371)
Epoch: [84][150/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 2.1607 (1.9936)	Acc@1 67.578 (71.645)	Acc@5 89.062 (94.291)
Epoch: [84][160/196]	Time 0.012 (0.014)	Data 0.013 (0.005)	Loss 2.2241 (1.9994)	Acc@1 69.141 (71.608)	Acc@5 91.797 (94.211)
Epoch: [84][170/196]	Time 0.016 (0.014)	Data 0.003 (0.005)	Loss 2.0444 (2.0038)	Acc@1 69.922 (71.500)	Acc@5 94.922 (94.120)
Epoch: [84][180/196]	Time 0.012 (0.014)	Data 0.015 (0.005)	Loss 1.9800 (2.0061)	Acc@1 71.484 (71.422)	Acc@5 97.266 (94.138)
Epoch: [84][190/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 2.1509 (2.0114)	Acc@1 66.797 (71.368)	Acc@5 92.578 (94.049)
num momentum params: 26
[0.1, 2.013473193359375, 1.8522489583492279, 71.342, 54.41, tensor(0.5191, device='cuda:0', grad_fn=<DivBackward0>), 2.7623789310455322, 0.3737044334411621]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [85 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [85][0/196]	Time 0.048 (0.048)	Data 0.209 (0.209)	Loss 2.1176 (2.1176)	Acc@1 67.188 (67.188)	Acc@5 93.750 (93.750)
Epoch: [85][10/196]	Time 0.017 (0.019)	Data 0.001 (0.021)	Loss 1.9135 (1.9604)	Acc@1 75.781 (72.479)	Acc@5 92.969 (95.135)
Epoch: [85][20/196]	Time 0.013 (0.017)	Data 0.004 (0.012)	Loss 1.7855 (1.9430)	Acc@1 76.562 (73.196)	Acc@5 95.703 (94.885)
Epoch: [85][30/196]	Time 0.015 (0.016)	Data 0.002 (0.009)	Loss 1.8504 (1.9310)	Acc@1 78.125 (73.349)	Acc@5 96.484 (95.086)
Epoch: [85][40/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 2.0135 (1.9285)	Acc@1 71.875 (73.466)	Acc@5 94.922 (95.027)
Epoch: [85][50/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.0241 (1.9274)	Acc@1 68.750 (73.399)	Acc@5 94.922 (95.152)
Epoch: [85][60/196]	Time 0.011 (0.015)	Data 0.009 (0.006)	Loss 2.0283 (1.9313)	Acc@1 73.438 (73.431)	Acc@5 92.578 (95.044)
Epoch: [85][70/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 2.0369 (1.9311)	Acc@1 68.750 (73.454)	Acc@5 94.141 (94.971)
Epoch: [85][80/196]	Time 0.012 (0.015)	Data 0.010 (0.006)	Loss 1.9745 (1.9413)	Acc@1 73.047 (73.302)	Acc@5 94.922 (94.845)
Epoch: [85][90/196]	Time 0.016 (0.015)	Data 0.001 (0.006)	Loss 2.0333 (1.9457)	Acc@1 68.750 (73.189)	Acc@5 93.750 (94.896)
Epoch: [85][100/196]	Time 0.011 (0.015)	Data 0.006 (0.006)	Loss 1.9937 (1.9536)	Acc@1 72.656 (72.942)	Acc@5 93.750 (94.810)
Epoch: [85][110/196]	Time 0.011 (0.015)	Data 0.005 (0.005)	Loss 1.9407 (1.9591)	Acc@1 75.781 (72.808)	Acc@5 93.359 (94.690)
Epoch: [85][120/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 1.9074 (1.9642)	Acc@1 73.438 (72.669)	Acc@5 96.094 (94.638)
Epoch: [85][130/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.3157 (1.9752)	Acc@1 62.891 (72.388)	Acc@5 92.188 (94.519)
Epoch: [85][140/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.2014 (1.9809)	Acc@1 66.797 (72.268)	Acc@5 92.969 (94.465)
Epoch: [85][150/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0979 (1.9867)	Acc@1 71.484 (72.139)	Acc@5 92.188 (94.417)
Epoch: [85][160/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.0712 (1.9935)	Acc@1 69.531 (71.989)	Acc@5 91.016 (94.340)
Epoch: [85][170/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0192 (1.9986)	Acc@1 71.094 (71.925)	Acc@5 92.969 (94.241)
Epoch: [85][180/196]	Time 0.016 (0.015)	Data 0.008 (0.005)	Loss 2.1971 (2.0034)	Acc@1 65.625 (71.752)	Acc@5 92.188 (94.195)
Epoch: [85][190/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.1732 (2.0086)	Acc@1 67.969 (71.664)	Acc@5 92.188 (94.114)
num momentum params: 26
[0.1, 2.0098298471450806, 1.9826832270622254, 71.606, 51.06, tensor(0.5199, device='cuda:0', grad_fn=<DivBackward0>), 2.920712947845459, 0.3782362937927246]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [86 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [86][0/196]	Time 0.056 (0.056)	Data 0.208 (0.208)	Loss 1.8651 (1.8651)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)
Epoch: [86][10/196]	Time 0.024 (0.021)	Data 0.002 (0.021)	Loss 1.8372 (1.8949)	Acc@1 75.391 (74.609)	Acc@5 97.266 (95.455)
Epoch: [86][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 1.8226 (1.8911)	Acc@1 75.781 (74.386)	Acc@5 96.094 (95.368)
Epoch: [86][30/196]	Time 0.014 (0.017)	Data 0.003 (0.009)	Loss 1.8923 (1.9055)	Acc@1 75.781 (74.395)	Acc@5 96.094 (95.023)
Epoch: [86][40/196]	Time 0.013 (0.017)	Data 0.005 (0.008)	Loss 2.0057 (1.9058)	Acc@1 71.484 (74.209)	Acc@5 95.703 (95.208)
Epoch: [86][50/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9620 (1.9067)	Acc@1 71.875 (73.958)	Acc@5 94.141 (95.236)
Epoch: [86][60/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9526 (1.9178)	Acc@1 73.438 (73.706)	Acc@5 95.312 (95.197)
Epoch: [86][70/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9987 (1.9204)	Acc@1 70.703 (73.630)	Acc@5 94.531 (95.114)
Epoch: [86][80/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0499 (1.9241)	Acc@1 71.875 (73.606)	Acc@5 94.531 (95.067)
Epoch: [86][90/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.9366 (1.9314)	Acc@1 75.000 (73.498)	Acc@5 95.703 (94.943)
Epoch: [86][100/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 2.0743 (1.9421)	Acc@1 70.312 (73.194)	Acc@5 92.578 (94.841)
Epoch: [86][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1591 (1.9524)	Acc@1 67.578 (72.920)	Acc@5 93.359 (94.781)
Epoch: [86][120/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 2.1249 (1.9644)	Acc@1 66.406 (72.527)	Acc@5 94.531 (94.689)
Epoch: [86][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.8837 (1.9687)	Acc@1 75.781 (72.486)	Acc@5 97.266 (94.654)
Epoch: [86][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9464 (1.9782)	Acc@1 73.438 (72.279)	Acc@5 92.969 (94.520)
Epoch: [86][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.9656 (1.9845)	Acc@1 75.000 (72.136)	Acc@5 96.094 (94.487)
Epoch: [86][160/196]	Time 0.013 (0.015)	Data 0.003 (0.005)	Loss 2.2085 (1.9927)	Acc@1 64.453 (71.924)	Acc@5 94.922 (94.408)
Epoch: [86][170/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9823 (1.9979)	Acc@1 71.094 (71.820)	Acc@5 92.969 (94.335)
Epoch: [86][180/196]	Time 0.011 (0.015)	Data 0.004 (0.004)	Loss 2.2545 (2.0040)	Acc@1 64.844 (71.627)	Acc@5 91.016 (94.261)
Epoch: [86][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1498 (2.0096)	Acc@1 65.234 (71.468)	Acc@5 93.359 (94.175)
num momentum params: 26
[0.1, 2.013528165512085, 1.917376263141632, 71.364, 52.87, tensor(0.5186, device='cuda:0', grad_fn=<DivBackward0>), 3.001892328262329, 0.3810300827026367]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [87 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [87][0/196]	Time 0.056 (0.056)	Data 0.190 (0.190)	Loss 1.9889 (1.9889)	Acc@1 73.438 (73.438)	Acc@5 96.094 (96.094)
Epoch: [87][10/196]	Time 0.014 (0.019)	Data 0.003 (0.020)	Loss 1.8905 (1.9973)	Acc@1 73.828 (71.733)	Acc@5 94.922 (94.638)
Epoch: [87][20/196]	Time 0.012 (0.017)	Data 0.004 (0.011)	Loss 1.8864 (1.9799)	Acc@1 73.828 (72.154)	Acc@5 95.703 (94.382)
Epoch: [87][30/196]	Time 0.018 (0.017)	Data 0.001 (0.008)	Loss 1.8820 (1.9678)	Acc@1 76.953 (72.606)	Acc@5 94.922 (94.481)
Epoch: [87][40/196]	Time 0.012 (0.016)	Data 0.008 (0.007)	Loss 1.8661 (1.9670)	Acc@1 76.172 (72.532)	Acc@5 94.922 (94.436)
Epoch: [87][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.8304 (1.9650)	Acc@1 77.344 (72.503)	Acc@5 96.094 (94.524)
Epoch: [87][60/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 1.9720 (1.9669)	Acc@1 71.484 (72.503)	Acc@5 94.531 (94.538)
Epoch: [87][70/196]	Time 0.015 (0.016)	Data 0.001 (0.005)	Loss 2.0745 (1.9686)	Acc@1 67.188 (72.530)	Acc@5 94.531 (94.509)
Epoch: [87][80/196]	Time 0.013 (0.016)	Data 0.010 (0.005)	Loss 2.1255 (1.9702)	Acc@1 67.188 (72.434)	Acc@5 92.188 (94.502)
Epoch: [87][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9641 (1.9729)	Acc@1 71.875 (72.291)	Acc@5 94.531 (94.471)
Epoch: [87][100/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 1.9503 (1.9775)	Acc@1 71.875 (72.146)	Acc@5 95.703 (94.458)
Epoch: [87][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0255 (1.9865)	Acc@1 68.750 (71.931)	Acc@5 95.312 (94.376)
Epoch: [87][120/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0567 (1.9922)	Acc@1 65.234 (71.733)	Acc@5 95.312 (94.328)
Epoch: [87][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0221 (1.9974)	Acc@1 70.703 (71.595)	Acc@5 95.312 (94.296)
Epoch: [87][140/196]	Time 0.026 (0.016)	Data 0.001 (0.005)	Loss 1.9802 (2.0008)	Acc@1 70.703 (71.479)	Acc@5 94.531 (94.312)
Epoch: [87][150/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9065 (2.0049)	Acc@1 75.000 (71.409)	Acc@5 95.703 (94.301)
Epoch: [87][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1950 (2.0102)	Acc@1 67.578 (71.290)	Acc@5 89.844 (94.211)
Epoch: [87][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.2483 (2.0166)	Acc@1 68.750 (71.105)	Acc@5 89.062 (94.131)
Epoch: [87][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 1.9030 (2.0253)	Acc@1 73.828 (70.919)	Acc@5 94.531 (94.009)
Epoch: [87][190/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.0237 (2.0281)	Acc@1 73.047 (70.832)	Acc@5 95.312 (94.000)
num momentum params: 26
[0.1, 2.0309121800231935, 1.920507354736328, 70.778, 52.41, tensor(0.5158, device='cuda:0', grad_fn=<DivBackward0>), 3.0231359004974365, 0.3769855499267578]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [88 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [88][0/196]	Time 0.051 (0.051)	Data 0.203 (0.203)	Loss 2.0467 (2.0467)	Acc@1 72.656 (72.656)	Acc@5 91.406 (91.406)
Epoch: [88][10/196]	Time 0.017 (0.019)	Data 0.001 (0.020)	Loss 2.0301 (1.9810)	Acc@1 69.531 (73.189)	Acc@5 96.094 (94.354)
Epoch: [88][20/196]	Time 0.016 (0.017)	Data 0.002 (0.012)	Loss 2.0036 (1.9645)	Acc@1 73.828 (72.824)	Acc@5 93.750 (94.587)
Epoch: [88][30/196]	Time 0.018 (0.016)	Data 0.001 (0.009)	Loss 1.9636 (1.9443)	Acc@1 73.047 (73.425)	Acc@5 95.312 (94.783)
Epoch: [88][40/196]	Time 0.012 (0.016)	Data 0.014 (0.008)	Loss 1.9413 (1.9340)	Acc@1 72.266 (73.666)	Acc@5 94.922 (94.979)
Epoch: [88][50/196]	Time 0.016 (0.016)	Data 0.000 (0.007)	Loss 2.0253 (1.9331)	Acc@1 70.703 (73.713)	Acc@5 93.750 (95.044)
Epoch: [88][60/196]	Time 0.012 (0.016)	Data 0.013 (0.006)	Loss 2.1028 (1.9373)	Acc@1 69.141 (73.540)	Acc@5 92.969 (95.031)
Epoch: [88][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9370 (1.9415)	Acc@1 74.219 (73.493)	Acc@5 94.141 (95.004)
Epoch: [88][80/196]	Time 0.012 (0.016)	Data 0.018 (0.006)	Loss 2.0005 (1.9465)	Acc@1 70.312 (73.278)	Acc@5 94.531 (94.951)
Epoch: [88][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1247 (1.9526)	Acc@1 69.922 (73.060)	Acc@5 91.406 (94.840)
Epoch: [88][100/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 2.1171 (1.9620)	Acc@1 66.406 (72.768)	Acc@5 92.578 (94.690)
Epoch: [88][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1958 (1.9671)	Acc@1 64.844 (72.684)	Acc@5 92.969 (94.552)
Epoch: [88][120/196]	Time 0.013 (0.015)	Data 0.008 (0.005)	Loss 2.0280 (1.9727)	Acc@1 73.438 (72.505)	Acc@5 93.750 (94.509)
Epoch: [88][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0560 (1.9801)	Acc@1 68.359 (72.310)	Acc@5 94.531 (94.451)
Epoch: [88][140/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 2.0706 (1.9885)	Acc@1 69.141 (72.058)	Acc@5 92.188 (94.398)
Epoch: [88][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1162 (1.9932)	Acc@1 66.797 (71.896)	Acc@5 92.188 (94.348)
Epoch: [88][160/196]	Time 0.012 (0.015)	Data 0.012 (0.004)	Loss 1.9976 (1.9971)	Acc@1 72.656 (71.793)	Acc@5 93.750 (94.296)
Epoch: [88][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0191 (1.9995)	Acc@1 74.609 (71.729)	Acc@5 91.406 (94.259)
Epoch: [88][180/196]	Time 0.011 (0.015)	Data 0.012 (0.004)	Loss 2.1522 (2.0030)	Acc@1 66.406 (71.607)	Acc@5 92.969 (94.223)
Epoch: [88][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.3118 (2.0112)	Acc@1 61.328 (71.378)	Acc@5 92.188 (94.137)
num momentum params: 26
[0.1, 2.013908267211914, 1.9630165469646454, 71.296, 51.85, tensor(0.5208, device='cuda:0', grad_fn=<DivBackward0>), 3.0289523601531982, 0.3865926265716553]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [89 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [89][0/196]	Time 0.055 (0.055)	Data 0.197 (0.197)	Loss 1.9878 (1.9878)	Acc@1 70.703 (70.703)	Acc@5 95.703 (95.703)
Epoch: [89][10/196]	Time 0.015 (0.019)	Data 0.003 (0.020)	Loss 2.0550 (1.9813)	Acc@1 71.094 (72.656)	Acc@5 93.750 (95.064)
Epoch: [89][20/196]	Time 0.014 (0.017)	Data 0.007 (0.012)	Loss 2.0581 (1.9774)	Acc@1 72.266 (73.177)	Acc@5 93.359 (94.959)
Epoch: [89][30/196]	Time 0.020 (0.017)	Data 0.001 (0.009)	Loss 1.8591 (1.9715)	Acc@1 71.094 (73.034)	Acc@5 96.875 (94.821)
Epoch: [89][40/196]	Time 0.012 (0.017)	Data 0.009 (0.007)	Loss 2.0394 (1.9673)	Acc@1 69.531 (73.047)	Acc@5 94.141 (94.684)
Epoch: [89][50/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8638 (1.9748)	Acc@1 76.562 (72.963)	Acc@5 95.312 (94.455)
Epoch: [89][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 2.0149 (1.9784)	Acc@1 69.531 (72.656)	Acc@5 95.703 (94.627)
Epoch: [89][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9490 (1.9790)	Acc@1 72.266 (72.469)	Acc@5 95.703 (94.669)
Epoch: [89][80/196]	Time 0.012 (0.016)	Data 0.013 (0.005)	Loss 1.9712 (1.9732)	Acc@1 72.656 (72.671)	Acc@5 93.359 (94.695)
Epoch: [89][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0272 (1.9739)	Acc@1 70.312 (72.600)	Acc@5 94.922 (94.673)
Epoch: [89][100/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.0372 (1.9774)	Acc@1 71.484 (72.552)	Acc@5 94.141 (94.605)
Epoch: [89][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.2364 (1.9808)	Acc@1 65.625 (72.473)	Acc@5 93.359 (94.514)
Epoch: [89][120/196]	Time 0.012 (0.015)	Data 0.016 (0.005)	Loss 2.0334 (1.9850)	Acc@1 73.047 (72.382)	Acc@5 92.969 (94.502)
Epoch: [89][130/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 1.9841 (1.9862)	Acc@1 74.609 (72.424)	Acc@5 94.141 (94.463)
Epoch: [89][140/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.0901 (1.9945)	Acc@1 71.484 (72.185)	Acc@5 92.578 (94.359)
Epoch: [89][150/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0347 (1.9981)	Acc@1 71.875 (72.085)	Acc@5 96.094 (94.324)
Epoch: [89][160/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.1876 (2.0044)	Acc@1 67.188 (71.924)	Acc@5 93.359 (94.252)
Epoch: [89][170/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.0576 (2.0125)	Acc@1 70.312 (71.731)	Acc@5 94.922 (94.163)
Epoch: [89][180/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.2295 (2.0218)	Acc@1 67.188 (71.398)	Acc@5 89.844 (94.082)
Epoch: [89][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9555 (2.0247)	Acc@1 67.188 (71.272)	Acc@5 96.484 (94.071)
num momentum params: 26
[0.1, 2.02744388961792, 1.994809284210205, 71.212, 51.37, tensor(0.5185, device='cuda:0', grad_fn=<DivBackward0>), 2.8644282817840576, 0.39114904403686523]
Non Pruning Epoch - module.conv1.weight: [48, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [48]
Non Pruning Epoch - module.bn1.bias: [48]
Non Pruning Epoch - module.conv2.weight: [128, 48, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [490, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [490]
Non Pruning Epoch - module.bn7.bias: [490]
Non Pruning Epoch - module.conv8.weight: [373, 490, 3, 3]
Non Pruning Epoch - module.bn8.weight: [373]
Non Pruning Epoch - module.bn8.bias: [373]
Non Pruning Epoch - module.fc.weight: [100, 373]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [90 | 180] LR: 0.100000
module.conv1.weight [48, 3, 3, 3]
module.conv2.weight [128, 48, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [490, 509, 3, 3]
module.conv8.weight [373, 490, 3, 3]
Epoch: [90][0/196]	Time 0.055 (0.055)	Data 0.199 (0.199)	Loss 1.9265 (1.9265)	Acc@1 74.219 (74.219)	Acc@5 95.703 (95.703)
Epoch: [90][10/196]	Time 0.014 (0.019)	Data 0.003 (0.021)	Loss 1.9001 (1.9793)	Acc@1 71.875 (72.408)	Acc@5 96.875 (95.099)
Epoch: [90][20/196]	Time 0.015 (0.017)	Data 0.003 (0.012)	Loss 1.9544 (1.9742)	Acc@1 72.656 (72.545)	Acc@5 94.141 (94.996)
Epoch: [90][30/196]	Time 0.017 (0.016)	Data 0.001 (0.009)	Loss 1.9631 (1.9610)	Acc@1 70.703 (73.085)	Acc@5 95.703 (95.136)
Epoch: [90][40/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9409 (1.9537)	Acc@1 74.219 (73.304)	Acc@5 94.922 (95.112)
Epoch: [90][50/196]	Time 0.016 (0.015)	Data 0.000 (0.007)	Loss 1.9392 (1.9592)	Acc@1 71.484 (73.024)	Acc@5 96.484 (94.998)
Epoch: [90][60/196]	Time 0.012 (0.015)	Data 0.010 (0.006)	Loss 1.9066 (1.9584)	Acc@1 73.438 (73.072)	Acc@5 95.703 (94.999)
Epoch: [90][70/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 2.1124 (1.9603)	Acc@1 64.062 (72.876)	Acc@5 92.188 (94.911)
Epoch: [90][80/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.0455 (1.9629)	Acc@1 67.969 (72.738)	Acc@5 94.141 (94.777)
Epoch: [90][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.8443 (1.9628)	Acc@1 76.172 (72.708)	Acc@5 96.484 (94.737)
Epoch: [90][100/196]	Time 0.012 (0.015)	Data 0.013 (0.005)	Loss 1.9803 (1.9708)	Acc@1 71.484 (72.459)	Acc@5 94.922 (94.620)
Epoch: [90][110/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 1.8870 (1.9738)	Acc@1 75.000 (72.420)	Acc@5 96.094 (94.623)
Epoch: [90][120/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 2.2135 (1.9786)	Acc@1 66.406 (72.321)	Acc@5 93.750 (94.576)
Epoch: [90][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0326 (1.9823)	Acc@1 70.312 (72.269)	Acc@5 92.969 (94.507)
Epoch: [90][140/196]	Time 0.012 (0.015)	Data 0.018 (0.005)	Loss 1.8215 (1.9872)	Acc@1 76.953 (72.146)	Acc@5 96.875 (94.473)
Epoch: [90][150/196]	Time 0.019 (0.015)	Data 0.000 (0.005)	Loss 2.2229 (1.9943)	Acc@1 62.500 (71.880)	Acc@5 91.406 (94.415)
Epoch: [90][160/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.1565 (1.9988)	Acc@1 68.750 (71.715)	Acc@5 91.797 (94.374)
Epoch: [90][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1949 (2.0018)	Acc@1 66.797 (71.599)	Acc@5 91.016 (94.346)
Epoch: [90][180/196]	Time 0.012 (0.015)	Data 0.014 (0.005)	Loss 2.1810 (2.0063)	Acc@1 65.625 (71.519)	Acc@5 91.016 (94.281)
Epoch: [90][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.2039 (2.0110)	Acc@1 66.406 (71.405)	Acc@5 92.969 (94.235)
num momentum params: 26
[0.1, 2.012689648361206, 1.9812205600738526, 71.348, 52.01, tensor(0.5222, device='cuda:0', grad_fn=<DivBackward0>), 2.9538207054138184, 0.39611649513244634]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [48, 3, 3, 3]
Before - module.bn1.weight: [48]
Before - module.bn1.bias: [48]
Before - module.conv2.weight: [128, 48, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [253, 128, 3, 3]
Before - module.bn3.weight: [253]
Before - module.bn3.bias: [253]
Before - module.conv4.weight: [256, 253, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [509, 510, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [490, 509, 3, 3]
Before - module.bn7.weight: [490]
Before - module.bn7.bias: [490]
Before - module.conv8.weight: [373, 490, 3, 3]
Before - module.bn8.weight: [373]
Before - module.bn8.bias: [373]
Before - module.fc.weight: [100, 373]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [48, 3, 3, 3] >> [45, 3, 3, 3]
[module.bn1.weight]: 48 >> 45
running_mean [45]
running_var [45]
num_batches_tracked []
[module.conv2.weight]: [128, 48, 3, 3] >> [128, 45, 3, 3]
[module.conv7.weight]: [490, 509, 3, 3] >> [487, 509, 3, 3]
[module.bn7.weight]: 490 >> 487
running_mean [487]
running_var [487]
num_batches_tracked []
[module.conv8.weight]: [373, 490, 3, 3] >> [358, 487, 3, 3]
[module.bn8.weight]: 373 >> 358
running_mean [358]
running_var [358]
num_batches_tracked []
[module.fc.weight]: [100, 373] >> [100, 358]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [45, 3, 3, 3]
After - module.bn1.weight: [45]
After - module.bn1.bias: [45]
After - module.conv2.weight: [128, 45, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [253, 128, 3, 3]
After - module.bn3.weight: [253]
After - module.bn3.bias: [253]
After - module.conv4.weight: [256, 253, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [509, 510, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [487, 509, 3, 3]
After - module.bn7.weight: [487]
After - module.bn7.bias: [487]
After - module.conv8.weight: [358, 487, 3, 3]
After - module.bn8.weight: [358]
After - module.bn8.bias: [358]
After - module.fc.weight: [100, 358]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [45, 3, 3, 3]
conv2 --> [128, 45, 3, 3]
conv3 --> [253, 128, 3, 3]
conv4 --> [256, 253, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [509, 510, 3, 3]
conv7 --> [487, 509, 3, 3]
conv8 --> [358, 487, 3, 3]
fc --> [358, 100]
1, 498286080, 1244160, 45
2, 5547294720, 13271040, 128
3, 8505851904, 18653184, 253
4, 17011703808, 37306368, 256
5, 10227548160, 18800640, 510
6, 20335242240, 37380960, 509
7, 6853469184, 8923788, 487
8, 4820318208, 6276456, 358
fc, 13747200, 35800, 0
===================
FLOP REPORT: 28833383400000.0 52443200000.0 141892396 131108 2546 15.788150787353516
[INFO] Storing checkpoint...

Epoch: [91 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [91][0/196]	Time 0.371 (0.371)	Data 0.214 (0.214)	Loss 1.8870 (1.8870)	Acc@1 74.609 (74.609)	Acc@5 96.875 (96.875)
Epoch: [91][10/196]	Time 0.016 (0.048)	Data 0.002 (0.021)	Loss 1.9265 (1.9568)	Acc@1 72.266 (73.082)	Acc@5 98.047 (95.490)
Epoch: [91][20/196]	Time 0.015 (0.033)	Data 0.002 (0.012)	Loss 1.8978 (1.9424)	Acc@1 75.000 (73.605)	Acc@5 94.531 (95.145)
Epoch: [91][30/196]	Time 0.017 (0.027)	Data 0.001 (0.009)	Loss 2.0140 (1.9281)	Acc@1 72.656 (73.929)	Acc@5 92.578 (95.262)
Epoch: [91][40/196]	Time 0.015 (0.024)	Data 0.002 (0.007)	Loss 1.7911 (1.9109)	Acc@1 75.781 (74.247)	Acc@5 95.312 (95.351)
Epoch: [91][50/196]	Time 0.019 (0.023)	Data 0.001 (0.006)	Loss 1.8292 (1.9116)	Acc@1 74.609 (74.150)	Acc@5 96.484 (95.328)
Epoch: [91][60/196]	Time 0.016 (0.021)	Data 0.003 (0.006)	Loss 1.9921 (1.9272)	Acc@1 70.703 (73.617)	Acc@5 92.578 (95.101)
Epoch: [91][70/196]	Time 0.016 (0.020)	Data 0.002 (0.005)	Loss 2.0045 (1.9389)	Acc@1 72.656 (73.300)	Acc@5 94.141 (94.927)
Epoch: [91][80/196]	Time 0.017 (0.020)	Data 0.000 (0.005)	Loss 2.0934 (1.9467)	Acc@1 70.312 (73.066)	Acc@5 92.188 (94.763)
Epoch: [91][90/196]	Time 0.015 (0.019)	Data 0.003 (0.005)	Loss 1.9468 (1.9581)	Acc@1 71.094 (72.927)	Acc@5 94.531 (94.720)
Epoch: [91][100/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 1.9510 (1.9651)	Acc@1 71.094 (72.726)	Acc@5 96.484 (94.678)
Epoch: [91][110/196]	Time 0.017 (0.019)	Data 0.002 (0.004)	Loss 1.8405 (1.9640)	Acc@1 75.781 (72.786)	Acc@5 96.094 (94.658)
Epoch: [91][120/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 2.0690 (1.9710)	Acc@1 69.922 (72.637)	Acc@5 91.797 (94.551)
Epoch: [91][130/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 2.0503 (1.9759)	Acc@1 69.922 (72.456)	Acc@5 93.359 (94.513)
Epoch: [91][140/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 2.0410 (1.9830)	Acc@1 71.094 (72.241)	Acc@5 93.359 (94.465)
Epoch: [91][150/196]	Time 0.016 (0.018)	Data 0.002 (0.004)	Loss 2.0402 (1.9891)	Acc@1 71.484 (72.069)	Acc@5 91.016 (94.368)
Epoch: [91][160/196]	Time 0.012 (0.018)	Data 0.006 (0.004)	Loss 2.1663 (1.9938)	Acc@1 66.406 (71.928)	Acc@5 91.016 (94.289)
Epoch: [91][170/196]	Time 0.018 (0.017)	Data 0.001 (0.004)	Loss 2.0917 (1.9983)	Acc@1 69.531 (71.797)	Acc@5 94.141 (94.209)
Epoch: [91][180/196]	Time 0.012 (0.017)	Data 0.007 (0.004)	Loss 1.9661 (2.0025)	Acc@1 71.094 (71.692)	Acc@5 95.312 (94.158)
Epoch: [91][190/196]	Time 0.016 (0.017)	Data 0.000 (0.004)	Loss 2.0993 (2.0045)	Acc@1 66.797 (71.603)	Acc@5 94.531 (94.145)
num momentum params: 26
[0.1, 2.007792981262207, 1.8864545392990113, 71.498, 53.11, tensor(0.5235, device='cuda:0', grad_fn=<DivBackward0>), 3.4964945316314697, 0.4348833560943604]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [92 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [92][0/196]	Time 0.057 (0.057)	Data 0.202 (0.202)	Loss 1.7405 (1.7405)	Acc@1 76.562 (76.562)	Acc@5 96.094 (96.094)
Epoch: [92][10/196]	Time 0.017 (0.020)	Data 0.002 (0.021)	Loss 2.0121 (1.9130)	Acc@1 69.531 (74.325)	Acc@5 95.703 (95.668)
Epoch: [92][20/196]	Time 0.013 (0.018)	Data 0.003 (0.012)	Loss 1.8813 (1.9171)	Acc@1 75.391 (74.033)	Acc@5 94.922 (95.406)
Epoch: [92][30/196]	Time 0.017 (0.017)	Data 0.002 (0.009)	Loss 1.9472 (1.9057)	Acc@1 75.391 (74.395)	Acc@5 94.531 (95.338)
Epoch: [92][40/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 1.8485 (1.9167)	Acc@1 74.219 (74.019)	Acc@5 96.484 (95.274)
Epoch: [92][50/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8406 (1.9124)	Acc@1 74.219 (74.058)	Acc@5 96.094 (95.305)
Epoch: [92][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 1.9777 (1.9173)	Acc@1 74.609 (74.033)	Acc@5 94.141 (95.280)
Epoch: [92][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9447 (1.9183)	Acc@1 73.438 (74.021)	Acc@5 94.141 (95.169)
Epoch: [92][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9393 (1.9207)	Acc@1 73.438 (73.987)	Acc@5 92.578 (95.129)
Epoch: [92][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0015 (1.9263)	Acc@1 73.047 (73.781)	Acc@5 93.359 (95.042)
Epoch: [92][100/196]	Time 0.013 (0.016)	Data 0.009 (0.005)	Loss 2.0802 (1.9399)	Acc@1 68.359 (73.376)	Acc@5 92.578 (94.868)
Epoch: [92][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0755 (1.9526)	Acc@1 70.312 (73.001)	Acc@5 93.359 (94.704)
Epoch: [92][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0030 (1.9631)	Acc@1 73.438 (72.708)	Acc@5 92.188 (94.583)
Epoch: [92][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0930 (1.9699)	Acc@1 66.797 (72.501)	Acc@5 92.969 (94.513)
Epoch: [92][140/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.2233 (1.9766)	Acc@1 65.625 (72.338)	Acc@5 91.406 (94.443)
Epoch: [92][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1988 (1.9822)	Acc@1 68.750 (72.162)	Acc@5 92.969 (94.397)
Epoch: [92][160/196]	Time 0.011 (0.015)	Data 0.008 (0.004)	Loss 2.1297 (1.9902)	Acc@1 65.234 (71.909)	Acc@5 92.969 (94.310)
Epoch: [92][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.1161 (1.9979)	Acc@1 68.750 (71.672)	Acc@5 91.406 (94.264)
Epoch: [92][180/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 1.9329 (2.0008)	Acc@1 75.781 (71.612)	Acc@5 94.141 (94.236)
Epoch: [92][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0467 (2.0061)	Acc@1 66.406 (71.462)	Acc@5 96.875 (94.224)
num momentum params: 26
[0.1, 2.0079477395629883, 1.811375857591629, 71.422, 54.16, tensor(0.5244, device='cuda:0', grad_fn=<DivBackward0>), 3.0102832317352295, 0.3853888511657715]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [93 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [93][0/196]	Time 0.056 (0.056)	Data 0.208 (0.208)	Loss 1.9584 (1.9584)	Acc@1 74.219 (74.219)	Acc@5 94.531 (94.531)
Epoch: [93][10/196]	Time 0.016 (0.020)	Data 0.002 (0.021)	Loss 1.9421 (1.9593)	Acc@1 72.656 (72.301)	Acc@5 95.703 (94.815)
Epoch: [93][20/196]	Time 0.016 (0.018)	Data 0.002 (0.012)	Loss 1.9126 (1.9357)	Acc@1 73.438 (73.214)	Acc@5 94.531 (95.071)
Epoch: [93][30/196]	Time 0.014 (0.017)	Data 0.003 (0.009)	Loss 1.9320 (1.9360)	Acc@1 75.391 (73.387)	Acc@5 95.312 (95.149)
Epoch: [93][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 2.0601 (1.9450)	Acc@1 67.969 (73.295)	Acc@5 96.484 (94.989)
Epoch: [93][50/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.8876 (1.9386)	Acc@1 76.172 (73.606)	Acc@5 94.141 (95.044)
Epoch: [93][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.8618 (1.9415)	Acc@1 74.609 (73.341)	Acc@5 95.312 (95.018)
Epoch: [93][70/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0129 (1.9483)	Acc@1 72.266 (73.146)	Acc@5 94.922 (95.010)
Epoch: [93][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1230 (1.9511)	Acc@1 66.016 (72.999)	Acc@5 96.484 (95.076)
Epoch: [93][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1542 (1.9571)	Acc@1 70.312 (72.849)	Acc@5 92.188 (95.029)
Epoch: [93][100/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.8966 (1.9632)	Acc@1 75.391 (72.707)	Acc@5 93.359 (94.906)
Epoch: [93][110/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1096 (1.9731)	Acc@1 67.969 (72.480)	Acc@5 94.922 (94.788)
Epoch: [93][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1417 (1.9803)	Acc@1 69.141 (72.372)	Acc@5 92.578 (94.706)
Epoch: [93][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9188 (1.9808)	Acc@1 73.047 (72.298)	Acc@5 96.875 (94.704)
Epoch: [93][140/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0757 (1.9860)	Acc@1 69.141 (72.160)	Acc@5 92.188 (94.639)
Epoch: [93][150/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9942 (1.9919)	Acc@1 71.094 (72.030)	Acc@5 96.094 (94.578)
Epoch: [93][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9778 (1.9979)	Acc@1 74.219 (71.858)	Acc@5 93.750 (94.502)
Epoch: [93][170/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 2.0725 (2.0037)	Acc@1 70.312 (71.665)	Acc@5 94.922 (94.435)
Epoch: [93][180/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.3578 (2.0110)	Acc@1 64.844 (71.443)	Acc@5 88.281 (94.361)
Epoch: [93][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1461 (2.0157)	Acc@1 69.141 (71.319)	Acc@5 92.188 (94.327)
num momentum params: 26
[0.1, 2.0182388132476805, 1.8847157490253448, 71.244, 53.49, tensor(0.5219, device='cuda:0', grad_fn=<DivBackward0>), 3.0113346576690674, 0.38043975830078125]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [94 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [94][0/196]	Time 0.058 (0.058)	Data 0.204 (0.204)	Loss 1.8147 (1.8147)	Acc@1 80.469 (80.469)	Acc@5 96.484 (96.484)
Epoch: [94][10/196]	Time 0.016 (0.020)	Data 0.003 (0.021)	Loss 1.9819 (1.9511)	Acc@1 73.047 (72.940)	Acc@5 93.359 (94.496)
Epoch: [94][20/196]	Time 0.016 (0.018)	Data 0.002 (0.012)	Loss 1.9014 (1.9507)	Acc@1 76.562 (73.084)	Acc@5 95.703 (94.754)
Epoch: [94][30/196]	Time 0.016 (0.017)	Data 0.003 (0.009)	Loss 1.9427 (1.9483)	Acc@1 72.656 (72.921)	Acc@5 96.094 (94.645)
Epoch: [94][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 2.0644 (1.9410)	Acc@1 70.312 (73.333)	Acc@5 94.531 (94.769)
Epoch: [94][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.8268 (1.9332)	Acc@1 77.734 (73.675)	Acc@5 97.656 (94.937)
Epoch: [94][60/196]	Time 0.015 (0.016)	Data 0.004 (0.006)	Loss 1.9687 (1.9345)	Acc@1 73.438 (73.578)	Acc@5 93.750 (95.024)
Epoch: [94][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 1.9021 (1.9392)	Acc@1 74.609 (73.421)	Acc@5 97.266 (95.026)
Epoch: [94][80/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.0586 (1.9490)	Acc@1 67.578 (73.240)	Acc@5 91.797 (94.893)
Epoch: [94][90/196]	Time 0.020 (0.016)	Data 0.001 (0.005)	Loss 2.1392 (1.9560)	Acc@1 66.797 (73.043)	Acc@5 92.188 (94.862)
Epoch: [94][100/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.1085 (1.9656)	Acc@1 69.922 (72.737)	Acc@5 92.578 (94.790)
Epoch: [94][110/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1152 (1.9717)	Acc@1 68.750 (72.593)	Acc@5 91.797 (94.704)
Epoch: [94][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1877 (1.9750)	Acc@1 67.188 (72.563)	Acc@5 91.016 (94.606)
Epoch: [94][130/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.0893 (1.9777)	Acc@1 71.484 (72.519)	Acc@5 90.625 (94.567)
Epoch: [94][140/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0230 (1.9796)	Acc@1 70.703 (72.451)	Acc@5 95.312 (94.551)
Epoch: [94][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0190 (1.9834)	Acc@1 73.047 (72.307)	Acc@5 94.141 (94.536)
Epoch: [94][160/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.1545 (1.9885)	Acc@1 66.016 (72.195)	Acc@5 91.797 (94.463)
Epoch: [94][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0781 (1.9936)	Acc@1 69.141 (72.042)	Acc@5 93.750 (94.406)
Epoch: [94][180/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.1314 (2.0048)	Acc@1 69.922 (71.793)	Acc@5 91.797 (94.266)
Epoch: [94][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.9536 (2.0093)	Acc@1 72.656 (71.650)	Acc@5 93.750 (94.224)
num momentum params: 26
[0.1, 2.0129640199279786, 1.8262047314643859, 71.536, 54.58, tensor(0.5236, device='cuda:0', grad_fn=<DivBackward0>), 3.133826732635498, 0.39469432830810547]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [95 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [95][0/196]	Time 0.059 (0.059)	Data 0.200 (0.200)	Loss 1.8998 (1.8998)	Acc@1 74.219 (74.219)	Acc@5 94.531 (94.531)
Epoch: [95][10/196]	Time 0.018 (0.021)	Data 0.002 (0.020)	Loss 1.9540 (1.9728)	Acc@1 72.656 (72.372)	Acc@5 96.094 (94.815)
Epoch: [95][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 1.8188 (1.9596)	Acc@1 76.953 (72.600)	Acc@5 96.484 (94.810)
Epoch: [95][30/196]	Time 0.016 (0.017)	Data 0.003 (0.009)	Loss 1.8387 (1.9380)	Acc@1 75.000 (73.664)	Acc@5 94.922 (94.871)
Epoch: [95][40/196]	Time 0.013 (0.017)	Data 0.005 (0.007)	Loss 1.8679 (1.9335)	Acc@1 76.953 (73.790)	Acc@5 96.484 (94.970)
Epoch: [95][50/196]	Time 0.017 (0.017)	Data 0.006 (0.006)	Loss 1.9456 (1.9272)	Acc@1 72.266 (73.889)	Acc@5 95.703 (95.052)
Epoch: [95][60/196]	Time 0.014 (0.017)	Data 0.002 (0.006)	Loss 1.8883 (1.9235)	Acc@1 75.781 (73.924)	Acc@5 97.266 (95.159)
Epoch: [95][70/196]	Time 0.017 (0.017)	Data 0.000 (0.005)	Loss 1.9157 (1.9334)	Acc@1 76.172 (73.669)	Acc@5 93.359 (95.153)
Epoch: [95][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9944 (1.9333)	Acc@1 73.828 (73.683)	Acc@5 94.141 (95.129)
Epoch: [95][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9674 (1.9382)	Acc@1 74.609 (73.528)	Acc@5 96.094 (95.106)
Epoch: [95][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9313 (1.9470)	Acc@1 73.047 (73.298)	Acc@5 94.531 (94.933)
Epoch: [95][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2241 (1.9585)	Acc@1 66.797 (72.976)	Acc@5 92.188 (94.813)
Epoch: [95][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1600 (1.9692)	Acc@1 66.797 (72.663)	Acc@5 90.625 (94.644)
Epoch: [95][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9738 (1.9760)	Acc@1 72.266 (72.507)	Acc@5 92.578 (94.570)
Epoch: [95][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0858 (1.9856)	Acc@1 67.188 (72.227)	Acc@5 93.750 (94.434)
Epoch: [95][150/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.2441 (1.9930)	Acc@1 64.453 (72.007)	Acc@5 91.406 (94.392)
Epoch: [95][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1209 (1.9967)	Acc@1 68.750 (71.882)	Acc@5 91.797 (94.344)
Epoch: [95][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1784 (2.0008)	Acc@1 65.625 (71.790)	Acc@5 92.969 (94.280)
Epoch: [95][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0181 (2.0020)	Acc@1 72.266 (71.761)	Acc@5 96.094 (94.257)
Epoch: [95][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1054 (2.0066)	Acc@1 66.797 (71.644)	Acc@5 92.578 (94.202)
num momentum params: 26
[0.1, 2.0108556929016115, 1.9171215331554412, 71.528, 53.16, tensor(0.5243, device='cuda:0', grad_fn=<DivBackward0>), 3.119307041168213, 0.3862631320953369]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [96 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [96][0/196]	Time 0.057 (0.057)	Data 0.211 (0.211)	Loss 2.0812 (2.0812)	Acc@1 68.750 (68.750)	Acc@5 90.625 (90.625)
Epoch: [96][10/196]	Time 0.015 (0.020)	Data 0.002 (0.021)	Loss 1.9611 (1.9590)	Acc@1 72.656 (72.798)	Acc@5 93.750 (94.425)
Epoch: [96][20/196]	Time 0.015 (0.018)	Data 0.003 (0.012)	Loss 1.9165 (1.9400)	Acc@1 75.781 (74.014)	Acc@5 96.875 (94.717)
Epoch: [96][30/196]	Time 0.017 (0.017)	Data 0.002 (0.009)	Loss 1.8677 (1.9322)	Acc@1 77.734 (74.320)	Acc@5 95.312 (94.922)
Epoch: [96][40/196]	Time 0.016 (0.017)	Data 0.003 (0.007)	Loss 1.9452 (1.9229)	Acc@1 73.438 (74.419)	Acc@5 97.266 (95.265)
Epoch: [96][50/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.9819 (1.9222)	Acc@1 74.609 (74.372)	Acc@5 95.703 (95.221)
Epoch: [96][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.8475 (1.9209)	Acc@1 75.781 (74.334)	Acc@5 96.094 (95.274)
Epoch: [96][70/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 2.0860 (1.9203)	Acc@1 67.969 (74.290)	Acc@5 94.922 (95.279)
Epoch: [96][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9055 (1.9251)	Acc@1 74.609 (74.055)	Acc@5 95.703 (95.264)
Epoch: [96][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1605 (1.9271)	Acc@1 66.406 (73.935)	Acc@5 91.797 (95.244)
Epoch: [96][100/196]	Time 0.017 (0.016)	Data 0.003 (0.005)	Loss 2.0899 (1.9363)	Acc@1 68.750 (73.766)	Acc@5 91.797 (95.150)
Epoch: [96][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0163 (1.9493)	Acc@1 70.312 (73.335)	Acc@5 94.531 (95.034)
Epoch: [96][120/196]	Time 0.012 (0.016)	Data 0.013 (0.004)	Loss 1.9691 (1.9590)	Acc@1 74.609 (73.092)	Acc@5 93.359 (94.848)
Epoch: [96][130/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 2.0401 (1.9656)	Acc@1 71.875 (72.925)	Acc@5 92.188 (94.755)
Epoch: [96][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0280 (1.9744)	Acc@1 68.359 (72.651)	Acc@5 96.094 (94.656)
Epoch: [96][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0749 (1.9832)	Acc@1 73.047 (72.475)	Acc@5 92.578 (94.516)
Epoch: [96][160/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.2412 (1.9895)	Acc@1 67.578 (72.234)	Acc@5 90.625 (94.439)
Epoch: [96][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1406 (1.9944)	Acc@1 70.703 (72.138)	Acc@5 92.578 (94.367)
Epoch: [96][180/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1455 (2.0032)	Acc@1 67.969 (71.946)	Acc@5 92.969 (94.274)
Epoch: [96][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1213 (2.0047)	Acc@1 71.094 (71.918)	Acc@5 93.359 (94.263)
num momentum params: 26
[0.1, 2.005324447631836, 1.7867389011383057, 71.896, 53.64, tensor(0.5251, device='cuda:0', grad_fn=<DivBackward0>), 3.039083242416382, 0.3787977695465088]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [97 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [97][0/196]	Time 0.057 (0.057)	Data 0.195 (0.195)	Loss 1.8615 (1.8615)	Acc@1 76.172 (76.172)	Acc@5 96.094 (96.094)
Epoch: [97][10/196]	Time 0.015 (0.020)	Data 0.002 (0.020)	Loss 1.9063 (1.8859)	Acc@1 76.172 (74.929)	Acc@5 94.922 (95.810)
Epoch: [97][20/196]	Time 0.020 (0.018)	Data 0.002 (0.011)	Loss 2.0444 (1.9007)	Acc@1 71.094 (74.609)	Acc@5 94.922 (95.406)
Epoch: [97][30/196]	Time 0.015 (0.017)	Data 0.003 (0.009)	Loss 2.0606 (1.9169)	Acc@1 69.531 (73.803)	Acc@5 93.359 (95.275)
Epoch: [97][40/196]	Time 0.014 (0.017)	Data 0.005 (0.007)	Loss 1.9401 (1.9158)	Acc@1 70.703 (73.780)	Acc@5 96.484 (95.379)
Epoch: [97][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0532 (1.9282)	Acc@1 70.703 (73.476)	Acc@5 94.531 (95.343)
Epoch: [97][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.7854 (1.9303)	Acc@1 79.297 (73.418)	Acc@5 97.266 (95.383)
Epoch: [97][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0463 (1.9398)	Acc@1 69.141 (73.140)	Acc@5 94.141 (95.230)
Epoch: [97][80/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0158 (1.9465)	Acc@1 70.312 (73.018)	Acc@5 93.750 (95.076)
Epoch: [97][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0386 (1.9560)	Acc@1 70.703 (72.686)	Acc@5 93.750 (94.952)
Epoch: [97][100/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 2.0493 (1.9701)	Acc@1 69.922 (72.393)	Acc@5 96.875 (94.837)
Epoch: [97][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0191 (1.9750)	Acc@1 71.484 (72.195)	Acc@5 95.312 (94.830)
Epoch: [97][120/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.1704 (1.9823)	Acc@1 67.969 (71.994)	Acc@5 92.969 (94.770)
Epoch: [97][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0791 (1.9930)	Acc@1 67.578 (71.806)	Acc@5 94.141 (94.588)
Epoch: [97][140/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0260 (1.9996)	Acc@1 71.875 (71.720)	Acc@5 94.531 (94.531)
Epoch: [97][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0415 (2.0076)	Acc@1 71.484 (71.575)	Acc@5 92.969 (94.415)
Epoch: [97][160/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0933 (2.0115)	Acc@1 69.141 (71.499)	Acc@5 93.750 (94.366)
Epoch: [97][170/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8828 (2.0154)	Acc@1 73.828 (71.384)	Acc@5 96.094 (94.326)
Epoch: [97][180/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 2.1982 (2.0198)	Acc@1 64.453 (71.290)	Acc@5 92.578 (94.264)
Epoch: [97][190/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0818 (2.0201)	Acc@1 66.797 (71.296)	Acc@5 93.359 (94.218)
num momentum params: 26
[0.1, 2.0193957241821288, 1.8154351758956908, 71.28, 53.82, tensor(0.5221, device='cuda:0', grad_fn=<DivBackward0>), 3.0612757205963135, 0.38062620162963867]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [98 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [98][0/196]	Time 0.057 (0.057)	Data 0.194 (0.194)	Loss 1.9023 (1.9023)	Acc@1 75.000 (75.000)	Acc@5 95.312 (95.312)
Epoch: [98][10/196]	Time 0.016 (0.020)	Data 0.002 (0.020)	Loss 1.8179 (1.9394)	Acc@1 76.172 (73.509)	Acc@5 96.094 (95.170)
Epoch: [98][20/196]	Time 0.012 (0.018)	Data 0.005 (0.012)	Loss 1.9609 (1.9437)	Acc@1 69.531 (73.177)	Acc@5 96.875 (95.015)
Epoch: [98][30/196]	Time 0.016 (0.017)	Data 0.003 (0.009)	Loss 1.9693 (1.9358)	Acc@1 69.141 (73.160)	Acc@5 96.484 (95.224)
Epoch: [98][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 2.0462 (1.9314)	Acc@1 71.484 (73.342)	Acc@5 94.922 (95.217)
Epoch: [98][50/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0002 (1.9329)	Acc@1 72.656 (73.376)	Acc@5 94.922 (95.221)
Epoch: [98][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 2.0638 (1.9357)	Acc@1 70.703 (73.380)	Acc@5 94.922 (95.210)
Epoch: [98][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0395 (1.9409)	Acc@1 71.094 (73.201)	Acc@5 91.797 (95.037)
Epoch: [98][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8453 (1.9451)	Acc@1 78.906 (73.119)	Acc@5 96.094 (94.980)
Epoch: [98][90/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9802 (1.9557)	Acc@1 74.609 (72.914)	Acc@5 92.578 (94.845)
Epoch: [98][100/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.9963 (1.9635)	Acc@1 72.266 (72.730)	Acc@5 94.141 (94.748)
Epoch: [98][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9005 (1.9694)	Acc@1 75.781 (72.603)	Acc@5 92.969 (94.577)
Epoch: [98][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.8723 (1.9749)	Acc@1 74.609 (72.417)	Acc@5 96.484 (94.509)
Epoch: [98][130/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0666 (1.9810)	Acc@1 69.141 (72.263)	Acc@5 92.578 (94.433)
Epoch: [98][140/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.2048 (1.9815)	Acc@1 66.797 (72.263)	Acc@5 93.359 (94.454)
Epoch: [98][150/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0615 (1.9873)	Acc@1 70.703 (72.085)	Acc@5 95.312 (94.415)
Epoch: [98][160/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.1907 (1.9933)	Acc@1 65.625 (71.887)	Acc@5 92.188 (94.366)
Epoch: [98][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.1507 (1.9991)	Acc@1 70.312 (71.795)	Acc@5 93.359 (94.285)
Epoch: [98][180/196]	Time 0.014 (0.015)	Data 0.005 (0.004)	Loss 2.0303 (2.0036)	Acc@1 73.047 (71.700)	Acc@5 92.578 (94.199)
Epoch: [98][190/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.2581 (2.0102)	Acc@1 63.281 (71.558)	Acc@5 90.625 (94.116)
num momentum params: 26
[0.1, 2.009351445388794, 1.7602327156066895, 71.598, 55.06, tensor(0.5248, device='cuda:0', grad_fn=<DivBackward0>), 3.0157926082611084, 0.3844752311706543]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [99 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [99][0/196]	Time 0.063 (0.063)	Data 0.199 (0.199)	Loss 1.9457 (1.9457)	Acc@1 73.828 (73.828)	Acc@5 96.094 (96.094)
Epoch: [99][10/196]	Time 0.016 (0.020)	Data 0.003 (0.020)	Loss 1.9438 (1.9049)	Acc@1 72.656 (73.686)	Acc@5 96.484 (95.881)
Epoch: [99][20/196]	Time 0.016 (0.018)	Data 0.003 (0.012)	Loss 2.1649 (1.9140)	Acc@1 66.797 (73.735)	Acc@5 91.406 (95.629)
Epoch: [99][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.7638 (1.8975)	Acc@1 78.516 (74.685)	Acc@5 97.266 (95.552)
Epoch: [99][40/196]	Time 0.013 (0.017)	Data 0.005 (0.007)	Loss 1.8537 (1.8904)	Acc@1 73.438 (74.876)	Acc@5 96.094 (95.684)
Epoch: [99][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8476 (1.8877)	Acc@1 73.828 (74.916)	Acc@5 96.094 (95.695)
Epoch: [99][60/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.8824 (1.8907)	Acc@1 76.172 (74.898)	Acc@5 94.922 (95.633)
Epoch: [99][70/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0639 (1.8968)	Acc@1 69.531 (74.785)	Acc@5 92.969 (95.527)
Epoch: [99][80/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 2.0289 (1.9017)	Acc@1 73.828 (74.691)	Acc@5 93.359 (95.428)
Epoch: [99][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9948 (1.9086)	Acc@1 72.266 (74.421)	Acc@5 95.703 (95.330)
Epoch: [99][100/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9191 (1.9216)	Acc@1 73.828 (74.014)	Acc@5 94.141 (95.196)
Epoch: [99][110/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0177 (1.9340)	Acc@1 73.438 (73.712)	Acc@5 94.531 (95.077)
Epoch: [99][120/196]	Time 0.014 (0.016)	Data 0.004 (0.004)	Loss 2.0058 (1.9414)	Acc@1 71.094 (73.505)	Acc@5 92.578 (95.032)
Epoch: [99][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0888 (1.9506)	Acc@1 69.922 (73.276)	Acc@5 92.188 (94.907)
Epoch: [99][140/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1241 (1.9583)	Acc@1 72.656 (73.102)	Acc@5 91.406 (94.808)
Epoch: [99][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.1352 (1.9649)	Acc@1 63.281 (72.905)	Acc@5 92.969 (94.715)
Epoch: [99][160/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9677 (1.9737)	Acc@1 68.750 (72.637)	Acc@5 96.875 (94.616)
Epoch: [99][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0753 (1.9824)	Acc@1 71.094 (72.398)	Acc@5 93.359 (94.527)
Epoch: [99][180/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 2.0837 (1.9903)	Acc@1 69.531 (72.175)	Acc@5 92.578 (94.443)
Epoch: [99][190/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 2.2262 (1.9949)	Acc@1 68.750 (72.069)	Acc@5 91.016 (94.400)
num momentum params: 26
[0.1, 1.997358301010132, 1.718792450428009, 72.02, 56.0, tensor(0.5283, device='cuda:0', grad_fn=<DivBackward0>), 3.082303285598755, 0.396165132522583]
Non Pruning Epoch - module.conv1.weight: [45, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [45]
Non Pruning Epoch - module.bn1.bias: [45]
Non Pruning Epoch - module.conv2.weight: [128, 45, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [253, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [253]
Non Pruning Epoch - module.bn3.bias: [253]
Non Pruning Epoch - module.conv4.weight: [256, 253, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [487, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [487]
Non Pruning Epoch - module.bn7.bias: [487]
Non Pruning Epoch - module.conv8.weight: [358, 487, 3, 3]
Non Pruning Epoch - module.bn8.weight: [358]
Non Pruning Epoch - module.bn8.bias: [358]
Non Pruning Epoch - module.fc.weight: [100, 358]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [100 | 180] LR: 0.100000
module.conv1.weight [45, 3, 3, 3]
module.conv2.weight [128, 45, 3, 3]
module.conv3.weight [253, 128, 3, 3]
module.conv4.weight [256, 253, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [487, 509, 3, 3]
module.conv8.weight [358, 487, 3, 3]
Epoch: [100][0/196]	Time 0.061 (0.061)	Data 0.199 (0.199)	Loss 1.8355 (1.8355)	Acc@1 78.125 (78.125)	Acc@5 97.266 (97.266)
Epoch: [100][10/196]	Time 0.016 (0.020)	Data 0.002 (0.020)	Loss 1.7590 (1.8732)	Acc@1 76.953 (75.568)	Acc@5 98.438 (96.307)
Epoch: [100][20/196]	Time 0.017 (0.018)	Data 0.003 (0.012)	Loss 1.9286 (1.9050)	Acc@1 73.828 (74.721)	Acc@5 96.094 (95.778)
Epoch: [100][30/196]	Time 0.018 (0.018)	Data 0.002 (0.009)	Loss 1.8985 (1.8969)	Acc@1 71.484 (74.622)	Acc@5 96.875 (95.691)
Epoch: [100][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.8348 (1.9144)	Acc@1 76.172 (74.190)	Acc@5 95.703 (95.398)
Epoch: [100][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9125 (1.9302)	Acc@1 71.484 (73.537)	Acc@5 96.094 (95.358)
Epoch: [100][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 2.0212 (1.9339)	Acc@1 69.141 (73.380)	Acc@5 94.922 (95.325)
Epoch: [100][70/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9330 (1.9385)	Acc@1 73.047 (73.228)	Acc@5 94.531 (95.257)
Epoch: [100][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.0469 (1.9453)	Acc@1 71.094 (73.057)	Acc@5 94.531 (95.144)
Epoch: [100][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0959 (1.9480)	Acc@1 69.922 (72.970)	Acc@5 93.750 (95.119)
Epoch: [100][100/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9034 (1.9496)	Acc@1 73.047 (72.985)	Acc@5 94.922 (95.088)
Epoch: [100][110/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9684 (1.9547)	Acc@1 75.391 (72.927)	Acc@5 94.141 (94.996)
Epoch: [100][120/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1417 (1.9622)	Acc@1 67.578 (72.698)	Acc@5 92.578 (94.903)
Epoch: [100][130/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0797 (1.9721)	Acc@1 69.531 (72.418)	Acc@5 93.359 (94.752)
Epoch: [100][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 1.9919 (1.9745)	Acc@1 73.828 (72.379)	Acc@5 94.531 (94.709)
Epoch: [100][150/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0927 (1.9830)	Acc@1 66.406 (72.167)	Acc@5 91.797 (94.547)
Epoch: [100][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9905 (1.9870)	Acc@1 73.828 (72.067)	Acc@5 92.578 (94.490)
Epoch: [100][170/196]	Time 0.016 (0.016)	Data 0.004 (0.004)	Loss 2.1473 (1.9930)	Acc@1 67.969 (71.937)	Acc@5 94.141 (94.449)
Epoch: [100][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.1625 (1.9992)	Acc@1 66.797 (71.758)	Acc@5 92.188 (94.350)
Epoch: [100][190/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1594 (2.0020)	Acc@1 65.625 (71.726)	Acc@5 91.406 (94.314)
num momentum params: 26
[0.1, 2.004481979980469, 1.8066468465328216, 71.656, 54.41, tensor(0.5262, device='cuda:0', grad_fn=<DivBackward0>), 3.066988945007324, 0.3893439769744873]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [45, 3, 3, 3]
Before - module.bn1.weight: [45]
Before - module.bn1.bias: [45]
Before - module.conv2.weight: [128, 45, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [253, 128, 3, 3]
Before - module.bn3.weight: [253]
Before - module.bn3.bias: [253]
Before - module.conv4.weight: [256, 253, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [509, 510, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [487, 509, 3, 3]
Before - module.bn7.weight: [487]
Before - module.bn7.bias: [487]
Before - module.conv8.weight: [358, 487, 3, 3]
Before - module.bn8.weight: [358]
Before - module.bn8.bias: [358]
Before - module.fc.weight: [100, 358]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [45, 3, 3, 3] >> [44, 3, 3, 3]
[module.bn1.weight]: 45 >> 44
running_mean [44]
running_var [44]
num_batches_tracked []
[module.conv2.weight]: [128, 45, 3, 3] >> [128, 44, 3, 3]
[module.conv3.weight]: [253, 128, 3, 3] >> [250, 128, 3, 3]
[module.bn3.weight]: 253 >> 250
running_mean [250]
running_var [250]
num_batches_tracked []
[module.conv4.weight]: [256, 253, 3, 3] >> [256, 250, 3, 3]
[module.conv7.weight]: [487, 509, 3, 3] >> [482, 509, 3, 3]
[module.bn7.weight]: 487 >> 482
running_mean [482]
running_var [482]
num_batches_tracked []
[module.conv8.weight]: [358, 487, 3, 3] >> [344, 482, 3, 3]
[module.bn8.weight]: 358 >> 344
running_mean [344]
running_var [344]
num_batches_tracked []
[module.fc.weight]: [100, 358] >> [100, 344]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [44, 3, 3, 3]
After - module.bn1.weight: [44]
After - module.bn1.bias: [44]
After - module.conv2.weight: [128, 44, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [250, 128, 3, 3]
After - module.bn3.weight: [250]
After - module.bn3.bias: [250]
After - module.conv4.weight: [256, 250, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [509, 510, 3, 3]
After - module.bn6.weight: [509]
After - module.bn6.bias: [509]
After - module.conv7.weight: [482, 509, 3, 3]
After - module.bn7.weight: [482]
After - module.bn7.bias: [482]
After - module.conv8.weight: [344, 482, 3, 3]
After - module.bn8.weight: [344]
After - module.bn8.bias: [344]
After - module.fc.weight: [100, 344]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [44, 3, 3, 3]
conv2 --> [128, 44, 3, 3]
conv3 --> [250, 128, 3, 3]
conv4 --> [256, 250, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [509, 510, 3, 3]
conv7 --> [482, 509, 3, 3]
conv8 --> [344, 482, 3, 3]
fc --> [344, 100]
1, 487213056, 1216512, 44
2, 5424021504, 12976128, 128
3, 8404992000, 18432000, 250
4, 16809984000, 36864000, 256
5, 10227548160, 18800640, 510
6, 20335242240, 37380960, 509
7, 6783105024, 8832168, 482
8, 4584259584, 5969088, 344
fc, 13209600, 34400, 0
===================
FLOP REPORT: 28542802800000.0 51926400000.0 140505896 129816 2523 15.57313346862793
[INFO] Storing checkpoint...

Epoch: [101 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [101][0/196]	Time 0.503 (0.503)	Data 0.211 (0.211)	Loss 1.8858 (1.8858)	Acc@1 73.828 (73.828)	Acc@5 96.094 (96.094)
Epoch: [101][10/196]	Time 0.016 (0.060)	Data 0.002 (0.021)	Loss 1.9016 (1.9368)	Acc@1 76.172 (73.189)	Acc@5 94.141 (95.348)
Epoch: [101][20/196]	Time 0.015 (0.039)	Data 0.002 (0.012)	Loss 2.0115 (1.9377)	Acc@1 72.656 (73.977)	Acc@5 92.188 (95.071)
Epoch: [101][30/196]	Time 0.017 (0.031)	Data 0.001 (0.009)	Loss 1.9742 (1.9372)	Acc@1 72.266 (74.017)	Acc@5 94.531 (95.086)
Epoch: [101][40/196]	Time 0.012 (0.027)	Data 0.002 (0.007)	Loss 1.8295 (1.9236)	Acc@1 76.172 (74.371)	Acc@5 96.484 (95.284)
Epoch: [101][50/196]	Time 0.014 (0.025)	Data 0.003 (0.006)	Loss 1.9030 (1.9229)	Acc@1 74.219 (74.249)	Acc@5 96.875 (95.305)
Epoch: [101][60/196]	Time 0.014 (0.023)	Data 0.002 (0.006)	Loss 1.8410 (1.9277)	Acc@1 76.562 (74.007)	Acc@5 96.484 (95.319)
Epoch: [101][70/196]	Time 0.017 (0.022)	Data 0.001 (0.005)	Loss 1.9605 (1.9344)	Acc@1 74.219 (73.834)	Acc@5 94.141 (95.202)
Epoch: [101][80/196]	Time 0.014 (0.021)	Data 0.002 (0.005)	Loss 1.8760 (1.9375)	Acc@1 75.391 (73.698)	Acc@5 94.531 (95.206)
Epoch: [101][90/196]	Time 0.019 (0.020)	Data 0.000 (0.005)	Loss 2.0846 (1.9450)	Acc@1 67.969 (73.442)	Acc@5 93.750 (95.128)
Epoch: [101][100/196]	Time 0.015 (0.020)	Data 0.003 (0.005)	Loss 2.0229 (1.9570)	Acc@1 69.922 (73.144)	Acc@5 94.531 (94.984)
Epoch: [101][110/196]	Time 0.015 (0.019)	Data 0.003 (0.004)	Loss 2.1641 (1.9662)	Acc@1 66.406 (72.832)	Acc@5 93.359 (94.855)
Epoch: [101][120/196]	Time 0.014 (0.019)	Data 0.003 (0.004)	Loss 2.0692 (1.9743)	Acc@1 70.312 (72.669)	Acc@5 93.359 (94.696)
Epoch: [101][130/196]	Time 0.014 (0.019)	Data 0.003 (0.004)	Loss 2.0011 (1.9843)	Acc@1 70.312 (72.358)	Acc@5 94.922 (94.633)
Epoch: [101][140/196]	Time 0.015 (0.018)	Data 0.003 (0.004)	Loss 2.1182 (1.9921)	Acc@1 71.484 (72.169)	Acc@5 90.234 (94.531)
Epoch: [101][150/196]	Time 0.017 (0.018)	Data 0.001 (0.004)	Loss 2.0121 (1.9923)	Acc@1 73.047 (72.193)	Acc@5 93.750 (94.500)
Epoch: [101][160/196]	Time 0.014 (0.018)	Data 0.002 (0.004)	Loss 2.1030 (2.0000)	Acc@1 67.969 (71.931)	Acc@5 94.531 (94.439)
Epoch: [101][170/196]	Time 0.016 (0.018)	Data 0.001 (0.004)	Loss 2.1412 (2.0054)	Acc@1 69.922 (71.790)	Acc@5 89.844 (94.326)
Epoch: [101][180/196]	Time 0.012 (0.017)	Data 0.008 (0.004)	Loss 2.1821 (2.0098)	Acc@1 66.797 (71.629)	Acc@5 92.969 (94.281)
Epoch: [101][190/196]	Time 0.016 (0.017)	Data 0.000 (0.004)	Loss 2.0846 (2.0120)	Acc@1 70.312 (71.621)	Acc@5 93.359 (94.267)
num momentum params: 26
[0.1, 2.016094597015381, 1.745368139743805, 71.5, 55.62, tensor(0.5244, device='cuda:0', grad_fn=<DivBackward0>), 3.5505609512329106, 0.44307374954223633]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [102 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [102][0/196]	Time 0.058 (0.058)	Data 0.193 (0.193)	Loss 1.8666 (1.8666)	Acc@1 75.000 (75.000)	Acc@5 96.094 (96.094)
Epoch: [102][10/196]	Time 0.014 (0.019)	Data 0.003 (0.020)	Loss 1.9271 (1.9056)	Acc@1 75.000 (74.432)	Acc@5 94.922 (95.561)
Epoch: [102][20/196]	Time 0.015 (0.017)	Data 0.002 (0.012)	Loss 1.8296 (1.9202)	Acc@1 78.516 (73.977)	Acc@5 96.484 (95.517)
Epoch: [102][30/196]	Time 0.013 (0.017)	Data 0.005 (0.009)	Loss 2.0617 (1.9259)	Acc@1 68.750 (73.803)	Acc@5 94.922 (95.413)
Epoch: [102][40/196]	Time 0.013 (0.016)	Data 0.005 (0.007)	Loss 1.9693 (1.9385)	Acc@1 73.828 (73.590)	Acc@5 94.141 (95.293)
Epoch: [102][50/196]	Time 0.015 (0.016)	Data 0.005 (0.007)	Loss 1.8726 (1.9424)	Acc@1 73.828 (73.284)	Acc@5 97.266 (95.381)
Epoch: [102][60/196]	Time 0.012 (0.016)	Data 0.010 (0.006)	Loss 1.9708 (1.9474)	Acc@1 71.875 (73.169)	Acc@5 96.875 (95.364)
Epoch: [102][70/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 2.1257 (1.9476)	Acc@1 69.531 (73.102)	Acc@5 91.797 (95.318)
Epoch: [102][80/196]	Time 0.011 (0.016)	Data 0.005 (0.006)	Loss 2.0010 (1.9512)	Acc@1 71.094 (72.975)	Acc@5 93.750 (95.264)
Epoch: [102][90/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9332 (1.9594)	Acc@1 74.219 (72.798)	Acc@5 95.312 (95.137)
Epoch: [102][100/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 2.0725 (1.9692)	Acc@1 69.141 (72.490)	Acc@5 94.922 (95.030)
Epoch: [102][110/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.0930 (1.9768)	Acc@1 70.312 (72.364)	Acc@5 92.578 (94.859)
Epoch: [102][120/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 1.9874 (1.9797)	Acc@1 69.531 (72.317)	Acc@5 94.141 (94.770)
Epoch: [102][130/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.0856 (1.9822)	Acc@1 68.359 (72.295)	Acc@5 90.625 (94.701)
Epoch: [102][140/196]	Time 0.014 (0.015)	Data 0.005 (0.005)	Loss 1.9661 (1.9843)	Acc@1 71.875 (72.246)	Acc@5 92.578 (94.634)
Epoch: [102][150/196]	Time 0.014 (0.015)	Data 0.007 (0.005)	Loss 2.1059 (1.9835)	Acc@1 70.703 (72.255)	Acc@5 94.531 (94.640)
Epoch: [102][160/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 1.9870 (1.9885)	Acc@1 72.656 (72.164)	Acc@5 94.141 (94.563)
Epoch: [102][170/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 1.9680 (1.9925)	Acc@1 70.312 (72.028)	Acc@5 96.094 (94.518)
Epoch: [102][180/196]	Time 0.012 (0.015)	Data 0.006 (0.004)	Loss 2.1886 (1.9985)	Acc@1 64.844 (71.853)	Acc@5 92.188 (94.443)
Epoch: [102][190/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.0657 (2.0029)	Acc@1 72.656 (71.720)	Acc@5 94.531 (94.388)
num momentum params: 26
[0.1, 2.0056166889190674, 2.2944268476963043, 71.632, 47.85, tensor(0.5271, device='cuda:0', grad_fn=<DivBackward0>), 2.9943695068359375, 0.38356709480285645]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [103 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [103][0/196]	Time 0.058 (0.058)	Data 0.217 (0.217)	Loss 1.8429 (1.8429)	Acc@1 77.734 (77.734)	Acc@5 96.484 (96.484)
Epoch: [103][10/196]	Time 0.017 (0.020)	Data 0.001 (0.021)	Loss 1.9395 (1.9536)	Acc@1 72.656 (73.118)	Acc@5 94.922 (94.780)
Epoch: [103][20/196]	Time 0.014 (0.017)	Data 0.007 (0.013)	Loss 1.9061 (1.9607)	Acc@1 75.391 (72.954)	Acc@5 96.484 (95.052)
Epoch: [103][30/196]	Time 0.015 (0.017)	Data 0.003 (0.009)	Loss 2.0886 (1.9653)	Acc@1 67.188 (72.606)	Acc@5 94.531 (94.897)
Epoch: [103][40/196]	Time 0.014 (0.016)	Data 0.003 (0.008)	Loss 1.9392 (1.9611)	Acc@1 72.656 (72.837)	Acc@5 96.875 (94.865)
Epoch: [103][50/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.9201 (1.9572)	Acc@1 71.484 (72.924)	Acc@5 94.922 (94.853)
Epoch: [103][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0440 (1.9634)	Acc@1 71.875 (72.784)	Acc@5 94.141 (94.851)
Epoch: [103][70/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.1027 (1.9671)	Acc@1 69.141 (72.678)	Acc@5 93.750 (94.856)
Epoch: [103][80/196]	Time 0.013 (0.015)	Data 0.011 (0.006)	Loss 2.0956 (1.9720)	Acc@1 67.969 (72.589)	Acc@5 94.141 (94.811)
Epoch: [103][90/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0021 (1.9743)	Acc@1 73.828 (72.570)	Acc@5 94.531 (94.763)
Epoch: [103][100/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.1037 (1.9827)	Acc@1 69.922 (72.382)	Acc@5 92.578 (94.663)
Epoch: [103][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.9662 (1.9862)	Acc@1 72.656 (72.322)	Acc@5 96.484 (94.661)
Epoch: [103][120/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 2.0361 (1.9946)	Acc@1 71.875 (72.117)	Acc@5 92.969 (94.512)
Epoch: [103][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0172 (1.9955)	Acc@1 72.266 (72.128)	Acc@5 92.578 (94.522)
Epoch: [103][140/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 1.9778 (1.9987)	Acc@1 75.000 (72.083)	Acc@5 94.141 (94.470)
Epoch: [103][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0208 (2.0037)	Acc@1 72.266 (71.942)	Acc@5 95.312 (94.441)
Epoch: [103][160/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0817 (2.0061)	Acc@1 72.266 (71.885)	Acc@5 93.359 (94.400)
Epoch: [103][170/196]	Time 0.018 (0.015)	Data 0.000 (0.004)	Loss 2.2089 (2.0104)	Acc@1 64.844 (71.832)	Acc@5 93.750 (94.355)
Epoch: [103][180/196]	Time 0.012 (0.015)	Data 0.007 (0.004)	Loss 1.9460 (2.0130)	Acc@1 73.438 (71.799)	Acc@5 95.312 (94.313)
Epoch: [103][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.0636 (2.0183)	Acc@1 68.750 (71.673)	Acc@5 93.359 (94.235)
num momentum params: 26
[0.1, 2.0200102210998536, 1.8825475347042084, 71.608, 53.92, tensor(0.5236, device='cuda:0', grad_fn=<DivBackward0>), 2.9471700191497803, 0.3825511932373047]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [104 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [104][0/196]	Time 0.063 (0.063)	Data 0.207 (0.207)	Loss 1.9101 (1.9101)	Acc@1 77.344 (77.344)	Acc@5 95.312 (95.312)
Epoch: [104][10/196]	Time 0.015 (0.020)	Data 0.002 (0.021)	Loss 2.0276 (1.9494)	Acc@1 71.094 (73.722)	Acc@5 93.359 (94.567)
Epoch: [104][20/196]	Time 0.014 (0.018)	Data 0.003 (0.012)	Loss 1.8996 (1.9062)	Acc@1 73.047 (74.591)	Acc@5 96.484 (95.126)
Epoch: [104][30/196]	Time 0.013 (0.017)	Data 0.005 (0.009)	Loss 1.7672 (1.8923)	Acc@1 77.734 (75.050)	Acc@5 97.266 (95.376)
Epoch: [104][40/196]	Time 0.013 (0.016)	Data 0.003 (0.007)	Loss 1.8808 (1.8951)	Acc@1 76.562 (74.762)	Acc@5 95.703 (95.598)
Epoch: [104][50/196]	Time 0.013 (0.016)	Data 0.015 (0.007)	Loss 1.8934 (1.8949)	Acc@1 73.047 (74.678)	Acc@5 95.703 (95.703)
Epoch: [104][60/196]	Time 0.013 (0.016)	Data 0.006 (0.007)	Loss 1.9942 (1.9066)	Acc@1 73.438 (74.468)	Acc@5 93.359 (95.511)
Epoch: [104][70/196]	Time 0.011 (0.016)	Data 0.007 (0.006)	Loss 1.8654 (1.9102)	Acc@1 76.172 (74.373)	Acc@5 94.531 (95.450)
Epoch: [104][80/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 2.0431 (1.9175)	Acc@1 71.094 (74.214)	Acc@5 95.312 (95.390)
Epoch: [104][90/196]	Time 0.011 (0.015)	Data 0.011 (0.006)	Loss 1.9651 (1.9248)	Acc@1 73.047 (73.948)	Acc@5 96.484 (95.325)
Epoch: [104][100/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.0726 (1.9314)	Acc@1 67.578 (73.666)	Acc@5 92.578 (95.235)
Epoch: [104][110/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0767 (1.9381)	Acc@1 70.703 (73.543)	Acc@5 91.797 (95.133)
Epoch: [104][120/196]	Time 0.020 (0.015)	Data 0.008 (0.005)	Loss 1.9851 (1.9426)	Acc@1 71.484 (73.360)	Acc@5 96.094 (95.057)
Epoch: [104][130/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 1.9829 (1.9492)	Acc@1 72.266 (73.229)	Acc@5 94.531 (94.952)
Epoch: [104][140/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0055 (1.9508)	Acc@1 69.922 (73.174)	Acc@5 93.750 (94.916)
Epoch: [104][150/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 2.0287 (1.9564)	Acc@1 71.094 (72.980)	Acc@5 94.531 (94.878)
Epoch: [104][160/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.0798 (1.9640)	Acc@1 70.312 (72.756)	Acc@5 93.750 (94.808)
Epoch: [104][170/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9939 (1.9709)	Acc@1 73.047 (72.549)	Acc@5 93.750 (94.721)
Epoch: [104][180/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 2.1794 (1.9788)	Acc@1 65.625 (72.350)	Acc@5 91.797 (94.618)
Epoch: [104][190/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 2.1957 (1.9876)	Acc@1 68.359 (72.145)	Acc@5 90.234 (94.501)
num momentum params: 26
[0.1, 1.9895741595458984, 1.8742952680587768, 72.09, 53.42, tensor(0.5319, device='cuda:0', grad_fn=<DivBackward0>), 2.8888556957244873, 0.39217424392700195]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [105 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [105][0/196]	Time 0.059 (0.059)	Data 0.220 (0.220)	Loss 1.8888 (1.8888)	Acc@1 75.000 (75.000)	Acc@5 94.922 (94.922)
Epoch: [105][10/196]	Time 0.015 (0.020)	Data 0.002 (0.022)	Loss 1.9071 (1.9095)	Acc@1 76.172 (74.858)	Acc@5 93.359 (95.490)
Epoch: [105][20/196]	Time 0.013 (0.018)	Data 0.004 (0.013)	Loss 1.9646 (1.9195)	Acc@1 74.609 (74.721)	Acc@5 94.141 (95.182)
Epoch: [105][30/196]	Time 0.018 (0.017)	Data 0.003 (0.009)	Loss 1.8577 (1.9137)	Acc@1 76.172 (74.572)	Acc@5 94.922 (95.199)
Epoch: [105][40/196]	Time 0.014 (0.016)	Data 0.004 (0.008)	Loss 1.8135 (1.9114)	Acc@1 76.172 (74.686)	Acc@5 95.312 (95.274)
Epoch: [105][50/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.8149 (1.9146)	Acc@1 75.781 (74.364)	Acc@5 96.094 (95.366)
Epoch: [105][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 1.8347 (1.9164)	Acc@1 78.516 (74.302)	Acc@5 94.922 (95.396)
Epoch: [105][70/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0389 (1.9213)	Acc@1 71.094 (74.175)	Acc@5 92.578 (95.290)
Epoch: [105][80/196]	Time 0.012 (0.015)	Data 0.005 (0.006)	Loss 1.9664 (1.9251)	Acc@1 70.703 (74.011)	Acc@5 94.531 (95.187)
Epoch: [105][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.1640 (1.9361)	Acc@1 63.672 (73.755)	Acc@5 95.703 (95.068)
Epoch: [105][100/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 2.1432 (1.9497)	Acc@1 68.359 (73.333)	Acc@5 91.797 (94.926)
Epoch: [105][110/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0143 (1.9594)	Acc@1 69.531 (73.005)	Acc@5 92.188 (94.813)
Epoch: [105][120/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0163 (1.9679)	Acc@1 72.266 (72.750)	Acc@5 94.531 (94.689)
Epoch: [105][130/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.1775 (1.9729)	Acc@1 64.062 (72.632)	Acc@5 94.531 (94.648)
Epoch: [105][140/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1596 (1.9811)	Acc@1 68.750 (72.507)	Acc@5 91.406 (94.479)
Epoch: [105][150/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.1435 (1.9852)	Acc@1 68.750 (72.379)	Acc@5 90.625 (94.428)
Epoch: [105][160/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.9701 (1.9903)	Acc@1 74.219 (72.278)	Acc@5 94.531 (94.357)
Epoch: [105][170/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0578 (1.9960)	Acc@1 72.266 (72.108)	Acc@5 93.750 (94.312)
Epoch: [105][180/196]	Time 0.022 (0.015)	Data 0.001 (0.004)	Loss 2.0124 (2.0002)	Acc@1 72.266 (72.024)	Acc@5 92.578 (94.231)
Epoch: [105][190/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 1.9618 (2.0035)	Acc@1 73.047 (71.951)	Acc@5 96.484 (94.212)
num momentum params: 26
[0.1, 2.006012742767334, 2.090469982624054, 71.852, 50.78, tensor(0.5278, device='cuda:0', grad_fn=<DivBackward0>), 2.9597859382629395, 0.4017148017883301]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [106 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [106][0/196]	Time 0.063 (0.063)	Data 0.212 (0.212)	Loss 1.8167 (1.8167)	Acc@1 76.562 (76.562)	Acc@5 95.312 (95.312)
Epoch: [106][10/196]	Time 0.017 (0.020)	Data 0.001 (0.022)	Loss 1.9204 (2.0175)	Acc@1 75.000 (71.236)	Acc@5 96.484 (94.425)
Epoch: [106][20/196]	Time 0.012 (0.018)	Data 0.006 (0.013)	Loss 1.8922 (1.9994)	Acc@1 75.391 (71.373)	Acc@5 95.703 (94.680)
Epoch: [106][30/196]	Time 0.018 (0.017)	Data 0.001 (0.009)	Loss 2.0300 (1.9863)	Acc@1 72.266 (71.825)	Acc@5 93.359 (94.909)
Epoch: [106][40/196]	Time 0.012 (0.016)	Data 0.007 (0.008)	Loss 1.8797 (1.9644)	Acc@1 76.562 (72.637)	Acc@5 94.922 (94.970)
Epoch: [106][50/196]	Time 0.019 (0.016)	Data 0.001 (0.007)	Loss 2.0309 (1.9592)	Acc@1 70.703 (73.009)	Acc@5 94.141 (95.037)
Epoch: [106][60/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.1694 (1.9614)	Acc@1 65.234 (72.906)	Acc@5 94.531 (95.191)
Epoch: [106][70/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.7911 (1.9631)	Acc@1 78.516 (72.937)	Acc@5 98.438 (95.037)
Epoch: [106][80/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.0277 (1.9607)	Acc@1 71.484 (72.931)	Acc@5 95.703 (95.086)
Epoch: [106][90/196]	Time 0.024 (0.016)	Data 0.001 (0.005)	Loss 1.9802 (1.9630)	Acc@1 75.000 (72.888)	Acc@5 93.750 (95.102)
Epoch: [106][100/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.0729 (1.9651)	Acc@1 73.047 (72.830)	Acc@5 92.578 (95.042)
Epoch: [106][110/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 2.0574 (1.9697)	Acc@1 69.531 (72.677)	Acc@5 93.359 (94.996)
Epoch: [106][120/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.2091 (1.9783)	Acc@1 63.281 (72.424)	Acc@5 94.531 (94.906)
Epoch: [106][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0596 (1.9833)	Acc@1 71.875 (72.358)	Acc@5 94.922 (94.850)
Epoch: [106][140/196]	Time 0.012 (0.015)	Data 0.008 (0.004)	Loss 2.2081 (1.9898)	Acc@1 69.922 (72.205)	Acc@5 92.188 (94.753)
Epoch: [106][150/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0966 (1.9971)	Acc@1 68.750 (71.997)	Acc@5 94.531 (94.666)
Epoch: [106][160/196]	Time 0.013 (0.015)	Data 0.010 (0.004)	Loss 2.1413 (2.0059)	Acc@1 64.453 (71.729)	Acc@5 92.578 (94.539)
Epoch: [106][170/196]	Time 0.013 (0.015)	Data 0.011 (0.004)	Loss 2.1007 (2.0113)	Acc@1 69.922 (71.672)	Acc@5 93.359 (94.486)
Epoch: [106][180/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 2.2059 (2.0157)	Acc@1 66.797 (71.633)	Acc@5 91.016 (94.389)
Epoch: [106][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1434 (2.0201)	Acc@1 68.750 (71.536)	Acc@5 92.969 (94.335)
num momentum params: 26
[0.1, 2.0210119747924806, 1.8516150307655335, 71.5, 53.63, tensor(0.5244, device='cuda:0', grad_fn=<DivBackward0>), 3.0173397064208984, 0.385012149810791]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [107 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [107][0/196]	Time 0.062 (0.062)	Data 0.194 (0.194)	Loss 1.8631 (1.8631)	Acc@1 75.391 (75.391)	Acc@5 95.703 (95.703)
Epoch: [107][10/196]	Time 0.017 (0.021)	Data 0.002 (0.020)	Loss 1.8700 (1.9231)	Acc@1 73.438 (73.544)	Acc@5 95.703 (96.058)
Epoch: [107][20/196]	Time 0.012 (0.018)	Data 0.006 (0.012)	Loss 1.9051 (1.9136)	Acc@1 73.828 (74.014)	Acc@5 96.875 (96.038)
Epoch: [107][30/196]	Time 0.015 (0.017)	Data 0.003 (0.009)	Loss 1.8892 (1.9009)	Acc@1 72.266 (74.282)	Acc@5 95.703 (95.968)
Epoch: [107][40/196]	Time 0.014 (0.017)	Data 0.005 (0.007)	Loss 1.8161 (1.9052)	Acc@1 78.906 (74.381)	Acc@5 94.922 (95.856)
Epoch: [107][50/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.8759 (1.9149)	Acc@1 75.391 (74.043)	Acc@5 96.094 (95.787)
Epoch: [107][60/196]	Time 0.012 (0.016)	Data 0.005 (0.006)	Loss 1.8225 (1.9137)	Acc@1 76.953 (74.071)	Acc@5 98.438 (95.786)
Epoch: [107][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9619 (1.9114)	Acc@1 71.484 (74.103)	Acc@5 98.828 (95.813)
Epoch: [107][80/196]	Time 0.012 (0.016)	Data 0.004 (0.005)	Loss 1.9231 (1.9160)	Acc@1 73.828 (73.978)	Acc@5 96.094 (95.674)
Epoch: [107][90/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1642 (1.9226)	Acc@1 67.578 (73.785)	Acc@5 92.969 (95.523)
Epoch: [107][100/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.1321 (1.9324)	Acc@1 66.797 (73.499)	Acc@5 93.750 (95.382)
Epoch: [107][110/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.0541 (1.9377)	Acc@1 70.312 (73.452)	Acc@5 94.922 (95.334)
Epoch: [107][120/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 2.1918 (1.9428)	Acc@1 66.797 (73.366)	Acc@5 92.578 (95.251)
Epoch: [107][130/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 1.9400 (1.9464)	Acc@1 76.172 (73.294)	Acc@5 94.922 (95.202)
Epoch: [107][140/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 2.1413 (1.9575)	Acc@1 67.578 (73.030)	Acc@5 91.797 (95.077)
Epoch: [107][150/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 2.0132 (1.9664)	Acc@1 68.750 (72.773)	Acc@5 95.703 (94.989)
Epoch: [107][160/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0194 (1.9734)	Acc@1 68.359 (72.639)	Acc@5 92.188 (94.844)
Epoch: [107][170/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.1551 (1.9831)	Acc@1 65.625 (72.368)	Acc@5 92.188 (94.705)
Epoch: [107][180/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.1859 (1.9906)	Acc@1 66.797 (72.188)	Acc@5 92.188 (94.637)
Epoch: [107][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0228 (1.9939)	Acc@1 73.047 (72.151)	Acc@5 92.969 (94.576)
num momentum params: 26
[0.1, 1.9976700131225587, 1.8707381093502045, 72.094, 53.06, tensor(0.5303, device='cuda:0', grad_fn=<DivBackward0>), 2.9754271507263184, 0.38398218154907227]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [108 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [108][0/196]	Time 0.066 (0.066)	Data 0.206 (0.206)	Loss 1.8356 (1.8356)	Acc@1 80.078 (80.078)	Acc@5 96.484 (96.484)
Epoch: [108][10/196]	Time 0.017 (0.021)	Data 0.002 (0.021)	Loss 1.9929 (1.9735)	Acc@1 73.828 (72.940)	Acc@5 95.703 (95.099)
Epoch: [108][20/196]	Time 0.014 (0.018)	Data 0.004 (0.012)	Loss 1.9413 (1.9497)	Acc@1 75.391 (73.642)	Acc@5 94.141 (94.978)
Epoch: [108][30/196]	Time 0.016 (0.018)	Data 0.003 (0.009)	Loss 1.9797 (1.9447)	Acc@1 73.047 (73.790)	Acc@5 94.531 (95.073)
Epoch: [108][40/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 1.9215 (1.9291)	Acc@1 73.438 (74.095)	Acc@5 95.703 (95.160)
Epoch: [108][50/196]	Time 0.014 (0.017)	Data 0.003 (0.007)	Loss 1.8679 (1.9285)	Acc@1 75.781 (74.226)	Acc@5 96.875 (95.152)
Epoch: [108][60/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 2.0142 (1.9319)	Acc@1 73.438 (74.065)	Acc@5 92.969 (95.031)
Epoch: [108][70/196]	Time 0.014 (0.016)	Data 0.005 (0.006)	Loss 1.8516 (1.9362)	Acc@1 78.125 (73.971)	Acc@5 95.703 (95.048)
Epoch: [108][80/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9295 (1.9421)	Acc@1 75.000 (73.804)	Acc@5 94.141 (95.009)
Epoch: [108][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9630 (1.9439)	Acc@1 70.703 (73.661)	Acc@5 96.875 (94.982)
Epoch: [108][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0057 (1.9468)	Acc@1 74.609 (73.615)	Acc@5 96.094 (94.984)
Epoch: [108][110/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9789 (1.9557)	Acc@1 71.484 (73.283)	Acc@5 95.703 (94.855)
Epoch: [108][120/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.2015 (1.9609)	Acc@1 64.844 (73.134)	Acc@5 93.359 (94.841)
Epoch: [108][130/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1753 (1.9697)	Acc@1 69.141 (72.874)	Acc@5 91.406 (94.725)
Epoch: [108][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0273 (1.9755)	Acc@1 72.656 (72.703)	Acc@5 93.750 (94.692)
Epoch: [108][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0268 (1.9813)	Acc@1 67.578 (72.553)	Acc@5 95.312 (94.624)
Epoch: [108][160/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 1.9711 (1.9844)	Acc@1 73.828 (72.472)	Acc@5 95.312 (94.599)
Epoch: [108][170/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0379 (1.9878)	Acc@1 69.141 (72.357)	Acc@5 94.531 (94.547)
Epoch: [108][180/196]	Time 0.012 (0.016)	Data 0.011 (0.004)	Loss 1.9437 (1.9924)	Acc@1 73.438 (72.216)	Acc@5 95.312 (94.533)
Epoch: [108][190/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.2764 (1.9983)	Acc@1 65.625 (72.100)	Acc@5 91.406 (94.443)
num momentum params: 26
[0.1, 2.0006866500854494, 2.0857752287387847, 72.052, 50.2, tensor(0.5291, device='cuda:0', grad_fn=<DivBackward0>), 3.040626287460327, 0.39008283615112305]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [109 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [109][0/196]	Time 0.065 (0.065)	Data 0.197 (0.197)	Loss 1.9913 (1.9913)	Acc@1 69.531 (69.531)	Acc@5 95.312 (95.312)
Epoch: [109][10/196]	Time 0.015 (0.020)	Data 0.003 (0.020)	Loss 1.8531 (1.9884)	Acc@1 76.953 (73.082)	Acc@5 95.703 (94.389)
Epoch: [109][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 1.8111 (1.9781)	Acc@1 78.125 (72.935)	Acc@5 97.656 (94.736)
Epoch: [109][30/196]	Time 0.014 (0.017)	Data 0.003 (0.009)	Loss 1.8135 (1.9527)	Acc@1 78.906 (73.891)	Acc@5 96.875 (94.972)
Epoch: [109][40/196]	Time 0.012 (0.017)	Data 0.004 (0.007)	Loss 1.8098 (1.9386)	Acc@1 74.219 (73.914)	Acc@5 96.875 (95.179)
Epoch: [109][50/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 2.0155 (1.9334)	Acc@1 69.141 (73.820)	Acc@5 96.484 (95.343)
Epoch: [109][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 1.8452 (1.9289)	Acc@1 78.516 (73.860)	Acc@5 95.703 (95.325)
Epoch: [109][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9807 (1.9259)	Acc@1 74.219 (73.911)	Acc@5 95.312 (95.434)
Epoch: [109][80/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.0632 (1.9334)	Acc@1 70.703 (73.688)	Acc@5 93.359 (95.356)
Epoch: [109][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9619 (1.9366)	Acc@1 70.312 (73.502)	Acc@5 94.922 (95.295)
Epoch: [109][100/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.1117 (1.9427)	Acc@1 71.484 (73.395)	Acc@5 94.531 (95.235)
Epoch: [109][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9185 (1.9472)	Acc@1 72.266 (73.255)	Acc@5 95.312 (95.228)
Epoch: [109][120/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 1.9575 (1.9594)	Acc@1 70.312 (72.937)	Acc@5 94.922 (95.109)
Epoch: [109][130/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.9971 (1.9680)	Acc@1 70.312 (72.683)	Acc@5 92.578 (94.993)
Epoch: [109][140/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 2.1397 (1.9749)	Acc@1 67.578 (72.557)	Acc@5 92.578 (94.861)
Epoch: [109][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1256 (1.9801)	Acc@1 68.750 (72.403)	Acc@5 94.141 (94.793)
Epoch: [109][160/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 1.9854 (1.9884)	Acc@1 74.219 (72.169)	Acc@5 95.312 (94.711)
Epoch: [109][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0113 (1.9934)	Acc@1 72.266 (72.085)	Acc@5 94.922 (94.632)
Epoch: [109][180/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 2.0194 (1.9961)	Acc@1 73.438 (71.983)	Acc@5 95.703 (94.624)
Epoch: [109][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1630 (2.0020)	Acc@1 68.750 (71.832)	Acc@5 92.578 (94.552)
num momentum params: 26
[0.1, 2.005289970169067, 1.9000216186046601, 71.712, 53.12, tensor(0.5287, device='cuda:0', grad_fn=<DivBackward0>), 3.0440855026245117, 0.3887791633605957]
Non Pruning Epoch - module.conv1.weight: [44, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [44]
Non Pruning Epoch - module.bn1.bias: [44]
Non Pruning Epoch - module.conv2.weight: [128, 44, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [250, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [250]
Non Pruning Epoch - module.bn3.bias: [250]
Non Pruning Epoch - module.conv4.weight: [256, 250, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [509, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [509]
Non Pruning Epoch - module.bn6.bias: [509]
Non Pruning Epoch - module.conv7.weight: [482, 509, 3, 3]
Non Pruning Epoch - module.bn7.weight: [482]
Non Pruning Epoch - module.bn7.bias: [482]
Non Pruning Epoch - module.conv8.weight: [344, 482, 3, 3]
Non Pruning Epoch - module.bn8.weight: [344]
Non Pruning Epoch - module.bn8.bias: [344]
Non Pruning Epoch - module.fc.weight: [100, 344]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [110 | 180] LR: 0.100000
module.conv1.weight [44, 3, 3, 3]
module.conv2.weight [128, 44, 3, 3]
module.conv3.weight [250, 128, 3, 3]
module.conv4.weight [256, 250, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [509, 510, 3, 3]
module.conv7.weight [482, 509, 3, 3]
module.conv8.weight [344, 482, 3, 3]
Epoch: [110][0/196]	Time 0.063 (0.063)	Data 0.205 (0.205)	Loss 1.9550 (1.9550)	Acc@1 75.000 (75.000)	Acc@5 95.703 (95.703)
Epoch: [110][10/196]	Time 0.015 (0.020)	Data 0.002 (0.020)	Loss 1.9470 (1.9227)	Acc@1 70.312 (74.077)	Acc@5 95.703 (95.135)
Epoch: [110][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 1.9418 (1.9316)	Acc@1 72.266 (73.735)	Acc@5 94.922 (94.866)
Epoch: [110][30/196]	Time 0.016 (0.017)	Data 0.002 (0.009)	Loss 1.8876 (1.9115)	Acc@1 73.828 (74.446)	Acc@5 95.312 (95.111)
Epoch: [110][40/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 1.9659 (1.9153)	Acc@1 73.047 (74.476)	Acc@5 96.484 (95.236)
Epoch: [110][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9536 (1.9187)	Acc@1 73.438 (74.341)	Acc@5 95.312 (95.328)
Epoch: [110][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.8638 (1.9236)	Acc@1 77.344 (74.308)	Acc@5 95.703 (95.351)
Epoch: [110][70/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.8580 (1.9165)	Acc@1 74.609 (74.439)	Acc@5 95.312 (95.439)
Epoch: [110][80/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9779 (1.9229)	Acc@1 70.703 (74.195)	Acc@5 94.531 (95.303)
Epoch: [110][90/196]	Time 0.017 (0.015)	Data 0.004 (0.005)	Loss 2.0709 (1.9368)	Acc@1 70.703 (73.854)	Acc@5 94.531 (95.162)
Epoch: [110][100/196]	Time 0.013 (0.015)	Data 0.017 (0.005)	Loss 2.1436 (1.9463)	Acc@1 69.922 (73.592)	Acc@5 92.969 (95.022)
Epoch: [110][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.1144 (1.9548)	Acc@1 67.188 (73.304)	Acc@5 92.578 (94.922)
Epoch: [110][120/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 1.9490 (1.9616)	Acc@1 75.391 (73.134)	Acc@5 96.484 (94.909)
Epoch: [110][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.1845 (1.9687)	Acc@1 64.453 (72.916)	Acc@5 92.188 (94.818)
Epoch: [110][140/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 2.0563 (1.9782)	Acc@1 69.531 (72.651)	Acc@5 96.094 (94.711)
Epoch: [110][150/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1011 (1.9812)	Acc@1 71.875 (72.568)	Acc@5 92.578 (94.661)
Epoch: [110][160/196]	Time 0.011 (0.015)	Data 0.007 (0.004)	Loss 2.1361 (1.9867)	Acc@1 68.750 (72.368)	Acc@5 91.406 (94.626)
Epoch: [110][170/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 2.0982 (1.9942)	Acc@1 69.531 (72.188)	Acc@5 93.750 (94.520)
Epoch: [110][180/196]	Time 0.014 (0.015)	Data 0.004 (0.004)	Loss 2.0725 (2.0010)	Acc@1 71.484 (72.007)	Acc@5 95.312 (94.471)
Epoch: [110][190/196]	Time 0.013 (0.015)	Data 0.005 (0.004)	Loss 2.2615 (2.0083)	Acc@1 66.797 (71.808)	Acc@5 92.578 (94.417)
num momentum params: 26
[0.1, 2.0102319036865235, 1.8012249898910522, 71.78, 54.02, tensor(0.5281, device='cuda:0', grad_fn=<DivBackward0>), 2.980729579925537, 0.3864476680755615]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [44, 3, 3, 3]
Before - module.bn1.weight: [44]
Before - module.bn1.bias: [44]
Before - module.conv2.weight: [128, 44, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [250, 128, 3, 3]
Before - module.bn3.weight: [250]
Before - module.bn3.bias: [250]
Before - module.conv4.weight: [256, 250, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [509, 510, 3, 3]
Before - module.bn6.weight: [509]
Before - module.bn6.bias: [509]
Before - module.conv7.weight: [482, 509, 3, 3]
Before - module.bn7.weight: [482]
Before - module.bn7.bias: [482]
Before - module.conv8.weight: [344, 482, 3, 3]
Before - module.bn8.weight: [344]
Before - module.bn8.bias: [344]
Before - module.fc.weight: [100, 344]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv1.weight]: [44, 3, 3, 3] >> [42, 3, 3, 3]
[module.bn1.weight]: 44 >> 42
running_mean [42]
running_var [42]
num_batches_tracked []
[module.conv2.weight]: [128, 44, 3, 3] >> [128, 42, 3, 3]
[module.conv3.weight]: [250, 128, 3, 3] >> [248, 128, 3, 3]
[module.bn3.weight]: 250 >> 248
running_mean [248]
running_var [248]
num_batches_tracked []
[module.conv4.weight]: [256, 250, 3, 3] >> [256, 248, 3, 3]
[module.conv6.weight]: [509, 510, 3, 3] >> [508, 510, 3, 3]
[module.bn6.weight]: 509 >> 508
running_mean [508]
running_var [508]
num_batches_tracked []
[module.conv7.weight]: [482, 509, 3, 3] >> [481, 508, 3, 3]
[module.bn7.weight]: 482 >> 481
running_mean [481]
running_var [481]
num_batches_tracked []
[module.conv8.weight]: [344, 482, 3, 3] >> [331, 481, 3, 3]
[module.bn8.weight]: 344 >> 331
running_mean [331]
running_var [331]
num_batches_tracked []
[module.fc.weight]: [100, 344] >> [100, 331]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [248, 128, 3, 3]
After - module.bn3.weight: [248]
After - module.bn3.bias: [248]
After - module.conv4.weight: [256, 248, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [508, 510, 3, 3]
After - module.bn6.weight: [508]
After - module.bn6.bias: [508]
After - module.conv7.weight: [481, 508, 3, 3]
After - module.bn7.weight: [481]
After - module.bn7.bias: [481]
After - module.conv8.weight: [331, 481, 3, 3]
After - module.bn8.weight: [331]
After - module.bn8.bias: [331]
After - module.fc.weight: [100, 331]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [248, 128, 3, 3]
conv4 --> [256, 248, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [508, 510, 3, 3]
conv7 --> [481, 508, 3, 3]
conv8 --> [331, 481, 3, 3]
fc --> [331, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8337752064, 18284544, 248
4, 16675504128, 36569088, 256
5, 10227548160, 18800640, 510
6, 20295290880, 37307520, 508
7, 6755733504, 8796528, 481
8, 4401865728, 5731596, 331
fc, 12710400, 33100, 0
===================
FLOP REPORT: 28261307400000.0 51027200000.0 139070536 127568 2504 15.413917541503906
[INFO] Storing checkpoint...

Epoch: [111 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [111][0/196]	Time 0.635 (0.635)	Data 0.210 (0.210)	Loss 1.9214 (1.9214)	Acc@1 74.609 (74.609)	Acc@5 94.531 (94.531)
Epoch: [111][10/196]	Time 0.015 (0.072)	Data 0.003 (0.021)	Loss 2.0411 (1.9833)	Acc@1 70.703 (72.656)	Acc@5 94.922 (94.993)
Epoch: [111][20/196]	Time 0.016 (0.045)	Data 0.002 (0.012)	Loss 1.7972 (1.9421)	Acc@1 77.734 (73.865)	Acc@5 96.484 (95.406)
Epoch: [111][30/196]	Time 0.015 (0.036)	Data 0.003 (0.009)	Loss 1.9065 (1.9259)	Acc@1 73.828 (73.992)	Acc@5 94.922 (95.653)
Epoch: [111][40/196]	Time 0.017 (0.031)	Data 0.002 (0.008)	Loss 1.8253 (1.9164)	Acc@1 73.047 (74.009)	Acc@5 97.656 (95.856)
Epoch: [111][50/196]	Time 0.017 (0.028)	Data 0.002 (0.007)	Loss 1.9789 (1.9099)	Acc@1 74.609 (74.211)	Acc@5 92.969 (95.918)
Epoch: [111][60/196]	Time 0.014 (0.025)	Data 0.003 (0.006)	Loss 1.8884 (1.9158)	Acc@1 76.172 (74.103)	Acc@5 96.094 (95.831)
Epoch: [111][70/196]	Time 0.016 (0.024)	Data 0.001 (0.005)	Loss 1.9992 (1.9254)	Acc@1 73.828 (73.795)	Acc@5 94.922 (95.747)
Epoch: [111][80/196]	Time 0.013 (0.023)	Data 0.006 (0.005)	Loss 2.0768 (1.9302)	Acc@1 69.141 (73.659)	Acc@5 92.578 (95.578)
Epoch: [111][90/196]	Time 0.016 (0.022)	Data 0.000 (0.005)	Loss 1.8257 (1.9303)	Acc@1 76.172 (73.639)	Acc@5 96.484 (95.531)
Epoch: [111][100/196]	Time 0.012 (0.021)	Data 0.011 (0.005)	Loss 2.0071 (1.9385)	Acc@1 72.656 (73.472)	Acc@5 94.141 (95.425)
Epoch: [111][110/196]	Time 0.016 (0.020)	Data 0.001 (0.005)	Loss 1.9525 (1.9436)	Acc@1 71.484 (73.300)	Acc@5 93.750 (95.348)
Epoch: [111][120/196]	Time 0.012 (0.020)	Data 0.011 (0.005)	Loss 2.0635 (1.9522)	Acc@1 67.578 (73.108)	Acc@5 94.141 (95.258)
Epoch: [111][130/196]	Time 0.016 (0.020)	Data 0.001 (0.005)	Loss 2.1017 (1.9584)	Acc@1 69.141 (72.975)	Acc@5 95.703 (95.199)
Epoch: [111][140/196]	Time 0.013 (0.019)	Data 0.008 (0.005)	Loss 2.0209 (1.9621)	Acc@1 69.922 (72.861)	Acc@5 95.703 (95.177)
Epoch: [111][150/196]	Time 0.017 (0.019)	Data 0.001 (0.004)	Loss 2.0085 (1.9708)	Acc@1 70.312 (72.690)	Acc@5 94.922 (95.077)
Epoch: [111][160/196]	Time 0.011 (0.019)	Data 0.006 (0.004)	Loss 2.2110 (1.9786)	Acc@1 69.922 (72.443)	Acc@5 90.234 (94.973)
Epoch: [111][170/196]	Time 0.017 (0.018)	Data 0.000 (0.004)	Loss 2.0698 (1.9896)	Acc@1 67.578 (72.188)	Acc@5 92.969 (94.821)
Epoch: [111][180/196]	Time 0.012 (0.018)	Data 0.004 (0.004)	Loss 2.0305 (1.9928)	Acc@1 72.266 (72.095)	Acc@5 95.703 (94.758)
Epoch: [111][190/196]	Time 0.017 (0.018)	Data 0.000 (0.004)	Loss 2.0512 (1.9956)	Acc@1 72.266 (72.080)	Acc@5 92.578 (94.713)
num momentum params: 26
[0.1, 1.99621162651062, 1.8238155090808867, 72.074, 54.5, tensor(0.5318, device='cuda:0', grad_fn=<DivBackward0>), 3.726606607437134, 0.4876382350921631]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [112 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [112][0/196]	Time 0.058 (0.058)	Data 0.215 (0.215)	Loss 1.9238 (1.9238)	Acc@1 75.781 (75.781)	Acc@5 94.922 (94.922)
Epoch: [112][10/196]	Time 0.017 (0.020)	Data 0.001 (0.021)	Loss 2.0750 (1.9057)	Acc@1 68.359 (74.574)	Acc@5 93.750 (95.170)
Epoch: [112][20/196]	Time 0.014 (0.018)	Data 0.003 (0.012)	Loss 1.7606 (1.8946)	Acc@1 80.078 (75.093)	Acc@5 96.875 (95.331)
Epoch: [112][30/196]	Time 0.017 (0.017)	Data 0.001 (0.009)	Loss 1.8776 (1.8911)	Acc@1 75.000 (75.454)	Acc@5 97.656 (95.476)
Epoch: [112][40/196]	Time 0.015 (0.016)	Data 0.003 (0.008)	Loss 1.9223 (1.8890)	Acc@1 71.875 (75.381)	Acc@5 97.656 (95.522)
Epoch: [112][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.7897 (1.8935)	Acc@1 79.297 (75.100)	Acc@5 95.312 (95.496)
Epoch: [112][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.8780 (1.8960)	Acc@1 75.391 (75.102)	Acc@5 97.266 (95.581)
Epoch: [112][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9110 (1.8951)	Acc@1 75.000 (75.061)	Acc@5 95.703 (95.582)
Epoch: [112][80/196]	Time 0.013 (0.015)	Data 0.012 (0.006)	Loss 2.0589 (1.9005)	Acc@1 69.922 (74.932)	Acc@5 92.969 (95.476)
Epoch: [112][90/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0208 (1.9058)	Acc@1 74.219 (74.790)	Acc@5 92.969 (95.437)
Epoch: [112][100/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 2.1269 (1.9107)	Acc@1 70.703 (74.691)	Acc@5 94.141 (95.320)
Epoch: [112][110/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.8766 (1.9197)	Acc@1 76.953 (74.423)	Acc@5 95.312 (95.228)
Epoch: [112][120/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 1.9476 (1.9283)	Acc@1 75.000 (74.203)	Acc@5 96.484 (95.132)
Epoch: [112][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0354 (1.9344)	Acc@1 71.484 (73.930)	Acc@5 91.797 (95.086)
Epoch: [112][140/196]	Time 0.012 (0.015)	Data 0.014 (0.005)	Loss 2.0864 (1.9438)	Acc@1 67.969 (73.643)	Acc@5 93.359 (95.024)
Epoch: [112][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0699 (1.9527)	Acc@1 69.141 (73.388)	Acc@5 94.141 (94.917)
Epoch: [112][160/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.1083 (1.9604)	Acc@1 70.703 (73.188)	Acc@5 94.922 (94.827)
Epoch: [112][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1299 (1.9683)	Acc@1 69.141 (72.999)	Acc@5 93.359 (94.773)
Epoch: [112][180/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.0908 (1.9759)	Acc@1 70.703 (72.792)	Acc@5 94.141 (94.672)
Epoch: [112][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1106 (1.9818)	Acc@1 69.141 (72.640)	Acc@5 92.578 (94.648)
num momentum params: 26
[0.1, 1.9861321256256104, 2.286309574842453, 72.512, 46.89, tensor(0.5345, device='cuda:0', grad_fn=<DivBackward0>), 2.9669737815856934, 0.38304948806762695]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [113 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [113][0/196]	Time 0.053 (0.053)	Data 0.211 (0.211)	Loss 1.8597 (1.8597)	Acc@1 73.047 (73.047)	Acc@5 95.703 (95.703)
Epoch: [113][10/196]	Time 0.016 (0.020)	Data 0.002 (0.021)	Loss 2.1760 (2.0456)	Acc@1 67.969 (71.058)	Acc@5 92.578 (94.354)
Epoch: [113][20/196]	Time 0.015 (0.018)	Data 0.003 (0.012)	Loss 1.9451 (1.9914)	Acc@1 72.266 (72.377)	Acc@5 95.312 (94.996)
Epoch: [113][30/196]	Time 0.016 (0.017)	Data 0.001 (0.009)	Loss 2.0079 (1.9736)	Acc@1 71.094 (72.946)	Acc@5 94.922 (95.010)
Epoch: [113][40/196]	Time 0.015 (0.016)	Data 0.003 (0.008)	Loss 1.8722 (1.9524)	Acc@1 74.219 (73.390)	Acc@5 96.484 (95.293)
Epoch: [113][50/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 2.0255 (1.9376)	Acc@1 69.922 (73.736)	Acc@5 94.922 (95.412)
Epoch: [113][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 1.9504 (1.9333)	Acc@1 73.828 (73.956)	Acc@5 94.922 (95.409)
Epoch: [113][70/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9771 (1.9308)	Acc@1 75.391 (74.059)	Acc@5 94.141 (95.373)
Epoch: [113][80/196]	Time 0.014 (0.015)	Data 0.007 (0.006)	Loss 2.1076 (1.9363)	Acc@1 70.703 (73.876)	Acc@5 93.359 (95.202)
Epoch: [113][90/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.8104 (1.9375)	Acc@1 73.828 (73.721)	Acc@5 96.484 (95.270)
Epoch: [113][100/196]	Time 0.011 (0.015)	Data 0.010 (0.006)	Loss 1.9600 (1.9388)	Acc@1 75.000 (73.697)	Acc@5 96.484 (95.231)
Epoch: [113][110/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 2.1192 (1.9434)	Acc@1 68.750 (73.582)	Acc@5 94.531 (95.140)
Epoch: [113][120/196]	Time 0.018 (0.015)	Data 0.003 (0.005)	Loss 2.0010 (1.9519)	Acc@1 73.828 (73.286)	Acc@5 94.531 (95.083)
Epoch: [113][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 1.8471 (1.9582)	Acc@1 76.562 (73.139)	Acc@5 95.703 (95.014)
Epoch: [113][140/196]	Time 0.012 (0.015)	Data 0.014 (0.005)	Loss 2.1085 (1.9680)	Acc@1 70.312 (72.919)	Acc@5 94.531 (94.914)
Epoch: [113][150/196]	Time 0.015 (0.015)	Data 0.001 (0.005)	Loss 1.9324 (1.9747)	Acc@1 72.656 (72.726)	Acc@5 96.094 (94.839)
Epoch: [113][160/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 2.0174 (1.9851)	Acc@1 75.781 (72.503)	Acc@5 92.578 (94.657)
Epoch: [113][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0744 (1.9918)	Acc@1 66.406 (72.268)	Acc@5 93.750 (94.579)
Epoch: [113][180/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 1.9657 (1.9975)	Acc@1 74.219 (72.145)	Acc@5 94.141 (94.492)
Epoch: [113][190/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.0795 (2.0000)	Acc@1 69.531 (72.051)	Acc@5 92.969 (94.460)
num momentum params: 26
[0.1, 2.000679227142334, 1.891105614900589, 72.024, 52.67, tensor(0.5311, device='cuda:0', grad_fn=<DivBackward0>), 2.9274518489837646, 0.38707661628723145]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [114 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [114][0/196]	Time 0.063 (0.063)	Data 0.217 (0.217)	Loss 1.8260 (1.8260)	Acc@1 74.609 (74.609)	Acc@5 97.656 (97.656)
Epoch: [114][10/196]	Time 0.016 (0.021)	Data 0.002 (0.022)	Loss 1.8087 (1.9204)	Acc@1 75.000 (73.828)	Acc@5 96.875 (95.739)
Epoch: [114][20/196]	Time 0.015 (0.019)	Data 0.003 (0.013)	Loss 1.7820 (1.9138)	Acc@1 78.516 (74.237)	Acc@5 97.266 (95.647)
Epoch: [114][30/196]	Time 0.016 (0.017)	Data 0.001 (0.009)	Loss 1.9959 (1.9219)	Acc@1 72.266 (74.017)	Acc@5 93.750 (95.527)
Epoch: [114][40/196]	Time 0.014 (0.017)	Data 0.004 (0.008)	Loss 1.8846 (1.9227)	Acc@1 71.875 (74.019)	Acc@5 96.094 (95.455)
Epoch: [114][50/196]	Time 0.015 (0.017)	Data 0.001 (0.007)	Loss 1.8572 (1.9214)	Acc@1 77.734 (73.912)	Acc@5 95.703 (95.535)
Epoch: [114][60/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.9783 (1.9210)	Acc@1 75.391 (74.052)	Acc@5 91.797 (95.421)
Epoch: [114][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.8876 (1.9240)	Acc@1 73.828 (73.922)	Acc@5 94.922 (95.434)
Epoch: [114][80/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 1.9219 (1.9254)	Acc@1 73.828 (73.852)	Acc@5 97.656 (95.361)
Epoch: [114][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9709 (1.9259)	Acc@1 73.438 (73.850)	Acc@5 94.531 (95.270)
Epoch: [114][100/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 1.9548 (1.9258)	Acc@1 73.828 (73.913)	Acc@5 94.922 (95.235)
Epoch: [114][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8977 (1.9279)	Acc@1 74.609 (73.884)	Acc@5 95.703 (95.203)
Epoch: [114][120/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.1323 (1.9325)	Acc@1 65.625 (73.786)	Acc@5 94.141 (95.135)
Epoch: [114][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1040 (1.9413)	Acc@1 72.266 (73.634)	Acc@5 94.922 (94.958)
Epoch: [114][140/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 1.8936 (1.9440)	Acc@1 75.781 (73.598)	Acc@5 96.094 (94.927)
Epoch: [114][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0902 (1.9562)	Acc@1 67.969 (73.311)	Acc@5 94.141 (94.803)
Epoch: [114][160/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 2.1807 (1.9616)	Acc@1 69.531 (73.178)	Acc@5 94.141 (94.737)
Epoch: [114][170/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 2.0272 (1.9676)	Acc@1 72.266 (73.019)	Acc@5 94.531 (94.671)
Epoch: [114][180/196]	Time 0.012 (0.015)	Data 0.011 (0.004)	Loss 2.1475 (1.9770)	Acc@1 69.922 (72.829)	Acc@5 90.625 (94.579)
Epoch: [114][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 1.9980 (1.9832)	Acc@1 71.875 (72.671)	Acc@5 94.922 (94.486)
num momentum params: 26
[0.1, 1.9847396812438964, 2.0217335343360903, 72.63, 50.84, tensor(0.5352, device='cuda:0', grad_fn=<DivBackward0>), 3.0105693340301514, 0.3850874900817871]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [115 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [115][0/196]	Time 0.056 (0.056)	Data 0.215 (0.215)	Loss 2.1036 (2.1036)	Acc@1 70.703 (70.703)	Acc@5 93.750 (93.750)
Epoch: [115][10/196]	Time 0.013 (0.019)	Data 0.003 (0.022)	Loss 1.8074 (1.9395)	Acc@1 78.125 (74.361)	Acc@5 96.094 (95.384)
Epoch: [115][20/196]	Time 0.015 (0.017)	Data 0.003 (0.013)	Loss 1.8626 (1.9304)	Acc@1 72.656 (74.107)	Acc@5 95.312 (95.480)
Epoch: [115][30/196]	Time 0.014 (0.017)	Data 0.003 (0.010)	Loss 1.8281 (1.9212)	Acc@1 76.562 (74.370)	Acc@5 95.703 (95.439)
Epoch: [115][40/196]	Time 0.015 (0.016)	Data 0.002 (0.008)	Loss 1.9064 (1.9076)	Acc@1 75.391 (74.962)	Acc@5 94.531 (95.455)
Epoch: [115][50/196]	Time 0.014 (0.016)	Data 0.002 (0.007)	Loss 2.0503 (1.9164)	Acc@1 71.094 (74.678)	Acc@5 92.578 (95.427)
Epoch: [115][60/196]	Time 0.012 (0.016)	Data 0.009 (0.007)	Loss 1.9050 (1.9215)	Acc@1 74.609 (74.462)	Acc@5 94.922 (95.428)
Epoch: [115][70/196]	Time 0.014 (0.015)	Data 0.003 (0.006)	Loss 1.9677 (1.9267)	Acc@1 76.562 (74.345)	Acc@5 92.969 (95.307)
Epoch: [115][80/196]	Time 0.012 (0.015)	Data 0.008 (0.006)	Loss 2.0194 (1.9337)	Acc@1 71.484 (74.084)	Acc@5 95.312 (95.221)
Epoch: [115][90/196]	Time 0.015 (0.015)	Data 0.002 (0.006)	Loss 2.0065 (1.9440)	Acc@1 72.266 (73.802)	Acc@5 95.312 (95.128)
Epoch: [115][100/196]	Time 0.014 (0.015)	Data 0.002 (0.006)	Loss 2.1080 (1.9455)	Acc@1 70.703 (73.778)	Acc@5 92.188 (95.123)
Epoch: [115][110/196]	Time 0.016 (0.015)	Data 0.000 (0.006)	Loss 1.8640 (1.9514)	Acc@1 77.734 (73.578)	Acc@5 94.922 (95.063)
Epoch: [115][120/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0127 (1.9574)	Acc@1 67.969 (73.347)	Acc@5 96.094 (95.022)
Epoch: [115][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.1480 (1.9638)	Acc@1 67.188 (73.169)	Acc@5 94.141 (94.946)
Epoch: [115][140/196]	Time 0.011 (0.015)	Data 0.005 (0.005)	Loss 1.8606 (1.9663)	Acc@1 76.172 (73.061)	Acc@5 96.875 (94.947)
Epoch: [115][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.1418 (1.9731)	Acc@1 68.359 (72.866)	Acc@5 93.750 (94.880)
Epoch: [115][160/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0140 (1.9794)	Acc@1 72.266 (72.671)	Acc@5 93.359 (94.776)
Epoch: [115][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1449 (1.9884)	Acc@1 66.406 (72.407)	Acc@5 94.141 (94.696)
Epoch: [115][180/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0117 (1.9932)	Acc@1 74.609 (72.279)	Acc@5 93.750 (94.654)
Epoch: [115][190/196]	Time 0.018 (0.015)	Data 0.000 (0.005)	Loss 2.2316 (2.0002)	Acc@1 69.922 (72.167)	Acc@5 91.797 (94.544)
num momentum params: 26
[0.1, 2.004175460205078, 2.0973373782634734, 72.054, 47.47, tensor(0.5305, device='cuda:0', grad_fn=<DivBackward0>), 2.867279052734375, 0.3967406749725341]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [116 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [116][0/196]	Time 0.057 (0.057)	Data 0.209 (0.209)	Loss 1.8954 (1.8954)	Acc@1 74.219 (74.219)	Acc@5 96.094 (96.094)
Epoch: [116][10/196]	Time 0.016 (0.020)	Data 0.002 (0.021)	Loss 2.1386 (1.9834)	Acc@1 66.016 (71.768)	Acc@5 91.797 (95.135)
Epoch: [116][20/196]	Time 0.014 (0.018)	Data 0.002 (0.012)	Loss 1.8484 (1.9471)	Acc@1 76.562 (73.140)	Acc@5 94.922 (95.424)
Epoch: [116][30/196]	Time 0.016 (0.017)	Data 0.003 (0.009)	Loss 1.9041 (1.9478)	Acc@1 75.000 (72.971)	Acc@5 95.703 (95.401)
Epoch: [116][40/196]	Time 0.016 (0.017)	Data 0.006 (0.008)	Loss 1.8766 (1.9270)	Acc@1 77.734 (73.752)	Acc@5 95.703 (95.570)
Epoch: [116][50/196]	Time 0.017 (0.017)	Data 0.001 (0.007)	Loss 1.8611 (1.9154)	Acc@1 76.953 (73.851)	Acc@5 95.312 (95.642)
Epoch: [116][60/196]	Time 0.012 (0.016)	Data 0.006 (0.006)	Loss 1.8476 (1.9107)	Acc@1 78.906 (74.027)	Acc@5 95.312 (95.620)
Epoch: [116][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 2.1159 (1.9195)	Acc@1 68.359 (73.735)	Acc@5 91.797 (95.555)
Epoch: [116][80/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9074 (1.9226)	Acc@1 76.172 (73.785)	Acc@5 95.703 (95.491)
Epoch: [116][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0945 (1.9293)	Acc@1 68.359 (73.738)	Acc@5 92.969 (95.385)
Epoch: [116][100/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 1.9242 (1.9335)	Acc@1 75.781 (73.635)	Acc@5 95.312 (95.343)
Epoch: [116][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.9081 (1.9416)	Acc@1 73.828 (73.455)	Acc@5 95.312 (95.221)
Epoch: [116][120/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9895 (1.9455)	Acc@1 71.875 (73.376)	Acc@5 93.750 (95.196)
Epoch: [116][130/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0961 (1.9533)	Acc@1 67.188 (73.127)	Acc@5 94.141 (95.125)
Epoch: [116][140/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 2.0908 (1.9620)	Acc@1 71.484 (72.931)	Acc@5 93.750 (95.055)
Epoch: [116][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0664 (1.9702)	Acc@1 71.875 (72.729)	Acc@5 94.531 (94.966)
Epoch: [116][160/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 2.1535 (1.9775)	Acc@1 66.016 (72.545)	Acc@5 93.750 (94.820)
Epoch: [116][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1371 (1.9825)	Acc@1 68.359 (72.476)	Acc@5 92.578 (94.748)
Epoch: [116][180/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1096 (1.9876)	Acc@1 69.531 (72.313)	Acc@5 93.359 (94.704)
Epoch: [116][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0655 (1.9940)	Acc@1 71.094 (72.139)	Acc@5 93.359 (94.631)
num momentum params: 26
[0.1, 1.9986206829833983, 1.8971958220005036, 72.04, 54.07, tensor(0.5326, device='cuda:0', grad_fn=<DivBackward0>), 3.0498886108398438, 0.39326000213623047]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [117 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [117][0/196]	Time 0.069 (0.069)	Data 0.208 (0.208)	Loss 1.8304 (1.8304)	Acc@1 77.344 (77.344)	Acc@5 94.531 (94.531)
Epoch: [117][10/196]	Time 0.015 (0.021)	Data 0.002 (0.021)	Loss 1.9457 (1.9574)	Acc@1 72.656 (73.047)	Acc@5 94.922 (94.176)
Epoch: [117][20/196]	Time 0.013 (0.018)	Data 0.005 (0.012)	Loss 1.9912 (1.9237)	Acc@1 72.266 (74.051)	Acc@5 93.750 (94.922)
Epoch: [117][30/196]	Time 0.014 (0.017)	Data 0.002 (0.009)	Loss 1.8715 (1.9158)	Acc@1 77.344 (74.231)	Acc@5 94.922 (95.098)
Epoch: [117][40/196]	Time 0.012 (0.016)	Data 0.006 (0.007)	Loss 1.8533 (1.9175)	Acc@1 73.828 (74.057)	Acc@5 96.875 (95.246)
Epoch: [117][50/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 1.9765 (1.9242)	Acc@1 72.266 (73.813)	Acc@5 95.703 (95.267)
Epoch: [117][60/196]	Time 0.012 (0.016)	Data 0.005 (0.006)	Loss 2.0136 (1.9335)	Acc@1 72.266 (73.623)	Acc@5 93.750 (95.236)
Epoch: [117][70/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9820 (1.9316)	Acc@1 72.266 (73.718)	Acc@5 95.703 (95.257)
Epoch: [117][80/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 2.0376 (1.9369)	Acc@1 73.438 (73.688)	Acc@5 94.531 (95.259)
Epoch: [117][90/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 2.0406 (1.9363)	Acc@1 70.312 (73.656)	Acc@5 92.969 (95.240)
Epoch: [117][100/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 1.9350 (1.9383)	Acc@1 71.094 (73.550)	Acc@5 94.141 (95.138)
Epoch: [117][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 2.0604 (1.9439)	Acc@1 71.094 (73.402)	Acc@5 94.531 (95.077)
Epoch: [117][120/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 1.9524 (1.9482)	Acc@1 71.094 (73.192)	Acc@5 93.750 (95.070)
Epoch: [117][130/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.1986 (1.9532)	Acc@1 70.312 (73.104)	Acc@5 91.016 (94.996)
Epoch: [117][140/196]	Time 0.012 (0.015)	Data 0.012 (0.005)	Loss 1.9966 (1.9600)	Acc@1 70.703 (72.928)	Acc@5 94.531 (94.908)
Epoch: [117][150/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.1739 (1.9701)	Acc@1 64.453 (72.698)	Acc@5 91.797 (94.785)
Epoch: [117][160/196]	Time 0.012 (0.015)	Data 0.012 (0.005)	Loss 2.3295 (1.9829)	Acc@1 64.453 (72.343)	Acc@5 91.406 (94.626)
Epoch: [117][170/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 1.9789 (1.9902)	Acc@1 72.266 (72.142)	Acc@5 95.312 (94.588)
Epoch: [117][180/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 2.1654 (1.9994)	Acc@1 66.406 (71.866)	Acc@5 91.797 (94.469)
Epoch: [117][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1446 (2.0057)	Acc@1 68.359 (71.703)	Acc@5 93.750 (94.370)
num momentum params: 26
[0.1, 2.0076593267822265, 2.2069434940814974, 71.666, 49.79, tensor(0.5306, device='cuda:0', grad_fn=<DivBackward0>), 2.9319965839385986, 0.40129017829895025]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [118 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [118][0/196]	Time 0.065 (0.065)	Data 0.208 (0.208)	Loss 1.8984 (1.8984)	Acc@1 73.828 (73.828)	Acc@5 95.312 (95.312)
Epoch: [118][10/196]	Time 0.015 (0.020)	Data 0.003 (0.022)	Loss 1.9879 (1.9806)	Acc@1 73.047 (73.011)	Acc@5 95.703 (95.206)
Epoch: [118][20/196]	Time 0.014 (0.018)	Data 0.003 (0.012)	Loss 1.8011 (1.9508)	Acc@1 79.297 (73.661)	Acc@5 96.875 (95.499)
Epoch: [118][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.9667 (1.9212)	Acc@1 73.047 (74.496)	Acc@5 95.312 (95.628)
Epoch: [118][40/196]	Time 0.015 (0.017)	Data 0.003 (0.008)	Loss 1.8501 (1.9224)	Acc@1 77.344 (74.581)	Acc@5 95.312 (95.465)
Epoch: [118][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 2.0251 (1.9131)	Acc@1 69.922 (74.617)	Acc@5 94.922 (95.542)
Epoch: [118][60/196]	Time 0.011 (0.016)	Data 0.003 (0.006)	Loss 1.9092 (1.9055)	Acc@1 73.438 (74.693)	Acc@5 96.875 (95.748)
Epoch: [118][70/196]	Time 0.016 (0.016)	Data 0.001 (0.006)	Loss 1.9468 (1.9108)	Acc@1 71.094 (74.609)	Acc@5 94.922 (95.709)
Epoch: [118][80/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.0169 (1.9205)	Acc@1 70.703 (74.349)	Acc@5 94.531 (95.631)
Epoch: [118][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1458 (1.9305)	Acc@1 67.578 (74.013)	Acc@5 92.578 (95.480)
Epoch: [118][100/196]	Time 0.014 (0.015)	Data 0.009 (0.005)	Loss 2.0227 (1.9401)	Acc@1 74.609 (73.708)	Acc@5 93.750 (95.371)
Epoch: [118][110/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0420 (1.9481)	Acc@1 70.703 (73.395)	Acc@5 94.922 (95.312)
Epoch: [118][120/196]	Time 0.012 (0.015)	Data 0.006 (0.005)	Loss 1.9361 (1.9503)	Acc@1 74.219 (73.386)	Acc@5 96.484 (95.258)
Epoch: [118][130/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.1652 (1.9557)	Acc@1 67.578 (73.271)	Acc@5 93.359 (95.172)
Epoch: [118][140/196]	Time 0.013 (0.015)	Data 0.005 (0.005)	Loss 2.0697 (1.9630)	Acc@1 69.922 (73.102)	Acc@5 93.750 (95.058)
Epoch: [118][150/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.9925 (1.9689)	Acc@1 72.266 (72.949)	Acc@5 96.094 (95.015)
Epoch: [118][160/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9816 (1.9729)	Acc@1 71.875 (72.814)	Acc@5 92.969 (94.936)
Epoch: [118][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 2.0843 (1.9779)	Acc@1 69.531 (72.750)	Acc@5 94.922 (94.856)
Epoch: [118][180/196]	Time 0.017 (0.015)	Data 0.002 (0.005)	Loss 2.1783 (1.9869)	Acc@1 69.922 (72.546)	Acc@5 91.406 (94.736)
Epoch: [118][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 2.0683 (1.9931)	Acc@1 68.359 (72.354)	Acc@5 94.141 (94.662)
num momentum params: 26
[0.1, 1.9963526817321777, 1.7649207508563995, 72.248, 55.57, tensor(0.5341, device='cuda:0', grad_fn=<DivBackward0>), 2.9282567501068115, 0.38991379737854004]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [119 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [119][0/196]	Time 0.060 (0.060)	Data 0.204 (0.204)	Loss 1.8245 (1.8245)	Acc@1 76.172 (76.172)	Acc@5 97.266 (97.266)
Epoch: [119][10/196]	Time 0.016 (0.020)	Data 0.003 (0.021)	Loss 2.0389 (1.9144)	Acc@1 71.094 (75.178)	Acc@5 92.578 (95.455)
Epoch: [119][20/196]	Time 0.014 (0.018)	Data 0.003 (0.012)	Loss 1.9569 (1.8909)	Acc@1 72.266 (75.744)	Acc@5 95.703 (96.038)
Epoch: [119][30/196]	Time 0.016 (0.017)	Data 0.003 (0.009)	Loss 1.6865 (1.8763)	Acc@1 84.375 (76.348)	Acc@5 96.094 (96.069)
Epoch: [119][40/196]	Time 0.014 (0.017)	Data 0.002 (0.007)	Loss 1.9521 (1.8702)	Acc@1 75.391 (76.553)	Acc@5 94.922 (96.046)
Epoch: [119][50/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.8918 (1.8657)	Acc@1 75.391 (76.624)	Acc@5 93.359 (96.025)
Epoch: [119][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 2.0041 (1.8751)	Acc@1 72.656 (76.140)	Acc@5 95.312 (95.998)
Epoch: [119][70/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.8417 (1.8789)	Acc@1 76.953 (76.040)	Acc@5 96.484 (95.951)
Epoch: [119][80/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.2547 (1.8932)	Acc@1 66.016 (75.584)	Acc@5 93.359 (95.804)
Epoch: [119][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0454 (1.9085)	Acc@1 70.312 (75.060)	Acc@5 96.094 (95.677)
Epoch: [119][100/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 1.9308 (1.9200)	Acc@1 75.000 (74.783)	Acc@5 95.312 (95.494)
Epoch: [119][110/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 1.9922 (1.9335)	Acc@1 71.875 (74.352)	Acc@5 94.531 (95.320)
Epoch: [119][120/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 1.9737 (1.9406)	Acc@1 72.266 (74.093)	Acc@5 96.094 (95.287)
Epoch: [119][130/196]	Time 0.016 (0.015)	Data 0.002 (0.004)	Loss 2.2847 (1.9506)	Acc@1 67.578 (73.822)	Acc@5 90.625 (95.134)
Epoch: [119][140/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 1.9937 (1.9578)	Acc@1 68.359 (73.507)	Acc@5 94.922 (95.041)
Epoch: [119][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0155 (1.9640)	Acc@1 69.531 (73.238)	Acc@5 95.312 (95.005)
Epoch: [119][160/196]	Time 0.017 (0.015)	Data 0.001 (0.004)	Loss 2.0520 (1.9681)	Acc@1 71.875 (73.137)	Acc@5 92.578 (94.970)
Epoch: [119][170/196]	Time 0.018 (0.015)	Data 0.004 (0.004)	Loss 2.0506 (1.9738)	Acc@1 71.094 (72.992)	Acc@5 95.703 (94.899)
Epoch: [119][180/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 2.0783 (1.9822)	Acc@1 71.094 (72.712)	Acc@5 95.703 (94.827)
Epoch: [119][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.1100 (1.9874)	Acc@1 73.828 (72.607)	Acc@5 92.969 (94.760)
num momentum params: 26
[0.1, 1.989999783630371, 2.00988622546196, 72.554, 50.94, tensor(0.5354, device='cuda:0', grad_fn=<DivBackward0>), 2.9656670093536377, 0.400421142578125]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [508, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [508]
Non Pruning Epoch - module.bn6.bias: [508]
Non Pruning Epoch - module.conv7.weight: [481, 508, 3, 3]
Non Pruning Epoch - module.bn7.weight: [481]
Non Pruning Epoch - module.bn7.bias: [481]
Non Pruning Epoch - module.conv8.weight: [331, 481, 3, 3]
Non Pruning Epoch - module.bn8.weight: [331]
Non Pruning Epoch - module.bn8.bias: [331]
Non Pruning Epoch - module.fc.weight: [100, 331]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [120 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [508, 510, 3, 3]
module.conv7.weight [481, 508, 3, 3]
module.conv8.weight [331, 481, 3, 3]
Epoch: [120][0/196]	Time 0.068 (0.068)	Data 0.219 (0.219)	Loss 1.7746 (1.7746)	Acc@1 78.516 (78.516)	Acc@5 98.047 (98.047)
Epoch: [120][10/196]	Time 0.015 (0.021)	Data 0.003 (0.022)	Loss 2.0211 (1.9028)	Acc@1 71.094 (74.822)	Acc@5 94.141 (95.384)
Epoch: [120][20/196]	Time 0.015 (0.018)	Data 0.003 (0.013)	Loss 1.8709 (1.8930)	Acc@1 76.562 (75.260)	Acc@5 97.656 (95.647)
Epoch: [120][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 1.8280 (1.8651)	Acc@1 76.953 (76.071)	Acc@5 96.094 (95.867)
Epoch: [120][40/196]	Time 0.013 (0.017)	Data 0.005 (0.008)	Loss 1.8376 (1.8616)	Acc@1 75.781 (76.067)	Acc@5 96.094 (96.008)
Epoch: [120][50/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.9891 (1.8703)	Acc@1 73.828 (75.812)	Acc@5 92.969 (95.948)
Epoch: [120][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 1.9008 (1.8853)	Acc@1 74.609 (75.263)	Acc@5 96.484 (95.812)
Epoch: [120][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9115 (1.8969)	Acc@1 73.438 (74.972)	Acc@5 97.266 (95.687)
Epoch: [120][80/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 2.0714 (1.9040)	Acc@1 68.359 (74.764)	Acc@5 95.703 (95.611)
Epoch: [120][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9546 (1.9111)	Acc@1 73.438 (74.635)	Acc@5 92.969 (95.437)
Epoch: [120][100/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9865 (1.9184)	Acc@1 70.312 (74.416)	Acc@5 94.141 (95.363)
Epoch: [120][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9964 (1.9229)	Acc@1 75.000 (74.374)	Acc@5 93.750 (95.256)
Epoch: [120][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0508 (1.9338)	Acc@1 69.531 (73.999)	Acc@5 96.094 (95.180)
Epoch: [120][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0384 (1.9397)	Acc@1 70.312 (73.813)	Acc@5 94.922 (95.125)
Epoch: [120][140/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9782 (1.9497)	Acc@1 73.828 (73.515)	Acc@5 94.531 (95.011)
Epoch: [120][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1362 (1.9569)	Acc@1 66.797 (73.300)	Acc@5 94.531 (94.948)
Epoch: [120][160/196]	Time 0.012 (0.016)	Data 0.009 (0.004)	Loss 1.9906 (1.9604)	Acc@1 72.656 (73.161)	Acc@5 92.969 (94.907)
Epoch: [120][170/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0636 (1.9699)	Acc@1 69.922 (72.878)	Acc@5 92.969 (94.831)
Epoch: [120][180/196]	Time 0.013 (0.015)	Data 0.007 (0.004)	Loss 1.9590 (1.9780)	Acc@1 72.656 (72.665)	Acc@5 94.531 (94.764)
Epoch: [120][190/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 2.2118 (1.9854)	Acc@1 65.625 (72.489)	Acc@5 91.406 (94.705)
num momentum params: 26
[0.1, 1.9863641792297364, 1.7759412837028503, 72.468, 55.85, tensor(0.5359, device='cuda:0', grad_fn=<DivBackward0>), 3.0337917804718018, 0.39201879501342773]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [248, 128, 3, 3]
Before - module.bn3.weight: [248]
Before - module.bn3.bias: [248]
Before - module.conv4.weight: [256, 248, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [508, 510, 3, 3]
Before - module.bn6.weight: [508]
Before - module.bn6.bias: [508]
Before - module.conv7.weight: [481, 508, 3, 3]
Before - module.bn7.weight: [481]
Before - module.bn7.bias: [481]
Before - module.conv8.weight: [331, 481, 3, 3]
Before - module.bn8.weight: [331]
Before - module.bn8.bias: [331]
Before - module.fc.weight: [100, 331]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv6.weight]: [508, 510, 3, 3] >> [507, 510, 3, 3]
[module.bn6.weight]: 508 >> 507
running_mean [507]
running_var [507]
num_batches_tracked []
[module.conv7.weight]: [481, 508, 3, 3] >> [480, 507, 3, 3]
[module.bn7.weight]: 481 >> 480
running_mean [480]
running_var [480]
num_batches_tracked []
[module.conv8.weight]: [331, 481, 3, 3] >> [318, 480, 3, 3]
[module.bn8.weight]: 331 >> 318
running_mean [318]
running_var [318]
num_batches_tracked []
[module.fc.weight]: [100, 331] >> [100, 318]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [248, 128, 3, 3]
After - module.bn3.weight: [248]
After - module.bn3.bias: [248]
After - module.conv4.weight: [256, 248, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [507, 510, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [480, 507, 3, 3]
After - module.bn7.weight: [480]
After - module.bn7.bias: [480]
After - module.conv8.weight: [318, 480, 3, 3]
After - module.bn8.weight: [318]
After - module.bn8.bias: [318]
After - module.fc.weight: [100, 318]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [248, 128, 3, 3]
conv4 --> [256, 248, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [507, 510, 3, 3]
conv7 --> [480, 507, 3, 3]
conv8 --> [318, 480, 3, 3]
fc --> [318, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8337752064, 18284544, 248
4, 16675504128, 36569088, 256
5, 10227548160, 18800640, 510
6, 20255339520, 37234080, 507
7, 6728417280, 8760960, 480
8, 4220190720, 5495040, 318
fc, 12211200, 31800, 0
===================
FLOP REPORT: 28163869200000.0 50998400000.0 138723672 127496 2489 15.272871017456055
[INFO] Storing checkpoint...

Epoch: [121 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [121][0/196]	Time 0.451 (0.451)	Data 0.210 (0.210)	Loss 1.8945 (1.8945)	Acc@1 75.781 (75.781)	Acc@5 94.922 (94.922)
Epoch: [121][10/196]	Time 0.015 (0.057)	Data 0.002 (0.021)	Loss 1.9252 (1.9255)	Acc@1 75.000 (74.254)	Acc@5 94.922 (95.774)
Epoch: [121][20/196]	Time 0.015 (0.037)	Data 0.002 (0.012)	Loss 1.7873 (1.9089)	Acc@1 80.859 (74.833)	Acc@5 96.484 (95.815)
Epoch: [121][30/196]	Time 0.016 (0.030)	Data 0.002 (0.009)	Loss 1.9227 (1.8930)	Acc@1 74.609 (75.189)	Acc@5 96.484 (95.930)
Epoch: [121][40/196]	Time 0.016 (0.027)	Data 0.002 (0.007)	Loss 1.9716 (1.8911)	Acc@1 70.312 (75.029)	Acc@5 95.703 (96.018)
Epoch: [121][50/196]	Time 0.013 (0.025)	Data 0.004 (0.006)	Loss 2.0374 (1.8973)	Acc@1 72.656 (74.747)	Acc@5 91.797 (96.009)
Epoch: [121][60/196]	Time 0.015 (0.023)	Data 0.002 (0.006)	Loss 2.1168 (1.8992)	Acc@1 67.578 (74.449)	Acc@5 94.141 (96.036)
Epoch: [121][70/196]	Time 0.014 (0.022)	Data 0.005 (0.005)	Loss 1.8682 (1.9063)	Acc@1 78.125 (74.422)	Acc@5 97.266 (95.901)
Epoch: [121][80/196]	Time 0.017 (0.021)	Data 0.001 (0.005)	Loss 1.8725 (1.9160)	Acc@1 74.609 (74.166)	Acc@5 95.312 (95.727)
Epoch: [121][90/196]	Time 0.013 (0.021)	Data 0.007 (0.005)	Loss 1.9084 (1.9225)	Acc@1 76.953 (74.094)	Acc@5 94.531 (95.639)
Epoch: [121][100/196]	Time 0.017 (0.020)	Data 0.001 (0.005)	Loss 2.0138 (1.9283)	Acc@1 70.312 (73.882)	Acc@5 92.578 (95.564)
Epoch: [121][110/196]	Time 0.012 (0.020)	Data 0.008 (0.004)	Loss 2.0509 (1.9389)	Acc@1 70.703 (73.635)	Acc@5 93.750 (95.481)
Epoch: [121][120/196]	Time 0.017 (0.020)	Data 0.001 (0.004)	Loss 2.1058 (1.9494)	Acc@1 66.406 (73.344)	Acc@5 93.750 (95.406)
Epoch: [121][130/196]	Time 0.013 (0.019)	Data 0.004 (0.004)	Loss 2.0284 (1.9584)	Acc@1 70.703 (73.172)	Acc@5 93.359 (95.253)
Epoch: [121][140/196]	Time 0.013 (0.019)	Data 0.005 (0.004)	Loss 1.8916 (1.9632)	Acc@1 75.391 (73.033)	Acc@5 95.703 (95.141)
Epoch: [121][150/196]	Time 0.017 (0.019)	Data 0.002 (0.004)	Loss 1.9658 (1.9714)	Acc@1 70.312 (72.866)	Acc@5 94.922 (95.031)
Epoch: [121][160/196]	Time 0.013 (0.019)	Data 0.005 (0.004)	Loss 2.1106 (1.9796)	Acc@1 69.922 (72.693)	Acc@5 92.578 (94.919)
Epoch: [121][170/196]	Time 0.015 (0.018)	Data 0.002 (0.004)	Loss 2.0923 (1.9841)	Acc@1 70.703 (72.643)	Acc@5 94.531 (94.872)
Epoch: [121][180/196]	Time 0.011 (0.018)	Data 0.006 (0.004)	Loss 2.0799 (1.9894)	Acc@1 68.750 (72.427)	Acc@5 94.141 (94.820)
Epoch: [121][190/196]	Time 0.014 (0.018)	Data 0.002 (0.004)	Loss 2.0887 (1.9963)	Acc@1 71.875 (72.245)	Acc@5 93.359 (94.746)
num momentum params: 26
[0.1, 1.9976880201721192, 1.8803476250171662, 72.2, 53.24, tensor(0.5337, device='cuda:0', grad_fn=<DivBackward0>), 3.6781952381134033, 0.4714269638061524]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [122 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [122][0/196]	Time 0.058 (0.058)	Data 0.213 (0.213)	Loss 1.8663 (1.8663)	Acc@1 76.953 (76.953)	Acc@5 97.266 (97.266)
Epoch: [122][10/196]	Time 0.018 (0.021)	Data 0.001 (0.022)	Loss 2.0789 (1.9406)	Acc@1 68.359 (73.651)	Acc@5 95.312 (95.987)
Epoch: [122][20/196]	Time 0.021 (0.020)	Data 0.004 (0.013)	Loss 1.8616 (1.9016)	Acc@1 78.125 (74.628)	Acc@5 97.266 (96.261)
Epoch: [122][30/196]	Time 0.022 (0.019)	Data 0.003 (0.009)	Loss 1.8676 (1.9029)	Acc@1 75.781 (74.786)	Acc@5 94.922 (95.968)
Epoch: [122][40/196]	Time 0.015 (0.018)	Data 0.003 (0.008)	Loss 1.8161 (1.9042)	Acc@1 77.734 (74.714)	Acc@5 97.266 (95.798)
Epoch: [122][50/196]	Time 0.013 (0.017)	Data 0.006 (0.007)	Loss 1.9648 (1.9027)	Acc@1 72.656 (74.809)	Acc@5 94.531 (95.718)
Epoch: [122][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 2.0113 (1.9045)	Acc@1 70.703 (74.699)	Acc@5 94.531 (95.684)
Epoch: [122][70/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.9377 (1.9052)	Acc@1 74.219 (74.785)	Acc@5 94.531 (95.626)
Epoch: [122][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.7449 (1.9035)	Acc@1 75.391 (74.797)	Acc@5 98.828 (95.689)
Epoch: [122][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 2.1454 (1.9198)	Acc@1 70.312 (74.425)	Acc@5 92.578 (95.488)
Epoch: [122][100/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9686 (1.9259)	Acc@1 73.438 (74.230)	Acc@5 96.094 (95.374)
Epoch: [122][110/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0974 (1.9391)	Acc@1 69.531 (73.895)	Acc@5 94.141 (95.214)
Epoch: [122][120/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 1.9641 (1.9455)	Acc@1 72.266 (73.760)	Acc@5 94.531 (95.138)
Epoch: [122][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9436 (1.9528)	Acc@1 71.094 (73.581)	Acc@5 94.922 (95.059)
Epoch: [122][140/196]	Time 0.013 (0.016)	Data 0.011 (0.005)	Loss 2.0582 (1.9592)	Acc@1 70.312 (73.354)	Acc@5 94.141 (95.047)
Epoch: [122][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.2206 (1.9664)	Acc@1 65.234 (73.140)	Acc@5 91.016 (94.963)
Epoch: [122][160/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0583 (1.9726)	Acc@1 71.094 (73.023)	Acc@5 94.531 (94.893)
Epoch: [122][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1963 (1.9805)	Acc@1 68.750 (72.839)	Acc@5 93.359 (94.835)
Epoch: [122][180/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 2.1679 (1.9865)	Acc@1 68.750 (72.691)	Acc@5 92.578 (94.758)
Epoch: [122][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1202 (1.9900)	Acc@1 67.578 (72.579)	Acc@5 92.969 (94.721)
num momentum params: 26
[0.1, 1.9913562690734863, 1.9362187933921815, 72.512, 52.76, tensor(0.5352, device='cuda:0', grad_fn=<DivBackward0>), 3.1278176307678227, 0.3997995853424073]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [123 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [123][0/196]	Time 0.072 (0.072)	Data 0.223 (0.223)	Loss 1.7687 (1.7687)	Acc@1 80.078 (80.078)	Acc@5 95.312 (95.312)
Epoch: [123][10/196]	Time 0.016 (0.022)	Data 0.003 (0.022)	Loss 1.9066 (1.9250)	Acc@1 75.781 (74.751)	Acc@5 95.703 (95.277)
Epoch: [123][20/196]	Time 0.015 (0.019)	Data 0.003 (0.013)	Loss 2.0463 (1.9277)	Acc@1 71.484 (74.516)	Acc@5 93.750 (95.257)
Epoch: [123][30/196]	Time 0.018 (0.018)	Data 0.002 (0.010)	Loss 1.8730 (1.9145)	Acc@1 77.734 (74.849)	Acc@5 96.094 (95.489)
Epoch: [123][40/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.9677 (1.9100)	Acc@1 71.484 (74.905)	Acc@5 96.484 (95.608)
Epoch: [123][50/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.8892 (1.9103)	Acc@1 76.172 (74.763)	Acc@5 96.094 (95.611)
Epoch: [123][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8506 (1.9177)	Acc@1 76.172 (74.533)	Acc@5 96.484 (95.517)
Epoch: [123][70/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.9553 (1.9279)	Acc@1 75.391 (74.202)	Acc@5 95.312 (95.351)
Epoch: [123][80/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9053 (1.9367)	Acc@1 73.828 (73.944)	Acc@5 96.094 (95.279)
Epoch: [123][90/196]	Time 0.013 (0.016)	Data 0.013 (0.005)	Loss 2.0210 (1.9407)	Acc@1 71.484 (73.845)	Acc@5 94.922 (95.201)
Epoch: [123][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8777 (1.9443)	Acc@1 75.391 (73.708)	Acc@5 96.484 (95.131)
Epoch: [123][110/196]	Time 0.012 (0.016)	Data 0.001 (0.005)	Loss 2.0474 (1.9494)	Acc@1 75.391 (73.606)	Acc@5 93.750 (95.059)
Epoch: [123][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1806 (1.9566)	Acc@1 70.312 (73.408)	Acc@5 89.844 (94.941)
Epoch: [123][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.8528 (1.9610)	Acc@1 76.953 (73.345)	Acc@5 94.922 (94.892)
Epoch: [123][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0588 (1.9640)	Acc@1 70.312 (73.246)	Acc@5 92.969 (94.858)
Epoch: [123][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0998 (1.9674)	Acc@1 68.359 (73.179)	Acc@5 91.797 (94.811)
Epoch: [123][160/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0767 (1.9752)	Acc@1 71.875 (72.972)	Acc@5 94.141 (94.733)
Epoch: [123][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.1574 (1.9801)	Acc@1 66.406 (72.846)	Acc@5 91.016 (94.693)
Epoch: [123][180/196]	Time 0.020 (0.016)	Data 0.001 (0.004)	Loss 2.0582 (1.9828)	Acc@1 69.922 (72.753)	Acc@5 93.750 (94.643)
Epoch: [123][190/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9984 (1.9869)	Acc@1 69.922 (72.613)	Acc@5 93.750 (94.623)
num momentum params: 26
[0.1, 1.9891209274291992, 1.8962194097042084, 72.554, 53.25, tensor(0.5355, device='cuda:0', grad_fn=<DivBackward0>), 3.1236574649810795, 0.4062044620513916]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [124 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [124][0/196]	Time 0.069 (0.069)	Data 0.212 (0.212)	Loss 1.9107 (1.9107)	Acc@1 74.219 (74.219)	Acc@5 96.484 (96.484)
Epoch: [124][10/196]	Time 0.016 (0.022)	Data 0.002 (0.022)	Loss 1.9236 (1.9399)	Acc@1 75.391 (74.751)	Acc@5 94.922 (95.206)
Epoch: [124][20/196]	Time 0.015 (0.019)	Data 0.002 (0.012)	Loss 1.9414 (1.9194)	Acc@1 74.609 (74.795)	Acc@5 96.094 (95.294)
Epoch: [124][30/196]	Time 0.018 (0.018)	Data 0.001 (0.009)	Loss 1.9585 (1.9183)	Acc@1 74.609 (74.887)	Acc@5 95.703 (95.262)
Epoch: [124][40/196]	Time 0.016 (0.017)	Data 0.003 (0.008)	Loss 1.8047 (1.9150)	Acc@1 76.562 (74.914)	Acc@5 97.656 (95.293)
Epoch: [124][50/196]	Time 0.016 (0.017)	Data 0.001 (0.007)	Loss 1.7974 (1.9108)	Acc@1 77.344 (74.900)	Acc@5 96.484 (95.458)
Epoch: [124][60/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 2.0357 (1.9147)	Acc@1 71.875 (74.942)	Acc@5 94.531 (95.441)
Epoch: [124][70/196]	Time 0.021 (0.017)	Data 0.001 (0.006)	Loss 2.0361 (1.9282)	Acc@1 71.875 (74.582)	Acc@5 93.359 (95.246)
Epoch: [124][80/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 1.9867 (1.9323)	Acc@1 71.875 (74.354)	Acc@5 94.141 (95.269)
Epoch: [124][90/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 1.9023 (1.9324)	Acc@1 75.781 (74.266)	Acc@5 96.094 (95.222)
Epoch: [124][100/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 1.9946 (1.9357)	Acc@1 71.875 (74.145)	Acc@5 93.359 (95.181)
Epoch: [124][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0179 (1.9393)	Acc@1 73.047 (74.008)	Acc@5 93.359 (95.133)
Epoch: [124][120/196]	Time 0.020 (0.016)	Data 0.001 (0.005)	Loss 1.9877 (1.9459)	Acc@1 73.438 (73.809)	Acc@5 94.922 (95.103)
Epoch: [124][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9568 (1.9515)	Acc@1 71.094 (73.688)	Acc@5 96.484 (95.005)
Epoch: [124][140/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1333 (1.9563)	Acc@1 71.094 (73.529)	Acc@5 94.141 (95.013)
Epoch: [124][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0458 (1.9623)	Acc@1 71.875 (73.368)	Acc@5 93.359 (94.901)
Epoch: [124][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1109 (1.9682)	Acc@1 71.875 (73.241)	Acc@5 92.969 (94.810)
Epoch: [124][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.2046 (1.9736)	Acc@1 67.188 (73.111)	Acc@5 94.141 (94.778)
Epoch: [124][180/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 2.0368 (1.9771)	Acc@1 71.484 (72.956)	Acc@5 94.531 (94.747)
Epoch: [124][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0388 (1.9830)	Acc@1 70.312 (72.707)	Acc@5 93.750 (94.730)
num momentum params: 26
[0.1, 1.9854815019226075, 1.8331740832328796, 72.608, 54.96, tensor(0.5358, device='cuda:0', grad_fn=<DivBackward0>), 3.198850154876709, 0.4015190601348877]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [125 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [125][0/196]	Time 0.063 (0.063)	Data 0.212 (0.212)	Loss 2.0129 (2.0129)	Acc@1 71.484 (71.484)	Acc@5 94.531 (94.531)
Epoch: [125][10/196]	Time 0.015 (0.020)	Data 0.002 (0.022)	Loss 1.9093 (1.9864)	Acc@1 72.656 (72.834)	Acc@5 96.484 (95.348)
Epoch: [125][20/196]	Time 0.015 (0.018)	Data 0.002 (0.012)	Loss 1.9805 (1.9652)	Acc@1 70.312 (73.289)	Acc@5 96.875 (95.406)
Epoch: [125][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 1.9411 (1.9452)	Acc@1 71.484 (73.677)	Acc@5 96.094 (95.602)
Epoch: [125][40/196]	Time 0.016 (0.017)	Data 0.003 (0.008)	Loss 1.9714 (1.9336)	Acc@1 73.828 (74.085)	Acc@5 94.141 (95.655)
Epoch: [125][50/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 1.9150 (1.9325)	Acc@1 72.656 (74.004)	Acc@5 96.094 (95.581)
Epoch: [125][60/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.8593 (1.9329)	Acc@1 77.734 (74.020)	Acc@5 96.875 (95.517)
Epoch: [125][70/196]	Time 0.016 (0.016)	Data 0.004 (0.006)	Loss 1.9729 (1.9402)	Acc@1 75.391 (73.801)	Acc@5 96.094 (95.351)
Epoch: [125][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.1099 (1.9455)	Acc@1 68.359 (73.640)	Acc@5 92.578 (95.264)
Epoch: [125][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9833 (1.9480)	Acc@1 69.922 (73.588)	Acc@5 96.094 (95.261)
Epoch: [125][100/196]	Time 0.018 (0.016)	Data 0.003 (0.005)	Loss 2.0136 (1.9501)	Acc@1 72.266 (73.565)	Acc@5 92.969 (95.162)
Epoch: [125][110/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9087 (1.9541)	Acc@1 75.391 (73.438)	Acc@5 95.312 (95.154)
Epoch: [125][120/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 1.9439 (1.9572)	Acc@1 73.828 (73.360)	Acc@5 96.094 (95.080)
Epoch: [125][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9616 (1.9582)	Acc@1 72.266 (73.360)	Acc@5 95.703 (95.071)
Epoch: [125][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1585 (1.9638)	Acc@1 70.312 (73.144)	Acc@5 91.797 (95.013)
Epoch: [125][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0110 (1.9711)	Acc@1 71.094 (72.972)	Acc@5 95.312 (94.917)
Epoch: [125][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1340 (1.9801)	Acc@1 70.312 (72.751)	Acc@5 92.188 (94.786)
Epoch: [125][170/196]	Time 0.017 (0.016)	Data 0.009 (0.004)	Loss 2.0787 (1.9866)	Acc@1 70.312 (72.608)	Acc@5 92.188 (94.698)
Epoch: [125][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0522 (1.9934)	Acc@1 71.875 (72.391)	Acc@5 92.578 (94.602)
Epoch: [125][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0831 (1.9974)	Acc@1 70.703 (72.300)	Acc@5 93.359 (94.550)
num momentum params: 26
[0.1, 1.9987530181884765, 1.687274911403656, 72.28, 57.27, tensor(0.5330, device='cuda:0', grad_fn=<DivBackward0>), 3.142348527908325, 0.4095602035522461]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [126 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [126][0/196]	Time 0.062 (0.062)	Data 0.210 (0.210)	Loss 1.9910 (1.9910)	Acc@1 75.391 (75.391)	Acc@5 95.312 (95.312)
Epoch: [126][10/196]	Time 0.014 (0.020)	Data 0.004 (0.022)	Loss 1.8784 (1.9229)	Acc@1 75.781 (74.503)	Acc@5 96.094 (95.739)
Epoch: [126][20/196]	Time 0.016 (0.018)	Data 0.004 (0.013)	Loss 1.8643 (1.8863)	Acc@1 72.656 (75.149)	Acc@5 96.875 (96.354)
Epoch: [126][30/196]	Time 0.013 (0.017)	Data 0.005 (0.010)	Loss 1.7659 (1.8760)	Acc@1 79.297 (75.391)	Acc@5 96.094 (96.245)
Epoch: [126][40/196]	Time 0.013 (0.017)	Data 0.005 (0.008)	Loss 1.8190 (1.8794)	Acc@1 76.562 (75.381)	Acc@5 95.703 (96.151)
Epoch: [126][50/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 2.0804 (1.8755)	Acc@1 70.703 (75.452)	Acc@5 92.188 (96.140)
Epoch: [126][60/196]	Time 0.020 (0.017)	Data 0.001 (0.006)	Loss 2.0538 (1.8823)	Acc@1 70.312 (75.410)	Acc@5 94.531 (96.062)
Epoch: [126][70/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0446 (1.8895)	Acc@1 74.219 (75.231)	Acc@5 93.750 (95.918)
Epoch: [126][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9166 (1.8953)	Acc@1 73.438 (75.024)	Acc@5 96.875 (95.838)
Epoch: [126][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.1388 (1.9036)	Acc@1 66.016 (74.815)	Acc@5 92.578 (95.746)
Epoch: [126][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0271 (1.9206)	Acc@1 67.969 (74.288)	Acc@5 93.750 (95.545)
Epoch: [126][110/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9759 (1.9275)	Acc@1 71.484 (74.089)	Acc@5 92.969 (95.457)
Epoch: [126][120/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 1.9709 (1.9365)	Acc@1 71.875 (73.883)	Acc@5 94.531 (95.332)
Epoch: [126][130/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 2.1053 (1.9476)	Acc@1 70.703 (73.542)	Acc@5 93.359 (95.211)
Epoch: [126][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1029 (1.9544)	Acc@1 67.969 (73.377)	Acc@5 94.531 (95.132)
Epoch: [126][150/196]	Time 0.013 (0.016)	Data 0.007 (0.004)	Loss 2.1400 (1.9605)	Acc@1 68.359 (73.233)	Acc@5 92.188 (95.046)
Epoch: [126][160/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0567 (1.9641)	Acc@1 70.312 (73.141)	Acc@5 95.703 (95.014)
Epoch: [126][170/196]	Time 0.016 (0.016)	Data 0.005 (0.004)	Loss 2.1445 (1.9716)	Acc@1 68.750 (72.910)	Acc@5 94.922 (94.945)
Epoch: [126][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1252 (1.9814)	Acc@1 67.578 (72.622)	Acc@5 94.531 (94.848)
Epoch: [126][190/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.0736 (1.9877)	Acc@1 72.266 (72.503)	Acc@5 94.922 (94.807)
num momentum params: 26
[0.1, 1.9905823610687257, 1.839796905517578, 72.416, 53.95, tensor(0.5361, device='cuda:0', grad_fn=<DivBackward0>), 3.103228569030762, 0.41612768173217773]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [127 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [127][0/196]	Time 0.064 (0.064)	Data 0.213 (0.213)	Loss 1.7480 (1.7480)	Acc@1 80.859 (80.859)	Acc@5 96.875 (96.875)
Epoch: [127][10/196]	Time 0.016 (0.021)	Data 0.003 (0.022)	Loss 1.9558 (1.9323)	Acc@1 72.266 (75.178)	Acc@5 96.094 (95.810)
Epoch: [127][20/196]	Time 0.013 (0.018)	Data 0.004 (0.013)	Loss 1.8687 (1.9140)	Acc@1 74.219 (75.167)	Acc@5 96.484 (95.852)
Epoch: [127][30/196]	Time 0.020 (0.018)	Data 0.002 (0.009)	Loss 1.9970 (1.9098)	Acc@1 73.438 (75.353)	Acc@5 95.312 (95.880)
Epoch: [127][40/196]	Time 0.015 (0.017)	Data 0.003 (0.008)	Loss 1.9504 (1.9042)	Acc@1 75.000 (75.381)	Acc@5 95.312 (95.741)
Epoch: [127][50/196]	Time 0.016 (0.017)	Data 0.003 (0.007)	Loss 1.8159 (1.9013)	Acc@1 75.000 (75.414)	Acc@5 97.656 (95.718)
Epoch: [127][60/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8882 (1.8974)	Acc@1 72.656 (75.320)	Acc@5 94.141 (95.716)
Epoch: [127][70/196]	Time 0.014 (0.017)	Data 0.008 (0.006)	Loss 1.8262 (1.8961)	Acc@1 75.781 (75.231)	Acc@5 96.484 (95.764)
Epoch: [127][80/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 1.9533 (1.8951)	Acc@1 75.000 (75.222)	Acc@5 92.969 (95.693)
Epoch: [127][90/196]	Time 0.014 (0.016)	Data 0.016 (0.005)	Loss 2.0807 (1.8974)	Acc@1 70.703 (75.193)	Acc@5 91.797 (95.656)
Epoch: [127][100/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 1.9757 (1.9144)	Acc@1 72.656 (74.776)	Acc@5 93.750 (95.382)
Epoch: [127][110/196]	Time 0.013 (0.017)	Data 0.009 (0.005)	Loss 1.9219 (1.9238)	Acc@1 75.000 (74.543)	Acc@5 94.922 (95.309)
Epoch: [127][120/196]	Time 0.017 (0.017)	Data 0.001 (0.004)	Loss 2.0442 (1.9370)	Acc@1 71.875 (74.161)	Acc@5 94.531 (95.170)
Epoch: [127][130/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1304 (1.9472)	Acc@1 69.531 (73.870)	Acc@5 93.359 (95.080)
Epoch: [127][140/196]	Time 0.017 (0.016)	Data 0.002 (0.004)	Loss 2.1003 (1.9567)	Acc@1 70.312 (73.631)	Acc@5 93.750 (94.991)
Epoch: [127][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0585 (1.9641)	Acc@1 69.922 (73.409)	Acc@5 96.094 (94.896)
Epoch: [127][160/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9397 (1.9696)	Acc@1 76.172 (73.314)	Acc@5 95.312 (94.827)
Epoch: [127][170/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.2045 (1.9747)	Acc@1 67.578 (73.138)	Acc@5 90.625 (94.799)
Epoch: [127][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0128 (1.9796)	Acc@1 69.922 (72.999)	Acc@5 93.750 (94.738)
Epoch: [127][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1232 (1.9836)	Acc@1 71.875 (72.883)	Acc@5 92.578 (94.699)
num momentum params: 26
[0.1, 1.9864559874725343, 1.7937027680873872, 72.816, 54.94, tensor(0.5376, device='cuda:0', grad_fn=<DivBackward0>), 3.146797657012939, 0.4277060031890869]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [128 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [128][0/196]	Time 0.065 (0.065)	Data 0.228 (0.228)	Loss 1.7109 (1.7109)	Acc@1 79.688 (79.688)	Acc@5 98.828 (98.828)
Epoch: [128][10/196]	Time 0.015 (0.021)	Data 0.002 (0.023)	Loss 1.8220 (1.9513)	Acc@1 76.562 (73.935)	Acc@5 97.266 (95.312)
Epoch: [128][20/196]	Time 0.015 (0.018)	Data 0.003 (0.013)	Loss 1.8509 (1.9184)	Acc@1 78.516 (74.535)	Acc@5 96.875 (95.499)
Epoch: [128][30/196]	Time 0.014 (0.018)	Data 0.004 (0.010)	Loss 1.8887 (1.9092)	Acc@1 76.562 (74.861)	Acc@5 95.312 (95.741)
Epoch: [128][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9144 (1.8955)	Acc@1 71.875 (75.171)	Acc@5 96.094 (95.827)
Epoch: [128][50/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 1.9132 (1.8951)	Acc@1 76.172 (75.268)	Acc@5 94.922 (95.665)
Epoch: [128][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8198 (1.8938)	Acc@1 75.781 (75.275)	Acc@5 96.875 (95.639)
Epoch: [128][70/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9969 (1.9009)	Acc@1 71.484 (75.193)	Acc@5 95.312 (95.555)
Epoch: [128][80/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.9530 (1.9056)	Acc@1 73.438 (75.063)	Acc@5 94.141 (95.544)
Epoch: [128][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0283 (1.9136)	Acc@1 71.875 (74.712)	Acc@5 95.703 (95.501)
Epoch: [128][100/196]	Time 0.024 (0.016)	Data 0.002 (0.005)	Loss 2.0897 (1.9212)	Acc@1 69.922 (74.474)	Acc@5 94.141 (95.394)
Epoch: [128][110/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 1.9747 (1.9304)	Acc@1 71.875 (74.173)	Acc@5 94.922 (95.288)
Epoch: [128][120/196]	Time 0.025 (0.016)	Data 0.003 (0.004)	Loss 2.1008 (1.9360)	Acc@1 71.094 (74.057)	Acc@5 94.531 (95.296)
Epoch: [128][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.8076 (1.9438)	Acc@1 77.344 (73.810)	Acc@5 97.656 (95.208)
Epoch: [128][140/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0590 (1.9500)	Acc@1 68.359 (73.662)	Acc@5 95.312 (95.127)
Epoch: [128][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0468 (1.9542)	Acc@1 71.875 (73.567)	Acc@5 93.359 (95.051)
Epoch: [128][160/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 1.9145 (1.9594)	Acc@1 74.609 (73.450)	Acc@5 96.875 (95.004)
Epoch: [128][170/196]	Time 0.012 (0.016)	Data 0.024 (0.004)	Loss 1.9501 (1.9620)	Acc@1 75.391 (73.358)	Acc@5 94.531 (94.990)
Epoch: [128][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0155 (1.9666)	Acc@1 73.047 (73.243)	Acc@5 92.969 (94.924)
Epoch: [128][190/196]	Time 0.011 (0.016)	Data 0.009 (0.004)	Loss 2.2009 (1.9716)	Acc@1 66.797 (73.078)	Acc@5 92.578 (94.897)
num momentum params: 26
[0.1, 1.974069418029785, 1.9754189825057984, 72.998, 52.11, tensor(0.5398, device='cuda:0', grad_fn=<DivBackward0>), 3.111727952957153, 0.4185078144073487]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [129 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [129][0/196]	Time 0.076 (0.076)	Data 0.216 (0.216)	Loss 1.9565 (1.9565)	Acc@1 74.219 (74.219)	Acc@5 95.312 (95.312)
Epoch: [129][10/196]	Time 0.017 (0.021)	Data 0.003 (0.022)	Loss 2.0017 (1.9399)	Acc@1 70.703 (73.970)	Acc@5 95.312 (95.526)
Epoch: [129][20/196]	Time 0.012 (0.019)	Data 0.002 (0.013)	Loss 2.0170 (1.9445)	Acc@1 71.875 (73.382)	Acc@5 92.578 (95.461)
Epoch: [129][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.8954 (1.9153)	Acc@1 76.172 (73.929)	Acc@5 94.922 (95.754)
Epoch: [129][40/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.9019 (1.9258)	Acc@1 73.438 (73.619)	Acc@5 95.703 (95.694)
Epoch: [129][50/196]	Time 0.013 (0.017)	Data 0.005 (0.007)	Loss 1.8567 (1.9246)	Acc@1 77.734 (73.775)	Acc@5 95.703 (95.619)
Epoch: [129][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.7854 (1.9147)	Acc@1 80.469 (74.078)	Acc@5 94.922 (95.690)
Epoch: [129][70/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8624 (1.9091)	Acc@1 76.953 (74.252)	Acc@5 96.875 (95.720)
Epoch: [129][80/196]	Time 0.014 (0.017)	Data 0.004 (0.005)	Loss 2.1056 (1.9094)	Acc@1 71.484 (74.383)	Acc@5 92.969 (95.660)
Epoch: [129][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 1.8928 (1.9164)	Acc@1 72.656 (74.099)	Acc@5 96.094 (95.592)
Epoch: [129][100/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0608 (1.9282)	Acc@1 72.266 (73.859)	Acc@5 93.359 (95.374)
Epoch: [129][110/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.1063 (1.9370)	Acc@1 68.359 (73.561)	Acc@5 94.141 (95.295)
Epoch: [129][120/196]	Time 0.012 (0.016)	Data 0.012 (0.004)	Loss 1.9998 (1.9438)	Acc@1 70.703 (73.386)	Acc@5 94.141 (95.174)
Epoch: [129][130/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0472 (1.9529)	Acc@1 69.922 (73.130)	Acc@5 94.922 (95.038)
Epoch: [129][140/196]	Time 0.016 (0.016)	Data 0.007 (0.004)	Loss 2.0099 (1.9585)	Acc@1 67.578 (72.975)	Acc@5 94.922 (94.980)
Epoch: [129][150/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 2.1778 (1.9679)	Acc@1 69.922 (72.731)	Acc@5 92.188 (94.852)
Epoch: [129][160/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 1.9862 (1.9759)	Acc@1 69.531 (72.547)	Acc@5 96.094 (94.774)
Epoch: [129][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1126 (1.9817)	Acc@1 68.750 (72.505)	Acc@5 92.969 (94.689)
Epoch: [129][180/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1669 (1.9875)	Acc@1 67.969 (72.356)	Acc@5 94.922 (94.659)
Epoch: [129][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0106 (1.9919)	Acc@1 73.047 (72.270)	Acc@5 92.969 (94.609)
num momentum params: 26
[0.1, 1.9951994845581054, 1.7814820873737336, 72.172, 55.82, tensor(0.5342, device='cuda:0', grad_fn=<DivBackward0>), 3.12354326248169, 0.4177076816558838]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [318, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [318]
Non Pruning Epoch - module.bn8.bias: [318]
Non Pruning Epoch - module.fc.weight: [100, 318]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [130 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [318, 480, 3, 3]
Epoch: [130][0/196]	Time 0.066 (0.066)	Data 0.219 (0.219)	Loss 1.9153 (1.9153)	Acc@1 78.125 (78.125)	Acc@5 95.703 (95.703)
Epoch: [130][10/196]	Time 0.015 (0.021)	Data 0.007 (0.022)	Loss 1.9459 (1.9567)	Acc@1 73.828 (73.722)	Acc@5 95.312 (94.922)
Epoch: [130][20/196]	Time 0.016 (0.019)	Data 0.002 (0.013)	Loss 1.9311 (1.9390)	Acc@1 72.656 (73.977)	Acc@5 94.141 (94.754)
Epoch: [130][30/196]	Time 0.012 (0.017)	Data 0.005 (0.010)	Loss 1.8157 (1.9148)	Acc@1 76.172 (74.521)	Acc@5 97.266 (95.086)
Epoch: [130][40/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.9003 (1.9039)	Acc@1 74.609 (74.867)	Acc@5 96.875 (95.284)
Epoch: [130][50/196]	Time 0.016 (0.017)	Data 0.003 (0.007)	Loss 1.8693 (1.8960)	Acc@1 75.781 (75.031)	Acc@5 97.266 (95.420)
Epoch: [130][60/196]	Time 0.013 (0.017)	Data 0.008 (0.006)	Loss 1.9026 (1.8964)	Acc@1 76.953 (75.109)	Acc@5 95.703 (95.357)
Epoch: [130][70/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.8937 (1.8991)	Acc@1 75.781 (74.983)	Acc@5 96.484 (95.368)
Epoch: [130][80/196]	Time 0.013 (0.017)	Data 0.012 (0.006)	Loss 1.9839 (1.9002)	Acc@1 70.703 (74.923)	Acc@5 96.484 (95.452)
Epoch: [130][90/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 1.9302 (1.9080)	Acc@1 74.609 (74.755)	Acc@5 94.141 (95.403)
Epoch: [130][100/196]	Time 0.014 (0.017)	Data 0.008 (0.005)	Loss 1.8228 (1.9138)	Acc@1 77.344 (74.656)	Acc@5 96.484 (95.282)
Epoch: [130][110/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.9316 (1.9199)	Acc@1 73.828 (74.497)	Acc@5 94.141 (95.228)
Epoch: [130][120/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 2.0759 (1.9293)	Acc@1 66.797 (74.283)	Acc@5 94.531 (95.170)
Epoch: [130][130/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0410 (1.9396)	Acc@1 69.531 (73.974)	Acc@5 94.531 (95.095)
Epoch: [130][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 2.1306 (1.9515)	Acc@1 67.969 (73.557)	Acc@5 94.922 (94.994)
Epoch: [130][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0991 (1.9618)	Acc@1 71.094 (73.256)	Acc@5 93.359 (94.860)
Epoch: [130][160/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1448 (1.9690)	Acc@1 68.359 (73.027)	Acc@5 93.750 (94.796)
Epoch: [130][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1188 (1.9734)	Acc@1 70.703 (72.919)	Acc@5 93.750 (94.744)
Epoch: [130][180/196]	Time 0.013 (0.016)	Data 0.008 (0.004)	Loss 2.0101 (1.9797)	Acc@1 69.922 (72.756)	Acc@5 95.703 (94.717)
Epoch: [130][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.2537 (1.9871)	Acc@1 65.625 (72.493)	Acc@5 92.188 (94.683)
num momentum params: 26
[0.1, 1.9895268293762207, 2.0553259658813476, 72.426, 50.47, tensor(0.5368, device='cuda:0', grad_fn=<DivBackward0>), 3.1662797927856445, 0.40133786201477056]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [248, 128, 3, 3]
Before - module.bn3.weight: [248]
Before - module.bn3.bias: [248]
Before - module.conv4.weight: [256, 248, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [507, 510, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [480, 507, 3, 3]
Before - module.bn7.weight: [480]
Before - module.bn7.bias: [480]
Before - module.conv8.weight: [318, 480, 3, 3]
Before - module.bn8.weight: [318]
Before - module.bn8.bias: [318]
Before - module.fc.weight: [100, 318]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv8.weight]: [318, 480, 3, 3] >> [307, 480, 3, 3]
[module.bn8.weight]: 318 >> 307
running_mean [307]
running_var [307]
num_batches_tracked []
[module.fc.weight]: [100, 318] >> [100, 307]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [248, 128, 3, 3]
After - module.bn3.weight: [248]
After - module.bn3.bias: [248]
After - module.conv4.weight: [256, 248, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [507, 510, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [480, 507, 3, 3]
After - module.bn7.weight: [480]
After - module.bn7.bias: [480]
After - module.conv8.weight: [307, 480, 3, 3]
After - module.bn8.weight: [307]
After - module.bn8.bias: [307]
After - module.fc.weight: [100, 307]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [248, 128, 3, 3]
conv4 --> [256, 248, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [507, 510, 3, 3]
conv7 --> [480, 507, 3, 3]
conv8 --> [307, 480, 3, 3]
fc --> [307, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8337752064, 18284544, 248
4, 16675504128, 36569088, 256
5, 10227548160, 18800640, 510
6, 20255339520, 37234080, 507
7, 6728417280, 8760960, 480
8, 4074209280, 5304960, 307
fc, 11788800, 30700, 0
===================
FLOP REPORT: 28106680200000.0 50980800000.0 138532492 127452 2478 15.180093765258789
[INFO] Storing checkpoint...

Epoch: [131 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [131][0/196]	Time 0.158 (0.158)	Data 0.230 (0.230)	Loss 1.8140 (1.8140)	Acc@1 78.906 (78.906)	Acc@5 96.484 (96.484)
Epoch: [131][10/196]	Time 0.018 (0.030)	Data 0.002 (0.022)	Loss 1.9275 (1.9525)	Acc@1 73.047 (73.757)	Acc@5 97.266 (95.206)
Epoch: [131][20/196]	Time 0.014 (0.023)	Data 0.003 (0.013)	Loss 2.0396 (1.9628)	Acc@1 71.484 (73.661)	Acc@5 95.312 (94.885)
Epoch: [131][30/196]	Time 0.015 (0.021)	Data 0.002 (0.010)	Loss 1.9144 (1.9314)	Acc@1 72.266 (74.244)	Acc@5 96.094 (95.174)
Epoch: [131][40/196]	Time 0.015 (0.019)	Data 0.002 (0.008)	Loss 1.9585 (1.9330)	Acc@1 75.000 (74.247)	Acc@5 94.141 (95.151)
Epoch: [131][50/196]	Time 0.016 (0.019)	Data 0.001 (0.007)	Loss 1.9710 (1.9283)	Acc@1 73.828 (74.357)	Acc@5 94.141 (95.067)
Epoch: [131][60/196]	Time 0.016 (0.018)	Data 0.018 (0.007)	Loss 2.0444 (1.9278)	Acc@1 71.484 (74.430)	Acc@5 93.359 (95.127)
Epoch: [131][70/196]	Time 0.018 (0.018)	Data 0.001 (0.006)	Loss 2.1005 (1.9348)	Acc@1 68.750 (74.153)	Acc@5 93.359 (95.081)
Epoch: [131][80/196]	Time 0.012 (0.017)	Data 0.006 (0.006)	Loss 1.8494 (1.9405)	Acc@1 75.000 (73.929)	Acc@5 95.312 (95.052)
Epoch: [131][90/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 1.9853 (1.9412)	Acc@1 71.484 (73.914)	Acc@5 94.141 (95.085)
Epoch: [131][100/196]	Time 0.012 (0.017)	Data 0.012 (0.005)	Loss 1.8411 (1.9464)	Acc@1 75.391 (73.832)	Acc@5 98.828 (95.065)
Epoch: [131][110/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 1.9712 (1.9510)	Acc@1 73.828 (73.719)	Acc@5 94.922 (95.042)
Epoch: [131][120/196]	Time 0.013 (0.017)	Data 0.006 (0.005)	Loss 2.0595 (1.9543)	Acc@1 71.094 (73.609)	Acc@5 91.016 (95.012)
Epoch: [131][130/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 1.9853 (1.9548)	Acc@1 74.609 (73.599)	Acc@5 94.141 (94.946)
Epoch: [131][140/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 2.0317 (1.9564)	Acc@1 70.312 (73.526)	Acc@5 95.703 (94.944)
Epoch: [131][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9860 (1.9610)	Acc@1 73.828 (73.316)	Acc@5 94.141 (94.930)
Epoch: [131][160/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0727 (1.9661)	Acc@1 68.359 (73.112)	Acc@5 92.578 (94.873)
Epoch: [131][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9372 (1.9764)	Acc@1 73.047 (72.878)	Acc@5 96.094 (94.762)
Epoch: [131][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.2210 (1.9820)	Acc@1 67.188 (72.712)	Acc@5 92.969 (94.700)
Epoch: [131][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.1720 (1.9884)	Acc@1 67.578 (72.507)	Acc@5 95.312 (94.666)
num momentum params: 26
[0.1, 1.990388863067627, 1.9056170403957366, 72.454, 53.53, tensor(0.5367, device='cuda:0', grad_fn=<DivBackward0>), 3.179175853729248, 0.42455291748046875]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [132 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [132][0/196]	Time 0.070 (0.070)	Data 0.220 (0.220)	Loss 2.0079 (2.0079)	Acc@1 74.609 (74.609)	Acc@5 93.750 (93.750)
Epoch: [132][10/196]	Time 0.017 (0.022)	Data 0.002 (0.022)	Loss 1.9259 (2.0321)	Acc@1 73.047 (71.733)	Acc@5 94.531 (93.430)
Epoch: [132][20/196]	Time 0.015 (0.020)	Data 0.003 (0.013)	Loss 1.9427 (1.9666)	Acc@1 72.656 (73.289)	Acc@5 95.703 (94.513)
Epoch: [132][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.9290 (1.9457)	Acc@1 75.781 (73.702)	Acc@5 93.750 (94.909)
Epoch: [132][40/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.8648 (1.9272)	Acc@1 75.000 (74.133)	Acc@5 94.141 (95.103)
Epoch: [132][50/196]	Time 0.014 (0.017)	Data 0.003 (0.007)	Loss 1.7662 (1.9154)	Acc@1 77.734 (74.510)	Acc@5 96.484 (95.389)
Epoch: [132][60/196]	Time 0.015 (0.017)	Data 0.004 (0.006)	Loss 1.8589 (1.9099)	Acc@1 74.609 (74.526)	Acc@5 96.094 (95.453)
Epoch: [132][70/196]	Time 0.013 (0.017)	Data 0.004 (0.006)	Loss 2.0209 (1.9078)	Acc@1 71.094 (74.631)	Acc@5 94.141 (95.478)
Epoch: [132][80/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.8569 (1.9108)	Acc@1 75.781 (74.518)	Acc@5 94.141 (95.409)
Epoch: [132][90/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 1.9986 (1.9193)	Acc@1 71.875 (74.249)	Acc@5 95.312 (95.300)
Epoch: [132][100/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0067 (1.9248)	Acc@1 71.094 (74.134)	Acc@5 94.922 (95.254)
Epoch: [132][110/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 1.9764 (1.9305)	Acc@1 73.047 (73.983)	Acc@5 92.578 (95.200)
Epoch: [132][120/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9462 (1.9349)	Acc@1 72.656 (73.835)	Acc@5 93.750 (95.170)
Epoch: [132][130/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.0522 (1.9423)	Acc@1 70.703 (73.593)	Acc@5 92.969 (95.128)
Epoch: [132][140/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 1.9055 (1.9506)	Acc@1 73.828 (73.368)	Acc@5 97.656 (95.030)
Epoch: [132][150/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0777 (1.9598)	Acc@1 69.141 (73.081)	Acc@5 94.141 (94.932)
Epoch: [132][160/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.0632 (1.9677)	Acc@1 71.484 (72.882)	Acc@5 94.141 (94.842)
Epoch: [132][170/196]	Time 0.013 (0.016)	Data 0.007 (0.004)	Loss 2.0713 (1.9692)	Acc@1 69.531 (72.860)	Acc@5 94.141 (94.865)
Epoch: [132][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1392 (1.9743)	Acc@1 67.188 (72.710)	Acc@5 95.312 (94.820)
Epoch: [132][190/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0913 (1.9787)	Acc@1 72.266 (72.619)	Acc@5 92.188 (94.785)
num momentum params: 26
[0.1, 1.9799121095275878, 1.8019419717788696, 72.6, 54.4, tensor(0.5398, device='cuda:0', grad_fn=<DivBackward0>), 3.1251602172851562, 0.4152185916900635]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [133 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [133][0/196]	Time 0.070 (0.070)	Data 0.234 (0.234)	Loss 1.9824 (1.9824)	Acc@1 74.609 (74.609)	Acc@5 95.312 (95.312)
Epoch: [133][10/196]	Time 0.015 (0.020)	Data 0.003 (0.023)	Loss 1.8986 (1.9485)	Acc@1 75.781 (74.254)	Acc@5 95.703 (95.312)
Epoch: [133][20/196]	Time 0.014 (0.018)	Data 0.004 (0.014)	Loss 1.9263 (1.9071)	Acc@1 72.656 (75.353)	Acc@5 95.312 (95.889)
Epoch: [133][30/196]	Time 0.013 (0.017)	Data 0.004 (0.010)	Loss 1.8488 (1.8968)	Acc@1 75.781 (75.504)	Acc@5 95.312 (95.854)
Epoch: [133][40/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 1.8277 (1.8981)	Acc@1 80.078 (75.333)	Acc@5 96.094 (95.684)
Epoch: [133][50/196]	Time 0.015 (0.016)	Data 0.002 (0.007)	Loss 1.8702 (1.9042)	Acc@1 78.516 (75.322)	Acc@5 94.922 (95.634)
Epoch: [133][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9050 (1.9044)	Acc@1 73.438 (75.243)	Acc@5 96.484 (95.645)
Epoch: [133][70/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.8742 (1.9119)	Acc@1 76.562 (74.840)	Acc@5 97.656 (95.698)
Epoch: [133][80/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0883 (1.9289)	Acc@1 69.141 (74.455)	Acc@5 93.359 (95.496)
Epoch: [133][90/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0080 (1.9352)	Acc@1 69.922 (74.167)	Acc@5 96.484 (95.471)
Epoch: [133][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0530 (1.9408)	Acc@1 72.266 (73.975)	Acc@5 94.531 (95.421)
Epoch: [133][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0840 (1.9474)	Acc@1 70.312 (73.694)	Acc@5 94.531 (95.344)
Epoch: [133][120/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 2.0694 (1.9558)	Acc@1 70.312 (73.402)	Acc@5 93.750 (95.245)
Epoch: [133][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9288 (1.9621)	Acc@1 70.703 (73.196)	Acc@5 96.094 (95.143)
Epoch: [133][140/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.9486 (1.9657)	Acc@1 75.781 (73.116)	Acc@5 94.141 (95.069)
Epoch: [133][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1353 (1.9703)	Acc@1 67.578 (72.993)	Acc@5 92.578 (94.981)
Epoch: [133][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.2075 (1.9734)	Acc@1 64.844 (72.923)	Acc@5 90.625 (94.924)
Epoch: [133][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.9492 (1.9772)	Acc@1 75.000 (72.814)	Acc@5 95.703 (94.874)
Epoch: [133][180/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0522 (1.9847)	Acc@1 69.141 (72.546)	Acc@5 92.969 (94.790)
Epoch: [133][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0548 (1.9915)	Acc@1 71.484 (72.339)	Acc@5 92.969 (94.705)
num momentum params: 26
[0.1, 1.9924136625671387, 1.9007141888141632, 72.324, 53.52, tensor(0.5370, device='cuda:0', grad_fn=<DivBackward0>), 3.097630739212036, 0.4109969139099121]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [134 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [134][0/196]	Time 0.066 (0.066)	Data 0.220 (0.220)	Loss 1.8263 (1.8263)	Acc@1 76.953 (76.953)	Acc@5 96.484 (96.484)
Epoch: [134][10/196]	Time 0.015 (0.021)	Data 0.002 (0.023)	Loss 1.9990 (1.9181)	Acc@1 69.922 (75.213)	Acc@5 96.875 (95.419)
Epoch: [134][20/196]	Time 0.017 (0.018)	Data 0.001 (0.014)	Loss 1.7153 (1.8904)	Acc@1 81.250 (75.893)	Acc@5 98.047 (95.629)
Epoch: [134][30/196]	Time 0.017 (0.018)	Data 0.002 (0.010)	Loss 2.0440 (1.8973)	Acc@1 74.609 (75.605)	Acc@5 92.188 (95.691)
Epoch: [134][40/196]	Time 0.015 (0.017)	Data 0.004 (0.009)	Loss 1.8115 (1.8866)	Acc@1 76.562 (75.705)	Acc@5 97.266 (95.894)
Epoch: [134][50/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8950 (1.8807)	Acc@1 77.344 (75.766)	Acc@5 96.484 (95.956)
Epoch: [134][60/196]	Time 0.013 (0.017)	Data 0.005 (0.007)	Loss 1.9461 (1.8839)	Acc@1 69.922 (75.519)	Acc@5 97.656 (95.927)
Epoch: [134][70/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.7487 (1.8897)	Acc@1 79.688 (75.341)	Acc@5 98.047 (95.841)
Epoch: [134][80/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.9850 (1.8980)	Acc@1 71.094 (75.019)	Acc@5 97.266 (95.795)
Epoch: [134][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9388 (1.9087)	Acc@1 73.438 (74.798)	Acc@5 95.312 (95.643)
Epoch: [134][100/196]	Time 0.014 (0.016)	Data 0.012 (0.005)	Loss 2.1228 (1.9261)	Acc@1 71.875 (74.281)	Acc@5 91.406 (95.479)
Epoch: [134][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0255 (1.9357)	Acc@1 72.266 (73.994)	Acc@5 95.703 (95.390)
Epoch: [134][120/196]	Time 0.013 (0.016)	Data 0.009 (0.005)	Loss 2.0401 (1.9424)	Acc@1 71.484 (73.838)	Acc@5 94.141 (95.287)
Epoch: [134][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0579 (1.9459)	Acc@1 71.094 (73.754)	Acc@5 94.922 (95.244)
Epoch: [134][140/196]	Time 0.017 (0.016)	Data 0.005 (0.005)	Loss 2.0012 (1.9536)	Acc@1 69.922 (73.462)	Acc@5 96.484 (95.188)
Epoch: [134][150/196]	Time 0.020 (0.016)	Data 0.001 (0.004)	Loss 2.0486 (1.9587)	Acc@1 72.266 (73.355)	Acc@5 92.578 (95.116)
Epoch: [134][160/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.9878 (1.9613)	Acc@1 71.484 (73.263)	Acc@5 94.922 (95.050)
Epoch: [134][170/196]	Time 0.014 (0.016)	Data 0.010 (0.004)	Loss 2.0017 (1.9650)	Acc@1 71.094 (73.184)	Acc@5 93.359 (95.016)
Epoch: [134][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0031 (1.9700)	Acc@1 71.484 (73.049)	Acc@5 93.750 (94.963)
Epoch: [134][190/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.0895 (1.9772)	Acc@1 68.359 (72.806)	Acc@5 93.750 (94.899)
num momentum params: 26
[0.1, 1.9803142614746094, 1.748078384399414, 72.698, 55.81, tensor(0.5404, device='cuda:0', grad_fn=<DivBackward0>), 3.0994646549224854, 0.41344451904296875]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [135 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [135][0/196]	Time 0.067 (0.067)	Data 0.225 (0.225)	Loss 1.8363 (1.8363)	Acc@1 78.125 (78.125)	Acc@5 96.484 (96.484)
Epoch: [135][10/196]	Time 0.018 (0.022)	Data 0.002 (0.022)	Loss 2.0146 (1.9353)	Acc@1 75.391 (74.112)	Acc@5 93.750 (95.632)
Epoch: [135][20/196]	Time 0.020 (0.019)	Data 0.001 (0.013)	Loss 1.8928 (1.9237)	Acc@1 74.609 (74.386)	Acc@5 96.484 (95.871)
Epoch: [135][30/196]	Time 0.021 (0.019)	Data 0.003 (0.010)	Loss 2.0433 (1.9101)	Acc@1 69.141 (75.113)	Acc@5 94.141 (96.069)
Epoch: [135][40/196]	Time 0.017 (0.018)	Data 0.002 (0.008)	Loss 1.8161 (1.8972)	Acc@1 75.391 (75.086)	Acc@5 96.875 (96.160)
Epoch: [135][50/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.9484 (1.8948)	Acc@1 75.391 (75.207)	Acc@5 95.703 (96.094)
Epoch: [135][60/196]	Time 0.016 (0.017)	Data 0.001 (0.006)	Loss 1.8527 (1.8934)	Acc@1 76.172 (75.339)	Acc@5 96.094 (95.959)
Epoch: [135][70/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 1.8981 (1.8918)	Acc@1 72.266 (75.182)	Acc@5 96.484 (95.929)
Epoch: [135][80/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 2.0207 (1.8983)	Acc@1 74.609 (75.010)	Acc@5 93.359 (95.804)
Epoch: [135][90/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 2.2222 (1.9098)	Acc@1 64.844 (74.687)	Acc@5 91.406 (95.656)
Epoch: [135][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0221 (1.9206)	Acc@1 71.484 (74.366)	Acc@5 93.359 (95.552)
Epoch: [135][110/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0477 (1.9309)	Acc@1 68.750 (74.145)	Acc@5 91.016 (95.330)
Epoch: [135][120/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1567 (1.9397)	Acc@1 66.797 (73.896)	Acc@5 93.359 (95.261)
Epoch: [135][130/196]	Time 0.012 (0.016)	Data 0.006 (0.005)	Loss 2.0288 (1.9459)	Acc@1 69.141 (73.763)	Acc@5 94.141 (95.196)
Epoch: [135][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0094 (1.9524)	Acc@1 70.703 (73.557)	Acc@5 94.141 (95.121)
Epoch: [135][150/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 2.0409 (1.9613)	Acc@1 71.875 (73.306)	Acc@5 96.484 (95.059)
Epoch: [135][160/196]	Time 0.012 (0.016)	Data 0.001 (0.005)	Loss 2.1155 (1.9677)	Acc@1 72.656 (73.144)	Acc@5 94.531 (94.973)
Epoch: [135][170/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 2.1124 (1.9719)	Acc@1 70.703 (73.024)	Acc@5 90.625 (94.894)
Epoch: [135][180/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1026 (1.9730)	Acc@1 69.531 (73.025)	Acc@5 93.359 (94.853)
Epoch: [135][190/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 2.1312 (1.9784)	Acc@1 68.359 (72.836)	Acc@5 93.750 (94.807)
num momentum params: 26
[0.1, 1.9798697495269775, 1.9344738984107972, 72.832, 53.98, tensor(0.5395, device='cuda:0', grad_fn=<DivBackward0>), 3.0476877689361572, 0.40602731704711914]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [136 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [136][0/196]	Time 0.073 (0.073)	Data 0.195 (0.195)	Loss 1.7808 (1.7808)	Acc@1 77.734 (77.734)	Acc@5 98.047 (98.047)
Epoch: [136][10/196]	Time 0.015 (0.022)	Data 0.002 (0.020)	Loss 1.8557 (1.8987)	Acc@1 76.953 (75.178)	Acc@5 96.094 (96.413)
Epoch: [136][20/196]	Time 0.013 (0.019)	Data 0.006 (0.012)	Loss 1.8405 (1.9298)	Acc@1 75.781 (73.847)	Acc@5 97.266 (95.945)
Epoch: [136][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.8821 (1.9268)	Acc@1 74.609 (73.879)	Acc@5 96.484 (95.968)
Epoch: [136][40/196]	Time 0.013 (0.017)	Data 0.005 (0.007)	Loss 1.8016 (1.9248)	Acc@1 78.906 (74.104)	Acc@5 96.484 (95.856)
Epoch: [136][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 1.8964 (1.9182)	Acc@1 78.125 (74.372)	Acc@5 95.703 (95.849)
Epoch: [136][60/196]	Time 0.014 (0.017)	Data 0.005 (0.006)	Loss 1.8905 (1.9202)	Acc@1 74.609 (74.315)	Acc@5 96.094 (95.774)
Epoch: [136][70/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 2.0111 (1.9236)	Acc@1 71.484 (74.202)	Acc@5 94.141 (95.687)
Epoch: [136][80/196]	Time 0.013 (0.016)	Data 0.012 (0.005)	Loss 2.0453 (1.9281)	Acc@1 74.219 (74.117)	Acc@5 94.141 (95.578)
Epoch: [136][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.1269 (1.9335)	Acc@1 69.141 (73.965)	Acc@5 92.578 (95.480)
Epoch: [136][100/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.9507 (1.9368)	Acc@1 72.266 (73.863)	Acc@5 94.922 (95.432)
Epoch: [136][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0393 (1.9411)	Acc@1 69.531 (73.705)	Acc@5 94.531 (95.436)
Epoch: [136][120/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0226 (1.9450)	Acc@1 71.484 (73.541)	Acc@5 94.922 (95.403)
Epoch: [136][130/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 1.9520 (1.9465)	Acc@1 73.438 (73.506)	Acc@5 94.141 (95.366)
Epoch: [136][140/196]	Time 0.013 (0.016)	Data 0.015 (0.004)	Loss 1.9919 (1.9509)	Acc@1 69.141 (73.363)	Acc@5 94.922 (95.299)
Epoch: [136][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0910 (1.9571)	Acc@1 71.094 (73.176)	Acc@5 91.016 (95.222)
Epoch: [136][160/196]	Time 0.014 (0.016)	Data 0.009 (0.004)	Loss 2.1340 (1.9656)	Acc@1 66.406 (72.926)	Acc@5 91.016 (95.075)
Epoch: [136][170/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 2.1683 (1.9746)	Acc@1 70.312 (72.707)	Acc@5 93.359 (94.961)
Epoch: [136][180/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 1.9731 (1.9792)	Acc@1 69.141 (72.563)	Acc@5 96.094 (94.894)
Epoch: [136][190/196]	Time 0.018 (0.016)	Data 0.000 (0.004)	Loss 2.0502 (1.9867)	Acc@1 70.703 (72.401)	Acc@5 92.188 (94.811)
num momentum params: 26
[0.1, 1.9890402579498292, 2.1103659200668337, 72.348, 51.07, tensor(0.5371, device='cuda:0', grad_fn=<DivBackward0>), 3.163039445877075, 0.40893149375915533]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [137 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [137][0/196]	Time 0.074 (0.074)	Data 0.217 (0.217)	Loss 1.8269 (1.8269)	Acc@1 78.125 (78.125)	Acc@5 96.484 (96.484)
Epoch: [137][10/196]	Time 0.016 (0.023)	Data 0.002 (0.022)	Loss 2.0576 (1.9776)	Acc@1 70.703 (72.159)	Acc@5 93.750 (95.384)
Epoch: [137][20/196]	Time 0.014 (0.020)	Data 0.003 (0.012)	Loss 1.8407 (1.9570)	Acc@1 76.172 (73.158)	Acc@5 96.094 (95.368)
Epoch: [137][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.9239 (1.9377)	Acc@1 74.219 (73.904)	Acc@5 97.266 (95.665)
Epoch: [137][40/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.8939 (1.9298)	Acc@1 74.609 (74.066)	Acc@5 94.141 (95.722)
Epoch: [137][50/196]	Time 0.014 (0.017)	Data 0.005 (0.007)	Loss 1.8044 (1.9257)	Acc@1 81.250 (74.134)	Acc@5 94.531 (95.741)
Epoch: [137][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 2.0379 (1.9258)	Acc@1 70.312 (74.129)	Acc@5 95.312 (95.684)
Epoch: [137][70/196]	Time 0.015 (0.017)	Data 0.004 (0.005)	Loss 1.8650 (1.9176)	Acc@1 77.344 (74.450)	Acc@5 94.922 (95.698)
Epoch: [137][80/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 1.8906 (1.9197)	Acc@1 75.781 (74.378)	Acc@5 96.484 (95.631)
Epoch: [137][90/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9824 (1.9171)	Acc@1 71.484 (74.378)	Acc@5 96.094 (95.673)
Epoch: [137][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.8567 (1.9191)	Acc@1 77.344 (74.312)	Acc@5 95.703 (95.637)
Epoch: [137][110/196]	Time 0.014 (0.016)	Data 0.006 (0.005)	Loss 2.0377 (1.9248)	Acc@1 68.359 (74.201)	Acc@5 95.312 (95.534)
Epoch: [137][120/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0153 (1.9306)	Acc@1 71.875 (74.009)	Acc@5 95.312 (95.471)
Epoch: [137][130/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9765 (1.9345)	Acc@1 73.047 (73.921)	Acc@5 93.750 (95.384)
Epoch: [137][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9799 (1.9401)	Acc@1 71.484 (73.762)	Acc@5 96.484 (95.340)
Epoch: [137][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1055 (1.9494)	Acc@1 71.094 (73.500)	Acc@5 94.531 (95.217)
Epoch: [137][160/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1619 (1.9606)	Acc@1 68.750 (73.256)	Acc@5 91.797 (95.065)
Epoch: [137][170/196]	Time 0.018 (0.016)	Data 0.003 (0.004)	Loss 2.1176 (1.9661)	Acc@1 70.312 (73.134)	Acc@5 92.969 (95.009)
Epoch: [137][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9123 (1.9718)	Acc@1 77.344 (72.997)	Acc@5 93.750 (94.961)
Epoch: [137][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0338 (1.9778)	Acc@1 73.047 (72.861)	Acc@5 96.094 (94.906)
num momentum params: 26
[0.1, 1.980637451019287, 1.9203897285461426, 72.766, 53.43, tensor(0.5410, device='cuda:0', grad_fn=<DivBackward0>), 3.1361544132232666, 0.4236595630645752]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [138 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [138][0/196]	Time 0.077 (0.077)	Data 0.216 (0.216)	Loss 2.1336 (2.1336)	Acc@1 70.312 (70.312)	Acc@5 93.359 (93.359)
Epoch: [138][10/196]	Time 0.016 (0.022)	Data 0.002 (0.021)	Loss 1.9456 (1.9878)	Acc@1 75.000 (73.935)	Acc@5 94.531 (94.567)
Epoch: [138][20/196]	Time 0.015 (0.020)	Data 0.005 (0.012)	Loss 1.8791 (1.9477)	Acc@1 76.562 (74.740)	Acc@5 95.703 (95.052)
Epoch: [138][30/196]	Time 0.020 (0.019)	Data 0.005 (0.009)	Loss 1.9200 (1.9315)	Acc@1 73.438 (74.887)	Acc@5 96.484 (95.186)
Epoch: [138][40/196]	Time 0.012 (0.018)	Data 0.005 (0.008)	Loss 1.9269 (1.9185)	Acc@1 72.656 (75.010)	Acc@5 97.656 (95.474)
Epoch: [138][50/196]	Time 0.014 (0.018)	Data 0.004 (0.007)	Loss 2.0927 (1.9141)	Acc@1 69.531 (75.069)	Acc@5 94.141 (95.550)
Epoch: [138][60/196]	Time 0.013 (0.017)	Data 0.005 (0.006)	Loss 1.9229 (1.9062)	Acc@1 73.047 (75.307)	Acc@5 94.531 (95.645)
Epoch: [138][70/196]	Time 0.014 (0.017)	Data 0.004 (0.006)	Loss 1.9378 (1.9109)	Acc@1 72.266 (75.121)	Acc@5 96.484 (95.538)
Epoch: [138][80/196]	Time 0.012 (0.017)	Data 0.011 (0.005)	Loss 2.0495 (1.9157)	Acc@1 69.531 (74.865)	Acc@5 95.312 (95.501)
Epoch: [138][90/196]	Time 0.016 (0.017)	Data 0.003 (0.005)	Loss 1.9087 (1.9191)	Acc@1 73.047 (74.721)	Acc@5 97.656 (95.536)
Epoch: [138][100/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9071 (1.9206)	Acc@1 76.172 (74.660)	Acc@5 94.922 (95.467)
Epoch: [138][110/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0301 (1.9265)	Acc@1 73.438 (74.493)	Acc@5 94.141 (95.344)
Epoch: [138][120/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.1065 (1.9348)	Acc@1 67.969 (74.228)	Acc@5 94.922 (95.303)
Epoch: [138][130/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9546 (1.9420)	Acc@1 73.047 (74.013)	Acc@5 93.750 (95.235)
Epoch: [138][140/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 2.0079 (1.9476)	Acc@1 75.000 (73.872)	Acc@5 95.703 (95.196)
Epoch: [138][150/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 2.0586 (1.9559)	Acc@1 71.875 (73.639)	Acc@5 93.750 (95.095)
Epoch: [138][160/196]	Time 0.014 (0.016)	Data 0.018 (0.004)	Loss 2.0760 (1.9655)	Acc@1 69.531 (73.362)	Acc@5 95.312 (94.995)
Epoch: [138][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0248 (1.9724)	Acc@1 72.266 (73.161)	Acc@5 95.703 (94.940)
Epoch: [138][180/196]	Time 0.014 (0.016)	Data 0.006 (0.004)	Loss 2.0018 (1.9770)	Acc@1 72.266 (73.045)	Acc@5 93.750 (94.892)
Epoch: [138][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0601 (1.9852)	Acc@1 71.484 (72.810)	Acc@5 93.750 (94.779)
num momentum params: 26
[0.1, 1.9876315159606934, 1.8512501525878906, 72.764, 53.47, tensor(0.5398, device='cuda:0', grad_fn=<DivBackward0>), 3.102293014526367, 0.4047262668609619]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [139 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [139][0/196]	Time 0.076 (0.076)	Data 0.224 (0.224)	Loss 1.8937 (1.8937)	Acc@1 76.172 (76.172)	Acc@5 94.531 (94.531)
Epoch: [139][10/196]	Time 0.013 (0.023)	Data 0.002 (0.022)	Loss 2.0881 (1.9779)	Acc@1 70.703 (73.366)	Acc@5 93.359 (94.886)
Epoch: [139][20/196]	Time 0.015 (0.020)	Data 0.003 (0.013)	Loss 1.8586 (1.9309)	Acc@1 77.344 (74.461)	Acc@5 96.875 (95.499)
Epoch: [139][30/196]	Time 0.015 (0.018)	Data 0.002 (0.010)	Loss 1.9075 (1.9177)	Acc@1 73.438 (74.836)	Acc@5 93.750 (95.615)
Epoch: [139][40/196]	Time 0.015 (0.018)	Data 0.002 (0.008)	Loss 1.9220 (1.9126)	Acc@1 73.047 (74.771)	Acc@5 97.656 (95.703)
Epoch: [139][50/196]	Time 0.018 (0.017)	Data 0.002 (0.007)	Loss 1.8123 (1.9131)	Acc@1 76.562 (74.586)	Acc@5 96.094 (95.657)
Epoch: [139][60/196]	Time 0.014 (0.017)	Data 0.004 (0.006)	Loss 1.8532 (1.9143)	Acc@1 77.344 (74.533)	Acc@5 94.531 (95.645)
Epoch: [139][70/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 2.0612 (1.9197)	Acc@1 67.188 (74.307)	Acc@5 94.531 (95.615)
Epoch: [139][80/196]	Time 0.015 (0.017)	Data 0.003 (0.005)	Loss 1.9978 (1.9298)	Acc@1 71.094 (74.074)	Acc@5 94.141 (95.505)
Epoch: [139][90/196]	Time 0.018 (0.017)	Data 0.003 (0.005)	Loss 1.9275 (1.9331)	Acc@1 71.875 (73.871)	Acc@5 95.703 (95.501)
Epoch: [139][100/196]	Time 0.018 (0.016)	Data 0.002 (0.005)	Loss 1.9916 (1.9414)	Acc@1 71.484 (73.646)	Acc@5 94.922 (95.440)
Epoch: [139][110/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 1.9050 (1.9446)	Acc@1 75.781 (73.666)	Acc@5 96.484 (95.411)
Epoch: [139][120/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.0157 (1.9478)	Acc@1 72.266 (73.567)	Acc@5 94.922 (95.367)
Epoch: [139][130/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9523 (1.9537)	Acc@1 75.000 (73.449)	Acc@5 94.922 (95.256)
Epoch: [139][140/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.0686 (1.9583)	Acc@1 70.703 (73.269)	Acc@5 92.578 (95.185)
Epoch: [139][150/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9384 (1.9639)	Acc@1 72.656 (73.192)	Acc@5 95.703 (95.082)
Epoch: [139][160/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.9481 (1.9676)	Acc@1 72.656 (73.091)	Acc@5 95.703 (95.029)
Epoch: [139][170/196]	Time 0.015 (0.016)	Data 0.012 (0.005)	Loss 2.0535 (1.9771)	Acc@1 69.922 (72.866)	Acc@5 94.531 (94.897)
Epoch: [139][180/196]	Time 0.024 (0.016)	Data 0.003 (0.005)	Loss 2.1798 (1.9833)	Acc@1 65.234 (72.727)	Acc@5 92.188 (94.801)
Epoch: [139][190/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 2.1344 (1.9885)	Acc@1 69.922 (72.605)	Acc@5 92.969 (94.777)
num momentum params: 26
[0.1, 1.9920648847961426, 1.8384956216812134, 72.482, 53.86, tensor(0.5383, device='cuda:0', grad_fn=<DivBackward0>), 3.099146604537964, 0.41770410537719727]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [248, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [248]
Non Pruning Epoch - module.bn3.bias: [248]
Non Pruning Epoch - module.conv4.weight: [256, 248, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [480, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [480]
Non Pruning Epoch - module.bn7.bias: [480]
Non Pruning Epoch - module.conv8.weight: [307, 480, 3, 3]
Non Pruning Epoch - module.bn8.weight: [307]
Non Pruning Epoch - module.bn8.bias: [307]
Non Pruning Epoch - module.fc.weight: [100, 307]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [140 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [248, 128, 3, 3]
module.conv4.weight [256, 248, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [480, 507, 3, 3]
module.conv8.weight [307, 480, 3, 3]
Epoch: [140][0/196]	Time 0.075 (0.075)	Data 0.238 (0.238)	Loss 1.8261 (1.8261)	Acc@1 76.562 (76.562)	Acc@5 95.703 (95.703)
Epoch: [140][10/196]	Time 0.015 (0.022)	Data 0.002 (0.024)	Loss 2.0217 (1.9965)	Acc@1 71.875 (72.443)	Acc@5 94.141 (94.531)
Epoch: [140][20/196]	Time 0.015 (0.019)	Data 0.004 (0.013)	Loss 1.9583 (1.9675)	Acc@1 74.219 (73.140)	Acc@5 95.703 (95.312)
Epoch: [140][30/196]	Time 0.018 (0.018)	Data 0.001 (0.010)	Loss 1.8917 (1.9363)	Acc@1 75.781 (74.042)	Acc@5 96.875 (95.413)
Epoch: [140][40/196]	Time 0.012 (0.017)	Data 0.006 (0.008)	Loss 1.8088 (1.9304)	Acc@1 79.688 (74.428)	Acc@5 94.531 (95.398)
Epoch: [140][50/196]	Time 0.016 (0.017)	Data 0.001 (0.007)	Loss 1.9573 (1.9365)	Acc@1 71.484 (74.119)	Acc@5 95.703 (95.389)
Epoch: [140][60/196]	Time 0.012 (0.017)	Data 0.017 (0.007)	Loss 1.9039 (1.9337)	Acc@1 76.172 (74.187)	Acc@5 96.484 (95.364)
Epoch: [140][70/196]	Time 0.017 (0.017)	Data 0.001 (0.007)	Loss 1.9890 (1.9330)	Acc@1 75.000 (74.340)	Acc@5 94.531 (95.263)
Epoch: [140][80/196]	Time 0.014 (0.017)	Data 0.008 (0.006)	Loss 1.9071 (1.9345)	Acc@1 78.125 (74.330)	Acc@5 94.922 (95.226)
Epoch: [140][90/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.9312 (1.9353)	Acc@1 71.875 (74.335)	Acc@5 98.047 (95.209)
Epoch: [140][100/196]	Time 0.013 (0.017)	Data 0.008 (0.006)	Loss 2.0503 (1.9335)	Acc@1 69.922 (74.389)	Acc@5 96.094 (95.293)
Epoch: [140][110/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 2.0528 (1.9423)	Acc@1 71.484 (74.074)	Acc@5 92.969 (95.232)
Epoch: [140][120/196]	Time 0.020 (0.017)	Data 0.015 (0.005)	Loss 1.9459 (1.9446)	Acc@1 74.609 (73.999)	Acc@5 94.922 (95.167)
Epoch: [140][130/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 2.0656 (1.9527)	Acc@1 71.094 (73.783)	Acc@5 91.406 (95.083)
Epoch: [140][140/196]	Time 0.014 (0.017)	Data 0.009 (0.005)	Loss 2.1346 (1.9644)	Acc@1 69.141 (73.446)	Acc@5 93.750 (94.994)
Epoch: [140][150/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 2.2517 (1.9754)	Acc@1 67.578 (73.163)	Acc@5 92.969 (94.852)
Epoch: [140][160/196]	Time 0.016 (0.017)	Data 0.004 (0.005)	Loss 2.1479 (1.9799)	Acc@1 64.844 (73.069)	Acc@5 93.750 (94.767)
Epoch: [140][170/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 2.0829 (1.9857)	Acc@1 71.094 (72.951)	Acc@5 92.969 (94.703)
Epoch: [140][180/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 2.1483 (1.9911)	Acc@1 70.703 (72.842)	Acc@5 91.797 (94.622)
Epoch: [140][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0647 (1.9947)	Acc@1 73.828 (72.742)	Acc@5 93.359 (94.595)
num momentum params: 26
[0.1, 1.9972753657531739, 1.908379077911377, 72.668, 52.49, tensor(0.5368, device='cuda:0', grad_fn=<DivBackward0>), 3.2043421268463135, 0.40480828285217285]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [248, 128, 3, 3]
Before - module.bn3.weight: [248]
Before - module.bn3.bias: [248]
Before - module.conv4.weight: [256, 248, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [507, 510, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [480, 507, 3, 3]
Before - module.bn7.weight: [480]
Before - module.bn7.bias: [480]
Before - module.conv8.weight: [307, 480, 3, 3]
Before - module.bn8.weight: [307]
Before - module.bn8.bias: [307]
Before - module.fc.weight: [100, 307]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv3.weight]: [248, 128, 3, 3] >> [247, 128, 3, 3]
[module.bn3.weight]: 248 >> 247
running_mean [247]
running_var [247]
num_batches_tracked []
[module.conv4.weight]: [256, 248, 3, 3] >> [256, 247, 3, 3]
[module.conv7.weight]: [480, 507, 3, 3] >> [479, 507, 3, 3]
[module.bn7.weight]: 480 >> 479
running_mean [479]
running_var [479]
num_batches_tracked []
[module.conv8.weight]: [307, 480, 3, 3] >> [300, 479, 3, 3]
[module.bn8.weight]: 307 >> 300
running_mean [300]
running_var [300]
num_batches_tracked []
[module.fc.weight]: [100, 307] >> [100, 300]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [247, 128, 3, 3]
After - module.bn3.weight: [247]
After - module.bn3.bias: [247]
After - module.conv4.weight: [256, 247, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [510, 256, 3, 3]
After - module.bn5.weight: [510]
After - module.bn5.bias: [510]
After - module.conv6.weight: [507, 510, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [479, 507, 3, 3]
After - module.bn7.weight: [479]
After - module.bn7.bias: [479]
After - module.conv8.weight: [300, 479, 3, 3]
After - module.bn8.weight: [300]
After - module.bn8.bias: [300]
After - module.fc.weight: [100, 300]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [247, 128, 3, 3]
conv4 --> [256, 247, 3, 3]
conv5 --> [510, 256, 3, 3]
conv6 --> [507, 510, 3, 3]
conv7 --> [479, 507, 3, 3]
conv8 --> [300, 479, 3, 3]
fc --> [300, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8304132096, 18210816, 247
4, 16608264192, 36421632, 256
5, 10227548160, 18800640, 510
6, 20255339520, 37234080, 507
7, 6714399744, 8742708, 479
8, 3973017600, 5173200, 300
fc, 11520000, 30000, 0
===================
FLOP REPORT: 28022173200000.0 50942400000.0 138160596 127356 2469 15.100605010986328
[INFO] Storing checkpoint...

Epoch: [141 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [141][0/196]	Time 0.407 (0.407)	Data 0.269 (0.269)	Loss 1.9400 (1.9400)	Acc@1 77.344 (77.344)	Acc@5 93.750 (93.750)
Epoch: [141][10/196]	Time 0.015 (0.052)	Data 0.002 (0.026)	Loss 1.9512 (1.9368)	Acc@1 77.344 (75.426)	Acc@5 95.312 (95.455)
Epoch: [141][20/196]	Time 0.015 (0.035)	Data 0.003 (0.015)	Loss 1.9103 (1.9382)	Acc@1 74.219 (74.740)	Acc@5 97.266 (95.368)
Epoch: [141][30/196]	Time 0.016 (0.028)	Data 0.005 (0.011)	Loss 1.7685 (1.9243)	Acc@1 78.125 (75.328)	Acc@5 96.484 (95.502)
Epoch: [141][40/196]	Time 0.015 (0.025)	Data 0.004 (0.009)	Loss 1.9559 (1.9237)	Acc@1 72.656 (75.057)	Acc@5 96.484 (95.436)
Epoch: [141][50/196]	Time 0.014 (0.023)	Data 0.004 (0.008)	Loss 2.0077 (1.9245)	Acc@1 72.656 (74.816)	Acc@5 93.359 (95.427)
Epoch: [141][60/196]	Time 0.016 (0.022)	Data 0.005 (0.007)	Loss 1.9099 (1.9222)	Acc@1 73.438 (74.693)	Acc@5 95.703 (95.453)
Epoch: [141][70/196]	Time 0.014 (0.021)	Data 0.003 (0.006)	Loss 1.9778 (1.9275)	Acc@1 73.828 (74.461)	Acc@5 93.750 (95.461)
Epoch: [141][80/196]	Time 0.012 (0.020)	Data 0.008 (0.006)	Loss 1.8961 (1.9324)	Acc@1 71.484 (74.378)	Acc@5 98.047 (95.361)
Epoch: [141][90/196]	Time 0.013 (0.020)	Data 0.005 (0.006)	Loss 1.9459 (1.9339)	Acc@1 74.609 (74.317)	Acc@5 94.531 (95.317)
Epoch: [141][100/196]	Time 0.012 (0.019)	Data 0.004 (0.006)	Loss 1.9794 (1.9393)	Acc@1 70.703 (74.033)	Acc@5 95.312 (95.309)
Epoch: [141][110/196]	Time 0.013 (0.019)	Data 0.005 (0.006)	Loss 2.0444 (1.9375)	Acc@1 68.750 (74.082)	Acc@5 92.969 (95.284)
Epoch: [141][120/196]	Time 0.019 (0.019)	Data 0.002 (0.005)	Loss 2.0086 (1.9371)	Acc@1 71.875 (74.077)	Acc@5 93.359 (95.251)
Epoch: [141][130/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 2.0010 (1.9402)	Acc@1 69.922 (73.950)	Acc@5 94.922 (95.241)
Epoch: [141][140/196]	Time 0.015 (0.018)	Data 0.003 (0.005)	Loss 1.9830 (1.9474)	Acc@1 71.484 (73.773)	Acc@5 94.141 (95.127)
Epoch: [141][150/196]	Time 0.012 (0.018)	Data 0.005 (0.005)	Loss 2.1540 (1.9490)	Acc@1 69.531 (73.748)	Acc@5 93.359 (95.131)
Epoch: [141][160/196]	Time 0.014 (0.018)	Data 0.003 (0.005)	Loss 2.2197 (1.9525)	Acc@1 67.969 (73.636)	Acc@5 91.797 (95.082)
Epoch: [141][170/196]	Time 0.014 (0.017)	Data 0.003 (0.005)	Loss 1.9953 (1.9574)	Acc@1 71.484 (73.508)	Acc@5 95.703 (95.000)
Epoch: [141][180/196]	Time 0.013 (0.017)	Data 0.023 (0.005)	Loss 2.0791 (1.9631)	Acc@1 68.359 (73.384)	Acc@5 93.750 (94.931)
Epoch: [141][190/196]	Time 0.014 (0.017)	Data 0.003 (0.005)	Loss 2.0793 (1.9679)	Acc@1 70.312 (73.268)	Acc@5 95.312 (94.908)
num momentum params: 26
[0.1, 1.9694341564941407, 1.9182537007331848, 73.212, 53.0, tensor(0.5439, device='cuda:0', grad_fn=<DivBackward0>), 3.473570346832275, 0.46386909484863287]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [142 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [142][0/196]	Time 0.064 (0.064)	Data 0.231 (0.231)	Loss 1.9368 (1.9368)	Acc@1 73.047 (73.047)	Acc@5 96.094 (96.094)
Epoch: [142][10/196]	Time 0.016 (0.020)	Data 0.002 (0.023)	Loss 1.8795 (1.9395)	Acc@1 76.172 (73.722)	Acc@5 94.922 (95.597)
Epoch: [142][20/196]	Time 0.013 (0.018)	Data 0.004 (0.013)	Loss 1.9306 (1.9448)	Acc@1 75.781 (73.605)	Acc@5 94.141 (95.257)
Epoch: [142][30/196]	Time 0.017 (0.017)	Data 0.001 (0.010)	Loss 1.8829 (1.9286)	Acc@1 77.344 (74.509)	Acc@5 96.484 (95.640)
Epoch: [142][40/196]	Time 0.016 (0.017)	Data 0.003 (0.008)	Loss 1.7396 (1.9204)	Acc@1 76.172 (74.705)	Acc@5 96.484 (95.675)
Epoch: [142][50/196]	Time 0.013 (0.016)	Data 0.004 (0.007)	Loss 1.8722 (1.9100)	Acc@1 73.828 (74.900)	Acc@5 97.266 (95.734)
Epoch: [142][60/196]	Time 0.014 (0.016)	Data 0.003 (0.007)	Loss 1.8570 (1.9104)	Acc@1 73.438 (74.814)	Acc@5 96.094 (95.678)
Epoch: [142][70/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.9513 (1.9154)	Acc@1 73.828 (74.703)	Acc@5 96.484 (95.582)
Epoch: [142][80/196]	Time 0.016 (0.016)	Data 0.003 (0.006)	Loss 2.0111 (1.9248)	Acc@1 73.438 (74.494)	Acc@5 94.531 (95.481)
Epoch: [142][90/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.1218 (1.9304)	Acc@1 70.703 (74.335)	Acc@5 93.359 (95.347)
Epoch: [142][100/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.8931 (1.9359)	Acc@1 75.000 (74.168)	Acc@5 96.094 (95.309)
Epoch: [142][110/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 2.0108 (1.9419)	Acc@1 71.484 (73.899)	Acc@5 94.922 (95.253)
Epoch: [142][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9867 (1.9498)	Acc@1 68.750 (73.660)	Acc@5 96.094 (95.151)
Epoch: [142][130/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0605 (1.9561)	Acc@1 69.531 (73.443)	Acc@5 94.531 (95.140)
Epoch: [142][140/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0962 (1.9645)	Acc@1 65.234 (73.130)	Acc@5 94.531 (95.055)
Epoch: [142][150/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0883 (1.9709)	Acc@1 68.359 (72.987)	Acc@5 94.531 (94.948)
Epoch: [142][160/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1733 (1.9775)	Acc@1 69.141 (72.821)	Acc@5 94.531 (94.890)
Epoch: [142][170/196]	Time 0.014 (0.015)	Data 0.005 (0.005)	Loss 2.0693 (1.9822)	Acc@1 68.359 (72.693)	Acc@5 93.750 (94.808)
Epoch: [142][180/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0922 (1.9892)	Acc@1 69.922 (72.568)	Acc@5 94.531 (94.719)
Epoch: [142][190/196]	Time 0.011 (0.015)	Data 0.005 (0.005)	Loss 2.1823 (1.9900)	Acc@1 67.188 (72.583)	Acc@5 91.406 (94.719)
num momentum params: 26
[0.1, 1.9912570838165282, 2.0306382977962496, 72.54, 50.82, tensor(0.5379, device='cuda:0', grad_fn=<DivBackward0>), 3.0161218643188477, 0.3961460590362549]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [143 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [143][0/196]	Time 0.061 (0.061)	Data 0.235 (0.235)	Loss 1.9319 (1.9319)	Acc@1 76.953 (76.953)	Acc@5 96.094 (96.094)
Epoch: [143][10/196]	Time 0.017 (0.021)	Data 0.002 (0.023)	Loss 1.9414 (1.9592)	Acc@1 72.656 (73.189)	Acc@5 94.922 (95.703)
Epoch: [143][20/196]	Time 0.017 (0.019)	Data 0.002 (0.013)	Loss 2.0033 (1.9748)	Acc@1 72.266 (73.419)	Acc@5 95.312 (95.108)
Epoch: [143][30/196]	Time 0.014 (0.018)	Data 0.003 (0.010)	Loss 1.8439 (1.9668)	Acc@1 76.562 (73.362)	Acc@5 94.922 (95.098)
Epoch: [143][40/196]	Time 0.013 (0.017)	Data 0.005 (0.008)	Loss 1.7552 (1.9495)	Acc@1 80.078 (73.876)	Acc@5 98.047 (95.389)
Epoch: [143][50/196]	Time 0.014 (0.017)	Data 0.003 (0.007)	Loss 1.8137 (1.9423)	Acc@1 77.734 (74.150)	Acc@5 97.656 (95.458)
Epoch: [143][60/196]	Time 0.014 (0.017)	Data 0.004 (0.006)	Loss 1.9849 (1.9326)	Acc@1 75.000 (74.347)	Acc@5 96.875 (95.652)
Epoch: [143][70/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8229 (1.9343)	Acc@1 75.781 (74.180)	Acc@5 96.484 (95.593)
Epoch: [143][80/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9137 (1.9352)	Acc@1 74.219 (74.243)	Acc@5 95.703 (95.496)
Epoch: [143][90/196]	Time 0.021 (0.016)	Data 0.003 (0.005)	Loss 1.8718 (1.9363)	Acc@1 74.609 (74.197)	Acc@5 97.266 (95.506)
Epoch: [143][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 2.0772 (1.9395)	Acc@1 69.141 (74.110)	Acc@5 93.359 (95.444)
Epoch: [143][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 2.0714 (1.9442)	Acc@1 69.141 (74.064)	Acc@5 94.531 (95.344)
Epoch: [143][120/196]	Time 0.015 (0.016)	Data 0.005 (0.005)	Loss 1.8140 (1.9445)	Acc@1 77.734 (74.061)	Acc@5 97.656 (95.325)
Epoch: [143][130/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.1399 (1.9491)	Acc@1 69.922 (73.918)	Acc@5 94.531 (95.307)
Epoch: [143][140/196]	Time 0.017 (0.016)	Data 0.003 (0.005)	Loss 2.0554 (1.9562)	Acc@1 72.656 (73.762)	Acc@5 93.359 (95.180)
Epoch: [143][150/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 2.0472 (1.9644)	Acc@1 71.875 (73.510)	Acc@5 92.578 (95.090)
Epoch: [143][160/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 2.0919 (1.9699)	Acc@1 72.656 (73.384)	Acc@5 90.234 (95.019)
Epoch: [143][170/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9601 (1.9718)	Acc@1 76.172 (73.339)	Acc@5 94.141 (94.974)
Epoch: [143][180/196]	Time 0.014 (0.015)	Data 0.008 (0.005)	Loss 2.0828 (1.9765)	Acc@1 70.703 (73.185)	Acc@5 94.922 (94.935)
Epoch: [143][190/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 1.9304 (1.9793)	Acc@1 73.828 (73.110)	Acc@5 96.484 (94.916)
num momentum params: 26
[0.1, 1.9811000553894043, 1.978301055431366, 73.07, 51.39, tensor(0.5405, device='cuda:0', grad_fn=<DivBackward0>), 2.974990129470825, 0.3925013542175293]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [144 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [144][0/196]	Time 0.061 (0.061)	Data 0.225 (0.225)	Loss 1.8613 (1.8613)	Acc@1 75.781 (75.781)	Acc@5 96.875 (96.875)
Epoch: [144][10/196]	Time 0.016 (0.020)	Data 0.002 (0.022)	Loss 1.8599 (1.9407)	Acc@1 77.734 (74.574)	Acc@5 95.703 (95.668)
Epoch: [144][20/196]	Time 0.015 (0.018)	Data 0.003 (0.013)	Loss 2.0780 (1.9073)	Acc@1 66.797 (75.130)	Acc@5 93.750 (95.833)
Epoch: [144][30/196]	Time 0.019 (0.017)	Data 0.001 (0.010)	Loss 1.9732 (1.9220)	Acc@1 73.047 (74.622)	Acc@5 94.141 (95.628)
Epoch: [144][40/196]	Time 0.015 (0.017)	Data 0.003 (0.008)	Loss 1.8995 (1.9140)	Acc@1 78.516 (74.838)	Acc@5 94.922 (95.675)
Epoch: [144][50/196]	Time 0.017 (0.016)	Data 0.001 (0.007)	Loss 1.8229 (1.9068)	Acc@1 75.781 (74.923)	Acc@5 96.875 (95.780)
Epoch: [144][60/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.8544 (1.9049)	Acc@1 77.734 (74.981)	Acc@5 96.875 (95.754)
Epoch: [144][70/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9063 (1.9094)	Acc@1 75.781 (74.851)	Acc@5 94.531 (95.670)
Epoch: [144][80/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 2.0081 (1.9143)	Acc@1 72.266 (74.638)	Acc@5 92.188 (95.583)
Epoch: [144][90/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 1.9586 (1.9272)	Acc@1 74.609 (74.296)	Acc@5 95.312 (95.398)
Epoch: [144][100/196]	Time 0.014 (0.016)	Data 0.004 (0.006)	Loss 1.9864 (1.9324)	Acc@1 72.656 (74.226)	Acc@5 93.750 (95.247)
Epoch: [144][110/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.0341 (1.9432)	Acc@1 70.312 (73.874)	Acc@5 96.094 (95.168)
Epoch: [144][120/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 2.0326 (1.9481)	Acc@1 75.391 (73.735)	Acc@5 94.531 (95.135)
Epoch: [144][130/196]	Time 0.012 (0.015)	Data 0.017 (0.005)	Loss 2.0016 (1.9539)	Acc@1 72.656 (73.557)	Acc@5 94.531 (95.053)
Epoch: [144][140/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 2.0372 (1.9635)	Acc@1 70.312 (73.293)	Acc@5 93.359 (94.952)
Epoch: [144][150/196]	Time 0.012 (0.015)	Data 0.012 (0.005)	Loss 1.9855 (1.9704)	Acc@1 74.219 (73.124)	Acc@5 94.531 (94.914)
Epoch: [144][160/196]	Time 0.018 (0.015)	Data 0.001 (0.005)	Loss 2.2353 (1.9776)	Acc@1 69.141 (72.938)	Acc@5 91.016 (94.791)
Epoch: [144][170/196]	Time 0.013 (0.015)	Data 0.011 (0.005)	Loss 2.0932 (1.9837)	Acc@1 72.656 (72.770)	Acc@5 93.750 (94.723)
Epoch: [144][180/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 2.0129 (1.9846)	Acc@1 73.438 (72.777)	Acc@5 95.703 (94.762)
Epoch: [144][190/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.9803 (1.9893)	Acc@1 74.609 (72.695)	Acc@5 94.531 (94.683)
num momentum params: 26
[0.1, 1.988576424636841, 2.249596393108368, 72.728, 47.89, tensor(0.5388, device='cuda:0', grad_fn=<DivBackward0>), 3.0441534519195557, 0.4109699726104737]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [145 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [145][0/196]	Time 0.064 (0.064)	Data 0.231 (0.231)	Loss 1.9264 (1.9264)	Acc@1 74.219 (74.219)	Acc@5 96.875 (96.875)
Epoch: [145][10/196]	Time 0.015 (0.021)	Data 0.002 (0.023)	Loss 1.8653 (1.9152)	Acc@1 76.953 (75.426)	Acc@5 97.266 (96.023)
Epoch: [145][20/196]	Time 0.016 (0.018)	Data 0.003 (0.013)	Loss 1.8221 (1.8910)	Acc@1 77.344 (75.856)	Acc@5 96.484 (96.001)
Epoch: [145][30/196]	Time 0.014 (0.017)	Data 0.004 (0.010)	Loss 1.8617 (1.8793)	Acc@1 76.172 (76.121)	Acc@5 96.875 (95.880)
Epoch: [145][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9562 (1.8860)	Acc@1 73.438 (75.696)	Acc@5 95.703 (95.884)
Epoch: [145][50/196]	Time 0.012 (0.016)	Data 0.015 (0.007)	Loss 1.8020 (1.8930)	Acc@1 78.906 (75.360)	Acc@5 97.266 (95.757)
Epoch: [145][60/196]	Time 0.016 (0.016)	Data 0.001 (0.007)	Loss 1.9224 (1.8959)	Acc@1 75.391 (75.218)	Acc@5 96.484 (95.857)
Epoch: [145][70/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 2.0687 (1.9041)	Acc@1 70.703 (74.989)	Acc@5 95.703 (95.725)
Epoch: [145][80/196]	Time 0.015 (0.016)	Data 0.001 (0.006)	Loss 2.1045 (1.9102)	Acc@1 69.141 (74.701)	Acc@5 91.797 (95.698)
Epoch: [145][90/196]	Time 0.014 (0.016)	Data 0.007 (0.006)	Loss 2.1815 (1.9177)	Acc@1 69.531 (74.506)	Acc@5 90.625 (95.609)
Epoch: [145][100/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 2.1067 (1.9241)	Acc@1 68.750 (74.254)	Acc@5 92.188 (95.560)
Epoch: [145][110/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.1004 (1.9353)	Acc@1 68.359 (73.951)	Acc@5 92.578 (95.453)
Epoch: [145][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0850 (1.9413)	Acc@1 68.750 (73.731)	Acc@5 94.141 (95.409)
Epoch: [145][130/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 2.1057 (1.9467)	Acc@1 71.094 (73.646)	Acc@5 92.969 (95.378)
Epoch: [145][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.9645 (1.9502)	Acc@1 73.047 (73.532)	Acc@5 94.922 (95.332)
Epoch: [145][150/196]	Time 0.013 (0.016)	Data 0.004 (0.004)	Loss 2.1461 (1.9555)	Acc@1 67.969 (73.388)	Acc@5 93.359 (95.250)
Epoch: [145][160/196]	Time 0.011 (0.016)	Data 0.003 (0.004)	Loss 2.0396 (1.9649)	Acc@1 69.922 (73.139)	Acc@5 94.922 (95.145)
Epoch: [145][170/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 1.9070 (1.9708)	Acc@1 75.000 (72.987)	Acc@5 97.656 (95.077)
Epoch: [145][180/196]	Time 0.016 (0.016)	Data 0.008 (0.004)	Loss 2.0668 (1.9774)	Acc@1 73.438 (72.829)	Acc@5 92.969 (94.993)
Epoch: [145][190/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 1.9891 (1.9806)	Acc@1 70.703 (72.761)	Acc@5 95.312 (94.936)
num momentum params: 26
[0.1, 1.9830036966705322, 1.8472768354415894, 72.67, 54.34, tensor(0.5402, device='cuda:0', grad_fn=<DivBackward0>), 3.0537028312683105, 0.406665563583374]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [146 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [146][0/196]	Time 0.072 (0.072)	Data 0.207 (0.207)	Loss 2.0861 (2.0861)	Acc@1 69.531 (69.531)	Acc@5 94.922 (94.922)
Epoch: [146][10/196]	Time 0.014 (0.021)	Data 0.003 (0.022)	Loss 2.0360 (1.9425)	Acc@1 72.266 (74.467)	Acc@5 94.531 (95.490)
Epoch: [146][20/196]	Time 0.015 (0.018)	Data 0.003 (0.013)	Loss 1.8105 (1.9482)	Acc@1 76.562 (73.958)	Acc@5 97.266 (95.443)
Epoch: [146][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 1.8955 (1.9436)	Acc@1 73.047 (74.105)	Acc@5 96.094 (95.602)
Epoch: [146][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.9033 (1.9293)	Acc@1 73.047 (74.285)	Acc@5 94.531 (95.713)
Epoch: [146][50/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 2.0386 (1.9218)	Acc@1 68.359 (74.472)	Acc@5 94.531 (95.695)
Epoch: [146][60/196]	Time 0.016 (0.017)	Data 0.003 (0.006)	Loss 1.9272 (1.9216)	Acc@1 71.875 (74.539)	Acc@5 95.312 (95.665)
Epoch: [146][70/196]	Time 0.012 (0.016)	Data 0.016 (0.006)	Loss 1.9199 (1.9284)	Acc@1 73.047 (74.378)	Acc@5 96.094 (95.577)
Epoch: [146][80/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 1.9009 (1.9288)	Acc@1 72.656 (74.325)	Acc@5 96.094 (95.583)
Epoch: [146][90/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 2.0691 (1.9374)	Acc@1 71.875 (74.073)	Acc@5 95.312 (95.506)
Epoch: [146][100/196]	Time 0.016 (0.016)	Data 0.040 (0.006)	Loss 1.9694 (1.9448)	Acc@1 72.656 (73.913)	Acc@5 93.750 (95.390)
Epoch: [146][110/196]	Time 0.012 (0.016)	Data 0.004 (0.006)	Loss 2.1028 (1.9535)	Acc@1 68.359 (73.613)	Acc@5 93.750 (95.263)
Epoch: [146][120/196]	Time 0.014 (0.016)	Data 0.014 (0.005)	Loss 1.8757 (1.9584)	Acc@1 78.125 (73.547)	Acc@5 96.875 (95.193)
Epoch: [146][130/196]	Time 0.017 (0.016)	Data 0.003 (0.005)	Loss 2.0694 (1.9643)	Acc@1 69.141 (73.396)	Acc@5 94.141 (95.157)
Epoch: [146][140/196]	Time 0.015 (0.016)	Data 0.005 (0.005)	Loss 2.1157 (1.9688)	Acc@1 69.531 (73.313)	Acc@5 94.141 (95.063)
Epoch: [146][150/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9623 (1.9721)	Acc@1 71.875 (73.179)	Acc@5 95.312 (95.010)
Epoch: [146][160/196]	Time 0.013 (0.016)	Data 0.020 (0.005)	Loss 2.2004 (1.9785)	Acc@1 66.016 (73.061)	Acc@5 92.578 (94.915)
Epoch: [146][170/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.9328 (1.9811)	Acc@1 71.094 (73.013)	Acc@5 94.922 (94.894)
Epoch: [146][180/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 2.1080 (1.9870)	Acc@1 69.141 (72.777)	Acc@5 92.188 (94.810)
Epoch: [146][190/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 2.1316 (1.9930)	Acc@1 69.141 (72.626)	Acc@5 94.531 (94.742)
num momentum params: 26
[0.1, 1.9951965911865235, 2.094082794189453, 72.578, 50.61, tensor(0.5375, device='cuda:0', grad_fn=<DivBackward0>), 3.035780668258667, 0.4004919528961181]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [147 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [147][0/196]	Time 0.071 (0.071)	Data 0.203 (0.203)	Loss 1.7197 (1.7197)	Acc@1 81.641 (81.641)	Acc@5 97.266 (97.266)
Epoch: [147][10/196]	Time 0.016 (0.021)	Data 0.003 (0.020)	Loss 2.0525 (1.9311)	Acc@1 73.438 (74.680)	Acc@5 93.359 (95.455)
Epoch: [147][20/196]	Time 0.013 (0.019)	Data 0.004 (0.012)	Loss 1.8604 (1.9167)	Acc@1 75.781 (74.572)	Acc@5 96.484 (95.778)
Epoch: [147][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 1.8695 (1.9103)	Acc@1 75.391 (74.609)	Acc@5 95.312 (95.779)
Epoch: [147][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8113 (1.9094)	Acc@1 75.000 (74.829)	Acc@5 97.656 (95.675)
Epoch: [147][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.8720 (1.9121)	Acc@1 76.562 (74.809)	Acc@5 95.703 (95.573)
Epoch: [147][60/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.9535 (1.9059)	Acc@1 75.781 (74.859)	Acc@5 95.703 (95.697)
Epoch: [147][70/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.9417 (1.9106)	Acc@1 71.875 (74.631)	Acc@5 94.141 (95.742)
Epoch: [147][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0491 (1.9174)	Acc@1 70.703 (74.412)	Acc@5 95.312 (95.742)
Epoch: [147][90/196]	Time 0.021 (0.016)	Data 0.001 (0.005)	Loss 1.9348 (1.9249)	Acc@1 74.609 (74.236)	Acc@5 95.312 (95.660)
Epoch: [147][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.1006 (1.9314)	Acc@1 70.703 (74.099)	Acc@5 93.359 (95.545)
Epoch: [147][110/196]	Time 0.014 (0.016)	Data 0.001 (0.005)	Loss 1.8838 (1.9374)	Acc@1 78.516 (73.881)	Acc@5 95.312 (95.450)
Epoch: [147][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9932 (1.9410)	Acc@1 74.219 (73.809)	Acc@5 94.922 (95.361)
Epoch: [147][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0015 (1.9487)	Acc@1 70.312 (73.673)	Acc@5 97.266 (95.283)
Epoch: [147][140/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 2.1549 (1.9552)	Acc@1 71.484 (73.462)	Acc@5 93.750 (95.243)
Epoch: [147][150/196]	Time 0.019 (0.016)	Data 0.001 (0.004)	Loss 1.9507 (1.9595)	Acc@1 73.047 (73.308)	Acc@5 96.484 (95.199)
Epoch: [147][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0683 (1.9652)	Acc@1 74.219 (73.205)	Acc@5 91.406 (95.121)
Epoch: [147][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0302 (1.9702)	Acc@1 70.312 (73.029)	Acc@5 92.969 (95.070)
Epoch: [147][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 2.0187 (1.9763)	Acc@1 71.875 (72.922)	Acc@5 95.312 (95.017)
Epoch: [147][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.1871 (1.9829)	Acc@1 63.281 (72.738)	Acc@5 95.312 (94.918)
num momentum params: 26
[0.1, 1.9845729970550536, 1.87247749209404, 72.716, 53.26, tensor(0.5404, device='cuda:0', grad_fn=<DivBackward0>), 3.126326322555542, 0.4060633182525635]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [148 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [148][0/196]	Time 0.071 (0.071)	Data 0.201 (0.201)	Loss 1.9438 (1.9438)	Acc@1 74.219 (74.219)	Acc@5 97.266 (97.266)
Epoch: [148][10/196]	Time 0.015 (0.022)	Data 0.002 (0.021)	Loss 2.0756 (1.9357)	Acc@1 68.750 (73.189)	Acc@5 95.312 (96.200)
Epoch: [148][20/196]	Time 0.014 (0.019)	Data 0.003 (0.012)	Loss 1.8031 (1.8970)	Acc@1 78.125 (74.535)	Acc@5 95.312 (96.168)
Epoch: [148][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.8909 (1.8834)	Acc@1 75.391 (75.126)	Acc@5 96.875 (96.106)
Epoch: [148][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.8703 (1.8766)	Acc@1 76.953 (75.457)	Acc@5 94.922 (95.998)
Epoch: [148][50/196]	Time 0.013 (0.017)	Data 0.004 (0.006)	Loss 1.8923 (1.8820)	Acc@1 76.953 (75.429)	Acc@5 94.531 (95.948)
Epoch: [148][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.8987 (1.8860)	Acc@1 75.391 (75.333)	Acc@5 95.312 (95.882)
Epoch: [148][70/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9214 (1.8894)	Acc@1 74.609 (75.314)	Acc@5 94.141 (95.813)
Epoch: [148][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 2.0131 (1.9011)	Acc@1 71.094 (75.024)	Acc@5 94.141 (95.679)
Epoch: [148][90/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.8177 (1.9071)	Acc@1 77.344 (74.897)	Acc@5 97.656 (95.587)
Epoch: [148][100/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.9361 (1.9120)	Acc@1 74.609 (74.795)	Acc@5 94.531 (95.568)
Epoch: [148][110/196]	Time 0.014 (0.016)	Data 0.005 (0.004)	Loss 1.9857 (1.9150)	Acc@1 73.438 (74.729)	Acc@5 96.484 (95.569)
Epoch: [148][120/196]	Time 0.020 (0.016)	Data 0.003 (0.004)	Loss 2.0197 (1.9214)	Acc@1 69.141 (74.522)	Acc@5 97.266 (95.513)
Epoch: [148][130/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 2.0997 (1.9265)	Acc@1 66.406 (74.269)	Acc@5 92.578 (95.426)
Epoch: [148][140/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 1.9317 (1.9336)	Acc@1 70.703 (74.025)	Acc@5 96.484 (95.362)
Epoch: [148][150/196]	Time 0.013 (0.016)	Data 0.010 (0.004)	Loss 2.1225 (1.9408)	Acc@1 70.312 (73.818)	Acc@5 91.797 (95.258)
Epoch: [148][160/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.1960 (1.9485)	Acc@1 67.578 (73.593)	Acc@5 93.359 (95.155)
Epoch: [148][170/196]	Time 0.015 (0.016)	Data 0.009 (0.004)	Loss 2.0363 (1.9526)	Acc@1 73.047 (73.495)	Acc@5 94.531 (95.132)
Epoch: [148][180/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 2.0856 (1.9574)	Acc@1 69.922 (73.377)	Acc@5 93.750 (95.084)
Epoch: [148][190/196]	Time 0.013 (0.016)	Data 0.003 (0.004)	Loss 1.9603 (1.9606)	Acc@1 75.000 (73.309)	Acc@5 96.484 (95.045)
num momentum params: 26
[0.1, 1.9663187631225585, 2.216039947271347, 73.174, 47.3, tensor(0.5451, device='cuda:0', grad_fn=<DivBackward0>), 3.0546703338623047, 0.4069490432739258]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [149 | 180] LR: 0.100000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [149][0/196]	Time 0.075 (0.075)	Data 0.203 (0.203)	Loss 1.9933 (1.9933)	Acc@1 72.656 (72.656)	Acc@5 94.531 (94.531)
Epoch: [149][10/196]	Time 0.016 (0.022)	Data 0.002 (0.020)	Loss 1.9621 (1.9897)	Acc@1 74.219 (72.940)	Acc@5 94.141 (95.170)
Epoch: [149][20/196]	Time 0.015 (0.019)	Data 0.002 (0.012)	Loss 2.0550 (1.9692)	Acc@1 74.219 (73.493)	Acc@5 94.141 (95.033)
Epoch: [149][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.8954 (1.9427)	Acc@1 73.438 (74.206)	Acc@5 96.094 (95.312)
Epoch: [149][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.7613 (1.9235)	Acc@1 79.297 (74.676)	Acc@5 98.047 (95.436)
Epoch: [149][50/196]	Time 0.013 (0.017)	Data 0.005 (0.006)	Loss 1.8693 (1.9183)	Acc@1 73.828 (74.586)	Acc@5 97.266 (95.565)
Epoch: [149][60/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.9078 (1.9148)	Acc@1 71.875 (74.501)	Acc@5 95.312 (95.626)
Epoch: [149][70/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.8965 (1.9145)	Acc@1 75.391 (74.648)	Acc@5 93.359 (95.555)
Epoch: [149][80/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.8000 (1.9148)	Acc@1 80.078 (74.715)	Acc@5 95.312 (95.530)
Epoch: [149][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9176 (1.9211)	Acc@1 76.172 (74.614)	Acc@5 97.266 (95.480)
Epoch: [149][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 2.0472 (1.9290)	Acc@1 70.312 (74.385)	Acc@5 94.141 (95.401)
Epoch: [149][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.9766 (1.9307)	Acc@1 69.922 (74.264)	Acc@5 96.484 (95.379)
Epoch: [149][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.0421 (1.9389)	Acc@1 73.438 (74.009)	Acc@5 92.969 (95.290)
Epoch: [149][130/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 2.0761 (1.9532)	Acc@1 71.875 (73.619)	Acc@5 94.531 (95.151)
Epoch: [149][140/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 2.0139 (1.9610)	Acc@1 76.562 (73.446)	Acc@5 93.359 (95.055)
Epoch: [149][150/196]	Time 0.016 (0.016)	Data 0.003 (0.004)	Loss 2.0819 (1.9688)	Acc@1 71.484 (73.241)	Acc@5 92.578 (94.956)
Epoch: [149][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.9767 (1.9746)	Acc@1 69.531 (73.129)	Acc@5 95.312 (94.939)
Epoch: [149][170/196]	Time 0.017 (0.016)	Data 0.003 (0.004)	Loss 1.8577 (1.9787)	Acc@1 77.344 (72.994)	Acc@5 96.484 (94.878)
Epoch: [149][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 2.1136 (1.9858)	Acc@1 70.703 (72.822)	Acc@5 94.531 (94.814)
Epoch: [149][190/196]	Time 0.011 (0.016)	Data 0.006 (0.004)	Loss 2.0083 (1.9925)	Acc@1 71.094 (72.656)	Acc@5 95.703 (94.732)
num momentum params: 26
[0.1, 1.9953534616851807, 1.9633770501613617, 72.598, 52.9, tensor(0.5381, device='cuda:0', grad_fn=<DivBackward0>), 3.0951461791992188, 0.41536736488342285]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [247, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [247]
Non Pruning Epoch - module.bn3.bias: [247]
Non Pruning Epoch - module.conv4.weight: [256, 247, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [510, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [510]
Non Pruning Epoch - module.bn5.bias: [510]
Non Pruning Epoch - module.conv6.weight: [507, 510, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [300, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [300]
Non Pruning Epoch - module.bn8.bias: [300]
Non Pruning Epoch - module.fc.weight: [100, 300]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [150 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [247, 128, 3, 3]
module.conv4.weight [256, 247, 3, 3]
module.conv5.weight [510, 256, 3, 3]
module.conv6.weight [507, 510, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [300, 479, 3, 3]
Epoch: [150][0/196]	Time 0.067 (0.067)	Data 0.216 (0.216)	Loss 1.8664 (1.8664)	Acc@1 77.734 (77.734)	Acc@5 97.266 (97.266)
Epoch: [150][10/196]	Time 0.015 (0.022)	Data 0.002 (0.022)	Loss 1.6970 (1.8370)	Acc@1 83.203 (77.344)	Acc@5 98.047 (96.804)
Epoch: [150][20/196]	Time 0.015 (0.019)	Data 0.003 (0.013)	Loss 1.7569 (1.7784)	Acc@1 76.562 (78.590)	Acc@5 98.047 (97.303)
Epoch: [150][30/196]	Time 0.015 (0.018)	Data 0.003 (0.009)	Loss 1.6329 (1.7379)	Acc@1 83.594 (80.129)	Acc@5 98.047 (97.467)
Epoch: [150][40/196]	Time 0.012 (0.017)	Data 0.006 (0.008)	Loss 1.6203 (1.7129)	Acc@1 83.984 (80.878)	Acc@5 98.438 (97.618)
Epoch: [150][50/196]	Time 0.012 (0.017)	Data 0.006 (0.007)	Loss 1.5692 (1.6966)	Acc@1 83.203 (81.426)	Acc@5 98.828 (97.763)
Epoch: [150][60/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.5167 (1.6779)	Acc@1 86.719 (82.082)	Acc@5 98.828 (97.855)
Epoch: [150][70/196]	Time 0.012 (0.016)	Data 0.008 (0.006)	Loss 1.4953 (1.6592)	Acc@1 89.844 (82.702)	Acc@5 98.438 (97.893)
Epoch: [150][80/196]	Time 0.017 (0.016)	Data 0.001 (0.006)	Loss 1.5530 (1.6448)	Acc@1 84.766 (83.140)	Acc@5 98.438 (97.979)
Epoch: [150][90/196]	Time 0.012 (0.016)	Data 0.007 (0.005)	Loss 1.4996 (1.6322)	Acc@1 85.547 (83.469)	Acc@5 99.609 (98.086)
Epoch: [150][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.4551 (1.6185)	Acc@1 89.844 (83.880)	Acc@5 98.828 (98.167)
Epoch: [150][110/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 1.4567 (1.6076)	Acc@1 88.281 (84.206)	Acc@5 99.609 (98.219)
Epoch: [150][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.4963 (1.5979)	Acc@1 88.281 (84.507)	Acc@5 99.219 (98.283)
Epoch: [150][130/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 1.4623 (1.5890)	Acc@1 88.281 (84.792)	Acc@5 99.219 (98.330)
Epoch: [150][140/196]	Time 0.013 (0.016)	Data 0.001 (0.005)	Loss 1.4562 (1.5810)	Acc@1 87.500 (84.979)	Acc@5 99.609 (98.379)
Epoch: [150][150/196]	Time 0.013 (0.016)	Data 0.010 (0.005)	Loss 1.4409 (1.5738)	Acc@1 89.062 (85.187)	Acc@5 98.828 (98.422)
Epoch: [150][160/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.3808 (1.5665)	Acc@1 91.406 (85.409)	Acc@5 99.609 (98.454)
Epoch: [150][170/196]	Time 0.012 (0.016)	Data 0.007 (0.004)	Loss 1.4340 (1.5591)	Acc@1 91.406 (85.659)	Acc@5 99.219 (98.476)
Epoch: [150][180/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 1.4214 (1.5521)	Acc@1 89.453 (85.873)	Acc@5 99.609 (98.522)
Epoch: [150][190/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 1.5291 (1.5446)	Acc@1 87.891 (86.113)	Acc@5 98.828 (98.558)
num momentum params: 26
[0.010000000000000002, 1.5428115504837037, 1.1904384410381317, 86.18, 68.3, tensor(0.6945, device='cuda:0', grad_fn=<DivBackward0>), 3.132169008255005, 0.4171175956726074]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [247, 128, 3, 3]
Before - module.bn3.weight: [247]
Before - module.bn3.bias: [247]
Before - module.conv4.weight: [256, 247, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [510, 256, 3, 3]
Before - module.bn5.weight: [510]
Before - module.bn5.bias: [510]
Before - module.conv6.weight: [507, 510, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [479, 507, 3, 3]
Before - module.bn7.weight: [479]
Before - module.bn7.bias: [479]
Before - module.conv8.weight: [300, 479, 3, 3]
Before - module.bn8.weight: [300]
Before - module.bn8.bias: [300]
Before - module.fc.weight: [100, 300]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
[module.conv3.weight]: [247, 128, 3, 3] >> [246, 128, 3, 3]
[module.bn3.weight]: 247 >> 246
running_mean [246]
running_var [246]
num_batches_tracked []
[module.conv4.weight]: [256, 247, 3, 3] >> [256, 246, 3, 3]
[module.conv5.weight]: [510, 256, 3, 3] >> [509, 256, 3, 3]
[module.bn5.weight]: 510 >> 509
running_mean [509]
running_var [509]
num_batches_tracked []
[module.conv6.weight]: [507, 510, 3, 3] >> [507, 509, 3, 3]
[module.conv8.weight]: [300, 479, 3, 3] >> [298, 479, 3, 3]
[module.bn8.weight]: 300 >> 298
running_mean [298]
running_var [298]
num_batches_tracked []
[module.fc.weight]: [100, 300] >> [100, 298]
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [246, 128, 3, 3]
After - module.bn3.weight: [246]
After - module.bn3.bias: [246]
After - module.conv4.weight: [256, 246, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [509, 256, 3, 3]
After - module.bn5.weight: [509]
After - module.bn5.bias: [509]
After - module.conv6.weight: [507, 509, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [479, 507, 3, 3]
After - module.bn7.weight: [479]
After - module.bn7.bias: [479]
After - module.conv8.weight: [298, 479, 3, 3]
After - module.bn8.weight: [298]
After - module.bn8.bias: [298]
After - module.fc.weight: [100, 298]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [246, 128, 3, 3]
conv4 --> [256, 246, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [507, 509, 3, 3]
conv7 --> [479, 507, 3, 3]
conv8 --> [298, 479, 3, 3]
fc --> [298, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8270512128, 18137088, 246
4, 16541024256, 36274176, 256
5, 10207494144, 18763776, 509
6, 20215623168, 37161072, 507
7, 6714399744, 8742708, 479
8, 3946530816, 5138712, 298
fc, 11443200, 29800, 0
===================
FLOP REPORT: 27949050600000.0 50907200000.0 137794852 127268 2465 15.064077377319336
[INFO] Storing checkpoint...

Epoch: [151 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [151][0/196]	Time 0.538 (0.538)	Data 0.233 (0.233)	Loss 1.4497 (1.4497)	Acc@1 89.062 (89.062)	Acc@5 98.047 (98.047)
Epoch: [151][10/196]	Time 0.017 (0.064)	Data 0.003 (0.023)	Loss 1.3662 (1.3656)	Acc@1 92.188 (91.548)	Acc@5 99.609 (99.396)
Epoch: [151][20/196]	Time 0.016 (0.041)	Data 0.003 (0.014)	Loss 1.3337 (1.3808)	Acc@1 92.188 (91.016)	Acc@5 100.000 (99.405)
Epoch: [151][30/196]	Time 0.015 (0.032)	Data 0.002 (0.010)	Loss 1.3934 (1.3757)	Acc@1 90.234 (91.280)	Acc@5 99.609 (99.320)
Epoch: [151][40/196]	Time 0.013 (0.028)	Data 0.004 (0.008)	Loss 1.3541 (1.3724)	Acc@1 92.188 (91.368)	Acc@5 99.219 (99.390)
Epoch: [151][50/196]	Time 0.012 (0.026)	Data 0.005 (0.007)	Loss 1.4097 (1.3742)	Acc@1 89.844 (91.261)	Acc@5 99.219 (99.395)
Epoch: [151][60/196]	Time 0.013 (0.024)	Data 0.011 (0.006)	Loss 1.3003 (1.3705)	Acc@1 94.531 (91.227)	Acc@5 100.000 (99.436)
Epoch: [151][70/196]	Time 0.014 (0.023)	Data 0.003 (0.006)	Loss 1.3466 (1.3655)	Acc@1 91.406 (91.401)	Acc@5 99.609 (99.450)
Epoch: [151][80/196]	Time 0.012 (0.022)	Data 0.012 (0.006)	Loss 1.3907 (1.3659)	Acc@1 90.234 (91.339)	Acc@5 100.000 (99.441)
Epoch: [151][90/196]	Time 0.013 (0.021)	Data 0.008 (0.006)	Loss 1.2890 (1.3645)	Acc@1 94.141 (91.342)	Acc@5 99.609 (99.451)
Epoch: [151][100/196]	Time 0.011 (0.021)	Data 0.005 (0.005)	Loss 1.3482 (1.3641)	Acc@1 90.625 (91.410)	Acc@5 99.609 (99.435)
Epoch: [151][110/196]	Time 0.016 (0.020)	Data 0.003 (0.005)	Loss 1.3320 (1.3622)	Acc@1 89.844 (91.410)	Acc@5 99.219 (99.423)
Epoch: [151][120/196]	Time 0.013 (0.020)	Data 0.004 (0.005)	Loss 1.3158 (1.3595)	Acc@1 92.188 (91.393)	Acc@5 100.000 (99.435)
Epoch: [151][130/196]	Time 0.012 (0.019)	Data 0.008 (0.005)	Loss 1.4087 (1.3597)	Acc@1 89.844 (91.338)	Acc@5 99.609 (99.442)
Epoch: [151][140/196]	Time 0.013 (0.019)	Data 0.019 (0.005)	Loss 1.3297 (1.3582)	Acc@1 91.797 (91.395)	Acc@5 100.000 (99.443)
Epoch: [151][150/196]	Time 0.019 (0.019)	Data 0.003 (0.005)	Loss 1.3347 (1.3582)	Acc@1 91.406 (91.318)	Acc@5 100.000 (99.439)
Epoch: [151][160/196]	Time 0.014 (0.018)	Data 0.003 (0.005)	Loss 1.3288 (1.3578)	Acc@1 92.188 (91.324)	Acc@5 99.219 (99.425)
Epoch: [151][170/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 1.3100 (1.3570)	Acc@1 91.016 (91.306)	Acc@5 99.609 (99.429)
Epoch: [151][180/196]	Time 0.016 (0.018)	Data 0.001 (0.005)	Loss 1.3234 (1.3558)	Acc@1 90.625 (91.313)	Acc@5 99.609 (99.439)
Epoch: [151][190/196]	Time 0.015 (0.018)	Data 0.002 (0.005)	Loss 1.3383 (1.3550)	Acc@1 92.578 (91.333)	Acc@5 99.609 (99.438)
num momentum params: 26
[0.010000000000000002, 1.3550078121185303, 1.1800532764196396, 91.314, 68.94, tensor(0.7764, device='cuda:0', grad_fn=<DivBackward0>), 3.6225187778472905, 0.4837760925292969]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [152 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [152][0/196]	Time 0.073 (0.073)	Data 0.222 (0.222)	Loss 1.2747 (1.2747)	Acc@1 94.922 (94.922)	Acc@5 99.219 (99.219)
Epoch: [152][10/196]	Time 0.016 (0.022)	Data 0.002 (0.023)	Loss 1.2551 (1.2788)	Acc@1 96.484 (94.567)	Acc@5 99.609 (99.467)
Epoch: [152][20/196]	Time 0.019 (0.020)	Data 0.002 (0.013)	Loss 1.3765 (1.2876)	Acc@1 91.797 (93.955)	Acc@5 99.219 (99.572)
Epoch: [152][30/196]	Time 0.016 (0.018)	Data 0.002 (0.010)	Loss 1.3072 (1.2906)	Acc@1 90.234 (93.586)	Acc@5 100.000 (99.584)
Epoch: [152][40/196]	Time 0.016 (0.018)	Data 0.002 (0.008)	Loss 1.3259 (1.2915)	Acc@1 92.188 (93.455)	Acc@5 99.219 (99.533)
Epoch: [152][50/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.2806 (1.2877)	Acc@1 94.141 (93.513)	Acc@5 100.000 (99.602)
Epoch: [152][60/196]	Time 0.017 (0.017)	Data 0.002 (0.006)	Loss 1.2682 (1.2867)	Acc@1 93.750 (93.519)	Acc@5 100.000 (99.590)
Epoch: [152][70/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.2612 (1.2866)	Acc@1 94.531 (93.513)	Acc@5 99.609 (99.593)
Epoch: [152][80/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 1.2749 (1.2859)	Acc@1 92.969 (93.417)	Acc@5 100.000 (99.605)
Epoch: [152][90/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 1.2597 (1.2853)	Acc@1 96.094 (93.411)	Acc@5 98.828 (99.584)
Epoch: [152][100/196]	Time 0.012 (0.016)	Data 0.015 (0.005)	Loss 1.3014 (1.2847)	Acc@1 91.797 (93.452)	Acc@5 100.000 (99.555)
Epoch: [152][110/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.2681 (1.2836)	Acc@1 93.750 (93.440)	Acc@5 99.609 (99.564)
Epoch: [152][120/196]	Time 0.012 (0.016)	Data 0.015 (0.005)	Loss 1.2820 (1.2821)	Acc@1 92.188 (93.476)	Acc@5 99.609 (99.577)
Epoch: [152][130/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.2864 (1.2814)	Acc@1 92.969 (93.467)	Acc@5 99.609 (99.577)
Epoch: [152][140/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.2710 (1.2810)	Acc@1 92.188 (93.429)	Acc@5 100.000 (99.573)
Epoch: [152][150/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.2502 (1.2799)	Acc@1 92.578 (93.414)	Acc@5 100.000 (99.578)
Epoch: [152][160/196]	Time 0.011 (0.016)	Data 0.007 (0.005)	Loss 1.2922 (1.2790)	Acc@1 89.062 (93.405)	Acc@5 100.000 (99.590)
Epoch: [152][170/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.2444 (1.2786)	Acc@1 95.703 (93.387)	Acc@5 99.219 (99.591)
Epoch: [152][180/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.2795 (1.2776)	Acc@1 92.578 (93.409)	Acc@5 99.609 (99.596)
Epoch: [152][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.2893 (1.2775)	Acc@1 92.188 (93.380)	Acc@5 100.000 (99.601)
num momentum params: 26
[0.010000000000000002, 1.2777733863067626, 1.184822554588318, 93.36, 69.07, tensor(0.8115, device='cuda:0', grad_fn=<DivBackward0>), 3.0366833209991455, 0.3987252712249756]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [153 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [153][0/196]	Time 0.070 (0.070)	Data 0.225 (0.225)	Loss 1.2410 (1.2410)	Acc@1 94.922 (94.922)	Acc@5 99.609 (99.609)
Epoch: [153][10/196]	Time 0.015 (0.021)	Data 0.002 (0.022)	Loss 1.2333 (1.2420)	Acc@1 94.922 (94.460)	Acc@5 99.609 (99.716)
Epoch: [153][20/196]	Time 0.015 (0.019)	Data 0.003 (0.013)	Loss 1.2627 (1.2367)	Acc@1 92.578 (94.438)	Acc@5 99.609 (99.721)
Epoch: [153][30/196]	Time 0.015 (0.018)	Data 0.005 (0.009)	Loss 1.1980 (1.2365)	Acc@1 96.484 (94.342)	Acc@5 100.000 (99.698)
Epoch: [153][40/196]	Time 0.016 (0.017)	Data 0.002 (0.008)	Loss 1.2358 (1.2341)	Acc@1 94.531 (94.436)	Acc@5 100.000 (99.714)
Epoch: [153][50/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 1.2149 (1.2322)	Acc@1 95.312 (94.455)	Acc@5 100.000 (99.740)
Epoch: [153][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.2129 (1.2325)	Acc@1 94.531 (94.384)	Acc@5 99.609 (99.737)
Epoch: [153][70/196]	Time 0.012 (0.016)	Data 0.013 (0.006)	Loss 1.2774 (1.2319)	Acc@1 92.578 (94.333)	Acc@5 100.000 (99.752)
Epoch: [153][80/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.2478 (1.2304)	Acc@1 92.969 (94.396)	Acc@5 100.000 (99.764)
Epoch: [153][90/196]	Time 0.014 (0.016)	Data 0.021 (0.006)	Loss 1.2171 (1.2297)	Acc@1 95.312 (94.407)	Acc@5 99.609 (99.760)
Epoch: [153][100/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.2248 (1.2289)	Acc@1 94.531 (94.435)	Acc@5 100.000 (99.760)
Epoch: [153][110/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 1.2567 (1.2281)	Acc@1 92.188 (94.461)	Acc@5 100.000 (99.764)
Epoch: [153][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.1723 (1.2269)	Acc@1 96.875 (94.499)	Acc@5 99.609 (99.758)
Epoch: [153][130/196]	Time 0.014 (0.016)	Data 0.007 (0.005)	Loss 1.1920 (1.2265)	Acc@1 95.703 (94.516)	Acc@5 100.000 (99.753)
Epoch: [153][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.1960 (1.2260)	Acc@1 94.531 (94.490)	Acc@5 100.000 (99.748)
Epoch: [153][150/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 1.2773 (1.2252)	Acc@1 91.797 (94.492)	Acc@5 99.219 (99.741)
Epoch: [153][160/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.1665 (1.2249)	Acc@1 96.484 (94.541)	Acc@5 100.000 (99.738)
Epoch: [153][170/196]	Time 0.013 (0.016)	Data 0.009 (0.005)	Loss 1.2042 (1.2243)	Acc@1 95.703 (94.556)	Acc@5 99.609 (99.742)
Epoch: [153][180/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.2368 (1.2236)	Acc@1 94.531 (94.555)	Acc@5 99.609 (99.754)
Epoch: [153][190/196]	Time 0.011 (0.016)	Data 0.008 (0.004)	Loss 1.2197 (1.2223)	Acc@1 94.531 (94.601)	Acc@5 99.609 (99.755)
num momentum params: 26
[0.010000000000000002, 1.2227775759887696, 1.2015382951498033, 94.572, 69.01, tensor(0.8360, device='cuda:0', grad_fn=<DivBackward0>), 3.0921468734741206, 0.4173879623413086]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [154 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [154][0/196]	Time 0.076 (0.076)	Data 0.236 (0.236)	Loss 1.1881 (1.1881)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [154][10/196]	Time 0.017 (0.022)	Data 0.003 (0.024)	Loss 1.2039 (1.2048)	Acc@1 93.359 (94.673)	Acc@5 100.000 (99.680)
Epoch: [154][20/196]	Time 0.016 (0.019)	Data 0.002 (0.013)	Loss 1.1461 (1.2005)	Acc@1 98.047 (95.015)	Acc@5 100.000 (99.702)
Epoch: [154][30/196]	Time 0.016 (0.018)	Data 0.002 (0.010)	Loss 1.1582 (1.1955)	Acc@1 96.484 (95.186)	Acc@5 100.000 (99.723)
Epoch: [154][40/196]	Time 0.015 (0.017)	Data 0.003 (0.008)	Loss 1.1877 (1.1894)	Acc@1 95.312 (95.427)	Acc@5 100.000 (99.790)
Epoch: [154][50/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 1.1949 (1.1887)	Acc@1 94.531 (95.420)	Acc@5 100.000 (99.809)
Epoch: [154][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.2047 (1.1901)	Acc@1 95.312 (95.421)	Acc@5 99.609 (99.801)
Epoch: [154][70/196]	Time 0.013 (0.016)	Data 0.017 (0.006)	Loss 1.1982 (1.1886)	Acc@1 95.312 (95.472)	Acc@5 100.000 (99.807)
Epoch: [154][80/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 1.2044 (1.1891)	Acc@1 95.703 (95.390)	Acc@5 99.609 (99.802)
Epoch: [154][90/196]	Time 0.014 (0.016)	Data 0.015 (0.006)	Loss 1.1481 (1.1879)	Acc@1 96.094 (95.407)	Acc@5 100.000 (99.798)
Epoch: [154][100/196]	Time 0.015 (0.016)	Data 0.001 (0.006)	Loss 1.1983 (1.1886)	Acc@1 96.094 (95.382)	Acc@5 100.000 (99.810)
Epoch: [154][110/196]	Time 0.012 (0.016)	Data 0.011 (0.005)	Loss 1.2157 (1.1873)	Acc@1 94.141 (95.411)	Acc@5 99.609 (99.821)
Epoch: [154][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.1807 (1.1862)	Acc@1 95.703 (95.435)	Acc@5 99.609 (99.816)
Epoch: [154][130/196]	Time 0.013 (0.016)	Data 0.007 (0.005)	Loss 1.2346 (1.1855)	Acc@1 93.359 (95.447)	Acc@5 99.609 (99.821)
Epoch: [154][140/196]	Time 0.019 (0.016)	Data 0.001 (0.005)	Loss 1.1452 (1.1849)	Acc@1 95.703 (95.418)	Acc@5 100.000 (99.820)
Epoch: [154][150/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.1877 (1.1857)	Acc@1 94.531 (95.349)	Acc@5 99.609 (99.816)
Epoch: [154][160/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.1841 (1.1847)	Acc@1 95.703 (95.356)	Acc@5 100.000 (99.818)
Epoch: [154][170/196]	Time 0.011 (0.016)	Data 0.020 (0.005)	Loss 1.1672 (1.1834)	Acc@1 94.141 (95.354)	Acc@5 99.609 (99.820)
Epoch: [154][180/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.1259 (1.1831)	Acc@1 96.484 (95.341)	Acc@5 100.000 (99.814)
Epoch: [154][190/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.1721 (1.1825)	Acc@1 96.484 (95.333)	Acc@5 100.000 (99.814)
num momentum params: 26
[0.010000000000000002, 1.1819065056228637, 1.1990265083312988, 95.342, 69.33, tensor(0.8525, device='cuda:0', grad_fn=<DivBackward0>), 3.0823826789855957, 0.41169047355651855]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [155 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [155][0/196]	Time 0.080 (0.080)	Data 0.219 (0.219)	Loss 1.1454 (1.1454)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [155][10/196]	Time 0.015 (0.023)	Data 0.002 (0.022)	Loss 1.1108 (1.1445)	Acc@1 96.094 (96.307)	Acc@5 100.000 (99.893)
Epoch: [155][20/196]	Time 0.014 (0.019)	Data 0.003 (0.013)	Loss 1.1143 (1.1489)	Acc@1 97.266 (96.224)	Acc@5 99.609 (99.870)
Epoch: [155][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 1.1419 (1.1440)	Acc@1 96.484 (96.472)	Acc@5 100.000 (99.899)
Epoch: [155][40/196]	Time 0.015 (0.018)	Data 0.003 (0.008)	Loss 1.1285 (1.1437)	Acc@1 96.484 (96.456)	Acc@5 100.000 (99.886)
Epoch: [155][50/196]	Time 0.016 (0.017)	Data 0.003 (0.007)	Loss 1.1459 (1.1438)	Acc@1 96.094 (96.377)	Acc@5 100.000 (99.870)
Epoch: [155][60/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.1174 (1.1411)	Acc@1 96.875 (96.440)	Acc@5 100.000 (99.891)
Epoch: [155][70/196]	Time 0.017 (0.017)	Data 0.001 (0.006)	Loss 1.1231 (1.1414)	Acc@1 96.484 (96.380)	Acc@5 100.000 (99.890)
Epoch: [155][80/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 1.1632 (1.1419)	Acc@1 94.922 (96.325)	Acc@5 100.000 (99.884)
Epoch: [155][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.1108 (1.1420)	Acc@1 97.266 (96.313)	Acc@5 100.000 (99.880)
Epoch: [155][100/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.1582 (1.1418)	Acc@1 95.703 (96.334)	Acc@5 100.000 (99.876)
Epoch: [155][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.1583 (1.1413)	Acc@1 95.312 (96.340)	Acc@5 100.000 (99.884)
Epoch: [155][120/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.1041 (1.1411)	Acc@1 97.656 (96.313)	Acc@5 100.000 (99.887)
Epoch: [155][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.0990 (1.1413)	Acc@1 98.047 (96.288)	Acc@5 100.000 (99.881)
Epoch: [155][140/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.1601 (1.1408)	Acc@1 95.312 (96.288)	Acc@5 100.000 (99.886)
Epoch: [155][150/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.1461 (1.1405)	Acc@1 94.922 (96.257)	Acc@5 100.000 (99.889)
Epoch: [155][160/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.1117 (1.1393)	Acc@1 96.875 (96.305)	Acc@5 100.000 (99.888)
Epoch: [155][170/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.1297 (1.1389)	Acc@1 96.484 (96.286)	Acc@5 99.609 (99.886)
Epoch: [155][180/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.1404 (1.1382)	Acc@1 96.094 (96.305)	Acc@5 100.000 (99.892)
Epoch: [155][190/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 1.1292 (1.1378)	Acc@1 97.266 (96.323)	Acc@5 99.609 (99.894)
num momentum params: 26
[0.010000000000000002, 1.137396213684082, 1.2153981137275696, 96.344, 69.38, tensor(0.8729, device='cuda:0', grad_fn=<DivBackward0>), 3.0597732067108154, 0.41404294967651367]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [156 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [156][0/196]	Time 0.074 (0.074)	Data 0.236 (0.236)	Loss 1.0982 (1.0982)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [156][10/196]	Time 0.016 (0.022)	Data 0.002 (0.023)	Loss 1.0993 (1.1071)	Acc@1 97.656 (97.301)	Acc@5 100.000 (99.964)
Epoch: [156][20/196]	Time 0.016 (0.019)	Data 0.002 (0.013)	Loss 1.1167 (1.1111)	Acc@1 96.484 (96.987)	Acc@5 100.000 (99.926)
Epoch: [156][30/196]	Time 0.018 (0.018)	Data 0.001 (0.010)	Loss 1.1357 (1.1102)	Acc@1 96.094 (97.001)	Acc@5 99.609 (99.924)
Epoch: [156][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.1239 (1.1082)	Acc@1 97.656 (97.066)	Acc@5 99.609 (99.905)
Epoch: [156][50/196]	Time 0.014 (0.017)	Data 0.004 (0.007)	Loss 1.1274 (1.1085)	Acc@1 96.094 (96.990)	Acc@5 100.000 (99.916)
Epoch: [156][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 1.1344 (1.1082)	Acc@1 95.703 (97.003)	Acc@5 99.219 (99.910)
Epoch: [156][70/196]	Time 0.012 (0.016)	Data 0.005 (0.006)	Loss 1.0808 (1.1078)	Acc@1 96.484 (97.035)	Acc@5 100.000 (99.912)
Epoch: [156][80/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 1.1137 (1.1069)	Acc@1 95.312 (97.049)	Acc@5 100.000 (99.913)
Epoch: [156][90/196]	Time 0.012 (0.016)	Data 0.011 (0.006)	Loss 1.1181 (1.1076)	Acc@1 96.484 (97.017)	Acc@5 100.000 (99.910)
Epoch: [156][100/196]	Time 0.017 (0.016)	Data 0.003 (0.005)	Loss 1.0640 (1.1055)	Acc@1 98.047 (97.084)	Acc@5 100.000 (99.919)
Epoch: [156][110/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.1305 (1.1060)	Acc@1 94.922 (97.062)	Acc@5 100.000 (99.919)
Epoch: [156][120/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 1.1030 (1.1051)	Acc@1 95.703 (97.062)	Acc@5 100.000 (99.916)
Epoch: [156][130/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.1103 (1.1054)	Acc@1 96.875 (97.030)	Acc@5 100.000 (99.914)
Epoch: [156][140/196]	Time 0.011 (0.016)	Data 0.005 (0.005)	Loss 1.1294 (1.1056)	Acc@1 96.484 (97.011)	Acc@5 99.609 (99.914)
Epoch: [156][150/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 1.1093 (1.1061)	Acc@1 96.875 (96.968)	Acc@5 100.000 (99.909)
Epoch: [156][160/196]	Time 0.013 (0.016)	Data 0.010 (0.005)	Loss 1.0709 (1.1062)	Acc@1 97.656 (96.953)	Acc@5 100.000 (99.905)
Epoch: [156][170/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.0831 (1.1058)	Acc@1 97.656 (96.964)	Acc@5 100.000 (99.902)
Epoch: [156][180/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.0829 (1.1053)	Acc@1 96.484 (96.925)	Acc@5 100.000 (99.905)
Epoch: [156][190/196]	Time 0.014 (0.016)	Data 0.007 (0.005)	Loss 1.1207 (1.1046)	Acc@1 96.094 (96.930)	Acc@5 99.609 (99.904)
num momentum params: 26
[0.010000000000000002, 1.1045871276855468, 1.2205328458547593, 96.918, 69.37, tensor(0.8857, device='cuda:0', grad_fn=<DivBackward0>), 3.0385196208953857, 0.3968324661254883]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [157 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [157][0/196]	Time 0.069 (0.069)	Data 0.219 (0.219)	Loss 1.0806 (1.0806)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [157][10/196]	Time 0.015 (0.022)	Data 0.002 (0.022)	Loss 1.0852 (1.0806)	Acc@1 97.656 (97.656)	Acc@5 100.000 (99.929)
Epoch: [157][20/196]	Time 0.016 (0.019)	Data 0.003 (0.013)	Loss 1.0994 (1.0772)	Acc@1 96.094 (97.842)	Acc@5 99.609 (99.926)
Epoch: [157][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 1.0626 (1.0786)	Acc@1 97.266 (97.669)	Acc@5 100.000 (99.899)
Epoch: [157][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 1.0911 (1.0788)	Acc@1 96.484 (97.561)	Acc@5 100.000 (99.914)
Epoch: [157][50/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.0810 (1.0796)	Acc@1 96.094 (97.449)	Acc@5 100.000 (99.923)
Epoch: [157][60/196]	Time 0.011 (0.016)	Data 0.005 (0.006)	Loss 1.0524 (1.0777)	Acc@1 98.047 (97.522)	Acc@5 100.000 (99.936)
Epoch: [157][70/196]	Time 0.013 (0.016)	Data 0.006 (0.006)	Loss 1.0723 (1.0782)	Acc@1 97.656 (97.453)	Acc@5 100.000 (99.934)
Epoch: [157][80/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.0635 (1.0778)	Acc@1 97.656 (97.425)	Acc@5 100.000 (99.942)
Epoch: [157][90/196]	Time 0.013 (0.016)	Data 0.010 (0.005)	Loss 1.0351 (1.0780)	Acc@1 99.219 (97.347)	Acc@5 100.000 (99.944)
Epoch: [157][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.0858 (1.0778)	Acc@1 97.656 (97.347)	Acc@5 99.609 (99.938)
Epoch: [157][110/196]	Time 0.014 (0.016)	Data 0.009 (0.005)	Loss 1.1124 (1.0780)	Acc@1 96.094 (97.315)	Acc@5 100.000 (99.933)
Epoch: [157][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.0999 (1.0767)	Acc@1 96.484 (97.356)	Acc@5 100.000 (99.939)
Epoch: [157][130/196]	Time 0.012 (0.016)	Data 0.008 (0.005)	Loss 1.0769 (1.0760)	Acc@1 98.047 (97.403)	Acc@5 99.609 (99.934)
Epoch: [157][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.1284 (1.0758)	Acc@1 95.312 (97.399)	Acc@5 99.609 (99.934)
Epoch: [157][150/196]	Time 0.012 (0.016)	Data 0.012 (0.004)	Loss 1.0693 (1.0759)	Acc@1 98.047 (97.385)	Acc@5 100.000 (99.933)
Epoch: [157][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.0676 (1.0760)	Acc@1 96.875 (97.377)	Acc@5 100.000 (99.932)
Epoch: [157][170/196]	Time 0.016 (0.016)	Data 0.016 (0.004)	Loss 1.0563 (1.0756)	Acc@1 97.656 (97.382)	Acc@5 100.000 (99.936)
Epoch: [157][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.0668 (1.0749)	Acc@1 98.047 (97.389)	Acc@5 100.000 (99.937)
Epoch: [157][190/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.0809 (1.0751)	Acc@1 97.266 (97.360)	Acc@5 100.000 (99.935)
num momentum params: 26
[0.010000000000000002, 1.0746735943984986, 1.2360969650745393, 97.368, 69.21, tensor(0.8969, device='cuda:0', grad_fn=<DivBackward0>), 3.105802297592163, 0.39332532882690424]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [158 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [158][0/196]	Time 0.079 (0.079)	Data 0.231 (0.231)	Loss 1.0552 (1.0552)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [158][10/196]	Time 0.016 (0.022)	Data 0.002 (0.023)	Loss 1.0372 (1.0520)	Acc@1 98.828 (98.118)	Acc@5 100.000 (100.000)
Epoch: [158][20/196]	Time 0.014 (0.019)	Data 0.004 (0.013)	Loss 1.0461 (1.0533)	Acc@1 98.828 (98.028)	Acc@5 100.000 (99.963)
Epoch: [158][30/196]	Time 0.015 (0.018)	Data 0.003 (0.010)	Loss 1.0553 (1.0493)	Acc@1 97.266 (98.034)	Acc@5 100.000 (99.975)
Epoch: [158][40/196]	Time 0.014 (0.017)	Data 0.004 (0.008)	Loss 1.0314 (1.0491)	Acc@1 98.438 (97.923)	Acc@5 99.609 (99.971)
Epoch: [158][50/196]	Time 0.013 (0.017)	Data 0.006 (0.007)	Loss 1.0104 (1.0484)	Acc@1 98.438 (97.855)	Acc@5 100.000 (99.962)
Epoch: [158][60/196]	Time 0.016 (0.017)	Data 0.017 (0.007)	Loss 1.0566 (1.0485)	Acc@1 96.875 (97.868)	Acc@5 100.000 (99.968)
Epoch: [158][70/196]	Time 0.021 (0.017)	Data 0.001 (0.006)	Loss 1.0415 (1.0504)	Acc@1 97.266 (97.766)	Acc@5 100.000 (99.972)
Epoch: [158][80/196]	Time 0.013 (0.017)	Data 0.008 (0.006)	Loss 1.0624 (1.0507)	Acc@1 97.266 (97.753)	Acc@5 99.609 (99.961)
Epoch: [158][90/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 1.0337 (1.0494)	Acc@1 96.484 (97.781)	Acc@5 100.000 (99.957)
Epoch: [158][100/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 1.0358 (1.0494)	Acc@1 98.438 (97.765)	Acc@5 100.000 (99.961)
Epoch: [158][110/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 1.0355 (1.0493)	Acc@1 98.828 (97.755)	Acc@5 100.000 (99.961)
Epoch: [158][120/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.0476 (1.0488)	Acc@1 97.656 (97.769)	Acc@5 100.000 (99.961)
Epoch: [158][130/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 1.0269 (1.0481)	Acc@1 98.047 (97.776)	Acc@5 100.000 (99.958)
Epoch: [158][140/196]	Time 0.013 (0.016)	Data 0.009 (0.005)	Loss 1.0457 (1.0479)	Acc@1 97.266 (97.756)	Acc@5 100.000 (99.958)
Epoch: [158][150/196]	Time 0.015 (0.016)	Data 0.001 (0.004)	Loss 1.0629 (1.0479)	Acc@1 97.656 (97.739)	Acc@5 99.609 (99.956)
Epoch: [158][160/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 1.0261 (1.0478)	Acc@1 98.438 (97.731)	Acc@5 100.000 (99.956)
Epoch: [158][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.0511 (1.0477)	Acc@1 98.047 (97.743)	Acc@5 100.000 (99.957)
Epoch: [158][180/196]	Time 0.011 (0.016)	Data 0.007 (0.004)	Loss 1.0466 (1.0477)	Acc@1 98.047 (97.745)	Acc@5 100.000 (99.957)
Epoch: [158][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 1.0402 (1.0471)	Acc@1 98.438 (97.748)	Acc@5 100.000 (99.959)
num momentum params: 26
[0.010000000000000002, 1.0468953922271729, 1.240794620513916, 97.736, 69.53, tensor(0.9069, device='cuda:0', grad_fn=<DivBackward0>), 3.1567556858062744, 0.39430952072143555]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [159 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [159][0/196]	Time 0.084 (0.084)	Data 0.213 (0.213)	Loss 1.0262 (1.0262)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [159][10/196]	Time 0.015 (0.023)	Data 0.003 (0.021)	Loss 1.0040 (1.0192)	Acc@1 100.000 (98.580)	Acc@5 100.000 (99.964)
Epoch: [159][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 1.0235 (1.0253)	Acc@1 98.828 (98.344)	Acc@5 100.000 (99.963)
Epoch: [159][30/196]	Time 0.016 (0.018)	Data 0.002 (0.009)	Loss 1.0254 (1.0266)	Acc@1 98.438 (98.274)	Acc@5 100.000 (99.962)
Epoch: [159][40/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 1.0159 (1.0255)	Acc@1 98.828 (98.266)	Acc@5 100.000 (99.962)
Epoch: [159][50/196]	Time 0.016 (0.017)	Data 0.001 (0.007)	Loss 1.0201 (1.0248)	Acc@1 98.438 (98.231)	Acc@5 100.000 (99.969)
Epoch: [159][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 1.0254 (1.0252)	Acc@1 98.828 (98.194)	Acc@5 100.000 (99.968)
Epoch: [159][70/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 0.9995 (1.0253)	Acc@1 99.219 (98.184)	Acc@5 100.000 (99.972)
Epoch: [159][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 1.0227 (1.0249)	Acc@1 97.656 (98.187)	Acc@5 100.000 (99.976)
Epoch: [159][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.0145 (1.0244)	Acc@1 98.828 (98.206)	Acc@5 100.000 (99.974)
Epoch: [159][100/196]	Time 0.013 (0.016)	Data 0.003 (0.005)	Loss 1.0521 (1.0251)	Acc@1 95.703 (98.144)	Acc@5 100.000 (99.977)
Epoch: [159][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.0321 (1.0256)	Acc@1 97.656 (98.107)	Acc@5 100.000 (99.979)
Epoch: [159][120/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.0033 (1.0254)	Acc@1 98.828 (98.118)	Acc@5 100.000 (99.977)
Epoch: [159][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 1.0410 (1.0252)	Acc@1 98.047 (98.112)	Acc@5 100.000 (99.979)
Epoch: [159][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 1.0043 (1.0250)	Acc@1 98.828 (98.088)	Acc@5 100.000 (99.981)
Epoch: [159][150/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.0173 (1.0253)	Acc@1 99.219 (98.068)	Acc@5 100.000 (99.979)
Epoch: [159][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.9930 (1.0246)	Acc@1 98.828 (98.078)	Acc@5 100.000 (99.981)
Epoch: [159][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 1.0400 (1.0245)	Acc@1 96.875 (98.054)	Acc@5 100.000 (99.979)
Epoch: [159][180/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 1.0169 (1.0244)	Acc@1 98.047 (98.034)	Acc@5 100.000 (99.974)
Epoch: [159][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 1.0386 (1.0240)	Acc@1 97.656 (98.049)	Acc@5 100.000 (99.975)
num momentum params: 26
[0.010000000000000002, 1.023670691757202, 1.2466749274730682, 98.046, 69.31, tensor(0.9136, device='cuda:0', grad_fn=<DivBackward0>), 3.092651128768921, 0.40257740020751953]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [160 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [160][0/196]	Time 0.079 (0.079)	Data 0.224 (0.224)	Loss 1.0220 (1.0220)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [160][10/196]	Time 0.016 (0.023)	Data 0.003 (0.022)	Loss 1.0026 (1.0023)	Acc@1 99.609 (98.651)	Acc@5 99.609 (99.929)
Epoch: [160][20/196]	Time 0.017 (0.019)	Data 0.002 (0.013)	Loss 0.9975 (1.0068)	Acc@1 98.438 (98.400)	Acc@5 100.000 (99.963)
Epoch: [160][30/196]	Time 0.015 (0.018)	Data 0.002 (0.010)	Loss 1.0064 (1.0038)	Acc@1 97.266 (98.400)	Acc@5 100.000 (99.975)
Epoch: [160][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 0.9909 (1.0024)	Acc@1 98.828 (98.447)	Acc@5 100.000 (99.971)
Epoch: [160][50/196]	Time 0.018 (0.017)	Data 0.001 (0.007)	Loss 1.0384 (1.0038)	Acc@1 96.875 (98.384)	Acc@5 100.000 (99.977)
Epoch: [160][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 0.9843 (1.0041)	Acc@1 99.609 (98.399)	Acc@5 100.000 (99.968)
Epoch: [160][70/196]	Time 0.018 (0.016)	Data 0.001 (0.006)	Loss 1.0066 (1.0044)	Acc@1 97.266 (98.344)	Acc@5 100.000 (99.967)
Epoch: [160][80/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 0.9902 (1.0041)	Acc@1 98.828 (98.327)	Acc@5 99.609 (99.966)
Epoch: [160][90/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.9951 (1.0042)	Acc@1 98.047 (98.292)	Acc@5 100.000 (99.961)
Epoch: [160][100/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 0.9736 (1.0030)	Acc@1 99.609 (98.314)	Acc@5 100.000 (99.965)
Epoch: [160][110/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 1.0034 (1.0024)	Acc@1 98.438 (98.335)	Acc@5 100.000 (99.968)
Epoch: [160][120/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 1.0044 (1.0025)	Acc@1 98.438 (98.337)	Acc@5 100.000 (99.971)
Epoch: [160][130/196]	Time 0.023 (0.016)	Data 0.001 (0.005)	Loss 0.9787 (1.0017)	Acc@1 99.219 (98.345)	Acc@5 100.000 (99.970)
Epoch: [160][140/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 1.0046 (1.0012)	Acc@1 97.266 (98.360)	Acc@5 100.000 (99.970)
Epoch: [160][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.9965 (1.0014)	Acc@1 98.438 (98.337)	Acc@5 100.000 (99.972)
Epoch: [160][160/196]	Time 0.011 (0.015)	Data 0.005 (0.005)	Loss 0.9862 (1.0007)	Acc@1 98.047 (98.338)	Acc@5 100.000 (99.973)
Epoch: [160][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 1.0098 (1.0005)	Acc@1 97.266 (98.328)	Acc@5 100.000 (99.975)
Epoch: [160][180/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.9789 (1.0000)	Acc@1 98.828 (98.340)	Acc@5 100.000 (99.974)
Epoch: [160][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 1.0063 (1.0002)	Acc@1 98.438 (98.329)	Acc@5 100.000 (99.975)
num momentum params: 26
[0.010000000000000002, 0.9998800058364868, 1.2612765765190124, 98.334, 69.24, tensor(0.9212, device='cuda:0', grad_fn=<DivBackward0>), 2.969604015350342, 0.4054253101348877]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [246, 128, 3, 3]
Before - module.bn3.weight: [246]
Before - module.bn3.bias: [246]
Before - module.conv4.weight: [256, 246, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [509, 256, 3, 3]
Before - module.bn5.weight: [509]
Before - module.bn5.bias: [509]
Before - module.conv6.weight: [507, 509, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [479, 507, 3, 3]
Before - module.bn7.weight: [479]
Before - module.bn7.bias: [479]
Before - module.conv8.weight: [298, 479, 3, 3]
Before - module.bn8.weight: [298]
Before - module.bn8.bias: [298]
Before - module.fc.weight: [100, 298]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [246, 128, 3, 3]
After - module.bn3.weight: [246]
After - module.bn3.bias: [246]
After - module.conv4.weight: [256, 246, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [509, 256, 3, 3]
After - module.bn5.weight: [509]
After - module.bn5.bias: [509]
After - module.conv6.weight: [507, 509, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [479, 507, 3, 3]
After - module.bn7.weight: [479]
After - module.bn7.bias: [479]
After - module.conv8.weight: [298, 479, 3, 3]
After - module.bn8.weight: [298]
After - module.bn8.bias: [298]
After - module.fc.weight: [100, 298]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [246, 128, 3, 3]
conv4 --> [256, 246, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [507, 509, 3, 3]
conv7 --> [479, 507, 3, 3]
conv8 --> [298, 479, 3, 3]
fc --> [298, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8270512128, 18137088, 246
4, 16541024256, 36274176, 256
5, 10207494144, 18763776, 509
6, 20215623168, 37161072, 507
7, 6714399744, 8742708, 479
8, 3946530816, 5138712, 298
fc, 11443200, 29800, 0
===================
FLOP REPORT: 27949050600000.0 50907200000.0 137794852 127268 2465 15.064077377319336
[INFO] Storing checkpoint...

Epoch: [161 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [161][0/196]	Time 0.077 (0.077)	Data 0.224 (0.224)	Loss 0.9735 (0.9735)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [161][10/196]	Time 0.016 (0.022)	Data 0.002 (0.022)	Loss 0.9822 (0.9810)	Acc@1 98.438 (98.544)	Acc@5 100.000 (100.000)
Epoch: [161][20/196]	Time 0.016 (0.019)	Data 0.002 (0.013)	Loss 0.9774 (0.9800)	Acc@1 98.828 (98.735)	Acc@5 100.000 (100.000)
Epoch: [161][30/196]	Time 0.015 (0.018)	Data 0.003 (0.009)	Loss 0.9871 (0.9809)	Acc@1 97.656 (98.702)	Acc@5 100.000 (99.975)
Epoch: [161][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 0.9727 (0.9796)	Acc@1 98.828 (98.695)	Acc@5 100.000 (99.981)
Epoch: [161][50/196]	Time 0.016 (0.017)	Data 0.002 (0.007)	Loss 0.9970 (0.9816)	Acc@1 98.438 (98.598)	Acc@5 100.000 (99.985)
Epoch: [161][60/196]	Time 0.018 (0.017)	Data 0.001 (0.006)	Loss 0.9739 (0.9819)	Acc@1 99.219 (98.598)	Acc@5 100.000 (99.987)
Epoch: [161][70/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 0.9747 (0.9821)	Acc@1 100.000 (98.614)	Acc@5 100.000 (99.989)
Epoch: [161][80/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.9894 (0.9819)	Acc@1 97.656 (98.592)	Acc@5 100.000 (99.990)
Epoch: [161][90/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 0.9620 (0.9812)	Acc@1 99.219 (98.588)	Acc@5 100.000 (99.991)
Epoch: [161][100/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.9950 (0.9810)	Acc@1 97.266 (98.577)	Acc@5 100.000 (99.988)
Epoch: [161][110/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.9711 (0.9804)	Acc@1 98.828 (98.606)	Acc@5 100.000 (99.989)
Epoch: [161][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.9546 (0.9797)	Acc@1 98.828 (98.631)	Acc@5 100.000 (99.990)
Epoch: [161][130/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 1.0017 (0.9796)	Acc@1 96.875 (98.607)	Acc@5 100.000 (99.991)
Epoch: [161][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.9551 (0.9793)	Acc@1 98.828 (98.595)	Acc@5 100.000 (99.992)
Epoch: [161][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 0.9756 (0.9790)	Acc@1 98.438 (98.590)	Acc@5 100.000 (99.992)
Epoch: [161][160/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.9974 (0.9797)	Acc@1 97.266 (98.552)	Acc@5 100.000 (99.985)
Epoch: [161][170/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.9687 (0.9798)	Acc@1 98.047 (98.533)	Acc@5 100.000 (99.984)
Epoch: [161][180/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.9989 (0.9795)	Acc@1 98.828 (98.539)	Acc@5 100.000 (99.985)
Epoch: [161][190/196]	Time 0.013 (0.015)	Data 0.004 (0.004)	Loss 0.9511 (0.9793)	Acc@1 99.609 (98.527)	Acc@5 100.000 (99.984)
num momentum params: 26
[0.010000000000000002, 0.9790377566337586, 1.2745854830741883, 98.528, 69.44, tensor(0.9265, device='cuda:0', grad_fn=<DivBackward0>), 3.004167079925537, 0.4005453586578369]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [162 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [162][0/196]	Time 0.081 (0.081)	Data 0.218 (0.218)	Loss 0.9541 (0.9541)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [162][10/196]	Time 0.015 (0.022)	Data 0.003 (0.022)	Loss 0.9576 (0.9648)	Acc@1 98.438 (98.580)	Acc@5 100.000 (100.000)
Epoch: [162][20/196]	Time 0.015 (0.019)	Data 0.003 (0.013)	Loss 0.9483 (0.9628)	Acc@1 98.438 (98.679)	Acc@5 100.000 (100.000)
Epoch: [162][30/196]	Time 0.013 (0.018)	Data 0.004 (0.010)	Loss 0.9676 (0.9621)	Acc@1 99.219 (98.753)	Acc@5 100.000 (100.000)
Epoch: [162][40/196]	Time 0.012 (0.017)	Data 0.007 (0.008)	Loss 0.9539 (0.9616)	Acc@1 98.828 (98.742)	Acc@5 100.000 (100.000)
Epoch: [162][50/196]	Time 0.015 (0.017)	Data 0.004 (0.007)	Loss 0.9548 (0.9615)	Acc@1 99.609 (98.744)	Acc@5 100.000 (100.000)
Epoch: [162][60/196]	Time 0.014 (0.017)	Data 0.002 (0.006)	Loss 0.9845 (0.9607)	Acc@1 97.656 (98.783)	Acc@5 100.000 (100.000)
Epoch: [162][70/196]	Time 0.013 (0.016)	Data 0.007 (0.006)	Loss 0.9749 (0.9609)	Acc@1 97.656 (98.718)	Acc@5 100.000 (100.000)
Epoch: [162][80/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 0.9734 (0.9606)	Acc@1 98.828 (98.741)	Acc@5 100.000 (100.000)
Epoch: [162][90/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 0.9550 (0.9602)	Acc@1 99.609 (98.777)	Acc@5 100.000 (99.996)
Epoch: [162][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.9386 (0.9595)	Acc@1 99.609 (98.801)	Acc@5 100.000 (99.992)
Epoch: [162][110/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 0.9921 (0.9594)	Acc@1 96.875 (98.793)	Acc@5 100.000 (99.993)
Epoch: [162][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.9441 (0.9594)	Acc@1 99.609 (98.783)	Acc@5 100.000 (99.994)
Epoch: [162][130/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.9520 (0.9594)	Acc@1 99.219 (98.786)	Acc@5 100.000 (99.994)
Epoch: [162][140/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.9385 (0.9591)	Acc@1 100.000 (98.775)	Acc@5 100.000 (99.992)
Epoch: [162][150/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.9818 (0.9591)	Acc@1 98.438 (98.769)	Acc@5 100.000 (99.992)
Epoch: [162][160/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.9307 (0.9589)	Acc@1 100.000 (98.760)	Acc@5 100.000 (99.993)
Epoch: [162][170/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.9567 (0.9585)	Acc@1 98.438 (98.762)	Acc@5 100.000 (99.993)
Epoch: [162][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.9634 (0.9581)	Acc@1 98.438 (98.757)	Acc@5 100.000 (99.994)
Epoch: [162][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.9920 (0.9582)	Acc@1 97.656 (98.742)	Acc@5 100.000 (99.994)
num momentum params: 26
[0.010000000000000002, 0.9579667339706421, 1.272056725025177, 98.742, 69.73, tensor(0.9324, device='cuda:0', grad_fn=<DivBackward0>), 3.106455564498902, 0.4239180088043213]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [163 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [163][0/196]	Time 0.086 (0.086)	Data 0.211 (0.211)	Loss 0.9439 (0.9439)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [163][10/196]	Time 0.016 (0.023)	Data 0.003 (0.021)	Loss 0.9437 (0.9446)	Acc@1 99.219 (98.899)	Acc@5 100.000 (100.000)
Epoch: [163][20/196]	Time 0.018 (0.019)	Data 0.002 (0.012)	Loss 0.9441 (0.9427)	Acc@1 98.828 (98.958)	Acc@5 100.000 (100.000)
Epoch: [163][30/196]	Time 0.014 (0.018)	Data 0.004 (0.009)	Loss 0.9708 (0.9438)	Acc@1 96.484 (98.866)	Acc@5 100.000 (100.000)
Epoch: [163][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 0.9302 (0.9418)	Acc@1 99.219 (98.923)	Acc@5 100.000 (100.000)
Epoch: [163][50/196]	Time 0.013 (0.017)	Data 0.009 (0.007)	Loss 0.9551 (0.9402)	Acc@1 97.656 (98.958)	Acc@5 100.000 (100.000)
Epoch: [163][60/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 0.9452 (0.9413)	Acc@1 98.828 (98.886)	Acc@5 100.000 (100.000)
Epoch: [163][70/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 0.9507 (0.9418)	Acc@1 98.828 (98.850)	Acc@5 100.000 (100.000)
Epoch: [163][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.9357 (0.9410)	Acc@1 98.828 (98.857)	Acc@5 100.000 (99.995)
Epoch: [163][90/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 0.9486 (0.9405)	Acc@1 97.656 (98.884)	Acc@5 100.000 (99.996)
Epoch: [163][100/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.9334 (0.9402)	Acc@1 98.047 (98.875)	Acc@5 100.000 (99.996)
Epoch: [163][110/196]	Time 0.012 (0.016)	Data 0.004 (0.005)	Loss 0.9440 (0.9399)	Acc@1 98.047 (98.884)	Acc@5 100.000 (99.996)
Epoch: [163][120/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 0.9275 (0.9393)	Acc@1 98.828 (98.899)	Acc@5 100.000 (99.997)
Epoch: [163][130/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.9211 (0.9390)	Acc@1 100.000 (98.888)	Acc@5 100.000 (99.997)
Epoch: [163][140/196]	Time 0.015 (0.016)	Data 0.004 (0.004)	Loss 0.9404 (0.9388)	Acc@1 98.438 (98.867)	Acc@5 100.000 (99.997)
Epoch: [163][150/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 0.9465 (0.9385)	Acc@1 98.438 (98.872)	Acc@5 100.000 (99.997)
Epoch: [163][160/196]	Time 0.016 (0.016)	Data 0.004 (0.004)	Loss 0.9115 (0.9383)	Acc@1 99.609 (98.865)	Acc@5 100.000 (99.998)
Epoch: [163][170/196]	Time 0.012 (0.016)	Data 0.011 (0.004)	Loss 0.9204 (0.9382)	Acc@1 99.219 (98.842)	Acc@5 100.000 (99.998)
Epoch: [163][180/196]	Time 0.013 (0.016)	Data 0.005 (0.004)	Loss 0.9367 (0.9380)	Acc@1 99.219 (98.835)	Acc@5 100.000 (99.998)
Epoch: [163][190/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 0.9318 (0.9377)	Acc@1 98.828 (98.842)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.937509170036316, 1.2798260176181793, 98.84, 69.24, tensor(0.9380, device='cuda:0', grad_fn=<DivBackward0>), 3.067654609680176, 0.3975248336791993]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [164 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [164][0/196]	Time 0.072 (0.072)	Data 0.212 (0.212)	Loss 0.9243 (0.9243)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [164][10/196]	Time 0.019 (0.023)	Data 0.002 (0.022)	Loss 0.9167 (0.9205)	Acc@1 98.828 (99.148)	Acc@5 100.000 (100.000)
Epoch: [164][20/196]	Time 0.017 (0.019)	Data 0.002 (0.012)	Loss 0.9235 (0.9213)	Acc@1 98.438 (99.144)	Acc@5 100.000 (99.981)
Epoch: [164][30/196]	Time 0.017 (0.018)	Data 0.002 (0.009)	Loss 0.9308 (0.9192)	Acc@1 99.609 (99.244)	Acc@5 100.000 (99.987)
Epoch: [164][40/196]	Time 0.013 (0.018)	Data 0.007 (0.008)	Loss 0.9182 (0.9201)	Acc@1 99.219 (99.190)	Acc@5 100.000 (99.990)
Epoch: [164][50/196]	Time 0.016 (0.017)	Data 0.001 (0.007)	Loss 0.9103 (0.9215)	Acc@1 99.219 (99.127)	Acc@5 100.000 (99.992)
Epoch: [164][60/196]	Time 0.014 (0.017)	Data 0.006 (0.006)	Loss 0.9049 (0.9215)	Acc@1 99.609 (99.103)	Acc@5 100.000 (99.994)
Epoch: [164][70/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 0.9202 (0.9215)	Acc@1 99.219 (99.087)	Acc@5 100.000 (99.994)
Epoch: [164][80/196]	Time 0.012 (0.016)	Data 0.005 (0.005)	Loss 0.9135 (0.9212)	Acc@1 99.609 (99.084)	Acc@5 100.000 (99.995)
Epoch: [164][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.9354 (0.9214)	Acc@1 98.828 (99.090)	Acc@5 100.000 (99.996)
Epoch: [164][100/196]	Time 0.013 (0.016)	Data 0.008 (0.005)	Loss 0.9131 (0.9209)	Acc@1 100.000 (99.118)	Acc@5 100.000 (99.996)
Epoch: [164][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.9306 (0.9211)	Acc@1 97.266 (99.085)	Acc@5 100.000 (99.996)
Epoch: [164][120/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 0.9080 (0.9211)	Acc@1 99.609 (99.083)	Acc@5 100.000 (99.997)
Epoch: [164][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8972 (0.9202)	Acc@1 100.000 (99.105)	Acc@5 100.000 (99.997)
Epoch: [164][140/196]	Time 0.012 (0.016)	Data 0.006 (0.004)	Loss 0.9147 (0.9199)	Acc@1 98.828 (99.105)	Acc@5 100.000 (99.997)
Epoch: [164][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 0.9422 (0.9198)	Acc@1 97.266 (99.079)	Acc@5 100.000 (99.997)
Epoch: [164][160/196]	Time 0.012 (0.016)	Data 0.008 (0.004)	Loss 0.9075 (0.9199)	Acc@1 98.828 (99.066)	Acc@5 100.000 (99.998)
Epoch: [164][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.9108 (0.9199)	Acc@1 99.609 (99.050)	Acc@5 100.000 (99.995)
Epoch: [164][180/196]	Time 0.012 (0.016)	Data 0.004 (0.004)	Loss 0.9123 (0.9197)	Acc@1 98.828 (99.053)	Acc@5 100.000 (99.996)
Epoch: [164][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.9095 (0.9194)	Acc@1 99.609 (99.051)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.9192780940628051, 1.2862250363826753, 99.046, 69.73, tensor(0.9417, device='cuda:0', grad_fn=<DivBackward0>), 3.0878238677978516, 0.4011292457580567]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [165 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [165][0/196]	Time 0.081 (0.081)	Data 0.222 (0.222)	Loss 0.9266 (0.9266)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [165][10/196]	Time 0.016 (0.023)	Data 0.002 (0.022)	Loss 0.9118 (0.9065)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [165][20/196]	Time 0.016 (0.020)	Data 0.002 (0.013)	Loss 0.9076 (0.9017)	Acc@1 99.219 (99.386)	Acc@5 100.000 (100.000)
Epoch: [165][30/196]	Time 0.015 (0.018)	Data 0.002 (0.010)	Loss 0.8884 (0.9007)	Acc@1 99.609 (99.420)	Acc@5 100.000 (100.000)
Epoch: [165][40/196]	Time 0.018 (0.018)	Data 0.002 (0.008)	Loss 0.8995 (0.9008)	Acc@1 99.219 (99.381)	Acc@5 100.000 (100.000)
Epoch: [165][50/196]	Time 0.017 (0.017)	Data 0.002 (0.007)	Loss 0.8879 (0.9008)	Acc@1 100.000 (99.357)	Acc@5 100.000 (100.000)
Epoch: [165][60/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 0.8954 (0.9004)	Acc@1 99.609 (99.347)	Acc@5 100.000 (100.000)
Epoch: [165][70/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 0.9243 (0.9013)	Acc@1 98.438 (99.323)	Acc@5 99.609 (99.994)
Epoch: [165][80/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 0.8980 (0.9016)	Acc@1 98.828 (99.310)	Acc@5 100.000 (99.995)
Epoch: [165][90/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.8941 (0.9011)	Acc@1 99.609 (99.305)	Acc@5 100.000 (99.996)
Epoch: [165][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.8910 (0.9008)	Acc@1 99.219 (99.288)	Acc@5 100.000 (99.996)
Epoch: [165][110/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.9067 (0.9007)	Acc@1 98.047 (99.268)	Acc@5 100.000 (99.996)
Epoch: [165][120/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.8952 (0.9006)	Acc@1 99.219 (99.254)	Acc@5 100.000 (99.997)
Epoch: [165][130/196]	Time 0.016 (0.016)	Data 0.003 (0.005)	Loss 0.9000 (0.9003)	Acc@1 98.828 (99.263)	Acc@5 100.000 (99.997)
Epoch: [165][140/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.8905 (0.8999)	Acc@1 99.609 (99.274)	Acc@5 100.000 (99.997)
Epoch: [165][150/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 0.8889 (0.8996)	Acc@1 99.609 (99.265)	Acc@5 100.000 (99.997)
Epoch: [165][160/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8942 (0.8991)	Acc@1 98.828 (99.279)	Acc@5 100.000 (99.998)
Epoch: [165][170/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.8941 (0.8988)	Acc@1 99.219 (99.283)	Acc@5 100.000 (99.998)
Epoch: [165][180/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8944 (0.8988)	Acc@1 99.219 (99.266)	Acc@5 100.000 (99.998)
Epoch: [165][190/196]	Time 0.012 (0.016)	Data 0.002 (0.004)	Loss 0.9090 (0.8988)	Acc@1 98.047 (99.249)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8988012544441223, 1.2866427201032637, 99.242, 69.67, tensor(0.9480, device='cuda:0', grad_fn=<DivBackward0>), 3.0768215656280518, 0.4079046249389648]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [166 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [166][0/196]	Time 0.080 (0.080)	Data 0.207 (0.207)	Loss 0.8850 (0.8850)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [166][10/196]	Time 0.015 (0.022)	Data 0.002 (0.020)	Loss 0.8812 (0.8844)	Acc@1 99.609 (99.467)	Acc@5 100.000 (100.000)
Epoch: [166][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 0.8934 (0.8858)	Acc@1 98.828 (99.386)	Acc@5 100.000 (100.000)
Epoch: [166][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 0.8966 (0.8865)	Acc@1 98.828 (99.383)	Acc@5 100.000 (100.000)
Epoch: [166][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 0.8755 (0.8861)	Acc@1 100.000 (99.428)	Acc@5 100.000 (100.000)
Epoch: [166][50/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 0.8801 (0.8862)	Acc@1 99.609 (99.433)	Acc@5 100.000 (100.000)
Epoch: [166][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 0.8865 (0.8868)	Acc@1 99.609 (99.360)	Acc@5 100.000 (100.000)
Epoch: [166][70/196]	Time 0.016 (0.017)	Data 0.000 (0.005)	Loss 0.8742 (0.8871)	Acc@1 100.000 (99.356)	Acc@5 100.000 (100.000)
Epoch: [166][80/196]	Time 0.018 (0.017)	Data 0.001 (0.005)	Loss 0.8876 (0.8869)	Acc@1 100.000 (99.359)	Acc@5 100.000 (100.000)
Epoch: [166][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.8944 (0.8867)	Acc@1 99.609 (99.352)	Acc@5 100.000 (100.000)
Epoch: [166][100/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8791 (0.8864)	Acc@1 99.219 (99.315)	Acc@5 100.000 (100.000)
Epoch: [166][110/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 0.8715 (0.8858)	Acc@1 100.000 (99.324)	Acc@5 100.000 (100.000)
Epoch: [166][120/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8999 (0.8857)	Acc@1 98.438 (99.322)	Acc@5 100.000 (100.000)
Epoch: [166][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8956 (0.8857)	Acc@1 99.219 (99.323)	Acc@5 99.609 (99.997)
Epoch: [166][140/196]	Time 0.018 (0.016)	Data 0.001 (0.004)	Loss 0.8861 (0.8854)	Acc@1 98.828 (99.316)	Acc@5 100.000 (99.997)
Epoch: [166][150/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8799 (0.8850)	Acc@1 98.828 (99.299)	Acc@5 100.000 (99.997)
Epoch: [166][160/196]	Time 0.016 (0.016)	Data 0.002 (0.004)	Loss 0.8968 (0.8850)	Acc@1 99.219 (99.284)	Acc@5 100.000 (99.998)
Epoch: [166][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8663 (0.8847)	Acc@1 100.000 (99.285)	Acc@5 100.000 (99.998)
Epoch: [166][180/196]	Time 0.015 (0.016)	Data 0.002 (0.004)	Loss 0.9028 (0.8848)	Acc@1 98.438 (99.273)	Acc@5 100.000 (99.998)
Epoch: [166][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 0.8874 (0.8848)	Acc@1 99.219 (99.258)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8846415692138672, 1.2887187242507934, 99.25, 69.59, tensor(0.9479, device='cuda:0', grad_fn=<DivBackward0>), 3.1720972061157227, 0.4053006172180176]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [167 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [167][0/196]	Time 0.084 (0.084)	Data 0.217 (0.217)	Loss 0.8648 (0.8648)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [167][10/196]	Time 0.016 (0.022)	Data 0.002 (0.022)	Loss 0.8651 (0.8710)	Acc@1 99.219 (99.467)	Acc@5 100.000 (100.000)
Epoch: [167][20/196]	Time 0.015 (0.019)	Data 0.002 (0.013)	Loss 0.8729 (0.8723)	Acc@1 99.219 (99.386)	Acc@5 100.000 (100.000)
Epoch: [167][30/196]	Time 0.016 (0.018)	Data 0.003 (0.010)	Loss 0.8692 (0.8707)	Acc@1 99.609 (99.458)	Acc@5 100.000 (100.000)
Epoch: [167][40/196]	Time 0.013 (0.017)	Data 0.004 (0.008)	Loss 0.8716 (0.8715)	Acc@1 99.219 (99.400)	Acc@5 100.000 (100.000)
Epoch: [167][50/196]	Time 0.013 (0.017)	Data 0.004 (0.007)	Loss 0.8636 (0.8717)	Acc@1 99.609 (99.380)	Acc@5 100.000 (100.000)
Epoch: [167][60/196]	Time 0.012 (0.016)	Data 0.007 (0.006)	Loss 0.8584 (0.8709)	Acc@1 100.000 (99.379)	Acc@5 100.000 (100.000)
Epoch: [167][70/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 0.8755 (0.8710)	Acc@1 99.219 (99.373)	Acc@5 100.000 (100.000)
Epoch: [167][80/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 0.8744 (0.8706)	Acc@1 99.219 (99.354)	Acc@5 100.000 (100.000)
Epoch: [167][90/196]	Time 0.013 (0.016)	Data 0.005 (0.006)	Loss 0.8611 (0.8701)	Acc@1 99.609 (99.339)	Acc@5 100.000 (100.000)
Epoch: [167][100/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 0.8546 (0.8699)	Acc@1 100.000 (99.354)	Acc@5 100.000 (100.000)
Epoch: [167][110/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8612 (0.8695)	Acc@1 100.000 (99.367)	Acc@5 100.000 (100.000)
Epoch: [167][120/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.8585 (0.8691)	Acc@1 99.609 (99.364)	Acc@5 100.000 (99.997)
Epoch: [167][130/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.8657 (0.8692)	Acc@1 98.828 (99.341)	Acc@5 100.000 (99.997)
Epoch: [167][140/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.8459 (0.8689)	Acc@1 100.000 (99.352)	Acc@5 100.000 (99.997)
Epoch: [167][150/196]	Time 0.014 (0.015)	Data 0.011 (0.005)	Loss 0.8529 (0.8687)	Acc@1 99.609 (99.356)	Acc@5 100.000 (99.997)
Epoch: [167][160/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.8539 (0.8687)	Acc@1 99.609 (99.347)	Acc@5 100.000 (99.998)
Epoch: [167][170/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.8582 (0.8686)	Acc@1 100.000 (99.344)	Acc@5 100.000 (99.998)
Epoch: [167][180/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.8652 (0.8684)	Acc@1 99.219 (99.337)	Acc@5 100.000 (99.998)
Epoch: [167][190/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.8623 (0.8680)	Acc@1 98.438 (99.329)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8681165184974671, 1.2892170655727386, 99.32, 69.68, tensor(0.9506, device='cuda:0', grad_fn=<DivBackward0>), 2.9207639694213867, 0.40605831146240234]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [168 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [168][0/196]	Time 0.080 (0.080)	Data 0.217 (0.217)	Loss 0.8512 (0.8512)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [168][10/196]	Time 0.015 (0.023)	Data 0.002 (0.021)	Loss 0.8761 (0.8562)	Acc@1 98.828 (99.538)	Acc@5 100.000 (100.000)
Epoch: [168][20/196]	Time 0.014 (0.020)	Data 0.004 (0.012)	Loss 0.8464 (0.8549)	Acc@1 99.219 (99.516)	Acc@5 100.000 (100.000)
Epoch: [168][30/196]	Time 0.015 (0.018)	Data 0.003 (0.009)	Loss 0.8518 (0.8536)	Acc@1 99.609 (99.509)	Acc@5 100.000 (100.000)
Epoch: [168][40/196]	Time 0.013 (0.018)	Data 0.004 (0.007)	Loss 0.8444 (0.8538)	Acc@1 100.000 (99.486)	Acc@5 100.000 (100.000)
Epoch: [168][50/196]	Time 0.016 (0.017)	Data 0.002 (0.006)	Loss 0.8628 (0.8535)	Acc@1 98.828 (99.464)	Acc@5 100.000 (100.000)
Epoch: [168][60/196]	Time 0.012 (0.017)	Data 0.005 (0.006)	Loss 0.8607 (0.8551)	Acc@1 99.219 (99.443)	Acc@5 100.000 (100.000)
Epoch: [168][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.8458 (0.8551)	Acc@1 99.609 (99.461)	Acc@5 100.000 (100.000)
Epoch: [168][80/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 0.8500 (0.8549)	Acc@1 99.219 (99.474)	Acc@5 100.000 (100.000)
Epoch: [168][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.8359 (0.8539)	Acc@1 100.000 (99.485)	Acc@5 100.000 (100.000)
Epoch: [168][100/196]	Time 0.021 (0.016)	Data 0.001 (0.005)	Loss 0.8399 (0.8531)	Acc@1 100.000 (99.497)	Acc@5 100.000 (100.000)
Epoch: [168][110/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 0.8770 (0.8531)	Acc@1 98.438 (99.493)	Acc@5 100.000 (100.000)
Epoch: [168][120/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8508 (0.8529)	Acc@1 99.219 (99.480)	Acc@5 100.000 (100.000)
Epoch: [168][130/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8410 (0.8531)	Acc@1 99.609 (99.454)	Acc@5 100.000 (100.000)
Epoch: [168][140/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8440 (0.8529)	Acc@1 99.609 (99.446)	Acc@5 100.000 (100.000)
Epoch: [168][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 0.8317 (0.8525)	Acc@1 100.000 (99.436)	Acc@5 100.000 (100.000)
Epoch: [168][160/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8448 (0.8523)	Acc@1 100.000 (99.444)	Acc@5 100.000 (100.000)
Epoch: [168][170/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.8392 (0.8523)	Acc@1 99.609 (99.438)	Acc@5 100.000 (100.000)
Epoch: [168][180/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 0.8338 (0.8520)	Acc@1 99.609 (99.432)	Acc@5 100.000 (100.000)
Epoch: [168][190/196]	Time 0.017 (0.016)	Data 0.000 (0.004)	Loss 0.8457 (0.8516)	Acc@1 99.219 (99.429)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.851480547504425, 1.2964211946725845, 99.428, 69.63, tensor(0.9537, device='cuda:0', grad_fn=<DivBackward0>), 3.1560235023498535, 0.4196105003356934]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [169 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [169][0/196]	Time 0.084 (0.084)	Data 0.203 (0.203)	Loss 0.8415 (0.8415)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [169][10/196]	Time 0.015 (0.022)	Data 0.003 (0.020)	Loss 0.8366 (0.8408)	Acc@1 100.000 (99.503)	Acc@5 100.000 (100.000)
Epoch: [169][20/196]	Time 0.014 (0.019)	Data 0.003 (0.012)	Loss 0.8293 (0.8399)	Acc@1 100.000 (99.442)	Acc@5 100.000 (100.000)
Epoch: [169][30/196]	Time 0.015 (0.018)	Data 0.003 (0.009)	Loss 0.8371 (0.8403)	Acc@1 99.609 (99.458)	Acc@5 100.000 (100.000)
Epoch: [169][40/196]	Time 0.015 (0.017)	Data 0.003 (0.007)	Loss 0.8253 (0.8400)	Acc@1 100.000 (99.486)	Acc@5 100.000 (100.000)
Epoch: [169][50/196]	Time 0.016 (0.017)	Data 0.001 (0.006)	Loss 0.8462 (0.8395)	Acc@1 99.219 (99.487)	Acc@5 100.000 (100.000)
Epoch: [169][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 0.8299 (0.8387)	Acc@1 99.609 (99.507)	Acc@5 100.000 (100.000)
Epoch: [169][70/196]	Time 0.016 (0.017)	Data 0.002 (0.005)	Loss 0.8473 (0.8386)	Acc@1 98.828 (99.505)	Acc@5 100.000 (100.000)
Epoch: [169][80/196]	Time 0.015 (0.017)	Data 0.002 (0.005)	Loss 0.8403 (0.8380)	Acc@1 99.219 (99.518)	Acc@5 100.000 (100.000)
Epoch: [169][90/196]	Time 0.017 (0.017)	Data 0.001 (0.005)	Loss 0.8295 (0.8378)	Acc@1 100.000 (99.506)	Acc@5 100.000 (100.000)
Epoch: [169][100/196]	Time 0.013 (0.016)	Data 0.005 (0.005)	Loss 0.8385 (0.8376)	Acc@1 99.609 (99.513)	Acc@5 100.000 (100.000)
Epoch: [169][110/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8478 (0.8379)	Acc@1 98.438 (99.490)	Acc@5 100.000 (100.000)
Epoch: [169][120/196]	Time 0.012 (0.016)	Data 0.005 (0.004)	Loss 0.8505 (0.8376)	Acc@1 99.219 (99.490)	Acc@5 100.000 (100.000)
Epoch: [169][130/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8291 (0.8375)	Acc@1 100.000 (99.487)	Acc@5 100.000 (100.000)
Epoch: [169][140/196]	Time 0.012 (0.016)	Data 0.004 (0.004)	Loss 0.8355 (0.8375)	Acc@1 98.828 (99.474)	Acc@5 100.000 (100.000)
Epoch: [169][150/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.8315 (0.8373)	Acc@1 99.609 (99.477)	Acc@5 100.000 (100.000)
Epoch: [169][160/196]	Time 0.014 (0.016)	Data 0.003 (0.004)	Loss 0.8404 (0.8370)	Acc@1 99.219 (99.474)	Acc@5 100.000 (100.000)
Epoch: [169][170/196]	Time 0.017 (0.016)	Data 0.001 (0.004)	Loss 0.8345 (0.8370)	Acc@1 99.609 (99.477)	Acc@5 100.000 (99.998)
Epoch: [169][180/196]	Time 0.015 (0.016)	Data 0.003 (0.004)	Loss 0.8312 (0.8370)	Acc@1 99.609 (99.463)	Acc@5 100.000 (99.998)
Epoch: [169][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 0.8347 (0.8365)	Acc@1 99.219 (99.470)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.8366321411323547, 1.2885025501251222, 99.466, 69.78, tensor(0.9549, device='cuda:0', grad_fn=<DivBackward0>), 3.0730807781219482, 0.4223744869232177]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [170 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [170][0/196]	Time 0.083 (0.083)	Data 0.222 (0.222)	Loss 0.8179 (0.8179)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [170][10/196]	Time 0.016 (0.022)	Data 0.002 (0.022)	Loss 0.8362 (0.8240)	Acc@1 99.219 (99.574)	Acc@5 100.000 (100.000)
Epoch: [170][20/196]	Time 0.015 (0.019)	Data 0.002 (0.013)	Loss 0.8203 (0.8259)	Acc@1 100.000 (99.572)	Acc@5 100.000 (100.000)
Epoch: [170][30/196]	Time 0.014 (0.018)	Data 0.005 (0.009)	Loss 0.8191 (0.8257)	Acc@1 100.000 (99.534)	Acc@5 100.000 (100.000)
Epoch: [170][40/196]	Time 0.015 (0.017)	Data 0.002 (0.008)	Loss 0.8442 (0.8246)	Acc@1 99.609 (99.581)	Acc@5 100.000 (100.000)
Epoch: [170][50/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 0.8167 (0.8240)	Acc@1 99.609 (99.548)	Acc@5 100.000 (100.000)
Epoch: [170][60/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 0.8359 (0.8248)	Acc@1 98.828 (99.494)	Acc@5 100.000 (100.000)
Epoch: [170][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 0.8356 (0.8245)	Acc@1 99.609 (99.510)	Acc@5 100.000 (100.000)
Epoch: [170][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.8146 (0.8238)	Acc@1 99.609 (99.527)	Acc@5 100.000 (100.000)
Epoch: [170][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.8319 (0.8232)	Acc@1 99.219 (99.541)	Acc@5 100.000 (100.000)
Epoch: [170][100/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.8193 (0.8229)	Acc@1 99.219 (99.540)	Acc@5 100.000 (100.000)
Epoch: [170][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.8114 (0.8226)	Acc@1 100.000 (99.525)	Acc@5 100.000 (100.000)
Epoch: [170][120/196]	Time 0.013 (0.015)	Data 0.013 (0.005)	Loss 0.8162 (0.8224)	Acc@1 100.000 (99.529)	Acc@5 100.000 (100.000)
Epoch: [170][130/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.8061 (0.8222)	Acc@1 99.609 (99.514)	Acc@5 100.000 (100.000)
Epoch: [170][140/196]	Time 0.013 (0.015)	Data 0.007 (0.005)	Loss 0.8064 (0.8220)	Acc@1 100.000 (99.512)	Acc@5 100.000 (100.000)
Epoch: [170][150/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.8134 (0.8220)	Acc@1 99.219 (99.506)	Acc@5 100.000 (100.000)
Epoch: [170][160/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 0.8151 (0.8216)	Acc@1 99.219 (99.500)	Acc@5 100.000 (100.000)
Epoch: [170][170/196]	Time 0.012 (0.015)	Data 0.005 (0.005)	Loss 0.8144 (0.8214)	Acc@1 99.609 (99.504)	Acc@5 100.000 (100.000)
Epoch: [170][180/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.8174 (0.8210)	Acc@1 99.609 (99.506)	Acc@5 100.000 (100.000)
Epoch: [170][190/196]	Time 0.016 (0.015)	Data 0.003 (0.005)	Loss 0.8171 (0.8212)	Acc@1 99.219 (99.483)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.8211710486221313, 1.2930415868759155, 99.48, 69.7, tensor(0.9570, device='cuda:0', grad_fn=<DivBackward0>), 2.953015089035034, 0.41648721694946295]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [246, 128, 3, 3]
Before - module.bn3.weight: [246]
Before - module.bn3.bias: [246]
Before - module.conv4.weight: [256, 246, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [509, 256, 3, 3]
Before - module.bn5.weight: [509]
Before - module.bn5.bias: [509]
Before - module.conv6.weight: [507, 509, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [479, 507, 3, 3]
Before - module.bn7.weight: [479]
Before - module.bn7.bias: [479]
Before - module.conv8.weight: [298, 479, 3, 3]
Before - module.bn8.weight: [298]
Before - module.bn8.bias: [298]
Before - module.fc.weight: [100, 298]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [246, 128, 3, 3]
After - module.bn3.weight: [246]
After - module.bn3.bias: [246]
After - module.conv4.weight: [256, 246, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [509, 256, 3, 3]
After - module.bn5.weight: [509]
After - module.bn5.bias: [509]
After - module.conv6.weight: [507, 509, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [479, 507, 3, 3]
After - module.bn7.weight: [479]
After - module.bn7.bias: [479]
After - module.conv8.weight: [298, 479, 3, 3]
After - module.bn8.weight: [298]
After - module.bn8.bias: [298]
After - module.fc.weight: [100, 298]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [246, 128, 3, 3]
conv4 --> [256, 246, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [507, 509, 3, 3]
conv7 --> [479, 507, 3, 3]
conv8 --> [298, 479, 3, 3]
fc --> [298, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8270512128, 18137088, 246
4, 16541024256, 36274176, 256
5, 10207494144, 18763776, 509
6, 20215623168, 37161072, 507
7, 6714399744, 8742708, 479
8, 3946530816, 5138712, 298
fc, 11443200, 29800, 0
===================
FLOP REPORT: 27949050600000.0 50907200000.0 137794852 127268 2465 15.064077377319336
[INFO] Storing checkpoint...

Epoch: [171 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [171][0/196]	Time 0.077 (0.077)	Data 0.203 (0.203)	Loss 0.8264 (0.8264)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [171][10/196]	Time 0.016 (0.021)	Data 0.002 (0.021)	Loss 0.8134 (0.8114)	Acc@1 99.609 (99.396)	Acc@5 100.000 (100.000)
Epoch: [171][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 0.8064 (0.8102)	Acc@1 99.609 (99.516)	Acc@5 100.000 (100.000)
Epoch: [171][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 0.8023 (0.8101)	Acc@1 99.609 (99.534)	Acc@5 100.000 (100.000)
Epoch: [171][40/196]	Time 0.012 (0.017)	Data 0.005 (0.007)	Loss 0.8081 (0.8092)	Acc@1 99.609 (99.562)	Acc@5 100.000 (100.000)
Epoch: [171][50/196]	Time 0.016 (0.017)	Data 0.000 (0.006)	Loss 0.8203 (0.8094)	Acc@1 98.828 (99.533)	Acc@5 100.000 (100.000)
Epoch: [171][60/196]	Time 0.012 (0.017)	Data 0.013 (0.006)	Loss 0.8103 (0.8090)	Acc@1 99.219 (99.520)	Acc@5 100.000 (100.000)
Epoch: [171][70/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.8053 (0.8086)	Acc@1 99.609 (99.527)	Acc@5 100.000 (100.000)
Epoch: [171][80/196]	Time 0.012 (0.016)	Data 0.014 (0.005)	Loss 0.8199 (0.8083)	Acc@1 99.219 (99.547)	Acc@5 100.000 (100.000)
Epoch: [171][90/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 0.8155 (0.8084)	Acc@1 99.219 (99.532)	Acc@5 100.000 (100.000)
Epoch: [171][100/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 0.8121 (0.8084)	Acc@1 99.219 (99.540)	Acc@5 100.000 (100.000)
Epoch: [171][110/196]	Time 0.016 (0.016)	Data 0.002 (0.005)	Loss 0.8010 (0.8081)	Acc@1 100.000 (99.550)	Acc@5 100.000 (100.000)
Epoch: [171][120/196]	Time 0.014 (0.016)	Data 0.007 (0.005)	Loss 0.7997 (0.8079)	Acc@1 99.609 (99.535)	Acc@5 100.000 (100.000)
Epoch: [171][130/196]	Time 0.014 (0.016)	Data 0.002 (0.004)	Loss 0.8082 (0.8075)	Acc@1 99.609 (99.538)	Acc@5 100.000 (100.000)
Epoch: [171][140/196]	Time 0.013 (0.016)	Data 0.009 (0.004)	Loss 0.8013 (0.8073)	Acc@1 100.000 (99.523)	Acc@5 100.000 (100.000)
Epoch: [171][150/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 0.8143 (0.8072)	Acc@1 99.219 (99.532)	Acc@5 100.000 (100.000)
Epoch: [171][160/196]	Time 0.015 (0.016)	Data 0.017 (0.004)	Loss 0.7931 (0.8069)	Acc@1 99.609 (99.532)	Acc@5 100.000 (100.000)
Epoch: [171][170/196]	Time 0.016 (0.016)	Data 0.001 (0.004)	Loss 0.7935 (0.8065)	Acc@1 99.609 (99.532)	Acc@5 100.000 (100.000)
Epoch: [171][180/196]	Time 0.011 (0.016)	Data 0.011 (0.004)	Loss 0.7902 (0.8062)	Acc@1 100.000 (99.532)	Acc@5 100.000 (100.000)
Epoch: [171][190/196]	Time 0.016 (0.016)	Data 0.000 (0.004)	Loss 0.7975 (0.8059)	Acc@1 99.609 (99.532)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.8059894891738891, 1.2981021189689637, 99.524, 69.62, tensor(0.9591, device='cuda:0', grad_fn=<DivBackward0>), 3.0965895652770996, 0.3978245258331299]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [172 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [172][0/196]	Time 0.076 (0.076)	Data 0.207 (0.207)	Loss 0.8108 (0.8108)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [172][10/196]	Time 0.015 (0.022)	Data 0.002 (0.021)	Loss 0.8077 (0.8007)	Acc@1 99.219 (99.396)	Acc@5 100.000 (100.000)
Epoch: [172][20/196]	Time 0.015 (0.019)	Data 0.002 (0.012)	Loss 0.7869 (0.7979)	Acc@1 100.000 (99.479)	Acc@5 100.000 (100.000)
Epoch: [172][30/196]	Time 0.017 (0.018)	Data 0.002 (0.009)	Loss 0.7892 (0.7968)	Acc@1 100.000 (99.546)	Acc@5 100.000 (100.000)
Epoch: [172][40/196]	Time 0.015 (0.017)	Data 0.002 (0.007)	Loss 0.7954 (0.7966)	Acc@1 100.000 (99.571)	Acc@5 100.000 (100.000)
Epoch: [172][50/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 0.7864 (0.7961)	Acc@1 100.000 (99.594)	Acc@5 100.000 (100.000)
Epoch: [172][60/196]	Time 0.013 (0.016)	Data 0.004 (0.006)	Loss 0.7900 (0.7954)	Acc@1 100.000 (99.629)	Acc@5 100.000 (100.000)
Epoch: [172][70/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 0.7898 (0.7954)	Acc@1 100.000 (99.620)	Acc@5 100.000 (100.000)
Epoch: [172][80/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 0.8019 (0.7957)	Acc@1 99.609 (99.590)	Acc@5 100.000 (100.000)
Epoch: [172][90/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.8056 (0.7959)	Acc@1 98.828 (99.571)	Acc@5 100.000 (100.000)
Epoch: [172][100/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 0.7835 (0.7952)	Acc@1 100.000 (99.590)	Acc@5 100.000 (100.000)
Epoch: [172][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7837 (0.7953)	Acc@1 100.000 (99.585)	Acc@5 100.000 (100.000)
Epoch: [172][120/196]	Time 0.011 (0.015)	Data 0.010 (0.005)	Loss 0.7952 (0.7953)	Acc@1 99.219 (99.561)	Acc@5 100.000 (100.000)
Epoch: [172][130/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 0.7918 (0.7951)	Acc@1 99.609 (99.553)	Acc@5 100.000 (100.000)
Epoch: [172][140/196]	Time 0.011 (0.015)	Data 0.007 (0.005)	Loss 0.7973 (0.7953)	Acc@1 99.219 (99.540)	Acc@5 100.000 (100.000)
Epoch: [172][150/196]	Time 0.013 (0.015)	Data 0.011 (0.005)	Loss 0.7833 (0.7951)	Acc@1 100.000 (99.529)	Acc@5 100.000 (100.000)
Epoch: [172][160/196]	Time 0.013 (0.015)	Data 0.004 (0.005)	Loss 0.7777 (0.7946)	Acc@1 100.000 (99.534)	Acc@5 100.000 (100.000)
Epoch: [172][170/196]	Time 0.012 (0.015)	Data 0.009 (0.005)	Loss 0.7805 (0.7944)	Acc@1 100.000 (99.536)	Acc@5 100.000 (100.000)
Epoch: [172][180/196]	Time 0.012 (0.015)	Data 0.004 (0.005)	Loss 0.8029 (0.7943)	Acc@1 98.438 (99.525)	Acc@5 100.000 (100.000)
Epoch: [172][190/196]	Time 0.011 (0.015)	Data 0.008 (0.005)	Loss 0.7859 (0.7939)	Acc@1 100.000 (99.530)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7937569416618347, 1.2869445544481277, 99.528, 69.81, tensor(0.9578, device='cuda:0', grad_fn=<DivBackward0>), 2.9068446159362793, 0.39790248870849615]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [173 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [173][0/196]	Time 0.077 (0.077)	Data 0.205 (0.205)	Loss 0.7818 (0.7818)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [173][10/196]	Time 0.015 (0.022)	Data 0.003 (0.020)	Loss 0.7759 (0.7784)	Acc@1 100.000 (99.893)	Acc@5 100.000 (100.000)
Epoch: [173][20/196]	Time 0.014 (0.019)	Data 0.003 (0.012)	Loss 0.7753 (0.7811)	Acc@1 100.000 (99.740)	Acc@5 100.000 (100.000)
Epoch: [173][30/196]	Time 0.019 (0.018)	Data 0.002 (0.009)	Loss 0.7708 (0.7795)	Acc@1 100.000 (99.761)	Acc@5 100.000 (100.000)
Epoch: [173][40/196]	Time 0.019 (0.017)	Data 0.002 (0.007)	Loss 0.7813 (0.7790)	Acc@1 99.609 (99.781)	Acc@5 100.000 (100.000)
Epoch: [173][50/196]	Time 0.014 (0.017)	Data 0.003 (0.006)	Loss 0.7699 (0.7788)	Acc@1 100.000 (99.755)	Acc@5 100.000 (100.000)
Epoch: [173][60/196]	Time 0.015 (0.017)	Data 0.003 (0.006)	Loss 0.7802 (0.7791)	Acc@1 99.609 (99.731)	Acc@5 100.000 (99.994)
Epoch: [173][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.7864 (0.7790)	Acc@1 99.609 (99.730)	Acc@5 100.000 (99.994)
Epoch: [173][80/196]	Time 0.013 (0.016)	Data 0.006 (0.005)	Loss 0.7767 (0.7791)	Acc@1 100.000 (99.720)	Acc@5 100.000 (99.995)
Epoch: [173][90/196]	Time 0.017 (0.016)	Data 0.001 (0.005)	Loss 0.7723 (0.7793)	Acc@1 100.000 (99.704)	Acc@5 100.000 (99.996)
Epoch: [173][100/196]	Time 0.012 (0.016)	Data 0.010 (0.005)	Loss 0.7792 (0.7791)	Acc@1 99.609 (99.691)	Acc@5 100.000 (99.996)
Epoch: [173][110/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 0.7844 (0.7788)	Acc@1 99.609 (99.673)	Acc@5 100.000 (99.996)
Epoch: [173][120/196]	Time 0.011 (0.015)	Data 0.011 (0.005)	Loss 0.7831 (0.7787)	Acc@1 99.219 (99.677)	Acc@5 100.000 (99.997)
Epoch: [173][130/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7622 (0.7784)	Acc@1 100.000 (99.684)	Acc@5 100.000 (99.997)
Epoch: [173][140/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 0.7805 (0.7780)	Acc@1 99.609 (99.690)	Acc@5 100.000 (99.997)
Epoch: [173][150/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 0.7809 (0.7779)	Acc@1 99.609 (99.674)	Acc@5 100.000 (99.997)
Epoch: [173][160/196]	Time 0.012 (0.015)	Data 0.011 (0.004)	Loss 0.7748 (0.7778)	Acc@1 99.219 (99.660)	Acc@5 100.000 (99.995)
Epoch: [173][170/196]	Time 0.014 (0.015)	Data 0.002 (0.004)	Loss 0.7715 (0.7778)	Acc@1 100.000 (99.655)	Acc@5 100.000 (99.995)
Epoch: [173][180/196]	Time 0.012 (0.015)	Data 0.005 (0.004)	Loss 0.7757 (0.7776)	Acc@1 99.609 (99.644)	Acc@5 100.000 (99.996)
Epoch: [173][190/196]	Time 0.011 (0.015)	Data 0.006 (0.004)	Loss 0.7807 (0.7775)	Acc@1 98.828 (99.634)	Acc@5 100.000 (99.996)
num momentum params: 26
[0.010000000000000002, 0.777476310749054, 1.29285303235054, 99.634, 69.83, tensor(0.9616, device='cuda:0', grad_fn=<DivBackward0>), 2.9333677291870117, 0.3969578742980957]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [174 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [174][0/196]	Time 0.081 (0.081)	Data 0.202 (0.202)	Loss 0.7585 (0.7585)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [174][10/196]	Time 0.016 (0.022)	Data 0.003 (0.020)	Loss 0.7688 (0.7673)	Acc@1 99.219 (99.716)	Acc@5 100.000 (100.000)
Epoch: [174][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 0.7755 (0.7691)	Acc@1 99.219 (99.647)	Acc@5 100.000 (100.000)
Epoch: [174][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 0.7622 (0.7675)	Acc@1 99.609 (99.723)	Acc@5 100.000 (100.000)
Epoch: [174][40/196]	Time 0.017 (0.017)	Data 0.002 (0.007)	Loss 0.7669 (0.7671)	Acc@1 99.609 (99.714)	Acc@5 100.000 (100.000)
Epoch: [174][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 0.7598 (0.7666)	Acc@1 100.000 (99.701)	Acc@5 100.000 (100.000)
Epoch: [174][60/196]	Time 0.018 (0.016)	Data 0.003 (0.006)	Loss 0.7670 (0.7667)	Acc@1 99.219 (99.693)	Acc@5 100.000 (100.000)
Epoch: [174][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.7695 (0.7665)	Acc@1 99.219 (99.670)	Acc@5 100.000 (100.000)
Epoch: [174][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.7680 (0.7664)	Acc@1 99.219 (99.648)	Acc@5 100.000 (100.000)
Epoch: [174][90/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.7701 (0.7661)	Acc@1 100.000 (99.657)	Acc@5 100.000 (100.000)
Epoch: [174][100/196]	Time 0.016 (0.016)	Data 0.000 (0.005)	Loss 0.7639 (0.7656)	Acc@1 100.000 (99.664)	Acc@5 100.000 (100.000)
Epoch: [174][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7657 (0.7658)	Acc@1 100.000 (99.652)	Acc@5 100.000 (100.000)
Epoch: [174][120/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7564 (0.7655)	Acc@1 100.000 (99.664)	Acc@5 100.000 (100.000)
Epoch: [174][130/196]	Time 0.018 (0.015)	Data 0.002 (0.005)	Loss 0.7606 (0.7653)	Acc@1 99.609 (99.657)	Acc@5 100.000 (100.000)
Epoch: [174][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7620 (0.7651)	Acc@1 99.609 (99.651)	Acc@5 100.000 (100.000)
Epoch: [174][150/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.7653 (0.7651)	Acc@1 99.609 (99.640)	Acc@5 100.000 (100.000)
Epoch: [174][160/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.7623 (0.7651)	Acc@1 99.609 (99.619)	Acc@5 100.000 (100.000)
Epoch: [174][170/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7625 (0.7650)	Acc@1 98.828 (99.612)	Acc@5 100.000 (100.000)
Epoch: [174][180/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.7598 (0.7650)	Acc@1 99.609 (99.586)	Acc@5 100.000 (100.000)
Epoch: [174][190/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.7756 (0.7648)	Acc@1 98.828 (99.583)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7645762621688843, 1.3015688061714172, 99.586, 69.77, tensor(0.9614, device='cuda:0', grad_fn=<DivBackward0>), 2.8893942832946777, 0.400465726852417]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [175 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [175][0/196]	Time 0.077 (0.077)	Data 0.208 (0.208)	Loss 0.7493 (0.7493)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [175][10/196]	Time 0.016 (0.022)	Data 0.002 (0.021)	Loss 0.7672 (0.7590)	Acc@1 98.828 (99.645)	Acc@5 100.000 (100.000)
Epoch: [175][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 0.7617 (0.7601)	Acc@1 99.219 (99.554)	Acc@5 100.000 (100.000)
Epoch: [175][30/196]	Time 0.015 (0.018)	Data 0.002 (0.009)	Loss 0.7542 (0.7585)	Acc@1 99.219 (99.546)	Acc@5 100.000 (100.000)
Epoch: [175][40/196]	Time 0.016 (0.017)	Data 0.003 (0.007)	Loss 0.7545 (0.7572)	Acc@1 100.000 (99.552)	Acc@5 100.000 (100.000)
Epoch: [175][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 0.7510 (0.7566)	Acc@1 99.609 (99.602)	Acc@5 100.000 (100.000)
Epoch: [175][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 0.7557 (0.7564)	Acc@1 99.609 (99.603)	Acc@5 100.000 (99.994)
Epoch: [175][70/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 0.7470 (0.7559)	Acc@1 100.000 (99.615)	Acc@5 100.000 (99.994)
Epoch: [175][80/196]	Time 0.016 (0.016)	Data 0.001 (0.005)	Loss 0.7594 (0.7553)	Acc@1 99.609 (99.629)	Acc@5 100.000 (99.995)
Epoch: [175][90/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.7606 (0.7546)	Acc@1 99.609 (99.648)	Acc@5 100.000 (99.996)
Epoch: [175][100/196]	Time 0.018 (0.016)	Data 0.001 (0.005)	Loss 0.7653 (0.7545)	Acc@1 99.219 (99.656)	Acc@5 100.000 (99.996)
Epoch: [175][110/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.7435 (0.7539)	Acc@1 100.000 (99.669)	Acc@5 100.000 (99.996)
Epoch: [175][120/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7446 (0.7534)	Acc@1 100.000 (99.677)	Acc@5 100.000 (99.997)
Epoch: [175][130/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.7458 (0.7530)	Acc@1 100.000 (99.690)	Acc@5 100.000 (99.997)
Epoch: [175][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7545 (0.7529)	Acc@1 99.609 (99.687)	Acc@5 100.000 (99.997)
Epoch: [175][150/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.7353 (0.7526)	Acc@1 100.000 (99.684)	Acc@5 100.000 (99.997)
Epoch: [175][160/196]	Time 0.016 (0.015)	Data 0.001 (0.004)	Loss 0.7458 (0.7523)	Acc@1 100.000 (99.677)	Acc@5 100.000 (99.998)
Epoch: [175][170/196]	Time 0.015 (0.015)	Data 0.003 (0.004)	Loss 0.7576 (0.7519)	Acc@1 99.609 (99.687)	Acc@5 100.000 (99.998)
Epoch: [175][180/196]	Time 0.017 (0.015)	Data 0.000 (0.004)	Loss 0.7479 (0.7519)	Acc@1 100.000 (99.681)	Acc@5 100.000 (99.998)
Epoch: [175][190/196]	Time 0.015 (0.015)	Data 0.002 (0.004)	Loss 0.7575 (0.7517)	Acc@1 99.219 (99.675)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.751498867149353, 1.3073903381824494, 99.678, 69.84, tensor(0.9617, device='cuda:0', grad_fn=<DivBackward0>), 2.983170986175537, 0.40441203117370605]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [176 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [176][0/196]	Time 0.083 (0.083)	Data 0.204 (0.204)	Loss 0.7356 (0.7356)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [176][10/196]	Time 0.017 (0.023)	Data 0.002 (0.020)	Loss 0.7362 (0.7419)	Acc@1 100.000 (99.822)	Acc@5 100.000 (100.000)
Epoch: [176][20/196]	Time 0.014 (0.019)	Data 0.002 (0.012)	Loss 0.7504 (0.7410)	Acc@1 99.219 (99.814)	Acc@5 100.000 (100.000)
Epoch: [176][30/196]	Time 0.014 (0.018)	Data 0.003 (0.009)	Loss 0.7764 (0.7409)	Acc@1 98.828 (99.786)	Acc@5 100.000 (100.000)
Epoch: [176][40/196]	Time 0.014 (0.017)	Data 0.003 (0.007)	Loss 0.7308 (0.7414)	Acc@1 100.000 (99.743)	Acc@5 100.000 (100.000)
Epoch: [176][50/196]	Time 0.015 (0.017)	Data 0.002 (0.006)	Loss 0.7427 (0.7412)	Acc@1 99.609 (99.747)	Acc@5 100.000 (100.000)
Epoch: [176][60/196]	Time 0.017 (0.016)	Data 0.003 (0.006)	Loss 0.7290 (0.7408)	Acc@1 100.000 (99.757)	Acc@5 100.000 (100.000)
Epoch: [176][70/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.7326 (0.7404)	Acc@1 99.609 (99.747)	Acc@5 100.000 (100.000)
Epoch: [176][80/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.7325 (0.7395)	Acc@1 100.000 (99.759)	Acc@5 100.000 (100.000)
Epoch: [176][90/196]	Time 0.014 (0.016)	Data 0.002 (0.005)	Loss 0.7340 (0.7392)	Acc@1 100.000 (99.760)	Acc@5 100.000 (100.000)
Epoch: [176][100/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.7362 (0.7391)	Acc@1 99.609 (99.752)	Acc@5 100.000 (100.000)
Epoch: [176][110/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.7403 (0.7388)	Acc@1 98.828 (99.747)	Acc@5 100.000 (100.000)
Epoch: [176][120/196]	Time 0.011 (0.015)	Data 0.006 (0.005)	Loss 0.7458 (0.7388)	Acc@1 99.219 (99.729)	Acc@5 100.000 (100.000)
Epoch: [176][130/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7326 (0.7387)	Acc@1 99.609 (99.717)	Acc@5 100.000 (100.000)
Epoch: [176][140/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.7267 (0.7384)	Acc@1 100.000 (99.720)	Acc@5 100.000 (100.000)
Epoch: [176][150/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7326 (0.7382)	Acc@1 99.609 (99.713)	Acc@5 100.000 (100.000)
Epoch: [176][160/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.7303 (0.7380)	Acc@1 100.000 (99.706)	Acc@5 100.000 (100.000)
Epoch: [176][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.7367 (0.7376)	Acc@1 99.219 (99.708)	Acc@5 100.000 (100.000)
Epoch: [176][180/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7279 (0.7373)	Acc@1 100.000 (99.711)	Acc@5 100.000 (100.000)
Epoch: [176][190/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7303 (0.7371)	Acc@1 99.609 (99.710)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.7370647101783753, 1.3055326843261719, 99.71, 69.88, tensor(0.9639, device='cuda:0', grad_fn=<DivBackward0>), 2.9223313331604004, 0.3956296443939209]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [177 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [177][0/196]	Time 0.079 (0.079)	Data 0.197 (0.197)	Loss 0.7277 (0.7277)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [177][10/196]	Time 0.015 (0.022)	Data 0.002 (0.020)	Loss 0.7224 (0.7304)	Acc@1 100.000 (99.716)	Acc@5 100.000 (100.000)
Epoch: [177][20/196]	Time 0.015 (0.019)	Data 0.002 (0.012)	Loss 0.7224 (0.7275)	Acc@1 99.609 (99.758)	Acc@5 100.000 (100.000)
Epoch: [177][30/196]	Time 0.014 (0.018)	Data 0.003 (0.009)	Loss 0.7256 (0.7289)	Acc@1 100.000 (99.698)	Acc@5 100.000 (100.000)
Epoch: [177][40/196]	Time 0.014 (0.017)	Data 0.003 (0.007)	Loss 0.7485 (0.7290)	Acc@1 99.219 (99.667)	Acc@5 100.000 (100.000)
Epoch: [177][50/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 0.7243 (0.7288)	Acc@1 100.000 (99.663)	Acc@5 100.000 (100.000)
Epoch: [177][60/196]	Time 0.016 (0.016)	Data 0.002 (0.006)	Loss 0.7412 (0.7282)	Acc@1 99.219 (99.673)	Acc@5 100.000 (100.000)
Epoch: [177][70/196]	Time 0.015 (0.016)	Data 0.003 (0.005)	Loss 0.7218 (0.7275)	Acc@1 100.000 (99.703)	Acc@5 100.000 (100.000)
Epoch: [177][80/196]	Time 0.011 (0.016)	Data 0.008 (0.005)	Loss 0.7227 (0.7271)	Acc@1 100.000 (99.706)	Acc@5 100.000 (100.000)
Epoch: [177][90/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7144 (0.7266)	Acc@1 100.000 (99.717)	Acc@5 100.000 (100.000)
Epoch: [177][100/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7248 (0.7262)	Acc@1 99.219 (99.725)	Acc@5 100.000 (100.000)
Epoch: [177][110/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7415 (0.7259)	Acc@1 99.609 (99.740)	Acc@5 100.000 (100.000)
Epoch: [177][120/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7338 (0.7256)	Acc@1 98.828 (99.735)	Acc@5 100.000 (100.000)
Epoch: [177][130/196]	Time 0.016 (0.015)	Data 0.001 (0.005)	Loss 0.7219 (0.7256)	Acc@1 99.609 (99.726)	Acc@5 100.000 (100.000)
Epoch: [177][140/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7153 (0.7254)	Acc@1 100.000 (99.717)	Acc@5 100.000 (100.000)
Epoch: [177][150/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.7166 (0.7251)	Acc@1 99.609 (99.715)	Acc@5 100.000 (100.000)
Epoch: [177][160/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7151 (0.7249)	Acc@1 100.000 (99.716)	Acc@5 100.000 (100.000)
Epoch: [177][170/196]	Time 0.014 (0.014)	Data 0.002 (0.005)	Loss 0.7279 (0.7247)	Acc@1 99.609 (99.708)	Acc@5 100.000 (100.000)
Epoch: [177][180/196]	Time 0.016 (0.014)	Data 0.000 (0.005)	Loss 0.7243 (0.7242)	Acc@1 100.000 (99.715)	Acc@5 100.000 (100.000)
Epoch: [177][190/196]	Time 0.014 (0.014)	Data 0.003 (0.005)	Loss 0.7145 (0.7239)	Acc@1 99.609 (99.720)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.723729173488617, 1.2943900060653686, 99.722, 70.02, tensor(0.9648, device='cuda:0', grad_fn=<DivBackward0>), 2.8201515674591064, 0.39390683174133306]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [178 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [178][0/196]	Time 0.082 (0.082)	Data 0.204 (0.204)	Loss 0.7060 (0.7060)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [178][10/196]	Time 0.014 (0.022)	Data 0.003 (0.021)	Loss 0.7394 (0.7155)	Acc@1 98.438 (99.680)	Acc@5 100.000 (100.000)
Epoch: [178][20/196]	Time 0.014 (0.019)	Data 0.003 (0.012)	Loss 0.7126 (0.7133)	Acc@1 100.000 (99.777)	Acc@5 100.000 (100.000)
Epoch: [178][30/196]	Time 0.015 (0.017)	Data 0.002 (0.009)	Loss 0.7184 (0.7143)	Acc@1 99.609 (99.773)	Acc@5 100.000 (100.000)
Epoch: [178][40/196]	Time 0.014 (0.017)	Data 0.003 (0.007)	Loss 0.7287 (0.7143)	Acc@1 99.609 (99.781)	Acc@5 100.000 (100.000)
Epoch: [178][50/196]	Time 0.017 (0.016)	Data 0.000 (0.006)	Loss 0.7187 (0.7151)	Acc@1 100.000 (99.747)	Acc@5 100.000 (100.000)
Epoch: [178][60/196]	Time 0.015 (0.016)	Data 0.002 (0.006)	Loss 0.7096 (0.7151)	Acc@1 99.609 (99.731)	Acc@5 100.000 (100.000)
Epoch: [178][70/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.7079 (0.7149)	Acc@1 100.000 (99.719)	Acc@5 100.000 (100.000)
Epoch: [178][80/196]	Time 0.013 (0.016)	Data 0.004 (0.005)	Loss 0.7069 (0.7147)	Acc@1 100.000 (99.720)	Acc@5 100.000 (100.000)
Epoch: [178][90/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.7158 (0.7144)	Acc@1 99.609 (99.712)	Acc@5 100.000 (100.000)
Epoch: [178][100/196]	Time 0.013 (0.015)	Data 0.020 (0.005)	Loss 0.7075 (0.7145)	Acc@1 99.609 (99.687)	Acc@5 100.000 (100.000)
Epoch: [178][110/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.7206 (0.7143)	Acc@1 99.609 (99.687)	Acc@5 100.000 (100.000)
Epoch: [178][120/196]	Time 0.012 (0.015)	Data 0.011 (0.005)	Loss 0.7065 (0.7137)	Acc@1 100.000 (99.697)	Acc@5 100.000 (100.000)
Epoch: [178][130/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.7195 (0.7139)	Acc@1 99.609 (99.672)	Acc@5 100.000 (100.000)
Epoch: [178][140/196]	Time 0.013 (0.015)	Data 0.012 (0.005)	Loss 0.7194 (0.7136)	Acc@1 98.828 (99.681)	Acc@5 100.000 (100.000)
Epoch: [178][150/196]	Time 0.015 (0.015)	Data 0.003 (0.005)	Loss 0.7118 (0.7133)	Acc@1 99.609 (99.677)	Acc@5 100.000 (99.997)
Epoch: [178][160/196]	Time 0.012 (0.015)	Data 0.018 (0.005)	Loss 0.7099 (0.7132)	Acc@1 99.219 (99.668)	Acc@5 100.000 (99.998)
Epoch: [178][170/196]	Time 0.017 (0.015)	Data 0.001 (0.005)	Loss 0.7005 (0.7129)	Acc@1 100.000 (99.678)	Acc@5 100.000 (99.998)
Epoch: [178][180/196]	Time 0.012 (0.015)	Data 0.007 (0.005)	Loss 0.7103 (0.7127)	Acc@1 99.219 (99.672)	Acc@5 100.000 (99.998)
Epoch: [178][190/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.7048 (0.7125)	Acc@1 99.609 (99.671)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.7124278454208374, 1.3048916566371918, 99.67, 69.63, tensor(0.9633, device='cuda:0', grad_fn=<DivBackward0>), 2.9256937503814697, 0.3996927738189698]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [179 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [179][0/196]	Time 0.081 (0.081)	Data 0.214 (0.214)	Loss 0.7045 (0.7045)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [179][10/196]	Time 0.016 (0.022)	Data 0.002 (0.021)	Loss 0.7171 (0.7040)	Acc@1 98.828 (99.680)	Acc@5 100.000 (100.000)
Epoch: [179][20/196]	Time 0.013 (0.019)	Data 0.003 (0.012)	Loss 0.7067 (0.7048)	Acc@1 100.000 (99.702)	Acc@5 100.000 (100.000)
Epoch: [179][30/196]	Time 0.018 (0.018)	Data 0.000 (0.009)	Loss 0.6972 (0.7043)	Acc@1 99.609 (99.735)	Acc@5 100.000 (100.000)
Epoch: [179][40/196]	Time 0.016 (0.017)	Data 0.003 (0.008)	Loss 0.6924 (0.7032)	Acc@1 100.000 (99.743)	Acc@5 100.000 (100.000)
Epoch: [179][50/196]	Time 0.016 (0.016)	Data 0.002 (0.007)	Loss 0.7009 (0.7028)	Acc@1 99.609 (99.755)	Acc@5 100.000 (100.000)
Epoch: [179][60/196]	Time 0.014 (0.016)	Data 0.002 (0.006)	Loss 0.6957 (0.7024)	Acc@1 99.609 (99.757)	Acc@5 100.000 (100.000)
Epoch: [179][70/196]	Time 0.016 (0.016)	Data 0.000 (0.006)	Loss 0.7053 (0.7018)	Acc@1 99.609 (99.758)	Acc@5 100.000 (100.000)
Epoch: [179][80/196]	Time 0.015 (0.016)	Data 0.003 (0.006)	Loss 0.6953 (0.7019)	Acc@1 100.000 (99.759)	Acc@5 100.000 (100.000)
Epoch: [179][90/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.6910 (0.7016)	Acc@1 100.000 (99.751)	Acc@5 100.000 (100.000)
Epoch: [179][100/196]	Time 0.015 (0.016)	Data 0.002 (0.005)	Loss 0.6961 (0.7015)	Acc@1 100.000 (99.733)	Acc@5 100.000 (100.000)
Epoch: [179][110/196]	Time 0.017 (0.016)	Data 0.000 (0.005)	Loss 0.6940 (0.7014)	Acc@1 100.000 (99.722)	Acc@5 100.000 (100.000)
Epoch: [179][120/196]	Time 0.016 (0.015)	Data 0.002 (0.005)	Loss 0.6938 (0.7009)	Acc@1 99.609 (99.726)	Acc@5 100.000 (100.000)
Epoch: [179][130/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6979 (0.7004)	Acc@1 100.000 (99.735)	Acc@5 100.000 (100.000)
Epoch: [179][140/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.6943 (0.7001)	Acc@1 99.609 (99.734)	Acc@5 100.000 (100.000)
Epoch: [179][150/196]	Time 0.017 (0.015)	Data 0.000 (0.005)	Loss 0.6972 (0.6998)	Acc@1 99.609 (99.741)	Acc@5 100.000 (100.000)
Epoch: [179][160/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.6987 (0.6998)	Acc@1 100.000 (99.738)	Acc@5 100.000 (100.000)
Epoch: [179][170/196]	Time 0.016 (0.015)	Data 0.000 (0.005)	Loss 0.6967 (0.6996)	Acc@1 99.609 (99.735)	Acc@5 100.000 (100.000)
Epoch: [179][180/196]	Time 0.014 (0.015)	Data 0.003 (0.004)	Loss 0.6956 (0.6997)	Acc@1 99.219 (99.728)	Acc@5 100.000 (100.000)
Epoch: [179][190/196]	Time 0.016 (0.015)	Data 0.000 (0.004)	Loss 0.6889 (0.6997)	Acc@1 100.000 (99.716)	Acc@5 100.000 (100.000)
num momentum params: 26
[0.010000000000000002, 0.6996710742950439, 1.3004856675863266, 99.714, 69.69, tensor(0.9638, device='cuda:0', grad_fn=<DivBackward0>), 2.980935573577881, 0.3908755779266357]
Non Pruning Epoch - module.conv1.weight: [42, 3, 3, 3]
Non Pruning Epoch - module.bn1.weight: [42]
Non Pruning Epoch - module.bn1.bias: [42]
Non Pruning Epoch - module.conv2.weight: [128, 42, 3, 3]
Non Pruning Epoch - module.bn2.weight: [128]
Non Pruning Epoch - module.bn2.bias: [128]
Non Pruning Epoch - module.conv3.weight: [246, 128, 3, 3]
Non Pruning Epoch - module.bn3.weight: [246]
Non Pruning Epoch - module.bn3.bias: [246]
Non Pruning Epoch - module.conv4.weight: [256, 246, 3, 3]
Non Pruning Epoch - module.bn4.weight: [256]
Non Pruning Epoch - module.bn4.bias: [256]
Non Pruning Epoch - module.conv5.weight: [509, 256, 3, 3]
Non Pruning Epoch - module.bn5.weight: [509]
Non Pruning Epoch - module.bn5.bias: [509]
Non Pruning Epoch - module.conv6.weight: [507, 509, 3, 3]
Non Pruning Epoch - module.bn6.weight: [507]
Non Pruning Epoch - module.bn6.bias: [507]
Non Pruning Epoch - module.conv7.weight: [479, 507, 3, 3]
Non Pruning Epoch - module.bn7.weight: [479]
Non Pruning Epoch - module.bn7.bias: [479]
Non Pruning Epoch - module.conv8.weight: [298, 479, 3, 3]
Non Pruning Epoch - module.bn8.weight: [298]
Non Pruning Epoch - module.bn8.bias: [298]
Non Pruning Epoch - module.fc.weight: [100, 298]
Non Pruning Epoch - module.fc.bias: [100]
[INFO] Storing checkpoint...

Epoch: [180 | 180] LR: 0.010000
module.conv1.weight [42, 3, 3, 3]
module.conv2.weight [128, 42, 3, 3]
module.conv3.weight [246, 128, 3, 3]
module.conv4.weight [256, 246, 3, 3]
module.conv5.weight [509, 256, 3, 3]
module.conv6.weight [507, 509, 3, 3]
module.conv7.weight [479, 507, 3, 3]
module.conv8.weight [298, 479, 3, 3]
Epoch: [180][0/196]	Time 0.082 (0.082)	Data 0.206 (0.206)	Loss 0.6818 (0.6818)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [180][10/196]	Time 0.017 (0.022)	Data 0.002 (0.020)	Loss 0.6834 (0.6915)	Acc@1 100.000 (99.751)	Acc@5 100.000 (100.000)
Epoch: [180][20/196]	Time 0.016 (0.019)	Data 0.002 (0.012)	Loss 0.6926 (0.6906)	Acc@1 99.219 (99.721)	Acc@5 100.000 (100.000)
Epoch: [180][30/196]	Time 0.016 (0.018)	Data 0.003 (0.009)	Loss 0.6863 (0.6908)	Acc@1 99.609 (99.710)	Acc@5 100.000 (100.000)
Epoch: [180][40/196]	Time 0.014 (0.017)	Data 0.003 (0.008)	Loss 0.6846 (0.6898)	Acc@1 100.000 (99.743)	Acc@5 100.000 (100.000)
Epoch: [180][50/196]	Time 0.014 (0.017)	Data 0.002 (0.007)	Loss 0.6909 (0.6901)	Acc@1 100.000 (99.747)	Acc@5 100.000 (100.000)
Epoch: [180][60/196]	Time 0.014 (0.016)	Data 0.003 (0.006)	Loss 0.7012 (0.6905)	Acc@1 99.219 (99.693)	Acc@5 100.000 (100.000)
Epoch: [180][70/196]	Time 0.014 (0.016)	Data 0.004 (0.005)	Loss 0.6968 (0.6899)	Acc@1 99.219 (99.708)	Acc@5 100.000 (100.000)
Epoch: [180][80/196]	Time 0.014 (0.016)	Data 0.003 (0.005)	Loss 0.6791 (0.6896)	Acc@1 100.000 (99.706)	Acc@5 100.000 (100.000)
Epoch: [180][90/196]	Time 0.012 (0.016)	Data 0.009 (0.005)	Loss 0.6878 (0.6892)	Acc@1 100.000 (99.721)	Acc@5 100.000 (100.000)
Epoch: [180][100/196]	Time 0.014 (0.015)	Data 0.002 (0.005)	Loss 0.6902 (0.6893)	Acc@1 99.219 (99.718)	Acc@5 100.000 (99.996)
Epoch: [180][110/196]	Time 0.011 (0.015)	Data 0.023 (0.005)	Loss 0.6962 (0.6892)	Acc@1 99.219 (99.715)	Acc@5 100.000 (99.996)
Epoch: [180][120/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.6778 (0.6887)	Acc@1 100.000 (99.716)	Acc@5 100.000 (99.997)
Epoch: [180][130/196]	Time 0.011 (0.015)	Data 0.009 (0.005)	Loss 0.6840 (0.6886)	Acc@1 99.609 (99.708)	Acc@5 100.000 (99.997)
Epoch: [180][140/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.6831 (0.6883)	Acc@1 100.000 (99.720)	Acc@5 100.000 (99.997)
Epoch: [180][150/196]	Time 0.012 (0.015)	Data 0.008 (0.005)	Loss 0.6887 (0.6882)	Acc@1 99.609 (99.713)	Acc@5 100.000 (99.997)
Epoch: [180][160/196]	Time 0.015 (0.015)	Data 0.002 (0.005)	Loss 0.6805 (0.6881)	Acc@1 100.000 (99.714)	Acc@5 100.000 (99.998)
Epoch: [180][170/196]	Time 0.012 (0.015)	Data 0.010 (0.005)	Loss 0.6787 (0.6876)	Acc@1 100.000 (99.717)	Acc@5 100.000 (99.998)
Epoch: [180][180/196]	Time 0.014 (0.015)	Data 0.003 (0.005)	Loss 0.6817 (0.6873)	Acc@1 100.000 (99.717)	Acc@5 100.000 (99.998)
Epoch: [180][190/196]	Time 0.013 (0.015)	Data 0.015 (0.005)	Loss 0.6744 (0.6870)	Acc@1 100.000 (99.716)	Acc@5 100.000 (99.998)
num momentum params: 26
[0.010000000000000002, 0.6868433976364136, 1.3043961197137832, 99.718, 69.63, tensor(0.9647, device='cuda:0', grad_fn=<DivBackward0>), 2.9026248455047607, 0.39539432525634766]
[INFO] Force the sparse filters to zero...
Before - module.conv1.weight: [42, 3, 3, 3]
Before - module.bn1.weight: [42]
Before - module.bn1.bias: [42]
Before - module.conv2.weight: [128, 42, 3, 3]
Before - module.bn2.weight: [128]
Before - module.bn2.bias: [128]
Before - module.conv3.weight: [246, 128, 3, 3]
Before - module.bn3.weight: [246]
Before - module.bn3.bias: [246]
Before - module.conv4.weight: [256, 246, 3, 3]
Before - module.bn4.weight: [256]
Before - module.bn4.bias: [256]
Before - module.conv5.weight: [509, 256, 3, 3]
Before - module.bn5.weight: [509]
Before - module.bn5.bias: [509]
Before - module.conv6.weight: [507, 509, 3, 3]
Before - module.bn6.weight: [507]
Before - module.bn6.bias: [507]
Before - module.conv7.weight: [479, 507, 3, 3]
Before - module.bn7.weight: [479]
Before - module.bn7.bias: [479]
Before - module.conv8.weight: [298, 479, 3, 3]
Before - module.bn8.weight: [298]
Before - module.bn8.bias: [298]
Before - module.fc.weight: [100, 298]
Before - module.fc.bias: [100]
[INFO] Squeezing the sparse model to dense one...
resetting momentum module.conv1.weight
resetting momentum module.bn1.weight
resetting momentum module.bn1.bias
resetting momentum module.conv2.weight
resetting momentum module.bn2.weight
resetting momentum module.bn2.bias
resetting momentum module.conv3.weight
resetting momentum module.bn3.weight
resetting momentum module.bn3.bias
resetting momentum module.conv4.weight
resetting momentum module.bn4.weight
resetting momentum module.bn4.bias
resetting momentum module.conv5.weight
resetting momentum module.bn5.weight
resetting momentum module.bn5.bias
resetting momentum module.conv6.weight
resetting momentum module.bn6.weight
resetting momentum module.bn6.bias
resetting momentum module.conv7.weight
resetting momentum module.bn7.weight
resetting momentum module.bn7.bias
resetting momentum module.conv8.weight
resetting momentum module.bn8.weight
resetting momentum module.bn8.bias
resetting momentum module.fc.weight
resetting momentum module.fc.bias
After - module.conv1.weight: [42, 3, 3, 3]
After - module.bn1.weight: [42]
After - module.bn1.bias: [42]
After - module.conv2.weight: [128, 42, 3, 3]
After - module.bn2.weight: [128]
After - module.bn2.bias: [128]
After - module.conv3.weight: [246, 128, 3, 3]
After - module.bn3.weight: [246]
After - module.bn3.bias: [246]
After - module.conv4.weight: [256, 246, 3, 3]
After - module.bn4.weight: [256]
After - module.bn4.bias: [256]
After - module.conv5.weight: [509, 256, 3, 3]
After - module.bn5.weight: [509]
After - module.bn5.bias: [509]
After - module.conv6.weight: [507, 509, 3, 3]
After - module.bn6.weight: [507]
After - module.bn6.bias: [507]
After - module.conv7.weight: [479, 507, 3, 3]
After - module.bn7.weight: [479]
After - module.bn7.bias: [479]
After - module.conv8.weight: [298, 479, 3, 3]
After - module.bn8.weight: [298]
After - module.bn8.bias: [298]
After - module.fc.weight: [100, 298]
After - module.fc.bias: [100]
Calculating FLOPS
conv1 --> [42, 3, 3, 3]
conv2 --> [128, 42, 3, 3]
conv3 --> [246, 128, 3, 3]
conv4 --> [256, 246, 3, 3]
conv5 --> [509, 256, 3, 3]
conv6 --> [507, 509, 3, 3]
conv7 --> [479, 507, 3, 3]
conv8 --> [298, 479, 3, 3]
fc --> [298, 100]
1, 465067008, 1161216, 42
2, 5177475072, 12386304, 128
3, 8270512128, 18137088, 246
4, 16541024256, 36274176, 256
5, 10207494144, 18763776, 509
6, 20215623168, 37161072, 507
7, 6714399744, 8742708, 479
8, 3946530816, 5138712, 298
fc, 11443200, 29800, 0
===================
FLOP REPORT: 27949050600000.0 50907200000.0 137794852 127268 2465 15.064077377319336
[INFO] Storing checkpoint...
Best acc:
70.02

Total time:
1280.819804
[2021-06-20T05:06:48.710666] Command finished with return code 0


[2021-06-20T05:06:48.711100] The experiment completed successfully. Finalizing run...
Cleaning up all outstanding Run operations, waiting 900.0 seconds
1 items cleaning up...
Cleanup took 0.0717768669128418 seconds
[2021-06-20T05:06:48.938150] Finished context manager injector.
2021/06/20 05:06:49 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status
2021/06/20 05:06:49 Not exporting to RunHistory as the exporter is either stopped or there is no data.
Stopped: false
OriginalData: 2
FilteredData: 0.
2021/06/20 05:06:49 Process Exiting with Code:  0
2021/06/20 05:06:50 All App Insights Logs was send successfully
